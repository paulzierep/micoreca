[
    {
        "create_time": "2025-10-31",
        "creators": [
            "VGP",
            " Galaxy"
        ],
        "description": "Evaluation of Pacbio Hifi Reads and genome profiling. Create Meryl Database used for the estimation of assembly parameters and quality control with Merqury. Part of the VGP pipeline.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in name",
        "id": "632",
        "keep": true,
        "latest_version": 13,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/632?version=13",
        "name": "kmer-profiling-hifi-VGP1/main",
        "number_of_steps": 17,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "",
            "compose_text_param",
            "meryl_count_kmers",
            "imagemagick_image_montage",
            "genomescope",
            "rdeval",
            "meryl_groups_kmers",
            "meryl_histogram_kmers",
            "rdeval_report",
            "param_value_from_file"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-31",
        "versions": 13
    },
    {
        "create_time": "2025-10-26",
        "creators": [],
        "description": "# 3D genome builder (3DGB)\r\n\r\n3D genome builder (3DGB) is a workflow to build 3D models of genomes from HiC raw data and to integrate omics data on the produced models for further visual exploration.\r\n3DGB bundles [HiC-Pro](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0831-x), [PASTIS](https://academic.oup.com/bioinformatics/article/30/12/i26/385087) and custom Python scripts into a unified Snakemake workflow with limited inputs (see *Preparing Required Files*). 3DGB produces annotated 3D models of genome in PDB and G3D formats.\r\n\r\n## Download this repository\r\n\r\n```bash\r\ngit clone https://github.com/data-fun/3d-genome-builder.git\r\ncd 3d-genome-builder\r\n```\r\n\r\n## Install dependencies\r\n\r\n### Singularity\r\n\r\nDownload the latest version [here](https://github.com/apptainer/singularity/releases)\r\n\r\nInstall Singularity:\r\n\r\n```bash\r\nsudo apt install -y ./singularity-container_3.8.7_amd64.deb\r\n```\r\n\r\nVerify version:\r\n\r\n```\r\n$ singularity --version\r\nsingularity version 3.8.7\r\n```\r\n\r\n### Conda environment\r\n\r\nInstall [conda](https://docs.conda.io/en/latest/miniconda.html).\r\n\r\nInstall mamba:\r\n\r\n```bash\r\nconda install mamba -n base -c conda-forge\r\n```\r\n\r\nCreate conda environment and install dependendies:\r\n\r\n```bash\r\nmamba env create -f binder/environment.yml\r\n```\r\n\r\nLoad conda environment:\r\n\r\n```bash\r\nconda activate 3DGB\r\n```\r\n\r\n### Download  HiC-Pro Singularity image\r\n\r\n\r\n```bash\r\nwget --ciphers=DEFAULT:@SECLEVEL=1 https://zerkalo.curie.fr/partage/HiC-Pro/hicpro_3.1.0_ubuntu.img -P images\r\n```\r\n\r\nIf this command fails, try with an [alternate download link](https://zenodo.org/record/8376626):\r\n\r\n```bash\r\nwget https://zenodo.org/record/8376626/files/hicpro_3.1.0_ubuntu.img -P images\r\n```\r\n\r\nCheck the integrity of the image:\r\n\r\n```bash\r\n$ md5sum images/hicpro_3.1.0_ubuntu.img\r\nd480e636397c14e187608e50309eb9af  images/hicpro_3.1.0_ubuntu.img\r\n```\r\n\r\nVerify HiC-Pro version with:\r\n\r\n```bash\r\n$ singularity exec images/hicpro_3.1.0_ubuntu.img HiC-Pro --version\r\n[...]\r\nHiC-Pro version 3.1.0\r\n```\r\n\r\nand bowtie2 version:\r\n\r\n```bash\r\n$ singularity exec images/hicpro_3.1.0_ubuntu.img bowtie2 --version  2>/dev/null | head -n 1\r\n/usr/local/conda/envs/hicpro/bin/bowtie2-align-s version 2.4.4\r\n```\r\n\r\n\r\n## Prepare required files\r\n\r\n### Create the config file\r\n\r\nCreate and edit a configuration file in [yaml](https://en.wikipedia.org/wiki/YAML) format. See for instance the template `config_template.yml`\r\n\r\n### Add the reference genome\r\n\r\nThe reference genome fasta file must be located in `WORKING_DIR/genome.fasta` where `WORKING_DIR` is the name of the working directory as specified in your config file.\r\n\r\n### Add FASTQ files (optional)\r\n\r\nIf you already have fastq files stored locally or some fastq files are not available on GEO or SRA, you can use these files providing they are in the proper directory structure:\r\n\r\n<img align=\"right\" width=\"200px\" \r\n    src=\"assets/SCERE_chromosome_13.gif\"\r\n    alt=\"3D structure of the chromosome 13 of S. cerevisiae at 5 kb resolution\">\r\n\r\n```\r\nWORKING_DIR/\r\n\u251c\u2500\u2500 fastq_files\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ID1\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ID1_R1.fastq.gz\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ID1_R2.fastq.gz\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ID2\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ID2_R1.fastq.gz\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ID2_R2.fastq.gz\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ID3\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ID3_R1.fastq.gz\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ID3_R2.fastq.gz\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ID4\r\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ID4_R1.fastq.gz\r\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ID4_R2.fastq.gz\r\n\u2514\u2500\u2500 genome.fasta\r\n```\r\n\r\n- `WORKING_DIR` is the name of the working directory as specified in your config file.\r\n- Paired-end fastq files are in the directory `WORKING_DIR/fastq_files/IDx` with `IDx` the identifier of the paired fastq files. Fastq identifiers are reported in the config file. Please note fastq files have to follow the pattern `<sample ID>_R<1 or 2>.fastq.gz`.\r\n\r\n> **Note**\r\n>\r\n> Please strictly follow this file organization as it is required by the 3DGB workflow.\r\n\r\n## Build model\r\n\r\nRun 3DGB:\r\n\r\n```bash\r\nsnakemake --profile smk_profile -j 4 --configfile YOUR-CONFIG.yml\r\n```\r\n\r\n> **Note**\r\n> - Adapt `YOUR-CONFIG.yml` to the exact name of the config file you created.\r\n> - Option `-j 4` tells Snakemake to use up to 4 cores. If you are more cores available, you can increase this value (*e.g.* `-j 16`).\r\n\r\nOr with debugging options:\r\n\r\n```bash\r\nsnakemake --profile smk_profile_debug -j 4 --configfile YOUR-CONFIG.yml --verbose\r\n```\r\n\r\nDepending on the number and size of fastq files, the 3D construction will take a couple of hours to run.\r\n\r\nFor troubleshooting, have a look to log files in `WORKING_DIR/logs`, where `WORKING_DIR` is the name of the working directory as specified in your config file.\r\n\r\n## Map quantitative values on the 3D model\r\n\r\nTo map quantitative values on the model run:\r\n\r\n```bash\r\npython ./scripts/map_parameter.py --pdb path/to/structure.pdb --bedgraph path/to/annotation.bedgraph --output path/to/output.pdb\r\n```\r\n\r\nQuantitative values should be formatted in a 4-column bedgraph file (chromosome/start/stop/value):\r\n\r\n```\r\nchr1\t0\t50000\t116.959\r\nchr1\t50000\t100000\t48.4495\r\nchr1\t100000\t150000\t22.8726\r\nchr1\t150000\t200000\t84.3106\r\nchr1\t200000\t250000\t113.109\r\n```\r\n\r\nEach bead of the model will be assigned a quantitative value. The resolution in the bedgraph file should match the resolution used to build the model.\r\n\r\n\r\n## Get results\r\n\r\nUpon completion, the `WORKING_DIR` should look like this:\r\n\r\n```\r\nWORKING_DIR/\r\n\u251c\u2500\u2500 contact_maps\r\n\u251c\u2500\u2500 dense_matrix\r\n\u251c\u2500\u2500 fastq_files\r\n\u251c\u2500\u2500 HiC-Pro\r\n\u251c\u2500\u2500 logs\r\n\u251c\u2500\u2500 pastis\r\n\u251c\u2500\u2500 sequence\r\n\u2514\u2500\u2500 structure\r\n```\r\n\r\nThe following paths contain the most interesting results:\r\n\r\n- `WORKING_DIR/contact_maps/*.png` : contact maps.\r\n- `WORKING_DIR/HiC-Pro/output/hic_results/pic/*/*.pdf` : graphical summaries of read alignments produced by Hi-C Pro.\r\n- `WORKING_DIR/pastis/structure_RESOLUTION.pdb` : raw 3D models (in PDB format) produced by Pastis.\r\n- `WORKING_DIR/structure/RESOLUTION/structure_cleaned.*` : final (annotated) 3D models in PDB and G3D formats.\r\n\r\n> **Note**\r\n> - `WORKING_DIR` is the name of the working directory as specified in your config file.\r\n> - `RESOLUTION` is the resolution of the Hi-C data specified in the config file.\r\n\r\n## Examples\r\n\r\n- [Wild type model for *Neurospora crassa*](examples/n_crassa.md)\r\n- [Models built for the 3DGB paper](examples/paper/paper.md)\r\n\r\n## Visualize 3D model structures\r\n\r\nTo visualize 3D model structures (.pdb and .g3d files), follow this quick [tutorial](visualization/visualization.md).\r\n\r\n\r\n## Build DAG graph\r\n\r\nFor visualization purpose, you can build the graph of all computational steps involved in the 3D construction of the genome.\r\n\r\n```bash\r\nsnakemake --profile smk_profile --configfile YOUR-CONFIG.yml --rulegraph  | dot -Tpdf > rules.pdf\r\n```\r\n\r\nwhere `YOUR-CONFIG.yml` should be replaced by the name of the config file you created.\r\n\r\nWith wildcards:\r\n\r\n```bash\r\nsnakemake --profile smk_profile --configfile YOUR-CONFIG.yml --dag  | dot -Tpdf > dag.pdf\r\n```\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Functional genomics",
            "Genomics"
        ],
        "filtered_on": "binn* in description",
        "id": "2010",
        "keep": true,
        "latest_version": 1,
        "license": "BSD-3-Clause",
        "link": "https:/workflowhub.eu/workflows/2010?version=1",
        "name": "3D genome builder (3DGB)",
        "number_of_steps": 0,
        "projects": [
            "datafun"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2025-10-26",
        "versions": 1
    },
    {
        "create_time": "2025-10-24",
        "creators": [],
        "description": "This workflow encodes the top-ranking predicted pathways from the previous workflow into plasmids intended to be expressed in the specified organism. BASIC is used as assembly method. ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "plasmid* in description",
        "id": "2009",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/2009?version=1",
        "name": "Genetic Design (BASIC Assembly)",
        "number_of_steps": 3,
        "projects": [
            "galaxy-SynBioCAD"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "brs"
        ],
        "tools": [
            "rpbasicdesign",
            "selenzy-wrapper",
            "dnabot"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-30",
        "versions": 1
    },
    {
        "create_time": "2025-10-24",
        "creators": [
            "Bert Droesbeke"
        ],
        "description": "# ENA Reads & Assembly Submission Workflow\r\n\r\nOriginally developed within the [EVORA project](https://evora-project.eu/), this two-step Galaxy workflow streamlines submissions to the [European Nucleotide Archive (ENA)](https://www.ebi.ac.uk/ena). The workflow first submits raw sequencing reads via the **Galaxy ENA upload tool**, then submits assembled sequences using the **Galaxy ENA Webin CLI tool**. The process is fully interactive and GUI-driven while retaining ENA\u2019s required validations leveraging the user-scoped credential management system and the data upload/management from Galaxy.\r\n\r\n\r\n## Step 1 \u2014 Raw reads: Galaxy ENA upload tool\r\n\r\nA Galaxy wrapper around the [ENA-upload-cli](https://github.com/usegalaxy-eu/ena-upload-cli) that brings a graphical interface and interactive checks to standard ENA read submissions.\r\n\r\n**Key features**\r\n\r\n- Raw read submission with  ENA-upload-cli at its core\r\n- Use [tabular or excel sheet templates](https://github.com/ELIXIR-Belgium/ENA-metadata-templates) to easily capture the metadata\r\n- Client side validation using ENA checklists\r\n- Create, add, and modify ENA objects (e.g., studies, experiments, samples, runs)\r\n\r\n## Step 2 \u2014 Consensus sequences: Galaxy ENA Webin CLI\r\n\r\nA Galaxy wrapper around ENA\u2019s [Webin-CLI](https://github.com/enasequence/webin-cli) for interactive submission of consensus sequences, with full checklist coverage.\r\n\r\n**Key features**\r\n\r\n- Seamless integration with the Galaxy ENA upload tool to fetch and reuse metadata\r\n- Interactive metadata submission and validation\r\n- Support for all assembly levels (contig, scaffold and chromosome)\r\n\r\n## Funding\r\n\r\nThe EVORA project has received funding from the European Union's HORIZON programme under grant agreement No 101131959\r\n",
        "doi": "10.48546/workflowhub.workflow.2008.1",
        "edam_operation": [],
        "edam_topic": [
            "FAIR data",
            "Genomics"
        ],
        "filtered_on": "metap* in description",
        "id": "2008",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/2008?version=1",
        "name": "ENA Reads and Assembly Submission Workflow",
        "number_of_steps": 2,
        "projects": [
            "ELIXIR Belgium"
        ],
        "source": "WorkflowHub",
        "tags": [
            "evora",
            "fair",
            "brokering",
            "ena",
            "publication",
            "submission"
        ],
        "tools": [
            "ena_webin_cli",
            "ena_upload"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-24",
        "versions": 1
    },
    {
        "create_time": "2025-10-24",
        "creators": [],
        "description": "This workflow encodes the top-ranking predicted pathways from the previous workflow into plasmids intended to be expressed in the specified organism. Assembly methods are Gibson, Golden or Ligation Chain Reaction.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Synthetic biology"
        ],
        "filtered_on": "plasmid* in description",
        "id": "2007",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/2007?version=1",
        "name": "Genetic Design (Gibson, Golden Gate, LCR)",
        "number_of_steps": 6,
        "projects": [
            "galaxy-SynBioCAD"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "dnaweaver",
            "LCRGenie",
            "PartsGenie",
            "sbml2sbol",
            "optdoe",
            "selenzy-wrapper"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-30",
        "versions": 1
    },
    {
        "create_time": "2025-10-24",
        "creators": [
            "BioRetroSynth"
        ],
        "description": "Annotation: Evaluating and ranking a set of pathways based on multiple metrics.\r\nGiven a set of pathways generated by RetroPath2.0, this workflow informs the user as to the theoretically best performing ones based on the four criteria (target product flux, thermodynamic feasibility, pathway length, and enzyme availability).",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "2005",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/2005?version=1",
        "name": "Pathway Analysis",
        "number_of_steps": 4,
        "projects": [
            "galaxy-SynBioCAD"
        ],
        "source": "WorkflowHub",
        "tags": [
            "retrosynthesis",
            "brs"
        ],
        "tools": [
            "rptools_rpthermo",
            "rptools_rpscore",
            "rptools_rpranker",
            "rptools_rpfba"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-30",
        "versions": 1
    },
    {
        "create_time": "2025-10-22",
        "creators": [
            "Evangelos Karatzas",
            "Martin Beracochea"
        ],
        "description": "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/codespaces/new/nf-core/proteinfamilies)\r\n[![GitHub Actions CI Status](https://github.com/nf-core/proteinfamilies/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/proteinfamilies/actions/workflows/nf-test.yml)\r\n[![GitHub Actions Linting Status](https://github.com/nf-core/proteinfamilies/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/proteinfamilies/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/proteinfamilies/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.14881993-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.14881993)\r\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\r\n\r\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A525.04.0-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\r\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.4.1-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.4.1)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/proteinfamilies)\r\n\r\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23proteinfamilies-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/proteinfamilies)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\r\n\r\n## Introduction\r\n\r\n**nf-core/proteinfamilies** is a bioinformatics pipeline that generates protein families from amino acid sequences and/or updates existing families with new sequences.\r\nIt takes a protein fasta file as input, clusters the sequences and then generates protein family Hidden Markov Models (HMMs) along with their multiple sequence alignments (MSAs).\r\nOptionally, paths to existing family HMMs and MSAs can be given (must have matching base filenames one-to-one) in order to update with new sequences in case of matching hits.\r\n\r\n### Check quality and pre-process\r\n\r\nGenerate input amino acid sequence statistics with ([`SeqFu`](https://github.com/telatin/seqfu2/)) and pre-process them (i.e., gap removal, convert to upper case, validate, filter by length, replace special characters such as `/`, and remove duplicate sequences) with ([`SeqKit`](https://github.com/shenwei356/seqkit/))\r\n\r\n### Create families\r\n\r\n1. Cluster sequences ([`MMseqs2`](https://github.com/soedinglab/MMseqs2/))\r\n2. Perform multiple sequence alignment (MSA) ([`FAMSA`](https://github.com/refresh-bio/FAMSA/) or [`mafft`](https://github.com/GSLBiotech/mafft/))\r\n3. Optionally, clip gap parts of the MSA ([`ClipKIT`](https://github.com/JLSteenwyk/ClipKIT/))\r\n4. Generate family HMMs and fish additional sequences into the family ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\r\n5. Optionally, remove redundant and/or merge similar families by comparing family representative sequences against family models with ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\r\n6. Optionally, from the remaining families, remove in-family redundant sequences by strictly clustering with ([`MMseqs2`](https://github.com/soedinglab/MMseqs2/)) and keep cluster representatives\r\n7. Optionally, if in-family redundancy was not removed, reformat the `.sto` full MSAs to `.fas` with ([`HH-suite3`](https://github.com/soedinglab/hh-suite))\r\n8. Present statistics for remaining/updated family size distributions and representative sequence lengths ([`MultiQC`](http://multiqc.info/))\r\n\r\n### Update families\r\n\r\n1. Find which families to update by comparing the input sequences against existing family models with ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\r\n2. For non hit sequences continue with the above: A. Create families. For hit sequences and families continue to: 3\r\n3. Extract family sequences ([`SeqKit`](https://github.com/shenwei356/seqkit/)) and concatenate with filtered hit sequences of each family\r\n4. Optionally, remove in-family redundant sequences by strictly clustering with ([`MMseqs2`](https://github.com/soedinglab/MMseqs2/)) and keeping cluster representatives\r\n5. Perform multiple sequence alignment (MSA) ([`FAMSA`](https://github.com/refresh-bio/FAMSA/) or [`mafft`](https://github.com/GSLBiotech/mafft/))\r\n6. Optionally, clip gap parts of the MSA ([`ClipKIT`](https://github.com/JLSteenwyk/ClipKIT/))\r\n7. Update family HMM with ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\r\n\r\n## Usage\r\n\r\n> [!NOTE]\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\r\n\r\nFirst, prepare a samplesheet with your input data that looks as follows:\r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\nsample,fasta,existing_hmms_to_update,existing_msas_to_update\r\nCONTROL_REP1,input/mgnifams_input_small.faa,,\r\n```\r\n\r\nEach row contains a fasta file with amino acid sequences (can be zipped or unzipped).\r\nOptionally, a row may contain tarball archives (tar.gz) of existing families' HMM and MSA folders, in order to be updated.\r\nIn this case, the HMM and MSA files must be matching in numbers and in base filenames (not the extension).\r\nHit families/sequences will be updated, while no hit sequences will create new families.\r\n\r\nNow, you can run the pipeline using:\r\n\r\n```bash\r\nnextflow run nf-core/proteinfamilies \\\r\n   -profile <docker/singularity/.../institute> \\\r\n   --input samplesheet.csv \\\r\n   --outdir <OUTDIR>\r\n```\r\n\r\n> [!WARNING]\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\r\n\r\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/proteinfamilies/usage) and the [parameter documentation](https://nf-co.re/proteinfamilies/parameters).\r\n\r\n## Pipeline output\r\n\r\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/proteinfamilies/results) tab on the nf-core website pipeline page.\r\nFor more details about the output files and reports, please refer to the\r\n[output documentation](https://nf-co.re/proteinfamilies/output).\r\n\r\n## Credits\r\n\r\nnf-core/proteinfamilies was originally written by Evangelos Karatzas.\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n- [Martin Beracochea](https://github.com/mberacochea)\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#proteinfamilies` channel](https://nfcore.slack.com/channels/proteinfamilies) (you can join with [this invite](https://nf-co.re/join/slack)).\r\n\r\n## Citations\r\n\r\nIf you use nf-core/proteinfamilies for your analysis, please cite the article as follows:\r\n\r\n> **nf-core/proteinfamilies: A scalable pipeline for the generation of protein families.**\r\n>\r\n> Evangelos Karatzas, Martin Beracochea, Fotis A. Baltoumas, Eleni Aplakidou, Lorna Richardson, James A. Fellows Yates, Daniel Lundin, nf-core community, Aydin Bulu\u00e7, Nikos C. Kyrpides, Ilias Georgakopoulos-Soares, Georgios A. Pavlopoulos & Robert D. Finn\r\n>\r\n> _biorxiv._ 2025 Aug. doi: [10.1101/2025.08.12.670010](https://dx.doi.org/10.1101/2025.08.12.670010).\r\n\r\nYou can cite the nf-core/proteinfamilies zenodo record for a specific version using the following doi: [10.5281/zenodo.14881993](https://doi.org/10.5281/zenodo.14881993).\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": "10.48546/workflowhub.workflow.1954.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1954",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1954?version=2",
        "name": "nf-core/proteinfamilies",
        "number_of_steps": 0,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "proteomics",
            "protein-families"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-10-22",
        "versions": 2
    },
    {
        "create_time": "2025-10-19",
        "creators": [
            "Andreas Wilm"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-bacass_logo_dark.png\">\n    <img alt=\"nf-core/bacass\" src=\"docs/images/nf-core-bacass_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/bacass/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/bacass/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/bacass/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/bacass/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/bacass/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/bacass)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23bacass-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/bacass)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/bacass** is a bioinformatics best-practice analysis pipeline for simple bacterial assembly and annotation. The pipeline is able to assemble short reads, long reads, or a mixture of short and long reads (hybrid assembly).\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/bacass/results).\n\n## Pipeline summary\n\n### Short Read Assembly\n\nThis pipeline is primarily for bacterial assembly of next-generation sequencing reads. It can be used to quality trim your reads using [FastP](https://github.com/OpenGene/fastp) and performs basic sequencing QC using [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/). Afterwards, the pipeline performs read assembly using [Unicycler](https://github.com/rrwick/Unicycler). Contamination of the assembly is checked using [Kraken2](https://ccb.jhu.edu/software/kraken2/) and [Kmerfinder](https://bitbucket.org/genomicepidemiology/kmerfinder/src/master/) to verify sample purity.\n\n### Long Read Assembly\n\nFor users that only have Nanopore data, the pipeline quality trims these using [PoreChop](https://github.com/rrwick/Porechop) or filter long reads by quality using [Filtlong](https://github.com/rrwick/Filtlong) and assesses basic sequencing QC utilizing [NanoPlot](https://github.com/wdecoster/NanoPlot) and [PycoQC](https://github.com/a-slide/pycoQC). Contamination of the assembly is checked using [Kraken2](https://ccb.jhu.edu/software/kraken2/) and [Kmerfinder](https://bitbucket.org/genomicepidemiology/kmerfinder/src/master/) to verify sample purity.\n\nThe pipeline can then perform long read assembly utilizing [Unicycler](https://github.com/rrwick/Unicycler), [Miniasm](https://github.com/lh3/miniasm) in combination with [Racon](https://github.com/isovic/racon), [Canu](https://github.com/marbl/canu) or [Flye](https://github.com/fenderglass/Flye) by using the [Dragonflye](https://github.com/rpetit3/dragonflye)(\\*) pipeline. Long reads assembly can be polished using [Medaka](https://github.com/nanoporetech/medaka) or [NanoPolish](https://github.com/jts/nanopolish) with Fast5 files.\n\n> [!NOTE]\n> Dragonflye is a comprehensive pipeline designed for genome assembly of Oxford Nanopore Reads. It facilitates the utilization of Flye (default), Miniasm, and Raven assemblers, along with Racon (default) and Medaka polishers. For more information, visit the [Dragonflye GitHub](https://github.com/rpetit3/dragonflye) repository.\n\n### Hybrid Assembly\n\nFor users specifying both short read and long read (NanoPore) data, the pipeline can perform a hybrid assembly approach utilizing [Unicycler](https://github.com/rrwick/Unicycler) (short read assembly followed by gap closing with long reads) or [Dragonflye](https://github.com/rpetit3/dragonflye) (long read assembly followed by polishing with short reads), taking the full set of information from short reads and long reads into account.\n\n### Assembly QC and annotation\n\nIn all cases, the assembly is assessed using [QUAST](http://bioinf.spbau.ru/quast) and [BUSCO](https://busco.ezlab.org/). The resulting bacterial assembly is furthermore annotated using [Prokka](https://github.com/tseemann/prokka), [Bakta](https://github.com/oschwengers/bakta) or [DFAST](https://github.com/nigyta/dfast_core).\n\nIf Kmerfinder is invoked, the pipeline will group samples according to the [Kmerfinder](https://bitbucket.org/genomicepidemiology/kmerfinder/src/master/)-estimated reference genomes. Afterwards, two QUAST steps will be carried out: an initial ('general') [QUAST](http://bioinf.spbau.ru/quast) of all samples without reference genomes, and subsequently, a 'by reference genome' [QUAST](http://bioinf.spbau.ru/quast) to aggregate samples with their reference genomes.\n\n> [!NOTE]\n> This scenario is supported when [Kmerfinder](https://bitbucket.org/genomicepidemiology/kmerfinder/src/master/) analysis is performed only.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.tsv`:\n\n```tsv\nID      R1                            R2                            LongFastQ                    Fast5    GenomeSize\nshortreads      ./data/S1_R1.fastq.gz       ./data/S1_R2.fastq.gz       NA                            NA      NA\nlongreads       NA                          NA                          ./data/S1_long_fastq.gz      ./data/FAST5  2.8m\nshortNlong      ./data/S1_R1.fastq.gz       ./data/S1_R2.fastq.gz       ./data/S1_long_fastq.gz      ./data/FAST5  2.8m\n\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\nShort read assembly with Unicycler, `--kraken2db` can be any [compressed database (`.tar.gz`/`.tgz`)](https://benlangmead.github.io/aws-indexes/k2):\n\n```console\nnextflow run nf-core/bacass -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> --input samplesheet.tsv --assembly_type 'short' --kraken2db \"https://genome-idx.s3.amazonaws.com/kraken/k2_standard_8gb_20210517.tar.gz\"\n```\n\nLong read assembly with Miniasm:\n\n```console\nnextflow run nf-core/bacass -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> --input samplesheet.tsv --assembly_type 'long' --assembler 'miniasm' --kraken2db \"https://genome-idx.s3.amazonaws.com/kraken/k2_standard_8gb_20210517.tar.gz\"\n```\n\n```bash\nnextflow run nf-core/bacass \\\n  -profile <docker/singularity/.../institute> \\\n  --input samplesheet.tsv \\\n  --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/bacass/usage) and the [parameter documentation](https://nf-co.re/bacass/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/bacass/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/bacass/output).\n\n## Credits\n\nnf-core/bacass was initiated by [Andreas Wilm](https://github.com/andreas-wilm), originally written by [Alex Peltzer](https://github.com/apeltzer) (DSL1), rewritten by [Daniel Straub](https://github.com/d4straub) (DSL2) and maintained by [Daniel Valle-Millares](https://github.com/Daniel-VM).\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#bacass` channel](https://nfcore.slack.com/channels/bacass) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/bacass for your analysis, please cite it using the following doi: [10.5281/zenodo.2669428](https://doi.org/10.5281/zenodo.2669428)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "966",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/966?version=10",
        "name": "nf-core/bacass",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "bacterial-genomes",
            "denovo",
            "denovo-assembly",
            "genome-assembly",
            "hybrid-assembly",
            "nanopore",
            "nanopore-sequencing"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-10-19",
        "versions": 10
    },
    {
        "create_time": "2025-10-17",
        "creators": [
            "daniel rickert"
        ],
        "description": "# pb_variants \r\nA snakemake 9 based Pipeline for hifi snp, sv, cnv calling, phasing and more\r\n\r\nOnly PacBio data for now\r\n\r\n__!!THIS PIPLINE IS IN-DEVELOPMENT AND EXPERIMENTAL, USE AT YOUR OWN RISK!!__\r\n\r\n## what this tool aims to deliver:\r\n    - newest and best tools suited for HiFi data (only for now)\r\n    - singletons and trio analysis (trio is coming sometime...)\r\n    - human-first (hg38 for now), others should be possible (untested...)\r\n\r\n## included tools:\r\n- deepvariant or bcftools for snp calling\r\n- snps get used for phasing with whatshap and longphase\r\n- paraphase \r\n- trgt\r\n- hificnv \r\n- pb-cpg-tools (uses whatshap phased .bam file)\r\n- mitorsaw (just hg38)\r\n- sniffles for sv calls that get phased with longphase\r\n- sawfish for svs and cnvs (results phased by sawfish)\r\n- mosdepth, multiqc\r\n- pbmm2 for mapping\r\n- kraken2 for contamination detection (downsamples massively, needs kraken2 database)\r\n- demultiplexing of input as option, will not split the files per barcode.\r\n- for now one .bam per sample\r\n- NanoCaller for phased snp/indel calls\r\n- Variants get annotated if configured through config.yaml:\r\n    - SVs with sansa and AnnotSV\r\n    - SNPs with vep and snpsift\r\n    - both only tested with hg38 for now\r\n- if enabled, SNPs and SVs get overlaped (compared)\r\n    - svs: truvari overlaps sniffles as the \"truth\" to sawfish data for each sample\r\n    - snps: rtg-tools overlaps deepvariant or bcftools snps as the \"truth\" to nanocaller snps for each sample\r\n- if enabled, paraphase results get visualized with paraviewer\r\n\r\n## how to run\r\n- make sure you have a conda environment active with snakemake9+ (called smk9 in the runPipeline_local.sh)\r\n    - this can also be achieved by running the included setupPipeline_hpc.sh\r\n        - that script uses conda to create the env smk9 - with snakemake 9 installed already (check file smk9.yaml)\r\n- cp/mv/ln your unmapped .bam file into the root folder of this directory (pb_variants/your_bam_file_here.bam)\r\n- edit samplesheet.csv with your filename \r\n    - one sample per line, do not delete the header line (add to line2: your_bam_file_here.bam)\r\n- edit config.yaml to your liking/ folder structure (enable only things you want / need. keep in mind some analysis are hg38 only)\r\n- make sure you are in an interactive terminal session inside a screen / tmux or similar\r\n- bash runPipeline_local.sh for local installment on single-server setups, \r\n- bash runPipeline.sh on HPC \r\n- non-hpc users need to edit the config.yaml and enable deepvariant and disable deepvariant_hpc in the config.yaml:\r\nuse_deepvariant_hpc: True <- only set this to True on HPC HILBERT\r\n\r\n\r\n\r\n# DAG\r\nBefore each start, a DAG is created visualising the planned tasks for each sample.\r\nAn example:\r\n\r\n![alt text](dag.png)\r\n\r\nAll options enabled:\r\n\r\n![alt text](full.png)\r\n\r\nMinimalistic execution (and bcftools instead of Deepvariant) with only mandatory tasks:\r\n\r\n![alt text](minimalistic.png)\r\n\r\n\r\n## output files\r\n- the first step of the pipeline is to strip the kinetics data out of the .bam input file, but keep the methylation data. This makes all following processes much faster without any real data loss. \r\n- for each input sample:\r\n    - mosdepth and kraken (optional) report that get summarized with multiqc\r\n    - mapped .bam file haplotaged with whatshap and longphase\r\n    - the with whatshap phased bam is used for methylation track generation with cpg_tools\r\n    - bed/bw file for methylation tracks for IGV, should be used together with the whatshap phased output .bam file\r\n    - .vcf(.gz) file for:\r\n        - snps / indels from deepvariant or bcftools, phased with whatshap and longphase\r\n        - trgt\r\n        - paraphase\r\n        - mitorsaw\r\n        - hificnv\r\n        - sawfish sv / cnv \r\n        - svs from sniffles phased with longphase\r\n        - snps / indels from nanocaller\r\n    - phased snps and svs get annotated with snpsift and sansa and annotsv and vep   \r\n- snakemake report, rulegraph, copy of samplesheet and config.yaml with timestamp\r\n\r\n\r\n# general workflow in short:\r\n\r\n## prep the files:\r\n\r\n1. - move all .bam files into the folder pb_variants. (up to 100 at once makes sense, more overloads the hpc for sure)\r\n\t- with methylation, no kinetics needed\r\n\r\n2. - edit the config.yaml according to your needs. Mostly True/False. If unsure, keep as is but CHANGE THE OUTPUT DIRECTORY\r\n\r\n3. - edit the samplesheet.csv: keep the first line and then list all .bam files that you want to analyze in the run. all files listed in that file NEED TO BE IN THE FOLDER \r\n\r\n## now the actual pipeline execution:\r\n\r\n1. - start a screen session on the hpc\r\n\r\n2. - start an interactive job inside the screen session with at least 2 days runtime\r\n\r\n3. - cd into the folder pb_variants\r\n\r\n4. - activate the correct conda env with: \"conda activate smk9\"\r\n\r\n5. - run the pipeline with:\"bash runPipeline.sh\"\r\n\r\n\r\n## supervising the run:\r\n\r\n- check the screen regulary if you want -red colour is bad\r\n\r\n- qstat -u your_hpc_username if you want\r\n\r\n- check outputfolder if files are appearing\r\n\r\n- check clusterlogs_your_hpc_username for errors\r\n\r\n- check outputfolder/logs for errors that occured\r\n\r\n- if a multiqc_report.html is in the outputfolder then the pipeline is done\r\n\r\n- if the interactive job is done, but the pipeline is not done yet, retry\r\n\r\n- if you want to understand more, email me\r\n\r\n\r\n## transfering results:\r\n\r\n- you can transfer the complete outputfolder if you want. all results are stored in subfolders that should explain themselves.\r\n\r\n\r\n## roadmap:\r\n  trio calling : deeptrio, glnexus  -> only if there are requests for this. \r\n  assembly : hifiasm or similar -> only if there are requests for this. \r\n  str profiling: strkit, currently testing\r\n\r\n## why this work is being done:\r\n- nf-core/pacvar: https://nf-co.re/pacvar/1.0.1/\r\n    - does not run without sudo for us\r\n    - seems not mature enough (imho)\r\n    - not newest tools included\r\n    - not all wanted tools included\r\n\r\n- pacbios wdl-based workflow: https://github.com/PacificBiosciences/HiFi-somatic-WDL\r\n    - doesnt run on our hardware\r\n\r\n- other, locally developed snakemake-based workflows: (eg: https://github.com/core-unit-bioinformatics/workflow-smk-longread-variant-calling)\r\n    - not all wanted tools included\r\n\r\n- Radboud's Valentine workflow:\r\n    - not available to us\r\n    - not all wanted tools included\r\n\r\n- smrtlinks internal pipeline:\r\n    - singularity not working, limited tool options\r\n    - not all wanted tools included\r\n",
        "doi": "10.48546/workflowhub.workflow.1965.2",
        "edam_operation": [],
        "edam_topic": [
            "Genomics",
            "Human genetics"
        ],
        "filtered_on": "annot* in description",
        "id": "1965",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1965?version=2",
        "name": "Snakemake workflow for PacBio WGS short and long variant calling, phasing and much more",
        "number_of_steps": 0,
        "projects": [
            "WGGC"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "snps",
            "snakemake",
            "dna-methylation",
            "structural variants"
        ],
        "tools": [
            "Snakemake",
            "DeepVariant",
            "Sniffles",
            "BCFtools",
            "SAMtools",
            "FastQC",
            "Variant Effect Predictor (VEP)",
            "SnpSift",
            "AnnotSV",
            "pbmm2"
        ],
        "type": "Snakemake",
        "update_time": "2025-10-28",
        "versions": 2
    },
    {
        "create_time": "2025-10-17",
        "creators": [],
        "description": "# Domestication of new parts and cloning simulation\r\nAdd new parts to your sequences before the cloning simulation and interact with the database.\r\n\r\n## input:\r\n\r\n* _input: csv file (without header)_ : The **CSV** file should contain the constraints line by line in the first column, along with their associated fragments on each line. This data will be passed to the seq_from_DB tool.\r\n* _JSON parameters file (optional)_ : The **JSON** file should contain the the parameters used in workflow tools. If this input is not set, user should set the parameters mannualy in the maystro tool.\r\n* _Missing Fragments (optional)_ : The **collection** of GenBank files that are not present in the database ( [make collection](https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/collections/tutorial.html)). If a fragment is listed in the Assembly Plan CSV but missing from the database, it must be provided through this input.\r\n* _Assembly Standard_ : The **CSV** file is crutial for the working of domestication tool.\r\n* _New fragments for cloning similation (optional)_ : The **collection** of GenBank files ( [make collection](https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/collections/tutorial.html)). This applies only when the user wishes to incorporate additional fragments into the cloning simulation that are not part of the domestication process.\r\n\r\n## steps:\r\n\r\n>_workflow_2 Parameter_Maystro_\r\n\r\n1. Distribute workflow parameters on the workflow tools\r\n2. Parameters can be set as a JSON file in input (optional)\r\n3.  Parameters can be manually instate of JSON file\r\n4.  For the first use, it is recommended to set the parameters manually in the tool. A JSON file will then be generated automatically for use in subsequent runs.\r\n\r\n>_seq_from_DB_\r\n\r\n1. Extract the fragments associated with each constraint from the CSV file.\r\n2. Check if all fragments are present in the database.\r\n        \r\n>_evaluate_manufacturability_\r\n        \r\n1. If any fragment is missing it will serve from the unannotated Genbenk provided to annotate them and add them to the finale gb collection\r\n2. If all fragments are present in the database, GenBank files for each fragment will be passed to the cloning_simulation tool.\r\n\r\n> _sculpt_sequences_\r\n\r\n1. Set the constraints for the sculpt.\r\n2. Generate sculpted and unsculpted sequences and pass all GenBank files to _domestication_of_new_parts_ tool.\r\n\r\n> _domestication_of_new_parts_\r\n\r\n1. Import new parts from a CSV file.\r\n2. Perform domestication of the new parts in the sequences, generate reports, and create domesticated GenBank files and pass them to _seq_to_db_ tool.\r\n\r\n >_clonning_simulation_\r\n\r\n1. GoldenGate cloning simulation based on constraints.\r\n2. Generate GenBank files for each simulated constraint.\r\n\r\n>_seq_to_db_\r\n\r\n1. Constraint GenBank files can be saved to the database, depending on the user's choice.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Synthetic biology"
        ],
        "filtered_on": "annot* in description",
        "id": "2001",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/2001?version=1",
        "name": "Domestication and cloning simulation workflow",
        "number_of_steps": 7,
        "projects": [
            "DNA Foundry"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "parameters_maystro_workflow_2",
            "sculpt_sequences",
            "seq_to_db",
            "evaluate_manufacturability",
            "cloning_simulation",
            "domestication_of_new_parts",
            "seq_form_db"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-17",
        "versions": 1
    },
    {
        "create_time": "2025-10-17",
        "creators": [],
        "description": "\r\n# Cloning simulation workflow for sequences present in DB\r\n\r\nRun the GoldenGate cloning simulation for a list of constructs in a CSV file and interact with a database\r\n\r\n## inputs:\r\n\r\n* _Assmbly Plan (without header)_ : The **CSV** file should contain the constraints line by line in the first column, along with their associated fragments on each line. This data will be passed to the _seq_from_DB_ tool.\r\n* _JSON parameters file (optional)_ : The **JSON** file should contain the the parameters used in workflow tools. If this input is not set, user should set the parameters mannualy in the maystro tool.\r\n* _Missing Fragments (optional)_ : The **collection** of GenBank files that are not present in the database ( [make collection](https://training.galaxyproject.org/training-material/topics/galaxy-interface/tutorials/collections/tutorial.html)). If a fragment is listed in the Assembly Plan CSV but missing from the database, it must be provided through this input.\r\n\r\n## steps:\r\n\r\n>_workflow_1 Parameter_Maystro_\r\n\r\n1. Distribute workflow parameters on the workflow tools\r\n2. Parameters can be set as a JSON file in input (optional)\r\n3.  Parameters can be manually instate of JSON file\r\n4.  For the first use, it is recommended to set the parameters manually in the tool. A JSON file will then be generated automatically for use in subsequent runs.\r\n\r\n>_seq_from_DB_\r\n\r\n1. Extract the fragments associated with each constraint from the CSV file.\r\n2. Check if all fragments are present in the database.\r\n        \r\n>_evaluate_manufacturability_\r\n        \r\n1. If any fragment is missing it will serve from the unannotated Genbenk provided to annotate them and add them to the finale gb collection\r\n2. If all fragments are present in the database, GenBank files for each fragment will be passed to the cloning_simulation tool.\r\n\r\n >_clonning_simulation_\r\n\r\n1. GoldenGate cloning simulation based on constraints.\r\n2. Generate GenBank files for each simulated constraint.\r\n\r\n>_seq_to_db_\r\n\r\n1. Constraint GenBank files can be saved to the database, depending on the user's choice.\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Synthetic biology"
        ],
        "filtered_on": "annot* in description",
        "id": "2000",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/2000?version=1",
        "name": "Cloning Simulation workflow",
        "number_of_steps": 5,
        "projects": [
            "DNA Foundry"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "parameters_maystro_workflow_1",
            "seq_to_db",
            "evaluate_manufacturability",
            "cloning_simulation",
            "seq_form_db"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-17",
        "versions": 1
    },
    {
        "create_time": "2025-10-17",
        "creators": [
            "Diane Duroux"
        ],
        "description": "\r\n# \ud83d\udcc4 Generalizable machine learning models for rapid antimicrobial resistance prediction in unseen healthcare settings\r\n\r\nThis repository contains the code used for the experiments in the paper:\r\n\r\n**_Generalizable machine learning models for rapid antimicrobial resistance prediction in unseen healthcare settings_**  \r\nby *Diane Duroux, Paul P. Meyer, Giovanni Vison\u00e0, and Niko Beerenwinkel*.\r\n\r\n## \u2699\ufe0f Install the dependencies\r\nYou can set up the project with either pip or uv.\r\n\r\n### Option A - pip:\r\nInstall the necessary dependencies listed in the requirements.txt file\r\n\r\n```bash\r\npip install -r requirements.txt\r\n```\r\n\r\n### Option B - uv:\r\nWe provide pyproject.toml and uv.lock for macOS, Windows, and Linux.\r\n\r\nNote: On a Linux or non-apple silicon  please use the pyproject.toml file for Mac and rewrite the uv.lock after installation. \r\n\r\n```bash\r\n# 0) Install uv (one-time)\r\n# mac/linux:\r\ncurl -LsSf https://astral.sh/uv/install.sh | sh\r\n# windows (PowerShell):\r\niwr https://astral.sh/uv/install.ps1 -UseBasicParsing | iex\r\n \r\n# 1) Ensure the pinned Python is available (adjust if your pyproject pins a version)\r\nuv python install 3.11\r\n \r\n# 2) Create the exact environment from the lockfile\r\nuv sync --frozen\r\n \r\n# 3) Run your code within the env\r\nuv run python -V\r\nuv run python your_script.py\r\n```\r\n\r\n## \ud83d\udcbb AMR Classifier Training with ResMLP and inference\r\n\r\nThe following command trains a ResMLP model for AMR classification using the preprocessed DRIAMS data.\r\n\r\n### \ud83d\udce6 Output\r\n\r\nIn `output/<experiment_group>/<experiment_name>_results/`, the script generates:\r\n\r\n- `test_set_seed0.csv`  \r\n  \u27a4 Contains predictions: `species`, `sample_id`, `drug`, `response`, and `Prediction`.\r\n\r\n### \ud83d\udee0 Required Arguments\r\n\r\n| Argument                | Description                                                                                     |\r\n|-------------------------|-------------------------------------------------------------------------------------------------|\r\n| `--driams_long_table`   | Path to the metadata file for the current dataset.                                              |\r\n| `--spectra_matrix`      | Path to the input mass spectra (either raw or MAE-encoded).                                     |\r\n| `--sample_embedding_dim`| Dimension of the spectra input (6000 for raw, or same as <encoding_dim> for MAE).               |\r\n| `--drugs_df`            | Path to the antimicrobial compound encoding file.                                               |\r\n| `--fingerprint_class`   | Type of encoding: `'morgan_1024'`, `'molformer_github'`, or `'selfies_flattened_one_hot'`.      |\r\n| `--fingerprint_size`    | Size of the encoding: 1024 (Morgan), 768 (Molformer), or 24160 (SELFIES).                       |\r\n| `--split_type`          | Set to `specific` if splits are pre-defined, else random.                                       |\r\n| `--split_ids`           | Path to the `data_splits.csv` file.                                                             |\r\n| `--experiment_group`    | Name of the output folder.                                                                      |\r\n| `--experiment_name`     | Name of the output subfolder.                                                                   |\r\n| `--seed`                | Random seed for reproducibility.                                                                |\r\n| `--n_epochs`            | Number of epochs for classifier training.                                                       |\r\n| `--learning_rate`       | Learning rate for the optimizer.                                                                |\r\n| `--patience`            | Number of epochs to wait before early stopping.                                                 |\r\n| `--batch_size`          | Batch size for classifier training.                                                             |\r\n\r\n### \ud83d\ude80 Example: ResMLP Training on DRIAMS B2018 with Raw Spectra + Morgan Fingerprints\r\n\r\n```bash\r\nulimit -Sn 10000  # Optional: increase file descriptor limit if needed\r\n\r\npython3 code/ResAMR_classifier.py \\\r\n    --driams_long_table ProcessedData/B2018/combined_long_table.csv \\\r\n    --spectra_matrix ProcessedData/B2018/rawSpectra_data.npy \\\r\n    --sample_embedding_dim 6000 \\\r\n    --drugs_df OriginalData/drug_fingerprints_Mol_selfies.csv \\\r\n    --fingerprint_class morgan_1024 \\\r\n    --fingerprint_size 1024 \\\r\n    --split_type specific \\\r\n    --split_ids ProcessedData/B2018/data_splits.csv \\\r\n    --experiment_group rawMS_MorganFing \\\r\n    --experiment_name ResMLP \\\r\n    --seed 0 \\\r\n    --n_epochs 2 \\\r\n    --learning_rate 0.0003 \\\r\n    --patience 10 \\\r\n    --batch_size 128\r\n```\r\n\r\n---\r\n\r\n## \ud83d\udcb0 Funding\r\n\r\nThis research was primarily supported by the ETH AI Center.\r\n",
        "doi": "10.48546/workflowhub.workflow.1999.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "antimicrobial in name",
        "id": "1999",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1999?version=1",
        "name": "Generalizable machine learning models for rapid antimicrobial resistance prediction in unseen healthcare settings",
        "number_of_steps": 0,
        "projects": [
            "AMRMALDI"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2025-10-17",
        "versions": 1
    },
    {
        "create_time": "2025-06-30",
        "creators": [
            "Marco Salvi"
        ],
        "description": "# [DTC-T1] WF6101: Tsunami impact forecasting\r\n\r\nThis repository contains a Common Workflow Language (CWL) and Ro-Crate metadata definition for DTC-T1 workflow 6101, which is designed for \r\nproviding tsunami impact forecasting following a tsunamigenic earthquake event, based on a probabilistic approach. The workflow integrates real-time\r\nearthquake data, runs HPC simulations, and generates tsunami hazard maps. \r\n\r\nThe main CWL implementation is found in WF6101.cwl, together with ST610106 and ST610109 describing complex steps. The file mini-ST610106.cwl is refereed to a specific\r\nrealization of the ST610106, where different alternative options are included.\r\n\r\nTo see a preview of the RO-Crate metadata the `ro-crate-preview.html` file can be opeens in the browser.\r\n\r\n## Overview\r\n\r\nThe DTC-T1 Digital Twin component operates a single workflow (WF6101) that runs 11 main steps. DTC-T1 is also referred to as PTF,\r\nProbabilistic Tsunami Forecasting. \r\n\r\nThe workflow is initialized by a potentially tsunamigenic earthquake event. Based on the real-time event parameters together with long-term information \r\nretrieved from a regional-scale tsunami hazard model, an ensemble of earthquake scenarios compatible with the occurring event is built, where each scenario\r\nis weighted by the probability of being a good representation of the actual event. The tsunami impact computed by numerical modeling and the scenario \r\nprobabilities are aggregated into exceedance probabilities for given tsunami intensities at given coastal points to calculate the hazard curves and\r\nsubsequently the hazard maps. \r\n\r\nThe workflow is designed to operate in (near-)real-time and to be employed at Tsunami Warning Centres for operational tsunami early warning and forecasting,\r\nas well as for rapid post-event assessment.\r\n\r\n## Workflow structure\r\n\r\nThe workflow consists of multiple steps (ST), datasets (DT), and software services (SS). Below is a simplified breakdown:\r\n\r\n- The Scenario Player (ST610101) acts like a data archive, storing observations (synthetic or measured) (DT6101) recorded by external data providers\r\n(i.e. seismic data, labeled as DT6103, sea level data, labeled as DT6104, and GNSS data, labeled as DT6105) and sending them to specific data listeners.\r\n\r\n- The Listener EQ (ST610102) evaluates the earthquake information that initialise the workflow execution, while Listeners SL (ST610103) and GNSS (ST610104)\r\nelaborate observational sea level and ground deformation data for further comparison with simulated data. \r\n\r\n- The Ensemble Manager (ST610105) combines the information from the Listener EQ with the long-term information from a regional hazard model (DT6102) and \r\ndefines a list of scenarios (DT6106) compatible with the triggering event, with their associated probabilities (DT6107). \r\n\r\n- The ensemble of earthquake scenarios is provided as input to the Scenario Modelling (ST610106), which makes use of pre-defined topo-bathymetric grids (DT6109) \r\nand computes tsunami intensities (DT6110) and ground deformation (DT6111). In the modeling step different alternative and/or combined models can be used \r\nin each single workflow realization. More specifically, different source and tsunami models can be combined through the following software: \r\nTsunami-HySEA (SS6107), SeisSol (SS6106), BingClaw (SS6109), Shaltop (SS6110), Landslide-HySEA (SS6108), Source-to-wave filter (SS6112), \r\nand the Inundation AI module (SS6111). User settings will dictate which model combinations are invoked. As an alternative, precomputed scenarios \r\ncan be simply retrieved (SS6119). \r\n\r\n- The Misfit Evaluator (ST610107) combines simulation outputs with eventual SL and GNSS data to evaluate the degree of confidence between the simulated \r\nscenarios and observations. According to the computed misfit, the initial scenario ensemble and related probabilities can be updated. \r\n\r\n- The Hazard Aggregator (ST610108) aggregates scenario probabilities and impact metrics to calculate hazard curves at multiple forecast points (DT6112).\r\n\r\n-  The AL (Alert level) and Visualization step (ST610109) makes the final processing of the results, converting hazard into Alert Levels and producing \r\nprobabilistic visual maps (DT6113). It is worth noting that only the Tsunami Service Providers (TSP) are allowed in producing and issuing the AL for \r\noperational tsunami early warning and forecasting.\r\n\r\n- In case of landslide-triggered tsunamis, input in the form of shake maps generated for the occurring event through the Earthquake Modeling \r\n(ST610111) is forwarded to the Landslide Scenario Manager (ST610110), which in turn feeds landslide scenarios (DT6108) to the Scenario Modelling.\r\n\r\n\r\n## Workflow diagram\r\n\r\nHere is a visual overview of the described workflow, and the corresponding CWL graph automatically generated from the metadata information stored in the \r\nCWL workflow file.\r\n\r\n![wf_diag](images/PTF_Workflow_Diagram_FROZEN.drawio.png)\r\n\r\n![cwl_wf](images/cwl_graph_wf.png)\r\n\r\n## The mini-ST610106 for operationalization\r\n\r\nWe focused on one realization of the Scenario Modeling step (hereinafter called mini-ST610106), where the T-HySEA code (SS6107) is used\r\nto model both the earthquake source and the following tsunami, and only tsunami intensities (DT6110) are saved. In this experiment, the singularity \r\nimage of the T-HySEA code created with the eFlows4HPC image service creation is used. Moreover, PyCOMPSs functionalities have been implemented in \r\norder to execute the mini-ST610106 as a COMPSs job on the Booster partition of the Leonardo cluster at the HPC CINECA infrastructure.\r\n\r\n![cwl_mini](images/ministep_graph.png)\r\n\r\nTo enable execution, an executable CWL file containing only the information required to run the process has been produced. In this case, \r\nthe processing step is implemented by the bash script `run_gpu_compss.sh` that uses PyCOMPSs to submit a job to the Leonardo cluster queue through \r\nthe Slurm scheduler. The executable CWL file does not explicitly specify input and output data because the underlying processing step manages \r\nthese details internally. An input configuration file `input.yaml` provides the necessary global input data.\r\n\r\n### How to execute the mini-ST610106\r\n\r\nThe mini-workflow depicted is executed using the following command:\r\n\r\n`cwltool --cachedir cache --preserve-entire-environment mini-ST610106.cwl input.yaml`\r\n\r\nIn this command, the --cachedir flag is employed to save all files generated during execution for debugging purposes, while the \r\n--preserve-entire-environment flag ensures that the current environment variables are transferred into the custom execution environment \r\nestablished by cwltool. Once the command is launched, cwltool creates an isolated execution environment where it runs the bash script, which, in turn, \r\nsubmits the PyCOMPSs job to the computational node via Slurm, with the standard output and error captured in the cwl.out and cwl.err files, respectively.\r\n\r\nThe execution exploits the singularity image of the Tsunami-HySEA code, built with the image creation service of eFlows4HPC using\r\nparameters that fit the architecture of the Cineca cluster Leonardo (i.e., linux/amd64, x86_64, openmpi@4.1.4, cuda@11.8).",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1231",
        "keep": true,
        "latest_version": 3,
        "license": "CC-BY-NC-ND-4.0",
        "link": "https:/workflowhub.eu/workflows/1231?version=3",
        "name": "[DTC-T1] WF6101: Tsunami impact forecasting",
        "number_of_steps": 11,
        "projects": [
            "WP6 - Tsunamis"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophysics"
        ],
        "tools": [
            "SS6113",
            "SS6104",
            "SS6114",
            "SS6101",
            "SS6103",
            "SS6117",
            "SS6118",
            "SS6105",
            "SS6102"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-15",
        "versions": 3
    },
    {
        "create_time": "2025-10-14",
        "creators": [],
        "description": "The workflow DT-AGEF4 will produce seismic hazard maps related to the nearfuture anthropogenic seismicity (induced or triggered seismicity). The anthropogenic seismicity hazard maps will be related to the time-varying technological factors. The rationales behind the proposed workflow are (Lasocki, S., Proc. Sixth Int. Symp. on Rockburst and Seismicity in Mines, 2005; Lasocki, S. and Orlecka-Sikora, B., Tectonophys., 2008; Orlecka-Sikora, B., Tectonophys., 2008; Lasocki, S., Rockburst Mechanisms, Monitoring, Warning, and Mitigation (Xia-Ting Feng, ed.), Butterworth-Heinemann (Elsevier), Oxford, 2017):\r\n1. Anthropogenic seismicity (AS) occurrence is partially controlled by time-varying operation works. Due to the time evolution, probabilistic representations of AS also change in time. Rockmass fracturing in an active source zone will be stationary unless conditions responsible for this process change.\r\n2. Activity of AS zones will cease after exploitation, to which they are linked. New zones will appear at locations where exploitation will commence. The source zones in AS are finite in both space and time. Results of the DT-AGEF4 are, in this case, predictions concerning a specified time period in the future.\r\n3. The DT-AGEF4 scheme is the general scheme of a short-term ground vibrations forecast that assumes that probabilistic functions describing activity in the whole lifetime of the future seismic zone do not change over a short D time. In result, the problem is solved for the sought value amax, with the exceedance probability p in a single seismic zone, where a is Peak Ground Acceleration (PGA) or Spectral Amplitude (SA). \r\n4. In the AGEF4 we estimate possible impacts due to seismic zone that will be active in the future, during the period [D1,D1+D]. In general, however, nothing or not much is known about zone activity during period [D1,D1+D]. In this situation we make use of estimates of probabilistic properties of seismic zones active in the past. The zones active in the past become activity models for future zones. Usually, many activity models for a single future zone are selected. Each alternative model has a\u2019priori probability of realization. Here we propose to use a Multi-Model Ensembles Approach.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1986",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1986?version=1",
        "name": "[DTC-AGEF] WF8104: Forecasting induced shake map",
        "number_of_steps": 15,
        "projects": [
            "WP8 - Anthropogenic geophysical extremes"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2025-10-15",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [],
        "description": "The workflow will primarily focus on estimation of the maximum magnitude using various deterministic and statistical models available in the literature. The workflow consists of 5 steps. The first four steps belong to the CB-AGEF1 and ultimately aim at creation of the advanced seismic catalog. The last 5th step (belonging to the CBAGEF3-1) takes the available hydraulic data and advanced seismicity catalog derived in steps 1-4 and performs the actual assessment of the maximum magnitude. Step 1 is an application existing on the platform performing picking of raw waveform data. The output of the Step 1 is passed to the Step 2 (Phase Association module, in development) which gathers the picks from waveform data originating from different stations and performs the coincidence trigger. The output sets of picks are then passed to the 3rd step which performs assessment of earthquake location using the TRMLoc module already available on the EPISODES platform. The information on the location event is then passed to step 4 which uses waveform data and location data to calculate basic source parameters. Steps 1-4 are perpetually performed and provide updates to the advanced seismic catalog. The advanced seismic catalog is then used in Step 5 to perform the assessment of the maximum magnitude. Last 5th step of the workflow is its key element - an application (software module) for the EPISODES platform to compute the maximum magnitude estimates based on the variety of deterministic and probabilistic models available in the literature. The input of the application is catalog data (location, time and magnitude) and hydraulic parameters (time and injection rate). The output data include general info (index and time) of time series, computed parameters as time series (e.g. b-value and seismogenic index), maximum magnitude estimates with corresponding standard errors of estimations and other supplementary parameters (number of events, cumulative fluid injection and seismic efficiency ratio). The maximum magnitude models consist of McGarr (2014), Hallo et al. (2014), Li et al. (2022), van der Elst et al. (2016) and Shapiro et al. (2013). In any model, where the Gutenberg-Richter (GR) b-value is needed there are four possibilities: constant b-value equal to 1, maximum-likelihood estimation assuming (a) the left-hand side truncated exponential (Aki 1965) and (b) the tapered GR (Pareto) frequency-magnitude distribution (Kagan 2002) and \u2018b-positive\u2019 estimator of van der Elst (2021). The user can compute simultaneously a maximum magnitude assessment using any combination of four b-values and, for example in van der Elst model, with different confidence levels. The application is at the moment tested on the EPISODES platform and is being implemented to be an official, stand-alone application as well.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1985",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1985?version=1",
        "name": "[DTC-AGEF] WF8103: Forecasting maximum magnitude",
        "number_of_steps": 5,
        "projects": [
            "WP8 - Anthropogenic geophysical extremes"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2025-10-15",
        "versions": 1
    },
    {
        "create_time": "2025-02-21",
        "creators": [
            "Marco Salvi"
        ],
        "description": "Forecasting the atmospheric dispersal of volcanic products requires accurate input parameters for transport models, including meteorological data and ash/gas emission terms. Such forecasting builds upon three basic ingredients:\r\n1. Meteorological Data. Typically derived from global, regional, or local-scale models, meteorological data drive the transport and deposition of volcanic particles.\r\n2. Transport Models. These models simulate atmospheric dispersal, incorporating processes such as wind advection, turbulent diffusion, and particle sedimentation. Besides atmospheric density and viscosity, sedimentation velocities depend also on particle size, density, and shape, which affect the particle drag coefficients and atmospheric residence times. Some transport models (e.g. FALL3D) include volcanic ash aggregation and aerosol chemistry, two aspects that can improve the accuracy of dispersal forecasts.\r\n3. Emission Models. Emission models provide the so-called Eruption Source Parameters (ESP), needed to characterize the near vent properties of volcanic plumes (that are a kind of clouds which are connected to their source). The ESP, including eruption start and duration, cloud injection height, vertical mass distribution across the eruption column, Total Grain Size Distribution (TGSD), and Mass Eruption Rate (MER) are essential inputs to transport models. While some parameters (e.g., eruption start and end times, column height) can be directly observed, others like MER must be estimated by means of indirect methods, which introduces significant input uncertainties that propagate through the modelling workflow.\r\n\r\nThe DTC-V2 workflow (WF5201) configures and runs an ensemble of FALL3D model realizations, potentially assimilating in the model different sources of data (groundbased or satellite-based observations). A major advantage of the ensemble model approach is that it allows delivering both deterministic (e.g. ensemble mean) and probabilistic (e.g. fraction of ensemble members exceeding a given condition) forecasts, reflecting the inherent uncertainties in the ESPs. On a coarse-grain, the workflow requires four main steps:\r\n1. Get atmospheric data from Numerical Weather Prediction (NWP) models, assumed to be run and delivered by a third party. DTC-V2 can ingest either NWP forecast data (up to a few days ahead) or reanalyses (for past events or \u201cwhat if\u201d scenarios). Different agencies (e.g. ECMWF, National Weather Services, etc.) serve global/regional data at various spatial and temporal resolutions.\r\n2. Get the Eruption Source Parameters (ESP) from different sources; e.g. from volcano monitoring data provided by State Volcano Observatories (SVO). In case ESPs from several sources are available, the workflow assigns a ranked\r\npriority as shown in Table 1.\r\n3. Setup the FALL3D model and run an ensemble of simulations in the FENIX RI or HPC (leonardo@CINECA). The different ensemble members, each representing a single deterministic scenario, are set by perturbing some critical ESPs (e.g. eruption column height) within their uncertainty range. The model allows introducing both absolute and relative errors for all ESP around the observed value.\r\n4. Post-process the results",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1230",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1230?version=2",
        "name": "[DTC-V2] WF5201: Forecast volcanic ash fallout and dispersal in the atmosphere",
        "number_of_steps": 15,
        "projects": [
            "WP5 - Volcanoes"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "ST520103",
            "ST520113",
            "ST520105",
            "ST520112",
            "ST520111",
            "ST520114",
            "ST520115",
            "ST520108",
            "ST520106",
            "ST520109",
            "ST520104",
            "ST520102",
            "ST520110",
            "ST520107"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-15",
        "versions": 2
    },
    {
        "create_time": "2025-06-12",
        "creators": [],
        "description": "Active volcanoes often host settlements in their vicinity, exposing populations to geohazards related to their activity, in particular during unrest phases, when the potential for eruption is higher. Mount Etna is one of the most active volcanoes\r\nworldwide, closely monitored by a sophisticated network, and surrounded by several villages and the city of Catania. While the frequent summit activity, characterized by lava fountains and ash-rich plumes, poses a significant hazard to civil aviation, the real threat to the population lies in lava flows, fed by the opening of lateral vents at low altitudes (Del Negro et al., 2020). These vents are fed by volcanic dikes intruding the volcano edifice and reaching the surface, sometimes on time scales of a few days only, as observed during the July-August 2001 event (Harris et al., 2011). Therefore, the development of reliable systems for early identification and monitoring of dike intrusion events is a crucial scientific challenge for supporting the hazard assessment of lava flow inundation (Yang et al., 2019). Massive data streams from in-situ monitoring networks and satellite observations now provide high-resolution spatial and temporal information on Earth system processes (Yang et al., 2010), including geodetic data, which is crucial for volcano monitoring. Additionally, the availability of HPC resources (Reed et al., 2015; Hinton et al., 2006) enables the efficient processing and analysis of that data and the execution of computationally intensive simulations. Moreover, advancements in machine learning (ML) methods, such as the deep learning architectures (Hinton et al., 2006; Liu et al., 2022), handling the high dimensionality and nonlinearity of complex natural phenomena, are driving the development of innovative applications for volcano monitoring. At the forefront of ML technologies is the concept of Digital Twins (DTs). A DT is a digital replica of the state and evolution of a physical entity, envisioned as datainformed systems for early warning, forecasting, and hazard assessments. A DT continuously updates its physics-based model through available observations of the target process (De Felipe et al., 2022; Li et al., 2023; Wright et al., 2020). Therefore, DTs are most effective when the object changes significantly over time, data is collected at a sufficiently high sampling rate to capture these changes, and modeling can be performed within the required timeframe for the DT updates (DT-GEO, https://dtgeo.eu/). \r\nThe DT prototype, presented here for Mount Etna, is designed to replicate volcanic unrest induced by dike intrusions (Figure 1). The goal is to generate near-real-time scenarios of potential magmatic sources by analyzing the evolution of ground deformation patterns. The workflow developed for such a purpose exploits both physics-based HPC and ML algorithms. A training component makes use of an order 10 million numerical simulations of deformation patterns due to dyke intrusion at Mount Etna, to train an AI to invert from the observed deformation to the endogenous forces that generated the deformation. An operational component is activated by another AI, which scans the stream of multiparametric data from the Mount Etna control room of INGV and recognises the occurrence of unrest. When that happens, the previous AI is activated. The output consists of the spatial probability distribution of forces representing the source body that produced the displacement recorded by the effective GNSS stations operating on the volcano. The overall workflow includes both the training and operational components, to allow both direct applications to Mount Etna and new training and application stages on the same or other volcanoes. Figure 1 illustrates the DT workflow, showing how each component is connected to the others. The data access layer (#1) provides near real-time updates to the time series analyzed by the ML-based unrest detection system (#8). This system is trained on background behavior during phases of inactivity (#7), and declares the state of unrest when anomalous trends occur, providing the displacement dataset (#2) based on which the AI-based inversion (#6) operates. An archive of simulations for dike sources, produced using GALES (#3), is used to train the AI (#4) to reconstruct the dike scenario based on the available data. By quickly processing the data feeding the Mount Etna control room, the whole workflow provides a near-real-time (2 to 4 evaluations per hour) picture of dike propagation, allowing a quick and robust interpretation of ground deformation data and assisting in early warning and volcanic crisis management. All of the software and procedures will be available open source at the end of the project, for direct use as well as for replicating the approach at other volcanoes. In the following sections, we discuss each component, initial results and potential advancements for enhancing volcano observatory surveillance capabilities.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "1728",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1728?version=1",
        "name": "[DTC-V1] WF5101: Volcanic unrest",
        "number_of_steps": 8,
        "projects": [
            "WP5 - Volcanoes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophysics",
            "geology"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2025-10-15",
        "versions": 1
    },
    {
        "create_time": "2025-10-13",
        "creators": [],
        "description": "The DTC-V4 workflow (WF5401) relies on an atmospheric dispersion model to build the relationship between the plume height and SO2 flux (which taken together are called Eruption Source Parameters, or ESPs) and the SO2 ground concentrations. Here we use FALL3D dispersion model, however as the HMC scheme requires many thousands of forward runs we replace it with an Emulator, a function that approximates the model but runs much faster. So far, a simple interpolate-scale-sum emulator that makes use of the linear relationship between flux and ground concentration for basic FALL3D runs and gives adequate results is implemented. However, more sophisticated neural network (NN) based solutions that are more generally applicable are in development.\r\n\r\nIn using emulators within a Bayesian framework we are adopting the philosophy of \"emulate deterministic relationships, and sample stochastic ones\". The idea is to treat the time evolution of the state of the volcano as our parameter space and use\r\nemulators to calculate all variables (e.g. estimates of measured environmental parameters) that follow deterministically. In this way we can compose emulators together to represent different combinations of physical processes within flexible,\r\nsophisticated statistical models that can still be sampled adequately in reasonable time. As an example of this flexibility, we are experimenting with adding empirical relationships between plume height and ambient weather conditions identified during the 2021 Fagradalsfjall effusive eruption (Iceland), which will aid with the forecasting element of this DTC (i.e. the posterior distribution over ground concentrations beyond the last observations incorporated into the model).\r\n\r\nThe workflow is structured over four main steps:\r\n* WATCH AND FETCH (ST540102): this service is intended to grab the real-time measurements of SO2 concentration at the ground. The package needs to be executed while internet connection is available as it needs to connect to the API developed and maintained by the Environmental Agency of Iceland (data provider). Data is available for up to 25 stations around the Reykjanes peninsula (as of April 2025) which collect measurements with a temporal resolution of 10-minutes up to\r\n1 hour.\r\n* EMULATE (ST540103): This package is the core of DTC-V4 and it generates emulators, or fast forward models. These are functions that adequately approximate the output of a physics-based model such as Fall3D, for a given set of inputs, but perform the calculation at much faster speeds, facilitating their insertion into Monte Carlo Bayesian frameworks. It needs the availability of NWP data provided by external institutions (Copernicus, ECMWF) as well as from IMO itself. For the simple interpolate-scale-sum emulator provided here, this step just manages weather data \u2013 forward runs of Fall3D have been moved to the next (INFER) stage as this allows us to only perform a run when requested by the sampling algorithm saving computation time and disk space.\r\n* INFER (ST540104): This repository is intended for finding the posterior distribution over Eruption Source Parameters (ESPs, here, plume height and SO2 flux), as well as ground SO2 concentration, considering observations, for a given atmospheric dispersion emulator. This can be thought of as a kind of Source Term Estimation (STE) or Data Assimilation (DA), resulting in probabilistic estimates of ESPs and probabilistic forecasts of air quality that are then used for calculating concentration exceedence probabilities. Bayesian inference is performed using PyMC, a Software for Bayesian Data Analysis, which allows empirical relationships (e.g. between plume height and ambient atmospheric conditions) to be included in the model (not yet implemented).\r\n* VISUALIZE (ST540105): a repository for displaying the posterior distribution over Eruption Source Parameters (ESPs) and ground SO2 concentrations in an interactive website. The repository consists of a website that visualizes the probability distributions as time series of violin plots and maps of exceedence probabilities and concentrations. ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1982",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1982?version=1",
        "name": "[DTC-V4] WF5401: Volcanic gas dispersal and deposition",
        "number_of_steps": 5,
        "projects": [
            "WP5 - Volcanoes"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2025-10-15",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Maren B\u00f6se",
            "Savas Ceylan",
            "Johannes Kemper"
        ],
        "description": "## Overview\r\nThis workflow generates **evolutionary ShakeMaps** by combining multiple parametric data sources:  \r\nevent alerts, automatic and manual peak motions, and crowdsourced felt reports.\r\n\r\nIt integrates **pyFinDer**, **FinDer**, **ShakeMap**, **RRSM**, and **EMSC** to continuously update ground motion maps as new information arrives.\r\n\r\nIn its full configuration, WF7602 is also capable of:\r\n- Sending **event alerts** for each ShakeMap update.\r\n\r\n**Note:** In this distribution, the alerting feature is **disabled**.\r\n\r\nThe main CWL definition is provided in `WF7602.cwl`.  \r\nWorkflow metadata follows the [workflow-ro-crate-1.0](https://w3id.org/workflowhub/workflow-ro-crate/1.0) profile.\r\n\r\n## Workflow Structure\r\nThe workflow consists of the following datasets (DT) and software services (SS):\r\n\r\n1. **Data Ingestion**\r\n   - **DT7602:** Event alerts from seismic services.\r\n   - **DT7603:** Automated peak motions from RRSM.\r\n   - **DT7604:** Not Implemented: Felt reports from EMSC.\r\n   - **DT7605:** Manual peak motions from ESM.\r\n\r\n2. **Processing & ShakeMap Generation**\r\n   - **SS7602:** Parametric web services and Python wrapper for FinDer (SS7603) \r\n   - **SS7603:** FinDer \u2013 finite fault estimation.\r\n   - **SS7601:** Swiss, Italian and European ShakeMap \u2013 produces ground motion maps.\r\n\r\n3. **Output Storage**\r\n   - **DT7606:** Evolutionary shake maps calculated at every update.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1998",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1998?version=1",
        "name": "[DTC-E6] WF7602: ShakeMaps from Parametric Data",
        "number_of_steps": 8,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophyics"
        ],
        "tools": [
            "Finite-source characterization using template matching (FinDer).",
            "Generate ground motion and shaking intensity maps.",
            "Automated peak motions from RRSM.",
            "EMSC felt reports - planned but not implemented",
            "ML-based algorithm to estimate peak motions at stations.",
            "Integrate manual peak motions into the workflow.",
            "Prepare evolutionary shake maps for output using finite fault characterization.",
            "Collect event alerts."
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Maren B\u00f6se",
            "Savas Ceylan",
            "Johannes Kemper"
        ],
        "description": "## Overview\r\nThis workflow produces **synthetic shaking simulations** for historical earthquake events.  \r\nIt integrates **SeisComP**, **FinDer**, and the **Swiss, Italian, and European ShakeMap** implementation to process continuous seismic data, estimate rupture parameters, and generate **evolutionary shake maps**.\r\n\r\nIn its full configuration, WF7601 is also capable of:\r\n- Sending **event alerts** when specific trigger conditions are met.\r\n- Pushing generated ShakeMaps to an external **web portal** for dissemination.\r\n\r\n**Note:** In this distribution, both the alerting and ShakeMap publishing features are **disabled**.\r\n\r\nThe main CWL definition is provided in `WF7601.cwl`.  \r\nWorkflow metadata follows the [workflow-ro-crate-1.0](https://w3id.org/workflowhub/workflow-ro-crate/1.0) profile.\r\n\r\n## Workflow Structure\r\nThe workflow consists of the following datasets (DT) and software services (SS):\r\n\r\n1. **Data Ingestion & Preprocessing**\r\n   - **DT7601:** Continuous seismic data (waveforms) from seismic stations. Replaced by a *playback* module to mimic data stream using past earthquakes.\r\n   - **SS7602:** SeisComP \u2013 for seismic data acquisition and processing.\r\n\r\n2. **Finite Fault Estimation**\r\n   - **SS7603:** FinDer \u2013 finite fault earthquake early warning algorithm.\r\n\r\n3. **ShakeMap Generation**\r\n   - **SS7601:** Swiss, Italian and European ShakeMap \u2013 generates synthetic ground motion maps.\r\n\r\n4. **Output Storage**\r\n   - **DT7606:** Evolutionary shake maps calculated at multiple update intervals between 0-60 s.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1997",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1997?version=1",
        "name": "[DTC-E6] WF7601: Synthetic Shaking Workflow",
        "number_of_steps": 6,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophysics"
        ],
        "tools": [
            "Generate ground motion and shaking intensity maps.",
            "Finite-source characterization using template matching (FinDer).",
            "ML-based algorithm to estimate peak motions at stations.",
            "Seedlink streaming to processing infrastructure.",
            "Receive continuous seismic waveform data.",
            "Prepare evolutionary shake maps for output using finite fault characterization."
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Mathilde Marchandon",
            "Alice-Agnes Gabriel",
            "Iris Christadler"
        ],
        "description": "## Overview\r\nThis repository contains the **Common Workflow Language (CWL)** and **RO-Crate metadata** definition for **WF7403**, the **Rupture Forecast** workflow of **DTC-E4**, developed under the [DT-GEO project](https://dtgeo.eu).  \r\n\r\nThe **WF7403** workflow performs **physics-based dynamic rupture simulations** to generate a catalog of rupture scenarios for a given fault system. For each scenario, the workflow computes synthetic data such as ground motions, static displacements, and moment-rate release, along with corresponding shakemaps.  \r\n\r\nIn the event of a moderate to large earthquake, WF7403 supports a **rapid-response mode** that automatically compares real-time seismic and geodetic observations with the precomputed scenario catalog to identify the best-fitting rupture model within minutes to hours after the event.\r\n\r\n## Workflow Structure\r\n\r\nThe workflow is organized into two interconnected sub-workflows:\r\n\r\n1. **A. Catalog Creation Workflow**\r\n   - Generates a large ensemble of forward dynamic rupture simulations using **SeisSol** or **SPECFEM3D**, producing synthetic observables and storing them in the **Simulation Data Lake (SDL)**.  \r\n   - This process typically requires high-performance computing (Tier-0/Tier-1 resources) for up to **100 SeisSol runs**, equivalent to ~500,000 core hours.  \r\n\r\n2. **B. Rapid Response Workflow**\r\n   - Continuously monitors real-time earthquake catalogs.\r\n   - When an event is detected, it downloads waveform data and performs a **scenario-matching analysis** against the precomputed catalog in the SDL.\r\n   - Outputs the best-matching dynamic rupture scenario for rapid situational awareness.\r\n\r\n### Workflow Inputs\r\n- **DT7406 \u2013 Input files for dynamic rupture simulation**: Mesh, velocity model, frictional and medium parameters, and initial stress conditions.  \r\n- **DT7408 \u2013 Seismic and geodetic data**: Real-time data streamed from EPOS infrastructures.  \r\n\r\n### Workflow Outputs\r\n- **DT7409 \u2013 Single Dynamic Rupture Scenario**: The best-fitting scenario output from the catalog, including ground-motion and displacement fields.\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1996",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1996?version=1",
        "name": "[DTC-E4] WF7403: Rupture Forecast Workflow",
        "number_of_steps": 2,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophysics"
        ],
        "tools": [
            "Catalog Creation Workflow.",
            "Rapid Response Workflow."
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Bertrand Delouis",
            "Jean-Paul Ampuero",
            "Johannes Kemper"
        ],
        "description": "## Overview\r\nThis repository contains the **Common Workflow Language (CWL)** and **RO-Crate metadata** definition for **WF7402**, the **Kinematic Inversion** workflow of **DTC-E4**, developed under the [DT-GEO project](https://dtgeo.eu).  \r\n\r\nThe **WF7402** workflow performs **automatic kinematic source inversion** using seismic and geodetic data to produce slip models and kinematically-informed dynamic rupture simulations. It integrates multiple data sources and inversion tools to reconstruct the spatial and temporal evolution of fault slip during an earthquake event.\r\n\r\nThis workflow supports rapid, automated inversion following real or simulated earthquakes, contributing to the DT-GEO digital twin framework for dynamic earthquake modeling and validation.\r\n\r\n## Workflow Structure\r\n\r\n1. **Workflow Inputs**\r\n   - **DT7403 \u2013 Seismograms, GPS offsets, Fault geometry, and Velocity model**: Input dataset including fault geometry, velocity models, and seismic and geodetic observations from arrays of seismic and GPS stations.  \r\n\r\n2. **Processing Steps**\r\n   - **ST740201:** Input data ingestion and preparation.  \r\n   - **ST740202:** Fault geometry and velocity model setup.  \r\n   - **ST740203:** Execution of the kinematic inversion using the *SLIPNEAR* inversion code.  \r\n   - **ST740204:** Model validation and generation of slip distribution results.  \r\n   - **ST740205:** Post-processing and export of kinematically-informed dynamic rupture models.  \r\n\r\n3. **Workflow Outputs**\r\n   - **DT7404 \u2013 Kinematic Slip Model**: Slip distribution and model predictions from the inversion process.  \r\n   - **DT7405 \u2013 Kinematically-informed Dynamic Rupture Model**: Dynamic rupture simulations derived from the kinematic results.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1995",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1995?version=1",
        "name": "[DTC-E4] WF7402: Kinematic Inversion Workflow",
        "number_of_steps": 5,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophysics"
        ],
        "tools": [
            "Prepare kinematic inversion data.",
            "Run kinematic source inversion using HPC.",
            "Run kinematically informed dynamic rupture simulation using HPC.",
            "Generate kinematic rupture model.",
            "Kinematically-informed dynamic rupture model."
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Nico Schliwa",
            "Johannes Kemper"
        ],
        "description": "## Overview\r\nThis repository contains the **Common Workflow Language (CWL)** and **RO-Crate metadata** definition for **WF7401**, the **Dynamic Source Inversion** workflow of **DTC-E4**, developed under the [DT-GEO project](https://dtgeo.eu).  \r\n\r\nThe **WF7401** workflow performs **dynamic source inversion of earthquakes** using seismic and geodetic data. It prepares input data, executes high-performance computing (HPC) inversions, and extracts rupture models to characterize the physical parameters of earthquake sources.\r\n\r\nThe workflow leverages the **FD3D_TSN** inversion code, combining a quasi-dynamic boundary element solver and a Bayesian inversion framework with Parallel Tempering Monte Carlo to estimate model parameters and uncertainties.\r\n\r\n## Workflow Structure\r\n\r\n1. **Workflow Inputs**\r\n   - **DT7401 \u2013 Dynamic Source Inversion Input**: Input dataset including fault geometry, velocity models, and seismic and geodetic data from arrays of seismic and GPS stations.  \r\n\r\n2. **Processing Steps**\r\n   - **ST740101:** Input data preparation and format standardization.  \r\n   - **ST740102:** Configuration of the FD3D_TSN inversion model.  \r\n   - **ST740103:** Execution of HPC-based inversion for rupture dynamics.  \r\n   - **ST740104:** Post-processing and extraction of rupture parameters (prestress, friction, misfits).  \r\n   - **ST740105:** Generation of final model products and metadata export.  \r\n\r\n3. **Workflow Outputs**\r\n   - **DT7402 \u2013 Dynamic Source Inversion Output**: Model input parameters, rupture characteristics, and misfit statistics in text and binary formats.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1994",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1994?version=1",
        "name": "[DTC-E4] WF7401: Dynamic Source Inversion Workflow",
        "number_of_steps": 3,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophysics"
        ],
        "tools": [
            "Generate data-driven dynamic rupture model.",
            "Prepare dynamic inversion data.",
            "Perform dynamic source inversion using HPC."
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Graeme Weatherill",
            "Riccardo Zaccarelli",
            "Johannes Kemper"
        ],
        "description": "## Overview\r\nThis repository contains the **Common Workflow Language (CWL)** and **RO-Crate metadata** definition for **WF7301**, the **Ground Motion Data Processing and Hazard Product Generation** workflow of **DTC-E3**, developed under the [DT-GEO project](https://dtgeo.eu).  \r\n\r\nThe **WF7301** workflow automates the **collection, processing, and validation of ground motion data** to produce updated **hazard and risk products**. It integrates observational datasets with computational tools for model calibration, anomaly detection, and intensity model testing.\r\n\r\nThe workflow supports both real-time and historical seismic data ingestion, providing consistent data products for subsequent digital twin simulations and validation efforts.\r\n\r\n## Workflow Structure\r\n\r\n1. **Workflow Inputs**\r\n   - **DT7301 \u2013 Observed Ground Motion Data**: Ground motion waveforms and intensity measurements from ORFEUS (ESM, RRSM, EIDA).  \r\n   - **Simulation Dataset**: Synthetic ground motion simulations generated from computational models.  \r\n\r\n2. **Processing Steps**\r\n   - **ST730101:** Data ingestion and preprocessing using *Stream2Segment*.  \r\n   - **ST730102:** Ground motion intensity calculation and statistical correction.  \r\n   - **ST730103:** Model validation and ground motion prediction testing with *eGSIM*.  \r\n   - **ST730104:** Seismic anomaly detection using *SDAAS* (Simple ML tool for anomaly scoring).  \r\n   - **ST730105:** Integration of observed and simulated data for hazard and risk product updates.  \r\n\r\n3. **Workflow Outputs**\r\n   - **DT7303 \u2013 Hazard and Risk Products**: Processed and validated datasets containing ground shaking intensities, hazard curves, and derived risk products.\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1993",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1993?version=1",
        "name": "[DTC-E3] WF7301: Ground Motion Models",
        "number_of_steps": 10,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophysics"
        ],
        "tools": [
            "Ground Motion Model (GMM)",
            "Simulation Data",
            "Observed Ground Motion Data: Stream2Segment subworkflow",
            "Data Assimilation and Harmonisation",
            "Dynamically Updated GMM Coefficients (includes eGSIM and SDAAS)",
            "Anomaly Detection (SDAAS)",
            "Simulated Ground Motion Comparison & Validation",
            "GMM Coefficient Set (OpenQuake Engine)",
            "Near-Fault Model",
            "Updated Hazard & Risk Products"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Leila Mizrahi",
            "Nicolas Schmid",
            "Johannes Kemper"
        ],
        "description": "## Overview\r\nThis repository contains the **Common Workflow Language (CWL)** and **RO-Crate metadata** definition for **WF7202**, the **Short-Term Earthquake Forecasting (STEF)** workflow of **DTC-E2**, developed under the [DT-GEO project](https://dtgeo.eu).  \r\n\r\nThe **WF7202** workflow generates **short-term earthquake forecasts** by combining real-time and high-resolution seismic event catalogs with the **Epidemic-Type Aftershock Sequence (ETAS)** model. It enables dynamic, data-driven forecast updates based on the latest seismic activity, contributing to the earthquake digital twin\u2019s predictive capabilities.\r\n\r\n## Workflow Structure\r\n\r\nThe workflow integrates **data ingestion**, **forecast computation**, and **testing** components that operate together to produce and validate earthquake forecasts.\r\n\r\n1. **Workflow Inputs**\r\n   - **DT7202 \u2013 Swiss National Earthquake Catalogue**: Regional earthquake catalog provided by the Swiss Seismological Service (SED).  \r\n   - **DT7203 \u2013 EMSC Catalog**: European catalog from the EMSC.  \r\n   - **DT7201 \u2013 Short-term Earthquake Forecasts**: Previously issued forecast data for model updates.  \r\n\r\n2. **Processing Steps**\r\n   - **ST720201:** Data ingestion from real-time catalog sources.  \r\n   - **ST720202:** Model calibration and ETAS parameter estimation.  \r\n   - **ST720203:** Short-term forecast generation.  \r\n   - **ST720204:** Evaluation and testing using the CSEP framework.  \r\n   - **ST720205:** Forecast visualization and dissemination through a web service.  \r\n\r\n3. **Workflow Outputs**\r\n   - **DT7201 \u2013 Short-term Earthquake Forecasts**: Gridded forecast probabilities and simulated catalogs.  \r\n   - **DT7204 \u2013 Forecast Test Results**: CSEP testing outputs for model performance assessment.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1992",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1992?version=1",
        "name": "[DTC-E2] WF7202: Short-Term Earthquake Forecasting",
        "number_of_steps": 8,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophysics"
        ],
        "tools": [
            "ETAS forecast process (subworkflow: inversion, update, simulations)",
            "HERMES \u2013 Harmonize and merge input data",
            "Web service for forecast access, including visualization",
            "Activity monitoring and scheduling (model and forecast update)",
            "Visualization of forecast",
            "Forecast database",
            "Prospective testing of forecasts, including CSEP Testing",
            "Ingest real-time input earthquake catalogs"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Margarita Segou",
            "Johannes Kemper"
        ],
        "description": "## Overview\r\nThis repository contains the **Common Workflow Language (CWL)** and **RO-Crate metadata** definition for **WF7201**, the **AI-based Seismic Catalogue Generation** workflow of **DTC-E2**, developed under the [DT-GEO project](https://dtgeo.eu).  \r\n\r\n**WF7201** integrates advanced machine learning models for seismic signal detection and catalog generation within a unified, scalable computational framework. It builds upon **[QuakeFlow](https://github.com/AI4EPS/QuakeFlow)** \u2014 a deep-learning-based earthquake monitoring pipeline \u2014 to automate waveform ingestion, event detection, association, location, and catalog generation.\r\n\r\nThe workflow enables near-real-time seismic catalogue creation from waveform data, supporting downstream digital twin components for earthquake analysis and forecasting.\r\n\r\n## Workflow Structure\r\n\r\nThe workflow integrates several sequential stages for data ingestion, AI-driven detection, and catalog production:\r\n\r\n1. **Workflow Inputs**\r\n   - **WAVEFORMS** \u2013 Continuous seismic waveform data from multiple stations.  \r\n   - **DT7205 (AlpArray Catalogue)** \u2013 Example dataset from the *AlpArray Network* (EIDA download via ObsPy).  \r\n\r\n2. **Processing Steps**\r\n   - **ST720101:** Waveform ingestion and pre-processing (e.g., filtering, normalization).  \r\n   - **ST720102:** AI-based phase picking using neural network models from QuakeFlow.  \r\n   - **ST720103:** Event association to link picks to potential earthquake events.  \r\n   - **ST720104:** Event location and magnitude estimation.  \r\n   - **ST720105:** High-resolution catalogue generation and export.  \r\n\r\n3. **Workflow Output**\r\n   - **HIGH_RESOLUTION_CATALOG** \u2013 AI-generated earthquake catalogue containing high-precision event locations and magnitudes.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1991",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1991?version=1",
        "name": "[DTC-E2] WF7201: AI-based Seismic Catalogue Generation (QuakeFlow)",
        "number_of_steps": 5,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Generate high-resolution event catalog",
            "AI-based picking of seismic phases",
            "Locate events, compute magnitudes and focal mechanisms",
            "Association of picks into seismic events",
            "Waveform ingestion from EPOS/EIDA"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Laurentiu Danciu",
            "Nicolas Schmid",
            "Johannes Kemper"
        ],
        "description": "## Overview\r\nThis repository contains the **Common Workflow Language (CWL)** and **RO-Crate metadata** definition for **WF7104**, the **Computational Risk** workflow of **DTC-E1**, developed under the [DT-GEO project](https://dtgeo.eu).  \r\n\r\nThe **WF7104** workflow performs **seismic risk computations**, transforming seismic hazard results into quantifiable risk metrics such as loss maps and loss curves. It integrates outputs from the **Computational Hazard Workflow (WF7103)** with exposure and vulnerability datasets from the *2020 European Seismic Risk Model (ESRM20)* to estimate potential economic and structural losses due to earthquakes.\r\n\r\n## Workflow Structure\r\n\r\nThe workflow builds on standardized **datasets (DT)** and computational **steps (ST)** to assess seismic risk across regions.\r\n\r\n1. **Workflow Inputs**\r\n   - **DT7105** \u2013 Input files for the *2020 European Seismic Risk Model (ESRM20)*.  \r\n   - **DT7108** \u2013 *Updated Seismogenic Source Model* generated by WF7102 and used for consistency with hazard calculations.  \r\n\r\n2. **Processing Steps**\r\n   - **ST710401:** Preprocess exposure and vulnerability data.  \r\n   - **ST710402:** Integrate hazard results with exposure and vulnerability datasets.  \r\n   - **ST710403:** Run loss estimation simulations using OpenQuake.  \r\n   - **ST710404:** Post-process results into loss maps, loss curves, and statistical summaries.  \r\n\r\n3. **Workflow Output**\r\n   - **DT7110** \u2013 *Seismic Risk Results: Loss Maps and Loss Curves*, suitable for visualization and decision-support systems.\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1990",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1990?version=1",
        "name": "[DTC-E1] WF7104: Computational Risk",
        "number_of_steps": 3,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophysics"
        ],
        "tools": [
            "E1: Get OQ Input Files",
            "Risk Calculation (OpenQuake)",
            "E3: Risk Output \u2013 Risk Maps, Risk Curves, etc."
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Laurentiu Danciu",
            "Nicolas Schmid",
            "Johannes Kemper"
        ],
        "description": "## Overview\r\nThis repository contains the **Common Workflow Language (CWL)** and **RO-Crate metadata** definition for **WF7103**, the **Computational Hazard** workflow of **DTC-E1**, developed under the [DT-GEO project](https://dtgeo.eu).  \r\n\r\nThe **WF7103** workflow performs **probabilistic seismic hazard computations** by combining seismogenic source models with computational hazard software. It integrates outputs from the **Seismogenic Source Models Workflow (WF7102)** and base datasets such as the *2020 European Seismic Hazard Model (ESHM20)* to produce hazard maps and related outputs that quantify ground-shaking probabilities across regions.\r\n\r\n## Workflow Structure\r\n\r\nThe workflow follows a modular structure, using standardized **datasets (DT)** and computational **steps (ST)** to run regional-scale seismic hazard simulations.\r\n\r\n1. **Workflow Inputs**\r\n   - **DT7104** \u2013 Seismogenic Source Model of the *2020 European Seismic Hazard Model (ESHM20)*.  \r\n   - **DT7108** \u2013 *Updated Seismogenic Source Model* generated by WF7102.  \r\n\r\n2. **Processing Steps**\r\n   - **ST710301:** Prepare input model configurations and hazard parameters.  \r\n   - **ST710302:** Run seismic hazard simulations using the OpenQuake engine.  \r\n   - **ST710303:** Post-process hazard outputs (e.g., ground motion fields, hazard curves).  \r\n   - **ST710304:** Validate and archive the results for subsequent risk modeling (WF7104).\r\n\r\n3. **Workflow Output**\r\n   - **DT7109** \u2013 *Seismic Hazard Results* containing probabilistic ground-shaking maps, hazard curves, and summary statistics.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1989",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1989?version=1",
        "name": "[DTC-E1] WF7103: Computational Hazard",
        "number_of_steps": 3,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophysics"
        ],
        "tools": [
            "Get OQ Input Hazard Files",
            "Hazard Output \u2013 Hazard Maps, Curves, UHS",
            "Hazard Calculation (OpenQuake)"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Laurentiu Danciu",
            "Nicolas Schmid",
            "Johannes Kemper"
        ],
        "description": "## Overview\r\nThis repository contains the **Common Workflow Language (CWL)** and **RO-Crate metadata** definition for **WF7102**, the **Seismogenic Source Models** workflow of **DTC-E1**, developed under the [DT-GEO project](https://dtgeo.eu).  \r\n\r\nThe **WF7102** workflow is responsible for **updating and generating the Seismogenic Source Model (SSM)** by integrating multiple geological datasets. It processes outputs from the **Data Input Workflow (WF7101)** and combines them with additional model data to create an updated representation of earthquake-generating sources used in seismic hazard modeling.  \r\n\r\n## Workflow Structure\r\n\r\nThe workflow comprises a series of **datasets (DT)** and **steps (ST)** that progressively refine and update the Seismogenic Source Model.\r\n\r\n1. **Workflow Inputs**\r\n   - **DT7104** \u2013 Seismogenic Source Model of the *2020 European Seismic Hazard Model (ESHM20)*.  \r\n   - **ACTIVE_FAULT_DATASET** \u2013 Output from WF7101 describing active fault geometries.  \r\n   - **CRUSTAL_FAULT_MODEL** \u2013 Validated crustal fault data.  \r\n   - **SUBDUCTION_ZONES** \u2013 Harmonized subduction zone dataset.  \r\n\r\n2. **Processing Steps**\r\n   - **ST710201:** Initial data ingestion from DT7104 and input datasets.  \r\n   - **ST710202:** Preprocessing and format harmonization.  \r\n   - **ST710203:** Integration of active fault, crustal fault, and subduction zone models.  \r\n   - **ST710204:** Consistency and completeness checks across regions.  \r\n   - **ST710205:** Final model assembly and export of the updated Seismogenic Source Model.\r\n\r\n3. **Workflow Output**\r\n   - **DT7108** \u2013 *Updated Seismogenic Source Model*, ready for downstream computational hazard workflows (WF7103).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1988",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1988?version=1",
        "name": "[DTC-E1] WF7102: Seismogenic Source Models",
        "number_of_steps": 5,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophysics"
        ],
        "tools": [
            "Data Retrieval and Transformation.",
            "Estimation of Activity Rates.",
            "Seismogenic Sources Model (SSM). EFEHR static repository.",
            "Update of Seismogenic Source Model.",
            "Long Term Earthquake Forecasts."
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Laurentiu Danciu",
            "Nicolas Schmid",
            "Johannes Kemper"
        ],
        "description": "## Overview\r\nThis repository contains the **Common Workflow Language (CWL)** and **RO-Crate metadata** definition for **WF7101**, the **Data Input** workflow of **DTC-E1**, developed under the [DT-GEO project](https://dtgeo.eu).  \r\n\r\nThe **Data Input workflow (WF7101)** serves as the entry point for the DTC-E1 pipeline. It collects, validates, and prepares essential datasets for downstream workflows, including **Seismogenic Source Models (WF7102)**, **Computational Hazard (WF7103)**, and **Computational Risk (WF7104)**.  \r\n\r\nIt integrates multiple **data sources (DT7101\u2013DT7103)** representing crustal faults, subduction zones, and active fault datasets, and ensures standardized outputs ready for subsequent modeling workflows.\r\n\r\n## Workflow Structure\r\n\r\nThe workflow is composed of **steps (ST)** and **datasets (DT)**, following DT-GEO\u2019s standardized schema. The general data flow is:\r\n\r\n1. **Workflow Inputs**\r\n   - **DT7101** \u2013 Active fault data repository.  \r\n   - **DT7102** \u2013 Regional crustal fault dataset.  \r\n   - **DT7103** \u2013 Subduction zone dataset.\r\n\r\n2. **Preprocessing Steps**\r\n   - **ST710101\u2013ST710105:** Validate, harmonize, and convert data into uniform formats compatible with downstream workflows.\r\n\r\n3. **Workflow Outputs**\r\n   - **SUBDUCTION_ZONES** \u2013 Standardized subduction zone model dataset.  \r\n   - **CRUSTAL_FAULT_MODEL** \u2013 Cleaned and validated crustal fault data.  \r\n   - **ACTIVE_FAULT_DATASET** \u2013 Finalized active fault model used by WF7102.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1987",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1987?version=1",
        "name": "[DTC-E1] WF7101: Data Input Workflow",
        "number_of_steps": 5,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophysics"
        ],
        "tools": [
            "Update Crustal Faults Model and Subduction Zones.",
            "Statistical Analysis of the Earthquake Catalog.",
            "Harmonize Active Fault Dataset.",
            "Data Cleaning and Feature Selection.",
            "Harmonization of Earthquake Catalogs."
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-10",
        "creators": [
            "Daniel Straub",
            "Alexander Peltzer"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-ampliseq_logo_dark.png\">\n    <img alt=\"nf-core/ampliseq\" src=\"docs/images/nf-core-ampliseq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/ampliseq/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/ampliseq/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/ampliseq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/ampliseq/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/ampliseq/results)[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.1493841-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.1493841)[![Cite Publication](https://img.shields.io/badge/Cite%20Us!-Cite%20Publication-important?labelColor=000000)](https://doi.org/10.3389/fmicb.2020.550420)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/ampliseq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23ampliseq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/ampliseq)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)[![Watch on YouTube](http://img.shields.io/badge/youtube-ampliseq-FFFF00?labelColor=000000&logo=youtube)](https://youtu.be/a0VOEeAvETs)\n\n## Introduction\n\n**nfcore/ampliseq** is a bioinformatics analysis pipeline used for amplicon sequencing, supporting denoising of any amplicon and supports a variety of taxonomic databases for taxonomic assignment including 16S, ITS, CO1 and 18S. Phylogenetic placement is also possible. Multiple region analysis such as 5R is implemented. Supported is paired-end Illumina or single-end Illumina, PacBio and IonTorrent data. Default is the analysis of 16S rRNA gene amplicons sequenced paired-end with Illumina.\n\nA video about relevance, usage and output of the pipeline (version 2.1.0; 26th Oct. 2021) can also be found in [YouTube](https://youtu.be/a0VOEeAvETs) and [billibilli](https://www.bilibili.com/video/BV1B44y1e7MM), the slides are deposited at [figshare](https://doi.org/10.6084/m9.figshare.16871008.v1).\n\n<p align=\"center\">\n    <img src=\"docs/images/ampliseq_workflow.png\" alt=\"nf-core/ampliseq workflow overview\" width=\"60%\">\n</p>\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/ampliseq/results).\n\n## Pipeline summary\n\nBy default, the pipeline currently performs the following:\n\n- Sequencing quality control ([FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n- Trimming of reads ([Cutadapt](https://journal.embnet.org/index.php/embnetjournal/article/view/200))\n- Infer Amplicon Sequence Variants (ASVs) ([DADA2](https://doi.org/10.1038/nmeth.3869))\n- Optional post-clustering with [VSEARCH](https://github.com/torognes/vsearch)\n- Predict whether ASVs are ribosomal RNA sequences ([Barrnap](https://github.com/tseemann/barrnap))\n- Phylogenetic placement ([EPA-NG](https://github.com/Pbdas/epa-ng))\n- Taxonomical classification using DADA2; alternatives are [SINTAX](https://doi.org/10.1101/074161), [Kraken2](https://doi.org/10.1186/s13059-019-1891-0), and [QIIME2](https://www.nature.com/articles/s41587-019-0209-9)\n- Excludes unwanted taxa, produces absolute and relative feature/taxa count tables and plots, plots alpha rarefaction curves, computes alpha and beta diversity indices and plots thereof ([QIIME2](https://www.nature.com/articles/s41587-019-0209-9))\n- Creates phyloseq R objects ([Phyloseq](https://www.bioconductor.org/packages/release/bioc/html/phyloseq.html) and [TreeSE](https://doi.org/10.12688/f1000research.26669.2))\n- Pipeline QC summaries ([MultiQC](https://multiqc.info/))\n- Pipeline summary report ([R Markdown](https://github.com/rstudio/rmarkdown))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, you need to know whether the sequencing files at hand are expected to contain primer sequences (usually yes) and if yes, what primer sequences. In the example below, the paired end sequencing data was produced with 515f (GTGYCAGCMGCCGCGGTAA) and 806r (GGACTACNVGGGTWTCTAAT) primers of the V4 region of the 16S rRNA gene. Please note, that those sequences should not contain any sequencing adapter sequences, only the sequence that matches the biological amplicon.\n\nNext, the data needs to be organized in a folder, here `data`, or detailed in a samplesheet (see [input documentation](https://nf-co.re/ampliseq/usage#input-specifications)).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/ampliseq \\\n   -profile <docker/singularity/.../institute> \\\n   --input \"data\" \\\n   --FW_primer \"GTGYCAGCMGCCGCGGTAA\" \\\n   --RV_primer \"GGACTACNVGGGTWTCTAAT\" \\\n   --outdir <OUTDIR>\n```\n\n> [!NOTE]\n> Adding metadata will considerably increase the output, see [metadata documentation](https://nf-co.re/ampliseq/usage#metadata).\n\n> [!TIP]\n> By default the taxonomic assignment will be performed with DADA2 on SILVA database, but there are various tools and databases readily available, see [taxonomic classification documentation](https://nf-co.re/ampliseq/usage#taxonomic-classification). Differential abundance testing with ([ANCOM](https://www.ncbi.nlm.nih.gov/pubmed/26028277)) or ([ANCOM-BC](https://www.ncbi.nlm.nih.gov/pubmed/32665548)) when opting in.\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/ampliseq/usage) and the [parameter documentation](https://nf-co.re/ampliseq/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/ampliseq/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/ampliseq/output).\n\n## Credits\n\nnf-core/ampliseq was originally written by Daniel Straub ([@d4straub](https://github.com/d4straub)) and Alexander Peltzer ([@apeltzer](https://github.com/apeltzer)) for use at the [Quantitative Biology Center (QBiC)](https://www.info.qbic.uni-tuebingen.de/) and [Microbial Ecology, Center for Applied Geosciences](http://www.uni-tuebingen.de/de/104325), part of Eberhard Karls Universit\u00e4t T\u00fcbingen (Germany). Daniel Lundin [@erikrikarddaniel](https://github.com/erikrikarddaniel) ([Linnaeus University, Sweden](https://lnu.se/)) joined before pipeline release 2.0.0 and helped to improve the pipeline considerably.\n\nWe thank the following people for their extensive assistance in the development of this pipeline (in alphabetical order):\n\n[Adam Bennett](https://github.com/a4000), [Diego Brambilla](https://github.com/DiegoBrambilla), [Emelie Nilsson](https://github.com/emnilsson), [Jeanette T\u00e5ngrot](https://github.com/jtangrot), [Lokeshwaran Manoharan](https://github.com/lokeshbio), [Marissa Dubbelaar](https://github.com/marissaDubbelaar), [Sabrina Krakau](https://github.com/skrakau), [Sam Minot](https://github.com/sminot), [Till Englert](https://github.com/tillenglert)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#ampliseq` channel](https://nfcore.slack.com/channels/ampliseq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use `nf-core/ampliseq` for your analysis, please cite the `ampliseq` article as follows:\n\n> **Interpretations of Environmental Microbial Community Studies Are Biased by the Selected 16S rRNA (Gene) Amplicon Sequencing Pipeline**\n>\n> Daniel Straub, Nia Blackwell, Adrian Langarica-Fuentes, Alexander Peltzer, Sven Nahnsen, Sara Kleindienst\n>\n> _Frontiers in Microbiology_ 2020, 11:2652 [doi: 10.3389/fmicb.2020.550420](https://doi.org/10.3389/fmicb.2020.550420).\n\nYou can cite the `nf-core/ampliseq` zenodo record for a specific version using the following [doi: 10.5281/zenodo.1493841](https://zenodo.org/badge/latestdoi/150448201)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "964",
        "keep": true,
        "latest_version": 28,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/964?version=28",
        "name": "nf-core/ampliseq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "16s",
            "18s",
            "its",
            "metabarcoding",
            "metagenomics",
            "amplicon-sequencing",
            "edna",
            "illumina",
            "iontorrent",
            "metataxonomics",
            "microbiome",
            "pacbio",
            "qiime2",
            "rrna",
            "taxonomic-classification",
            "taxonomic-profiling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-10-10",
        "versions": 28
    },
    {
        "create_time": "2025-10-07",
        "creators": [
            "Alem Gusinac",
            "Thomas Ederveen",
            "Jos Boekhorst",
            "Annemarie Boleij"
        ],
        "description": "[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.10.0-23aa62.svg?labelColor=000000)](https://www.nextflow.io/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![nf-test](https://img.shields.io/badge/tested_with-nf--test-337ab7.svg)](https://code.askimed.com/nf-test)\r\n\r\n## Introduction: **metaBIOMx**\r\n\r\nThe metagenomics microbiomics pipeline is a best-practice suite for the decontamination and annotation of sequencing data obtained via short-read shotgun sequencing. The pipeline contains [NF-core modules](https://github.com/nf-core/modules) and other local modules that are in the similar format. It can be runned via both docker and singularity containers.\r\n\r\n## Pipeline summary\r\nThe pipeline is able to perform different taxonomic annotation on either (single/paired) reads or contigs. The different subworkflows can be defined via `--bypass_<method>` flags, a full overview is shown by running `--help`. By default the pipeline will check if the right databases are present in the right formats, when the path is provided. If this is not the case, compatible databases will be automatically downloaded.\r\n\r\nFor both subworkflows the pipeline will perform read trimming via [Trimmomatic](https://github.com/timflutre/trimmomatic) and/or [AdapterRemoval](https://github.com/MikkelSchubert/adapterremoval), followed by human removal via [Kneaddata](https://huttenhower.sph.harvard.edu/kneaddata/). Before and after each step the quality control will be assessed via [fastqc](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) and a [multiqc](https://github.com/MultiQC/MultiQC) report is created as output. Then taxonomy annotation is done as follows:\r\n\r\n**Read annotation**\r\n- paired reads are interleaved using [BBTools](https://archive.jgi.doe.gov/data-and-tools/software-tools/bbtools/).\r\n- [MetaPhlAn3](https://huttenhower.sph.harvard.edu/metaphlan/) and [HUMAnN3](https://huttenhower.sph.harvard.edu/humann/) are used for taxonomy and functional profiling.\r\n- taxonomy profiles are merged into a single BIOM file using [biom-format](https://github.com/biocore/biom-format).\r\n\r\n**Contig annotation**\r\n- read assembly is performed via [SPAdes](http://cab.spbu.ru/software/spades/).\r\n- Quality assesment of contigs is done via [Busco](https://busco.ezlab.org/).\r\n- taxonomy profiles are created using [CAT](https://github.com/dutilh/CAT).\r\n- Read abundance estimation is performed on the contigs using [Bowtie2]() and [BCFtools](http://samtools.github.io/bcftools/bcftools.html).\r\n- Contigs are selected if a read can be aligned against a contig and a BIOM file is generated using [biom-format](https://github.com/biocore/biom-format).\r\n\r\n## Installation\r\n> [!NOTE]\r\n> Make sure you have installed the latest [nextflow](https://www.nextflow.io/docs/latest/install.html#install-nextflow) version! \r\n\r\nClone the repository in a directory of your choice:\r\n```bash\r\ngit clone https://github.com/CMG-GUTS/metabiomx.git\r\n```\r\n\r\nThe pipeline is containerised, meaning it can be runned via docker or singularity images. No further actions need to be performed when using the docker profile, except a docker registery needs to be set on your local system, see [docker](https://docs.docker.com/engine/install/). In case singularity is used, images are automatically cached within the project directory.\r\n\r\n## Usage\r\nSince the latest version, metaBIOMx works with both a samplesheet (CSV) format or a path to the input files. Preferably, samplesheets should be provided.\r\n```bash\r\nnextflow run main.nf --input <samplesheet.csv> -work-dir work -profile singularity\r\nnextflow run main.nf --input <'*_{1,R1,2,R2}.{fq,fq.gz,fastq,fastq.gz}'> -work-dir work -profile singularity\r\n```\r\n\r\n### \ud83d\udccb Sample Metadata File Specification\r\n\r\nmetaBIOMx expects your sample input data to follow a **simple, but strict** structure to ensure compatibility and allow upfront validation. The input should be provided as a **CSV** file where **each entry = one sample** with specified sequencing file paths. Additional properties not mentioned here will be ignored by the validation step.\r\n\r\n### **Properties and Validation Rules**\r\n\r\n#### \ud83d\udd39 Required properties\r\n\r\n| Property     | Type   | Rules / Description                                                                                   |\r\n|--------------|--------|----------------------------------------------------------------------------------------------------|\r\n| `sample_id`     | string | Unique sample ID with no spaces (`^\\S+$`). Serves as an identifier.                                  |\r\n| `forward_read` | string | File path to forward sequencing read. Must be non-empty string matching FASTQ gzipped pattern. File must exist. |\r\n\r\n#### \ud83d\udd39 Optional property\r\n\r\n| Property       | Type   | Rules / Description                                                                                   |\r\n|----------------|--------|----------------------------------------------------------------------------------------------------|\r\n| `reverse_read` | string | File path to reverse sequencing read. Same constraints as `forward_read`. Required if specified.   |\r\n\r\n#### \ud83d\udd39 Pattern\u2011based columns \r\nYou can define extra variables using special prefixes:\r\n- **`CONTRAST_...`** \u2192 grouping/category labels used in differential comparisons  \r\n  Example: `CONTRAST_Treatment` with values `Drug` / `Placebo`\r\nThese prefixes are used to generate an automated `OmicFlow` report with alpha, beta diversity and compositional plots. For more information see [OmicFlow](https://github.com/agusinac/OmicFlow).\r\n\r\n### Example cases\r\n#### \ud83d\udd39 Read annotation\r\n```bash\r\nnextflow run main.nf \\\r\n    --input <samplesheet.csv> \\\r\n    # (optional) --bypass_trim \\\r\n    # (optional) --bypass_decon \\\r\n    --bypass_contig_annotation \\\r\n    -work-dir work \\\r\n    -profile singularity\r\n```\r\n\r\n#### \ud83d\udd39 Contig annotation\r\n```bash\r\nnextflow run main.nf \\\r\n    --input <samplesheet.csv> \\\r\n    # (optional) --bypass_trim \\\r\n    # (optional) --bypass_decon \\\r\n    --bypass_read_annotation \\\r\n    -work-dir work \\\r\n    -profile singularity\r\n```\r\n\r\nIn case you only have assemblies and wish to perform contig annotation:\r\n```bash\r\nnextflow run main.nf \\\r\n    --input <samplesheet.csv> \\\r\n    --bypass_assembly \\\r\n    --bypass_read_annotation \\\r\n    -work-dir work \\\r\n    -profile singularity\r\n```\r\n\r\n## Automatic database setup\r\nThe pipeline requires a set of databases which are used by the different tools within this workflow. The user is required to specify the location in where the databases will be downloaded. It is also possible to download the databases manually. The `configure` subworkflow will evaluate the database format and presence of the compatible files automatically.\r\n```bash\r\nnextflow run main.nf \\\r\n    --bowtie_db path/to/db/bowtie2 \\\r\n    --metaphlan_db path/to/db/metaphlan \\\r\n    --humann_db path/to/db/humann \\\r\n    --cat_pack_db path/to/db/catpack \\\r\n    --busco_db path/to/db/busco_downloads \\\r\n    -work-dir <work/dir> \\\r\n    -profile <singularity,docker>\r\n```\r\n\r\n<details>\r\n<summary>Manual database setup</summary>\r\n\r\n### HUMAnN3 and MetaPhlan3 DB\r\nMake sure the `path/to/db/humann` should contain a `chocophlan`, `uniref` and `utility_mapping` directory. These can be obtained by the following command:\r\n```bash\r\ndocker pull biobakery/humann:latest\r\n\r\ndocker run --rm -v $(pwd):/scripts biobakery/humann:latest \\\r\n    humann_databases --download chocophlan full ./path/to/db/humann \\\r\n    && humann_databases --download uniref uniref90_diamond ./path/to/db/humann \\\r\n    && humann_databases --download utility_mapping full ./path/to/db/humann\r\n```\r\n\r\n### MetaPhlAn DB\r\n```bash\r\nwget http://cmprod1.cibio.unitn.it/biobakery4/metaphlan_databases/mpa_vJun23_CHOCOPhlAnSGB_202403.tar \\\r\n    && tar -xvf mpa_vJun23_CHOCOPhlAnSGB_202403.tar -C path/to/db/metaphlan \\\r\n    && rm mpa_vJun23_CHOCOPhlAnSGB_202403.tar\r\n\r\nwget http://cmprod1.cibio.unitn.it/biobakery4/metaphlan_databases/bowtie2_indexes/mpa_vJun23_CHOCOPhlAnSGB_202403_bt2.tar \\\r\n    && tar -xvf mpa_vJun23_CHOCOPhlAnSGB_202403_bt2.tar -C path/to/db/metaphlan \\\r\n    && rm mpa_vJun23_CHOCOPhlAnSGB_202403_bt2.tar\r\n\r\necho 'mpa_vJun23_CHOCOPhlAnSGB_202403' > path/to/db/metaphlan/mpa_latest\r\n```\r\n\r\n### Kneaddata DB\r\n```bash\r\ndocker pull agusinac/kneaddata:latest\r\n\r\ndocker run --rm -v $(pwd):/scripts agusinac/kneaddata:latest \\\r\n    kneaddata_database \\\r\n        --download human_genome bowtie2 ./path/to/db/bowtie2\r\n```\r\n\r\n### CAT_pack DB\r\nA pre-constructed diamond database can be [downloaded](https://tbb.bio.uu.nl/tina/CAT_pack_prepare/) manually or by command:\r\n```bash\r\ndocker pull agusinac/catpack:latest\r\n\r\ndocker run --rm -v $(pwd):/scripts agusinac/catpack:latest \\\r\n    CAT_pack download \\\r\n        --db nr \\\r\n        -o path/to/db/catpack\r\n\r\n```\r\n\r\n### busco DB\r\nBUSCO expects that the directory is called `busco_downloads`.\r\n```bash\r\ndocker pull ezlabgva/busco:v5.8.2_cv1\r\n\r\ndocker run --rm -v $(pwd):/scripts ezlabgva/busco:v5.8.2_cv1 \\\r\n    busco \\\r\n        --download bacteria_odb12 \\\r\n        --download_path path/to/db/busco_downloads\r\n```\r\n</details>\r\n\r\n## Support\r\n\r\nIf you are having issues, please [create an issue](https://github.com/CMG-GUTS/metabiomx/issues)\r\n\r\n## Citations\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md)\r\nfile.\r\n",
        "doi": "10.48546/workflowhub.workflow.1787.6",
        "edam_operation": [
            "Gene functional annotation",
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metagenomic sequencing",
            "Metagenomics",
            "Microbial ecology",
            "Microbiology"
        ],
        "filtered_on": "edam",
        "id": "1787",
        "keep": true,
        "latest_version": 6,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1787?version=6",
        "name": "metaBIOMx: Metagenomics pipeline for Microbial shot-gun sequencing data",
        "number_of_steps": 0,
        "projects": [
            "CMG-GUTS"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "metagenomics",
            "nextflow"
        ],
        "tools": [
            "AdapterRemoval",
            "BCFtools",
            "BBTools",
            "Trimmomatic",
            "FastQC",
            "MultiQC",
            "humann",
            "MetaPhlAn",
            "SPAdes",
            "BUSCO",
            "Bowtie 2",
            "CAT and BAT",
            "OmicFlow"
        ],
        "type": "Nextflow",
        "update_time": "2025-10-08",
        "versions": 3
    },
    {
        "create_time": "2025-10-10",
        "creators": [
            "Isabella M Brown",
            "Paul Whatmore",
            "Kylie Munyard"
        ],
        "description": "The bioinformatic workflow presented here enables the analysis of RNA sequencing data obtained from human reproductive tissues in unexplained recurrent pregnancy loss (uRPL) research. This pipeline requires a sample sheet containing the sample information (example_input_data.csv) and gene expression matrices generated using the Salmon tool in the nf-core/rnaseq bioinformatics pipeline (example_count_data.csv). For more information on how to use the nf-core/rnaseq pipeline including the required inputs and expected outputs, please refer to their documentation. The processes used to download publicly available high throughput RNA-seq datasets and generate the Salmon gene expression matrices (e.g. counts files) can be found in our Github repository (also available as a file through WorkflowHub - Data_Preparation.md) alongside documentation showing the expected outputs from this pipeline. \r\n\r\nThe workflow developed during this project was designed with the intent to be used to compare datasets generated using different RNA sequencing methods by looking for concordance in differential expression analysis results, including differentially expressed genes and enriched functional pathways. This workflow can be accessed and used by others to help improve the standardisation and reproducibility of RNA-seq analytical processes, through consistent analysis methods and documentation. \r\n\r\nThis workflow can be split into different sections to complete the following analyses with the main packages used listed (tool versions available in the attached R script)\r\n\r\nSection 1: Intialising environment and loading required packages and files\r\n\r\nSection 2: Principal Component Analysis ([PCAtools](https://bioconductor.org/packages/devel/bioc/vignettes/PCAtools/inst/doc/PCAtools.html))\r\n\r\n* Section 2.1: Generating PCA objects to be used in Sections 2.2-2.4 ([PCAtools](https://bioconductor.org/packages/devel/bioc/vignettes/PCAtools/inst/doc/PCAtools.html))\r\n\r\n* Section 2.2: Principal Component Retention ([PCAtools](https://bioconductor.org/packages/devel/bioc/vignettes/PCAtools/inst/doc/PCAtools.html))\r\n\r\n* Section 2.3: Confounding factor identification using Eigencor plots and Pearson's Correlation coefficients ([PCAtools](https://bioconductor.org/packages/devel/bioc/vignettes/PCAtools/inst/doc/PCAtools.html))\r\n\r\n* Section 2.4: Generate PCA plots with arrows representing confounding numeric variables ([ggplot2](https://ggplot2.tidyverse.org/index.html))\r\n\r\nSection 3: Differential Expression Analysis ([DESeq2](https://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html))\r\n* \tAssess concordance in differential expression between datasets ([ggVenn](https://cran.r-project.org/web/packages/ggvenn/index.html))\r\n\r\nSection 4: Functional Annotation of KEGG pathways ([clusterProfiler](https://www.bioconductor.org/packages//2.10/bioc/html/clusterProfiler.html), [pathview](https://pathview.r-forge.r-project.org/))\r\n\r\nFor the most up-to-date versions of the workflow script please check our [GitHub page](https://github.com/bbrown2371/RNAseq-analysis-workflow-for-unexplained-Recurrent-Pregnancy-Loss.git)\r\n",
        "doi": null,
        "edam_operation": [
            "Differential gene expression profiling",
            "Expression profile clustering",
            "Gene functional annotation",
            "Principal component analysis"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Biomedical science",
            "RNA-Seq",
            "Reproductive health",
            "Workflows"
        ],
        "filtered_on": "annot* in description",
        "id": "1966",
        "keep": true,
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1966?version=3",
        "name": "R workflow for RNA-seq analysis in unexplained recurrent pregnancy loss",
        "number_of_steps": 0,
        "projects": [
            "Transcriptomics in unexplained recurrent pregnancy loss"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "nextflow",
            "r",
            "transcriptomics",
            "workflows",
            "data reproducibility",
            "rna",
            "standardisation"
        ],
        "tools": [
            "nf-core-rnaseq",
            "DESeq2",
            "clusterProfiler",
            "pathview"
        ],
        "type": "R markdown",
        "update_time": "2025-10-10",
        "versions": 3
    },
    {
        "create_time": "2025-10-06",
        "creators": [
            "Jasmin Frangenberg",
            "Anan Ibrahim",
            "James A. Fellows Yates"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-funcscan_logo_flat_dark.png\">\n    <img alt=\"nf-core/funcscan\" src=\"nf-core-funcscan_logo_flat_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/funcscan/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/funcscan/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/funcscan/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/funcscan/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/funcscan/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7643099-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7643099)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/funcscan)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23funcscan-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/funcscan)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n![HiRSE Code Promo Badge](https://img.shields.io/badge/Promo-8db427?style=plastic&label=HiRSE&labelColor=005aa0&link=https%3A%2F%2Fgo.fzj.de%2FCodePromo)\n\n## Introduction\n\n**nf-core/funcscan** is a bioinformatics best-practice analysis pipeline for the screening of nucleotide sequences such as assembled contigs for functional genes. It currently features mining for antimicrobial peptides, antibiotic resistance genes and biosynthetic gene clusters.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/funcscan/results).\n\nThe nf-core/funcscan AWS full test dataset are contigs generated by the MGnify service from the ENA. We used contigs generated from assemblies of chicken cecum shotgun metagenomes (study accession: MGYS00005631).\n\n## Pipeline summary\n\n1. Quality control of input sequences with [`SeqKit`](https://bioinf.shenwei.me/seqkit/)\n2. Taxonomic classification of contigs of **prokaryotic origin** with [`MMseqs2`](https://github.com/soedinglab/MMseqs2)\n3. Annotation of assembled prokaryotic contigs with [`Prodigal`](https://github.com/hyattpd/Prodigal), [`Pyrodigal`](https://github.com/althonos/pyrodigal), [`Prokka`](https://github.com/tseemann/prokka), or [`Bakta`](https://github.com/oschwengers/bakta)\n4. Annotation of coding sequences from 3. to obtain general protein families and domains with [`InterProScan`](https://github.com/ebi-pf-team/interproscan)\n5. Screening contigs for antimicrobial peptide-like sequences with [`ampir`](https://cran.r-project.org/web/packages/ampir/index.html), [`Macrel`](https://github.com/BigDataBiology/macrel), [`HMMER`](http://hmmer.org/), [`AMPlify`](https://github.com/bcgsc/AMPlify)\n6. Screening contigs for antibiotic resistant gene-like sequences with [`ABRicate`](https://github.com/tseemann/abricate), [`AMRFinderPlus`](https://github.com/ncbi/amr), [`fARGene`](https://github.com/fannyhb/fargene), [`RGI`](https://card.mcmaster.ca/analyze/rgi), [`DeepARG`](https://bench.cs.vt.edu/deeparg). [`argNorm`](https://github.com/BigDataBiology/argNorm) is used to map the outputs of `DeepARG`, `AMRFinderPlus`, and `ABRicate` to the [`Antibiotic Resistance Ontology`](https://www.ebi.ac.uk/ols4/ontologies/aro) for consistent ARG classification terms.\n7. Screening contigs for biosynthetic gene cluster-like sequences with [`antiSMASH`](https://antismash.secondarymetabolites.org), [`DeepBGC`](https://github.com/Merck/deepbgc), [`GECCO`](https://gecco.embl.de/), [`HMMER`](http://hmmer.org/)\n8. Creating aggregated reports for all samples across the workflows with [`AMPcombi`](https://github.com/Darcy220606/AMPcombi) for AMPs, [`hAMRonization`](https://github.com/pha4ge/hAMRonization) for ARGs, and [`comBGC`](https://raw.githubusercontent.com/nf-core/funcscan/master/bin/comBGC.py) for BGCs\n9. Software version and methods text reporting with [`MultiQC`](http://multiqc.info/)\n\n![funcscan metro workflow](docs/images/funcscan_metro_workflow.png)\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fasta\nCONTROL_REP1,AEG588A1_001.fasta\nCONTROL_REP2,AEG588A1_002.fasta\nCONTROL_REP3,AEG588A1_003.fasta\n```\n\nEach row represents a (multi-)fasta file of assembled contig sequences.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/funcscan \\\n   -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR> \\\n   --run_amp_screening \\\n   --run_arg_screening \\\n   --run_bgc_screening\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/funcscan/usage) and the [parameter documentation](https://nf-co.re/funcscan/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/funcscan/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/funcscan/output).\n\n## Credits\n\nnf-core/funcscan was originally written by Jasmin Frangenberg, Anan Ibrahim, Louisa Perelo, Moritz E. Beber, James A. Fellows Yates.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\nAdam Talbot, Alexandru Mizeranschi, Hugo Tavares, J\u00falia Mir Pedrol, Martin Klapper, Mehrdad Jaberi, Robert Syme, Rosa Herbst, Vedanth Ramji, @Microbion.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#funcscan` channel](https://nfcore.slack.com/channels/funcscan) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/funcscan for your analysis, please cite it using the following doi: [10.5281/zenodo.7643099](https://doi.org/10.5281/zenodo.7643099)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "987",
        "keep": true,
        "latest_version": 12,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/987?version=12",
        "name": "nf-core/funcscan",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amr",
            "assembly",
            "metagenomics",
            "amp",
            "antibiotic-resistance",
            "antimicrobial-peptides",
            "antimicrobial-resistance-genes",
            "arg",
            "bgc",
            "biosynthetic-gene-clusters",
            "contigs",
            "function",
            "natural-products",
            "screening",
            "secondary-metabolites"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-10-06",
        "versions": 12
    },
    {
        "create_time": "2025-10-03",
        "creators": [
            "Alexander Hambley",
            "Oliver Woolland",
            "Eli Chadwick",
            "Volodymyr Savchenko",
            "Jos\u00e9 M\u00aa Fern\u00e1ndez",
            "Stian Soiland-Reyes"
        ],
        "description": "# WorkflowHub Knowledge Graph\r\n\r\nA tool to generate a knowledge graph from a source of RO Crates. By default, this tool sources and generates an RDF graph of crates from [WorkflowHub](https://workflowhub.eu/). \r\n\r\n[![Docker build and push](https://github.com/workflowhub-eu/workflowhub-graph/actions/workflows/push-docker.yaml/badge.svg)](https://github.com/workflowhub-eu/workflowhub-graph/actions/workflows/push-docker.yaml)\r\n[![Python linting and tests](https://github.com/workflowhub-eu/workflowhub-graph/actions/workflows/lint-and-test.yaml/badge.svg)](https://github.com/workflowhub-eu/workflowhub-graph/actions/workflows/lint-and-test.yaml)\r\n[![Build and publish knowledge graph](https://github.com/workflowhub-eu/workflowhub-graph/actions/workflows/publish-kg.yaml/badge.svg)](https://github.com/workflowhub-eu/workflowhub-graph/actions/workflows/publish-kg.yaml)\r\n\r\n## v1.0.0\r\nThis version of the workflow was used to generate version 2025-08-29 of the WorkflowHub Knowledge Graph (https://doi.org/10.5281/zenodo.16995374).\r\n\r\nIt is documented by the report \"EuroScienceGateway MS6: Integrated EuroScienceGateway knowledge graph\" https://doi.org/10.5281/zenodo.16992674.\r\n\r\n## Getting Started\r\n\r\nThis tool is run as a Snakemake workflow. We recommend building a Docker container to run the workflow:\r\n\r\n```bash \r\ndocker build -t knowledgegraph .\r\n```\r\n\r\nThen, you can run the workflow using the following command:\r\n\r\n```bash\r\ndocker run --rm -v ./workflow-output:/app/output --user $(id -u):$(id -g) knowledgegraph\r\n```\r\n\r\nWhere `./workflow-output` is the directory where the output will be stored (already created for you in this repo) and the `--user` flag ensures that the output files are created with the correct permissions.\r\n\r\n## Structure\r\n\r\n![workflow dag](docs/images/dag.svg)\r\n\r\n- **`source_ro_crates`**: This rule sources RO crates from the WorkflowHub API (`source_crates.py`) \r\n- **`create_graph`**: This rule merges the individual RO crates into a single RDF graph\r\n- **`enrich_graph`**: This rule processes the base graph and adds additional metadata from external sources e.g. WikiData, Orcid\r\n- **`merge_graphs`**: This rule merges the base graph and enrichment graphs\r\n- **`consolidate`**: This rule collapses duplicate entries around canonical objects to make the graph easier to navigate\r\n\r\n[!TIP]\r\n\r\nThis diagram is generated with:\r\n\r\n`docker run --entrypoint '' knowledgegraph snakemake --dag | dot -Tsvg > docs/images/dag.svg`\r\n\r\n## Visualisation / exploration \r\n\r\nBundled in this repo is a stack which allows the knowledge graph to be explored visually and interactively.\r\n\r\nThe containers in the stack provide:\r\n- A triplestore to make SPARQL queries against\r\n- A visualisation tool\r\n- A one-shot tool to configure the visualisation tool\r\n\r\nTo view the visualisation run:\r\n\r\n```bash\r\n# run the workflow as above\r\ncd vis\r\ndocker compose down -v # clears configuration, skip if first run, refine if confident with Docker\r\ndocker compose up\r\n# View visualisation on localhost:4200\r\n```\r\n\r\n## Contributing\r\n\r\n### Coding Style\r\n\r\n- **Code Formatting**: We use [Python Black](https://black.readthedocs.io/en/stable/) for code formatting. Please format your code using Black before submitting a pull request (PR)\r\n- **Type Hinting**: Please use type hints ([PEP 484](https://www.python.org/dev/peps/pep-0484/)), and docstrings ([PEP 257](https://www.python.org/dev/peps/pep-0257/)) in methods and classes.\r\n\r\n### Branching Strategy\r\n\r\n- **Branch Naming**: When working on a new feature or bug fix, create a branch from `develop`. e.g. `feature/description` or `bugfix/description`.\r\n- **Development Branch**: The `develop` branch is currently our main integration branch. Features and fixes should target `develop` through PRs.\r\n- **Feature Branches**: These feature branches should be short-lived and focused. Once done, please create a pull request to merge it into `develop`.\r\n\r\n## License\r\n\r\n[BSD 2-Clause License](https://opensource.org/license/bsd-2-clause)\r\n",
        "doi": "10.48546/workflowhub.workflow.1967.1",
        "edam_operation": [],
        "edam_topic": [
            "Workflows"
        ],
        "filtered_on": "metap* in description",
        "id": "1967",
        "keep": true,
        "latest_version": 1,
        "license": "BSD-2-Clause",
        "link": "https:/workflowhub.eu/workflows/1967?version=1",
        "name": "Build WorkflowHub Knowledge Graph",
        "number_of_steps": 0,
        "projects": [
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "rdf",
            "workflow ro-crate",
            "workflowhub",
            "workflows",
            "ro-crate"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2025-10-21",
        "versions": 1
    },
    {
        "create_time": "2025-10-03",
        "creators": [
            "Jasper Koehorst",
            "Bart Nijsse"
        ],
        "description": "  **Workflow for Metagenomics binning from assembly.<br>**\r\n\r\n  Minimal inputs are: Identifier, assembly (fasta) and an associated sorted BAM file\r\n\r\n  Summary<br>\r\n    - MetaBAT2 (binning)<br>\r\n    - MaxBin2 (binning)<br>\r\n    - SemiBin2 (binning)<br>\r\n    - Binette (bin merging)<br>\r\n    - EukRep (eukaryotic classification)<br>\r\n    - CheckM2 (bin completeness and contamination)<br>\r\n    - BUSCO (bin completeness)<br>\r\n    - GTDB-Tk (bin taxonomic classification)<br>\r\n    - CoverM (bin abundances)<br>\r\n    \r\nIncluding:<br>\r\n   **Bin annotation (workflow: https://workflowhub.eu/workflows/1170):**<br>\r\n        - Bakta<br>\r\n        - Interproscan<br>\r\n        - Eggnog<br>\r\n        - KOfamscan<br>\r\n        - To RDF conversion with SAPP (optional, default on) --> https://workflowhub.eu/workflows/1174/<br>\r\n\r\n  Other UNLOCK workflows on WorkflowHub: https://workflowhub.eu/projects/16/workflows?view=default<br>\r\n  \r\n  **All tool CWL files and other workflows can be found here:**<br>\r\n    https://gitlab.com/m-unlock/cwl<br>\r\n\r\n  **How to setup and use an UNLOCK workflow:**<br>\r\n  https://docs.m-unlock.nl/docs/workflows/setup.html<br>\r\n",
        "doi": null,
        "edam_operation": [
            "Sequence assembly"
        ],
        "edam_topic": [
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "64",
        "keep": true,
        "latest_version": 12,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/64?version=12",
        "name": "Metagenomic Binning from Assembly",
        "number_of_steps": 31,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "binning",
            "metagenome",
            "microbial"
        ],
        "tools": [
            "MetaBAT 2",
            "SemiBin",
            "MaxBin",
            "Binette",
            "BUSCO",
            "CheckM2"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-03",
        "versions": 6
    },
    {
        "create_time": "2025-10-02",
        "creators": [
            "Ryo Mameda",
            "Sora Yonezawa"
        ],
        "description": "\r\n![GitHub last commit (branch)](https://img.shields.io/github/last-commit/RyoMameda/workflow_cwl/main)\r\n![Status](https://img.shields.io/badge/status-development-yellow)\r\n[![cwltool](https://img.shields.io/badge/cwltool-3.1.20250110105449-success)](https://github.com/common-workflow-language/cwltool/releases/tag/3.1.20250110105449)\r\n[![License](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE)\r\n![Version](https://img.shields.io/badge/version-1.0-brightgreen)\r\n[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=python3.11&color=blue&logo=docker)](https://github.com/yonesora56/plant2human/tree/main/.devcontainer)\r\n\r\n# Gene Expression Analysis Workflow in Complex Microbiomes\r\n\r\n&nbsp;\r\n\r\n## Workflow Schema \r\n- more details: [Optimization of Mapping Tools and Investigation of Ribosomal RNA Influence for Data-Driven Gene Expression Analysis in Complex Microbiomes](https://doi.org/10.3390/microorganisms13050995)\r\n\r\n![image](./image/microorganisms-13-00995-g001.png)\r\n\r\n&nbsp;\r\n\r\n### 1. Overview of the Workflow\r\n\r\nThis analysis focuses on transcriptional profiling of complex microbiomes. It requires both metagenomic and metatranscriptomic NGS short-read data, along with annotation reference information (e.g., ribosomal RNA sequences and referenced protein databases, listed below). The metagenomic and metatranscriptomic reads should be derived from the same microbiome samples. Assembled metagenomic contigs are then used as reference sequences to map both types of reads, enabling gene-level quantification.\r\n\r\n&nbsp;\r\n\r\n### 2. Minimum Requirements\r\n\r\n- `Docker`\r\n- `cwltool`\r\n\r\n&nbsp;\r\n\r\n### 3. Workflow Component\r\n\r\nThis analysis workflow is composed of three sub-workflows; metagenomic contig assembling, reads mapping and annotation. \r\n\r\n&nbsp;\r\n\r\n#### Metagenomic contig assembling\r\n\r\nIn this process, the following steps are performed:\r\n\r\n\r\n1. Assembly process using `Megahit`. \r\n2. Prediction Protein sequences using `Prodigal`.\r\n3. Statical analysis of contigs useing `SeqKit`.\r\n\r\n&nbsp;\r\n\r\n#### Reads mapping\r\n\r\nIn this process, the following steps are performed:\r\n\r\n1. Mapping process using `BWA MEM`.\r\n2. Statical analysis of mapping results using `SAMtools`\r\n\r\n&nbsp;\r\n\r\n#### Annotation\r\n\r\nIn this process, the following steps are performed:\r\n\r\n1. Searching contaminated ribosomal RNA sequences using `BLAST`.\r\n2. Searching referenced proteins using `DIAMOND`.\r\n3. Creation GTF formated file contained annotation informations.\r\n\r\n&nbsp;\r\n\r\n### 4. Test Dataset and Your Own Dataset\r\n\r\n- If you are testing with the following files, please place them in the `Data` directory!\r\n- You can also obtain metagenomic and metatranscriptomic FASTQ files either by downloading them from public databases or by using your own samples, and then place them in your `Data` directory.\r\n\r\n#### Metagenome data\r\n\r\n- [SRR27548858](https://www.ncbi.nlm.nih.gov/sra/?term=SRR27548858)\r\n\r\n#### Metatranscriptome data\r\n\r\n- [SRR27548863](https://www.ncbi.nlm.nih.gov/sra/?term=SRR27548863)\r\n- [SRR27548864](https://www.ncbi.nlm.nih.gov/sra/?term=SRR27548864)\r\n- [SRR27548865](https://www.ncbi.nlm.nih.gov/sra/?term=SRR27548865)\r\n\r\n&nbsp;\r\n\r\n### 5. Annotation References\r\n\r\nThese reference files are used in the BLAST and DIAMOND processes. The downloaded files are available in the `Data` directory (accessed on September 17, 2025). If you wish to use the latest versions of the references, please download them using the following scripts.\r\n\r\n```bash\r\n# rRNA data from SILVA website (release138.1; accessed on 17,September,2025)\r\ncurl -O https://ftp.arb-silva.de/release_138.1/Exports/SILVA_138.1_LSUParc_tax_silva.fasta.gz\r\ncurl -O https://ftp.arb-silva.de/release_138.1/Exports/SILVA_138.1_SSUParc_tax_silva.fasta.gz\r\n\r\n# Swiss-Prot data from UniProt for diamond makedb process (accessed on 17,September,2025)\r\ncurl -O https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\r\n\r\n# Pfam data from InterPro (accessed on 17,September,2025)) for hmmscan proess. Appling HMMER process in this workflow is on going, however this process takes time. This step will be optional.\r\n# curl -O https://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz\r\n```\r\n\r\n&nbsp;\r\n\r\n### 6. Command Execution\r\n\r\nWe recommend creating a `cache` directory to store cache and intermediate files. Since metagenomic and metatranscriptomic reads are mapped to contigs, the assembled results can be reused to reduce analytical costs. The `cwltool` properly recognizes caches when the `--cachedir` option is specified.\r\n\r\n```bash\r\n# main workflow\r\n\r\ncwltool --debug --cachedir <cache directory> --outdir <output directory> ./Worlkflow/main_w.cwl ./config/main_w.yml\r\n\r\n```\r\n\r\n&nbsp;\r\n\r\n### 7. based shell script & python script\r\n\r\nGitHub: https://github.com/RyoMameda/workflow\r\n\r\nThis workflow is developed at [DBCLS BioHackathon 2025](https://github.com/dbcls/bh25/), and the preprint of developing project is https://doi.org/10.37044/osf.io/qd5sz_v1.\r\n",
        "doi": "10.48546/workflowhub.workflow.1955.2",
        "edam_operation": [
            "Gene expression profiling",
            "Genome annotation",
            "Sequence annotation",
            "Sequence assembly",
            "Sequence trimming"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Microbiology",
            "Sequence analysis",
            "Sequence assembly"
        ],
        "filtered_on": "metage* in tags",
        "id": "1955",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1955?version=2",
        "name": "Gene Expression Analysis Workflow in Complex Microbiomes",
        "number_of_steps": 3,
        "projects": [
            "bonohulab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "metagenomics",
            "metatranscriptomics"
        ],
        "tools": [
            "BLASTN rRNA annotation process"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-02",
        "versions": 2
    },
    {
        "create_time": "2025-10-01",
        "creators": [
            "Galaxy",
            " VGP"
        ],
        "description": "Generate a genome assembly based on PacBio HiFi reads. Part of the VGP suite, it needs to be run after the VGP1 k-mer profiling workflow. The assembly contigs are built using HiFiasm, and the workflow generates assembly statistics, BUSCO reports, Merqury plots, and the contigs in fasta and GFA formats.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "612",
        "keep": true,
        "latest_version": 25,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/612?version=25",
        "name": "Assembly-Hifi-only-VGP3/main",
        "number_of_steps": 45,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "",
            "Add_a_column1",
            "tp_grep_tool",
            "Cut1",
            "gfastats",
            "join1",
            "param_value_from_file",
            "tp_cut_tool",
            "pick_value",
            "busco",
            "tp_awk_tool",
            "merqury",
            "multiqc",
            "tp_find_and_replace",
            "__EXTRACT_DATASET__",
            "cutadapt",
            "compose_text_param",
            "tp_replace_in_line",
            "compleasm",
            "Convert characters1",
            "bandage_image",
            "hifiasm"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-01",
        "versions": 25
    },
    {
        "create_time": "2025-10-01",
        "creators": [
            "Galaxy",
            " VGP"
        ],
        "description": "Generate phased assembly based on PacBio HiFi reads and parental Illumina data for phasing. Part of the VGP workflow suite, it needs to be run after the Trio k-mer Profiling workflow VGP2. This workflow uses HiFiasm for contigging, and generates assembly statistics, BUSCO reports, Merqury plots, and the genome assembly contigs in fasta and GFA format. ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "642",
        "keep": true,
        "latest_version": 25,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/642?version=25",
        "name": "Assembly-Hifi-Trio-phasing-VGP5/main",
        "number_of_steps": 47,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "",
            "Add_a_column1",
            "tp_grep_tool",
            "Cut1",
            "gfastats",
            "join1",
            "param_value_from_file",
            "tp_cut_tool",
            "pick_value",
            "busco",
            "tp_awk_tool",
            "merqury",
            "multiqc",
            "tp_find_and_replace",
            "__EXTRACT_DATASET__",
            "cutadapt",
            "compose_text_param",
            "tp_replace_in_line",
            "compleasm",
            "Convert characters1",
            "bandage_image",
            "hifiasm"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-01",
        "versions": 25
    },
    {
        "create_time": "2025-09-17",
        "creators": [
            "Sora Yonezawa"
        ],
        "description": "# plant2human workflow \ud83c\udf3e \u2194 \ud83d\udd7a\r\n\r\n![GitHub last commit (branch)](https://img.shields.io/github/last-commit/yonesora56/plant2human/main)\r\n![Status](https://img.shields.io/badge/status-development-yellow)\r\n[![cwltool](https://img.shields.io/badge/cwltool-3.1.20250110105449-success)](https://github.com/common-workflow-language/cwltool/releases/tag/3.1.20250110105449)\r\n[![License](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE)\r\n[![Version](https://img.shields.io/badge/version-2.0-brightgreen)](https://github.com/yonesora56/plant2human/releases/tag/v2.0)\r\n[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=python3.11&color=blue&logo=docker)](https://github.com/yonesora56/plant2human/tree/main/.devcontainer)\r\n[![X (@sorayone56)](https://img.shields.io/badge/X-sorayone56-black?style=flat&logo=x&logoColor=white)](https://x.com/sorayone56)\r\n\r\n## Introduction\r\n\r\nThis analysis workflow is centered on [foldseek](https://github.com/steineggerlab/foldseek), which enables fast structural similarity searches and supports the discovery of understudied genes by comparing plants, which are distantly related species, with humans, for which there is a wealth of information.\r\nBased on the list of genes you are interested in, you can easily create a scatter plot of **\u201cstructural similarity vs. sequence similarity\u201d** by retrieving structural data from the [AlphaFold protein structure database](https://alphafold.ebi.ac.uk/).\r\n\r\n&nbsp;\r\n\r\n## \ud83d\udce3 Report \r\n\r\n- \u2714 2025-02-02: fix `foldseek easy-search` command process\r\n- \u2714 2025-09-02: update `makeblstadb` command process\r\n- \u2714 2025-09-28: main workflow update! [plant2human_v2.cwl](https://github.com/yonesora56/plant2human/blob/main/Workflow/plant2human_v2.cwl)\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n## Implementation Background\r\n\r\nIn recent years, with the [AlphaFold protein structure database](https://alphafold.ebi.ac.uk/), it has become easier to obtain protein structure prediction data and perform structural similarity searches even for plant species such as rice. Against this background, searching for hits with **\u201clow sequence similarity and high structural similarity\u201d** for the gene groups being focused on has become possible. This approach may allow us to discover proteins that are conserved in distantly related species and to consider the characteristics of these proteins based on the wealth of knowledge we have about humans.\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n## Analysis Environment\r\n\r\n### Prerequisites\r\n\r\n- Docker\r\n- [`cwltool`](https://github.com/common-workflow-language/cwltool) >= 3.1.20250110105449\r\n\r\n&nbsp;\r\n\r\n### \u26a0 Prerequisites (Python Environment)\r\n\r\nI've already checked python 3.11 and package version below. \r\nPlease install the following packages beforehand!\r\n\r\n```python3\r\npolars==1.17.1\r\nmatplotlib==3.8.2\r\nseaborn==0.13.2\r\nunipressed==1.4.0\r\npapermill==2.6.0\r\n```\r\n\r\nUsing \u201cDevcontainers\u201d makes it easy to reproduce your execution environment!\r\n\r\n&nbsp;\r\n\r\n### **Using Dev Containers (Docker and VScode extension)**\r\n\r\nMost processes, such as Foldseek, use container ([BioContainers](https://quay.io/organization/biocontainers)), but some involve processing with jupyter notebook, which requires the preparation of some python libraries (e.g., polars.).\r\nIf you want to experiment with a simple workflow, you can create an analysis environment easily using [Dev Containers](./.devcontainer/devcontainer.json) system, a VScode extension.\r\nUsing this environment, the version of the python library is probably the one used during development, so errors are unlikely to occur (see [Dockerfile](./.devcontainer/Dockerfile) for the package version).\r\n\r\nPlease check the official website for Dev Container details.\r\n- [Developing inside a Container](https://code.visualstudio.com/docs/devcontainers/containers)\r\n- [Development Containers](https://containers.dev/)\r\n\r\n&nbsp;\r\n\r\n### The machine used for testing\r\n\r\n- Machine: \ud83c\udf4e MacBook Pro \ud83c\udf4e\r\n- Chip: Apple M3 Max\r\n- memory: 128 GB\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n## \ud83c\udf3e Example 1 ( *Oryza sativa 100 genes* vs *Homo sapiens*) \ud83c\udf3e\r\n\r\nHere, we will explain how to use the list of 100 rice genes as an example.\r\n\r\n&nbsp;\r\n\r\n### **1. Creation of a TSV file of gene and UniProt ID correspondences**\r\n\r\nFirst, you need the following [gene list tsv file](https://github.com/yonesora56/plant2human/blob/main/test/oryza_sativa_test_100genes_202509/oryza_sativa_random_100genes_list.tsv). (Please set the column name as \"From\")\r\n\r\n```tsv\r\nFrom\r\nOs12g0269700\r\nOs10g0410900\r\nOs05g0403000\r\nOs06g0127250\r\nOs02g0249600\r\nOs09g0349700\r\nOs03g0735150\r\nOs08g0547350\r\nOs06g0282400\r\nOs05g0576750\r\nOs07g0216600\r\nOs10g0164500\r\nOs07g0201300\r\nOs01g0567200\r\nOs05g0563050\r\nOs03g0660050\r\nOs11g0436450\r\n...\r\n```\r\n\r\nThe following [TSV file](https://github.com/yonesora56/plant2human/blob/main/test/oryza_sativa_test_100genes_202509/os_100_genes_idmapping_all.tsv) is required to execute the following workflow. \r\n\r\n```tsv\r\nFrom\tUniProt Accession\r\nOs01g0104800\tA0A0N7KC66\r\nOs01g0104800\tQ657Z6\r\nOs01g0104800\tQ658C6\r\nOs01g0152300\tQ9LGI2\r\nOs01g0322300\tA0A9K3Y6N1\r\nOs01g0322300\tQ657N1\r\nOs01g0567200\tA0A0N7KD66\r\nOs01g0567200\tQ657K0\r\nOs01g0571133\tA0A0P0V4A8\r\nOs01g0664500\tA0A8J8XFG3\r\nOs01g0664500\tQ5SN58\r\nOs01g0810800\tA0A8J8XDQ1\r\nOs01g0810800\tB7FAC9\r\nOs01g0875300\tA0A0P0VB72\r\nOs01g0924300\tA0A0P0VCB7\r\n...\r\n```\r\nTo do this, you need to follow the CWL workflow command below.\r\nThis [yaml file](https://github.com/yonesora56/plant2human/blob/main/job/oryza_sativa_100_genes/os_uniprot_idmapping.yml) is the parameter file for the workflow, for example.\r\n\r\n```bash\r\ncwltool --debug --outdir ./test/oryza_sativa_test_100genes_202509/ ./Tools/01_uniprot_idmapping.cwl ./job/oryza_sativa_100_genes/os_uniprot_idmapping.yml\r\n```\r\nIn this execution, [mmcif files](https://github.com/yonesora56/plant2human/tree/main/test/oryza_sativa_test_100genes_202509/os_100_genes_mmcif) are also retrieved.\r\nThe execution results are output with the [jupyter notebook](https://github.com/yonesora56/plant2human/blob/main/test/oryza_sativa_test_100genes_202509/oryza_sativa_100_genes_uniprot_idmapping.ipynb).\r\n\r\n**Note**: Network access required in this process!\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n### **2. Creating and Preparing Indexes**\r\n\r\nI'm sorry, but the [main workflow](https://github.com/yonesora56/plant2human/blob/main/Workflow/plant2human_v2.cwl) does not currently include the creation of an index process (both for foldseek index and BLAST index).\r\nPlease perform the following processes in advance.\r\n\r\n&nbsp;\r\n\r\n#### 2-1. Creating a Foldseek Index for structural alignment\r\n\r\nIn this workflow, the target of the structural similarity search is specified as the AlphaFold database to perform comparisons across a broader range of species.\r\nIndex creation using the `foldseek databases` command is through the following command.\r\n\r\nPlease select the database you want to use from `Alphafold/UniProt,` `Alphafold/UniProt50-minimal`, `Alphafold/UniProt50`, `Alphafold/Proteome,` `Alphafold/Swiss-Prot.`\r\n\r\n```python\r\n# Supported databases in this workflow\r\nAlphafold/UniProt\r\nAlphafold/UniProt50-minimal\r\nAlphafold/UniProt50\r\nAlphafold/Proteome\r\nAlphafold/Swiss-Prot\r\n```\r\n\r\n&nbsp;\r\n\r\nYou can check the details of this database using the following command.\r\n\r\n```bash\r\ndocker run --rm quay.io/biocontainers/foldseek:9.427df8a--pl5321h5021889_2 foldseek databases --help\r\n```\r\n\r\nFor example, if you want to specify AlphaFold/Swiss-Prot as the index, you can do so with the following CWL file;\r\n\r\n```bash\r\n# execute creation of foldseek index using \"foldseek databases\"\r\ncwltool --debug --outdir ./index/ ./Tools/02_foldseek_database.cwl \\\r\n--database Alphafold/Swiss-Prot \\\r\n--index_dir_name index_swissprot \\\r\n--index_name swissprot \\\r\n--threads 16\r\n```\r\n**\ud83d\udcdd Note:**: urrently, this index creation process is not incorporated into the main analysis workflow (plant2human workflow). \r\nThis is because the process requires network access, and maybe to be able to respond immediately if there are any changes to the tool requirements.\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n#### 2-2. Creating a BLAST Index for sequence alignment\r\n\r\nAn index FASTA file must be downloaded to obtain the amino acid sequence using the `blastdbcmd` command from the AlphaFold Protein Structure Database. This workflow uses the version of the protein sequence that was used for structure prediction.\r\n\r\nDownload URL: https://ftp.ebi.ac.uk/pub/databases/alphafold/sequences.fasta\r\n\r\n**Note:**: This FASTA file is extremely large (**92GB**), so it's probably best to delete it after creating the index.\r\n\r\n```bash\r\n# Preparation for BLAST index\r\nmkdir cwl_cache\r\ncd ./index\r\ncurl -O https://ftp.ebi.ac.uk/pub/databases/alphafold/sequences.fasta\r\nmv sequences.fasta afdb_all_sequneces.fasta\r\n```\r\n\r\n&nbsp;\r\n\r\n```bash\r\n# execute creation of BLAST index using \"makeblastdb\"\r\nmv ../\r\ncwltool --debug --cachedir ./cwl_cache/ --outdir ./index/ ./Tools/14_makeblastdb_v2.cwl\r\n```\r\n\r\n**\ud83d\udcdd Note \ud83d\udcdd :**: It is estimated to take 2 hours for creating index. We are currently investigating whether it can be executed by another method.\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n### 3. Execution of the [Main Workflow](https://github.com/yonesora56/plant2human/blob/main/Workflow/plant2human_v2.cwl)\r\n\r\nIn this process, we perform a structural similarity search using the `foldseek easy-search` command and then perform a pairwise alignment of the amino acid sequences of the hit pairs using the `needle` and `water` commands.\r\nFinally, based on this information, we create a scatter plot and output a [jupyter notebook](https://github.com/yonesora56/plant2human/blob/main/test/oryza_sativa_test_100genes_202509/os_100_genes_plant2human_report.ipynb) as a report.\r\nExamples of commands are as follows.\r\n\r\n```bash\r\ncwltool --debug --outdir ./test/oryza_sativa_test_100genes_202509/ ./Workflow/plant2human_v2.cwl ./job/oryza_sativa_100_genes/plant2human_job_example_os100.yml\r\n```\r\n\r\n&nbsp;\r\n\r\nFor example, you can visualize the results of structural similarity and global alignment, as shown below.\r\nIn this case, the x-axis represents the global alignment similarity match (%), and the y-axis represents the average lDDT score (an indicator of structural alignment).\r\n\r\nThe hit pairs in the upper-right plot indicate higher sequence similarity and structural similarity.\r\n\r\nhttps://github.com/yonesora56/plant2human/blob/main/image/rice_test_100genes_global_scatterplot.png\r\n\r\n&nbsp;\r\n\r\nIn this case, the x-axis represents the local alignment similarity match (%), and the y-axis represents the average lDDT score (an indicator of structural alignment).\r\n\r\nhttps://github.com/yonesora56/plant2human/blob/main/image/rice_test_100genes_local_scatterplot.png\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n### After Filtering\r\n\r\nThe report notebook for the plant2human workflow also outputs scatter plots after applying the filtering conditions set in this workflow.\r\n\r\n#### Filtering criteria\r\n\r\n1. structural alignment coverage >= 50%\r\n2. If there are hits with the same target for the same gene-derived UniProt ID, the one with the highest qcov is selected, and if the qcov is the same, the one with the highest lDDT is selected.\r\n\r\n    **Note that in this study, we leave the states with the same foldseek hit even if the rice genes are different.**\r\n\r\n3. Select hits that can be converted to Ensembl gene id and HGNC Gene nomenclature with TogoID API\r\n\r\n&nbsp;\r\n\r\nBy applying these filtering conditions, you can examine hit pairs that are easier to investigate!\r\n\r\n&nbsp;\r\n\r\n##### Global alignment\r\n\r\nhttps://github.com/yonesora56/plant2human/blob/main/image/rice_test_100genes_global_scatterplot_filter.png\r\n\r\n&nbsp;\r\n\r\n##### local alignment\r\n\r\nhttps://github.com/yonesora56/plant2human/blob/main/image/rice_test_100genes_local_scatterplot_filter.png",
        "doi": "10.48546/workflowhub.workflow.1206.8",
        "edam_operation": [
            "Global alignment",
            "Local alignment",
            "Pairwise sequence alignment",
            "Structure-based sequence alignment"
        ],
        "edam_topic": [
            "Function analysis",
            "Genetics",
            "Protein structure analysis",
            "Proteins"
        ],
        "filtered_on": "ITS in description",
        "id": "1206",
        "keep": true,
        "latest_version": 8,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1206?version=8",
        "name": "plant2human workflow",
        "number_of_steps": 7,
        "projects": [
            "bonohulab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "bioinformatics",
            "cwl",
            "docker",
            "functional analysis",
            "plant",
            "python",
            "human",
            "protein-structure"
        ],
        "tools": [
            "Foldseek",
            "EMBOSS",
            "BLAST",
            "BioContainers"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-09-28",
        "versions": 8
    },
    {
        "create_time": "2025-09-18",
        "creators": [
            "Samuel Chaffron",
            "Audrey Bihouee",
            "Benjamin Churcheward",
            "Maxime Millet",
            "Guillaume Fertin",
            "Hugo Lefeuvre"
        ],
        "description": "MAGNETO is an automated snakemake workflow dedicated to MAG (Metagenome-Assembled Genomes) reconstruction from metagenomic data. \r\n\r\nIt includes a fully-automated coassembly step informed by optimal clustering of metagenomic distances, and implements complementary genome binning strategies, for improving MAG recovery.\r\n\r\n# Key Features\r\n\r\n - **Quality Control (QC)**: Automatically assesses the quality and the contamination of input reads, ensuring that low-quality data are filtered out to improve downstream analyses.\r\n\r\n - **Assembly**: MAGNETO uses high-performance assembler to construct contigs from metagenomic reads.\r\n\r\n - **Gene Collection**: Extracts and compiles gene sequences from contigs, providing a comprehensive gene catalog directly after assembly.\r\n\r\n - **Binning**: Groups contigs into probable genomes using composition signatures and abundance profiles.\r\n\r\n - **Genomes collection**: Provides taxonomic and functional annotation of reconstructed MAGs.\r\n\r\n - **Metatranscriptomic mapping**: Mapping of transcriptomic reads on genes collection and/or MAGs obtained with previous analysis.\r\n\r\n# Documentation\r\n\r\n**Full description in the [wiki pages](https://gitlab.univ-nantes.fr/bird_pipeline_registry/magneto/-/wikis/home)**\r\n\r\n# Citing the pipeline \r\nChurcheward B, Millet M, Bihou\u00e9e A, Fertin G, Chaffron S.<br>\r\nMAGNETO: An Automated Workflow for Genome-Resolved Metagenomics.<br>\r\nmSystems. 2022 Jun 15:e0043222. doi: [10.1128/msystems.00432-22](https://doi.org/10.1128/msystems.00432-22)\r\n",
        "doi": null,
        "edam_operation": [
            "Gene functional annotation",
            "Genome assembly",
            "Read binning",
            "Read mapping",
            "Sequence assembly",
            "Sequencing quality control",
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metagenomics",
            "Metatranscriptomics"
        ],
        "filtered_on": "edam",
        "id": "1815",
        "keep": true,
        "latest_version": 3,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1815?version=3",
        "name": "MAGNETO (automated workflow dedicated to MAG reconstruction)",
        "number_of_steps": 0,
        "projects": [
            "BiRD"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2025-09-25",
        "versions": 3
    },
    {
        "create_time": "2025-09-25",
        "creators": [
            "\u00c9ric Charpentier",
            "Audrey Bihouee"
        ],
        "description": "This project is an analysis pipeline using **Snakemake** for RNAseq analysis in order to find differentially expressed genes.\r\nIt has been widely tested on human RNA sequencing from an Illumina HiSeq but should work on most systems and many other species, provided the necessary resource files can be downloaded.  \r\n\r\n### Description\r\n\r\n**This pipeline is set for paired-end data only from Illumina HiSeq output files.**\r\n\r\nThe main steps of the pipeline are:\r\n\r\n- optionnal cleaning data with [prinseq](http://prinseq.sourceforge.net/) and/or [cutadapt](http://cutadapt.readthedocs.io/en/stable/guide.html)\r\n- alignment of reads on reference genome with [STAR](https://github.com/alexdobin/STAR)\r\n- counting features with [HTSeq](http://htseq.readthedocs.io/)\r\n- detection of DGE genes with [DESeq2](https://bioconductor.org/packages/release/bioc/html/DESeq2.html)\r\n- functional annotations with [ClusterProfiler](http://bioconductor.org/packages/release/bioc/html/clusterProfiler.html) on GO and KEGG databases\r\n",
        "doi": "10.48546/workflowhub.workflow.1963.2",
        "edam_operation": [
            "Gene expression profiling",
            "Gene functional annotation",
            "Sequence alignment",
            "Sequencing quality control"
        ],
        "edam_topic": [
            "RNA-Seq"
        ],
        "filtered_on": "binn* in description",
        "id": "1963",
        "keep": true,
        "latest_version": 2,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1963?version=2",
        "name": "RNAseq-quantif-pipeline",
        "number_of_steps": 0,
        "projects": [
            "BiRD"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2025-09-26",
        "versions": 2
    },
    {
        "create_time": "2025-09-19",
        "creators": [
            "Caner Bagci"
        ],
        "description": "# Soil Metagenome Pipeline\r\n\r\nSoil Metagenome Pipeline is a modular, Nextflow DSL2 workflow for assembling, polishing, binning, annotating, and functionally characterizing complex soil metagenomes. It orchestrates state-of-the-art tools for long- and short-read metagenomics, generates high-quality MAGs, assigns taxonomy, and screens for biosynthetic gene clusters (BGCs).\r\n\r\n## What it does\r\n- Assembles long-read metagenomes (e.g., ONT) with Flye and optionally polishes with Medaka and/or NextPolish using short reads.\r\n- Maps short/long reads to assemblies to compute coverage/depth for downstream binning and QC.\r\n- Bins contigs with multiple strategies (SemiBin2, VAMB, MetaCoAG, ComeBIN) and can integrate results.\r\n- Evaluates MAG quality with CheckM2 and assigns taxonomy with GTDB-Tk.\r\n- Annotates bins and/or assemblies (Bakta, eggNOG) and detects BGCs (antiSMASH) with network-based clustering (BiG-SCAPE).\r\n- Produces organized outputs suitable for downstream comparative genomics.\r\n\r\n## Key features\r\n- Modular DSL2 design: swap/extend modules under `modules/` and `submodules/`.\r\n- Reproducible runtime via Conda/containers (profiles in `conf/`).\r\n- Sensible defaults with overridable parameters via `nextflow.config` or CLI.\r\n- Caching and resumability: supports `-resume` for efficient re-runs.\r\n\r\n## Modules at a glance (non-exhaustive)\r\n- Assembly and polishing: Flye, Medaka, NextPolish\r\n- Coverage mapping: minimap2/samtools, coverm, strobealign\r\n- Binning: SemiBin2, VAMB, MetaCoAG, ComeBIN, plus bin collection utilities\r\n- QC and taxonomy: CheckM2, GTDB-Tk\r\n- Annotation and function: Bakta (assemblies/bins), eggNOG\r\n- BGC discovery: antiSMASH (assemblies/bins), BiG-SCAPE networks\r\n- Taxonomic profiling: MMseqs2/MetaBuli helpers\r\n\r\n## Inputs\r\n- Reads: long reads (ONT/PacBio), optional short reads (Illumina).\r\n- Sample sheet: a tab-separated file like `data/samples.tsv` describing sample IDs and file paths.\r\n- Reference databases: external DBs required by some tools (e.g., GTDB-Tk, antiSMASH, BiG-SCAPE) are not bundled. Configure their locations via params or environment as appropriate.\r\n\r\n## Quick start\r\n- Dry run / graph preview:\r\n  nextflow run . -dsl2 -preview\r\n\r\n- Example execution (adjust paths and profile to your environment):\r\n  nextflow run . -profile conda -resume \\\r\n    --reads '/path/to/*_{R1,R2}.fastq.gz' \\\r\n    --longreads '/path/to/*.fastq.gz' \\\r\n    --samples 'data/samples.tsv' \\\r\n    --outdir 'results'\r\n\r\nSee `conf/` for example profiles (conda, docker, singularity, slurm). Tune resources via `nextflow.config` using `withName:` blocks for process-specific CPU, memory, and time.\r\n\r\n## Citation\r\nIf you use Soil Metagenome Pipeline in your research, please cite the corresponding preprint:\r\n\r\n- bioRxiv abstract: https://www.biorxiv.org/content/10.1101/2025.05.28.656579v1.abstract\r\n- DOI: https://doi.org/10.1101/2025.05.28.656579\r\n\r\nA machine-readable citation file (CITATION.cff) is included in the repository root. GitHub will display a \"Cite this repository\" button.\r\n\r\n## License\r\nThis project is licensed under the GNU General Public License v3.0 or later (GPL-3.0-or-later). See the LICENSE file for the full text.\r\n",
        "doi": "10.48546/workflowhub.workflow.1960.1",
        "edam_operation": [
            "Genome annotation",
            "Sequence assembly",
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Metagenomics",
            "Sequence assembly"
        ],
        "filtered_on": "edam",
        "id": "1960",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1960?version=1",
        "name": "Soil Metagenome Pipeline",
        "number_of_steps": 0,
        "projects": [
            "ZiemertLab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "assembly",
            "bioinformatics",
            "genomics",
            "metagenomics",
            "biosynthetic-gene-clusters"
        ],
        "tools": [
            "Flye",
            "Medaka",
            "NextPolish",
            "Minimap2",
            "SAMtools",
            "CoverM",
            "strobealign",
            "SemiBin",
            "VAMB",
            "MetaCoAG",
            "CheckM2",
            "CheckM",
            "GTDB",
            "Bakta",
            "eggNOG-mapper v2",
            "antiSMASH",
            "MMseqs2",
            "metabuli"
        ],
        "type": "Nextflow",
        "update_time": "2025-09-19",
        "versions": 1
    },
    {
        "create_time": "2025-09-19",
        "creators": [],
        "description": "The workflow main goal is to count reads aligned or pseudo-aligned to reference genome annotation using featureCount, HTSeq-count, RSEM, Kallisto or Salmon. Finally it runs Multi QC to gather tool metrics. The workflow was designed to be run in the SeqUIa (http://cfb.ceitec.muni.cz/sequia) application.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1957",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1957?version=1",
        "name": "Count Feature RNA",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics Core CEITEC"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "FeatureCounts",
            "HTSeq",
            "htseqcount",
            "RSEM",
            "kallisto",
            "Salmon"
        ],
        "type": "Snakemake",
        "update_time": "2025-09-19",
        "versions": 1
    },
    {
        "create_time": "2021-09-30",
        "creators": [
            "Bart Nijsse",
            "Jasper Koehorst"
        ],
        "description": "Workflow for quality assessment of paired reads and classification using NGTax 2.0 and functional annotation using picrust2.<br>\r\nIn addition files are exported to their respective subfolders for easier data management in a later stage.<br><br>\r\n\r\nSteps:\r\n  - Quality plots (FastQC)\r\n  - NG-TAX 2 High-throughput Amplicon Analysis\r\n  - PICRUSt 2 - Function prediction from marker gene sequences\r\n  - Export module for ngtax",
        "doi": "10.48546/workflowhub.workflow.154.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in tags",
        "id": "154",
        "keep": true,
        "latest_version": 2,
        "license": "AFL-3.0",
        "link": "https:/workflowhub.eu/workflows/154?version=2",
        "name": "Quality assessment, amplicon classification and functional prediction",
        "number_of_steps": 10,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "cwl",
            "classification"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2025-09-11",
        "versions": 2
    },
    {
        "create_time": "2025-09-10",
        "creators": [
            "Bart Nijsse",
            "Jasper Koehorst"
        ],
        "description": "Workflow for quality assessment and taxonomic classification of amplicon long read sequences.<br>\r\nIn addition files are exported to their respective subfolders for easier data management in a later stage.<br>\r\n<br>\r\nInputs are expected to be basecalled fastq files<br>\r\n<br>\r\n**Steps:**<br>\r\n    - NanoPlot read quality control, before and after filtering<br>\r\n    - fastplong read quality and length filtering<br>\r\n    - Emu abundance; species-level taxonomic abundance for full-length 16S read <br>\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Workflows"
        ],
        "filtered_on": "amplicon in tags",
        "id": "1952",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1952?version=1",
        "name": "Longread 16S classification workflow",
        "number_of_steps": 4,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "bioinformatics",
            "long-reads",
            "oxford-nanopore",
            "pacbio"
        ],
        "tools": [
            "NanoPlot",
            "fastplong"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-09-10",
        "versions": 1
    },
    {
        "create_time": "2025-09-09",
        "creators": [
            "Bart Nijsse",
            "Jasper Koehorst",
            "Changlin Ke"
        ],
        "description": "**Workflow (hybrid) metagenomic assembly and binning**<br>\r\n  - Workflow Illumina Quality: \r\n    - Sequali (control)\r\n    - hostile contamination filter\r\n    - fastp (quality trimming)\r\n  - Workflow Longread Quality:\t\r\n    - NanoPlot (control)\r\n    - fastplong (quality trimming)\r\n    - hostile contamination filter\r\n  - Kraken2 taxonomic classification of FASTQ reads\r\n  - SPAdes/Flye (Assembly)\r\n  - Medaka/PyPolCA (Assembly polishing)\r\n  - QUAST (Assembly quality report)\r\n\r\n  (optional)\r\n  - Workflow binnning\r\n    - Metabat2/MaxBin2/SemiBin\r\n    - Binette\r\n    - BUSCO\r\n    - GTDB-Tk\r\n\r\n  (optional)\r\n  - Workflow Genome-scale metabolic models https://workflowhub.eu/workflows/372\r\n    - CarveMe (GEM generation)\r\n    - MEMOTE (GEM test suite)\r\n    - SMETANA (Species METabolic interaction ANAlysis)\r\n\r\nOther UNLOCK workflows on WorkflowHub: https://workflowhub.eu/projects/16/workflows?view=default<br><br>\r\n\r\n**All tool CWL files and other workflows can be found here:**<br>\r\n  https://gitlab.com/m-unlock/cwl/ <br>\r\n\r\n**How to setup and use an UNLOCK workflow:**<br>\r\nhttps://docs.m-unlock.nl/docs/workflows/setup.html<br>\r\n",
        "doi": null,
        "edam_operation": [
            "Sequence assembly"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Metagenomic sequencing",
            "Metagenomics",
            "Sequence assembly"
        ],
        "filtered_on": "edam",
        "id": "367",
        "keep": true,
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/367?version=3",
        "name": "(Hybrid) Metagenomics workflow",
        "number_of_steps": 23,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "metagenomics",
            "binning",
            "illumina"
        ],
        "tools": [
            "SPAdes",
            "Flye",
            "Minimap2",
            "NanoPlot",
            "QUAST",
            "SemiBin",
            "MetaBAT 2",
            "MaxBin",
            "kraken2",
            "BUSCO",
            "CheckM",
            "InterProScan (EBI)",
            "eggNOG",
            "kofamscan",
            "KofamKOALA",
            "Sequali",
            "Hostile",
            "Binette"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-09-09",
        "versions": 3
    },
    {
        "create_time": "2025-08-13",
        "creators": [
            "Hana Ple\u0161ingerov\u00e1"
        ],
        "description": "KNIME workflow describing the analysis of mass spectrometry dataset related to the publication \"Multisite phosphorylation of intrinsically disordered region of DVL facilitates Wnt signaling\". Workflow was built using the KNIME software container environment, version 4.7.7, which can be created using \"docker pull cfprot/knime:4.7.7\" command in Docker.\r\n\r\nBriefly, it contains the contaminants removal, log2 intensities transformation, data filtering, normalization, imputation of missing values and statistical evaluation using the limma test.\r\n",
        "doi": "10.48546/workflowhub.workflow.1870.1",
        "edam_operation": [],
        "edam_topic": [
            "Proteomics"
        ],
        "filtered_on": "binn* in description",
        "id": "1870",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1870?version=1",
        "name": "Multisite phosphorylation of intrinsically disordered region of DVL facilitates Wnt signaling",
        "number_of_steps": 0,
        "projects": [
            "Proteomics CEITEC"
        ],
        "source": "WorkflowHub",
        "tags": [
            "knime",
            "proteomics"
        ],
        "tools": [
            "Konstanz Information Miner (KNIME)"
        ],
        "type": "KNIME",
        "update_time": "2025-09-08",
        "versions": 1
    },
    {
        "create_time": "2025-09-07",
        "creators": [
            "Leah Kemp",
            "Andre Reis",
            "Ira Deveson",
            "Kisaru Liyanage",
            "Matthew Downton",
            "Hardip Patel",
            "Kirat Alreja"
        ],
        "description": "# Pipeface\r\n\r\n## Overview\r\n\r\nPipefaceee.\r\n\r\nNextflow pipeline to process long read [ONT](https://nanoporetech.com/) and/or [pacbio](https://www.pacb.com/) HiFi data.\r\n\r\nPipeface's future hold's mitochondrial, STR, CNV and tandem repeat calling.\r\n\r\n<p align=\"center\">\r\n    <img src=\"./images/pipeface.png\">\r\n\r\n## Workflow\r\n\r\n### Singleton\r\n\r\n```mermaid\r\n%%{init: {'theme':'dark'}}%%\r\nflowchart LR\r\n\r\ninput_data(\"Input data: <br><br> ONT fastq.gz <br> and/or <br> ONT fastq <br> and/or <br> ONT uBAM <br> and/or <br> pacbio HiFi uBAM\")\r\nmerging{{\"Merge runs (if needed)\"}}\r\nalignment{{\"bam to fastq conversion (if needed), alignment, sorting\"}}\r\ndepth{{\"Calculate alignment depth\"}}\r\nsnp_indel_calling{{\"SNP/indel variant calling\"}}\r\nsplit_multiallele{{\"Split multiallelic variants into biallelic variants\"}}\r\nsnp_indel_phasing{{\"SNP/indel phasing\"}}\r\nsnp_indel_annotation{{\"SNP/indel annotation (hg38 only)\"}}\r\nhaplotagging{{\"Haplotagging bams\"}}\r\ncalculate_base_mod_freqs{{\"Calculate base modificiation frequencies (uBAM's containing base modifications only)\"}}\r\nsv_calling{{\"Structural variant calling\"}}\r\nsv_annotation{{\"Structural variant annotation (hg38 only)\"}}\r\n\r\ninput_data-.->merging-.->alignment-.->snp_indel_calling-.->split_multiallele-.->snp_indel_phasing-.->haplotagging-.->sv_calling\r\nalignment-.->depth\r\nalignment-.->haplotagging\r\nhaplotagging-.->calculate_base_mod_freqs\r\nsnp_indel_phasing-.->snp_indel_annotation\r\nsv_calling-.->sv_annotation\r\n```\r\n\r\n### Duo\r\n\r\n```mermaid\r\n%%{init: {'theme':'dark'}}%%\r\nflowchart LR\r\n\r\ninput_data(\"Input data: <br><br> ONT fastq.gz <br> and/or <br> ONT fastq <br> and/or <br> ONT uBAM <br> and/or <br> pacbio HiFi uBAM\")\r\nmerging{{\"Merge runs (if needed)\"}}\r\nalignment{{\"bam to fastq conversion (if needed), alignment, sorting\"}}\r\ndepth{{\"Calculate alignment depth\"}}\r\nsnp_indel_calling{{\"SNP/indel variant calling\"}}\r\nsplit_multiallele{{\"Split multiallelic variants into biallelic variants\"}}\r\nsnp_indel_phasing{{\"SNP/indel phasing\"}}\r\njoint_somalier{{\"Joint somalier relatedness check\"}}\r\ngvcf_merging{{\"gVCF merging\"}}\r\njoint_split_multiallele{{\"Split multiallelic variants into biallelic variants\"}}\r\njoint_snp_indel_phasing{{\"Joint SNP/indel phasing\"}}\r\njoint_snp_indel_annotation{{\"Joint SNP/indel annotation (hg38 only)\"}}\r\nhaplotagging{{\"Haplotagging bams\"}}\r\ncalculate_base_mod_freqs{{\"Calculate base modificiation frequencies (uBAM's containing base modifications only)\"}}\r\nsv_calling{{\"Structural variant calling\"}}\r\nsv_vcf_merging{{\"Structural variant VCF merging\"}}\r\njoint_sv_annotation{{\"Joint structural variant annotation (hg38 only)\"}}\r\n\r\ninput_data-.->merging-.->alignment-.->snp_indel_calling-.->split_multiallele-.->snp_indel_phasing-.->haplotagging-.->sv_calling\r\nalignment-.->depth\r\nalignment-.->haplotagging\r\nhaplotagging-.->calculate_base_mod_freqs\r\nhaplotagging-.->joint_somalier\r\nsnp_indel_calling-.->gvcf_merging-.->joint_split_multiallele-.->joint_snp_indel_phasing-.->joint_snp_indel_annotation\r\nsv_calling-.->sv_vcf_merging-.->joint_sv_annotation\r\n\r\n```\r\n\r\n### Trio\r\n\r\n```mermaid\r\n%%{init: {'theme':'dark'}}%%\r\nflowchart LR\r\n\r\ninput_data(\"Input data: <br><br> ONT fastq.gz <br> and/or <br> ONT fastq <br> and/or <br> ONT uBAM <br> and/or <br> pacbio HiFi uBAM\")\r\nmerging{{\"Merge runs (if needed)\"}}\r\nalignment{{\"bam to fastq conversion (if needed), alignment, sorting\"}}\r\ndepth{{\"Calculate alignment depth\"}}\r\nsnp_indel_calling{{\"SNP/indel variant calling\"}}\r\nsplit_multiallele{{\"Split multiallelic variants into biallelic variants\"}}\r\nsnp_indel_phasing{{\"SNP/indel phasing\"}}\r\njoint_snp_indel_calling{{\"Joint SNP/indel variant calling\"}}\r\njoint_somalier{{\"Joint somalier relatedness check\"}}\r\ngvcf_merging{{\"gVCF merging\"}}\r\njoint_split_multiallele{{\"Split multiallelic variants into biallelic variants\"}}\r\njoint_snp_indel_phasing{{\"Joint SNP/indel phasing\"}}\r\njoint_snp_indel_annotation{{\"Joint SNP/indel annotation (hg38 only)\"}}\r\nhaplotagging{{\"Haplotagging bams\"}}\r\ncalculate_base_mod_freqs{{\"Calculate base modificiation frequencies (uBAM's containing base modifications only)\"}}\r\nsv_calling{{\"Structural variant calling\"}}\r\nsv_vcf_merging{{\"Structural variant VCF merging\"}}\r\njoint_sv_annotation{{\"Joint structural variant annotation (hg38 only)\"}}\r\n\r\ninput_data-.->merging-.->alignment-.->snp_indel_calling-.->split_multiallele-.->snp_indel_phasing-.->haplotagging-.->sv_calling\r\nalignment-.->depth\r\nalignment-.->haplotagging\r\nhaplotagging-.->calculate_base_mod_freqs\r\nhaplotagging-.->joint_somalier\r\nsnp_indel_phasing-.->joint_snp_indel_calling-.->gvcf_merging-.->joint_split_multiallele-.->joint_snp_indel_phasing-.->joint_snp_indel_annotation\r\nsv_calling-.->sv_vcf_merging-.->joint_sv_annotation\r\n\r\n```\r\n\r\n## Main analyses\r\n\r\n- ONT and/or pacbio HiFi data\r\n- Singletons, duos or trios\r\n- WGS and/or targeted\r\n- hg38 or hs1 reference genome\r\n\r\n## Main tools\r\n\r\n- [Minimap2](https://github.com/lh3/minimap2)\r\n- [Clair3](https://github.com/HKU-BAL/Clair3) or [DeepVariant](https://github.com/google/deepvariant)/[DeepTrio](https://github.com/google/deepvariant/blob/r1.8/docs/deeptrio-details.md)\r\n- [WhatsHap](https://github.com/whatshap/whatshap)\r\n- [GLnexus](https://github.com/dnanexus-rnd/GLnexus)\r\n- [Sniffles2](https://github.com/fritzsedlazeck/Sniffles) and/or [cuteSV](https://github.com/tjiangHIT/cuteSV)\r\n- [Jasmine (customised)](https://github.com/bioinfomethods/Jasmine)\r\n- [somalier](https://github.com/brentp/somalier)\r\n- [Samtools](https://github.com/samtools/samtools)\r\n- [mosdepth](https://github.com/brentp/mosdepth)\r\n- [minimod](https://github.com/warp9seq/minimod?tab=readme-ov-file)\r\n- [ensembl-vep](https://github.com/Ensembl/ensembl-vep)\r\n\r\n## Main input files\r\n\r\n### Required\r\n\r\n- ONT/pacbio HiFi FASTQ (gzipped or uncompressed) or unaligned BAM\r\n- Indexed reference genome\r\n- Clair3 models (if running Clair3)\r\n\r\n### Optional\r\n\r\n- Regions of interest BED file\r\n- Tandem repeat BED file\r\n- PAR regions BED file (if running in haploid aware mode)\r\n\r\n## Main output files\r\n\r\n### Singleton\r\n\r\n- Aligned, sorted and haplotagged bam\r\n- Alignment depth per chromosome (and per region in the case of targeted sequencing)\r\n- Phased Clair3 or DeepVariant SNP/indel VCF file\r\n- Phased and annotated Clair3 or DeepVariant SNP/indel VCF file (hg38 only)\r\n- Clair3 or DeepVariant SNP/indel gVCF file\r\n- Bed and bigwig base modification frequencies for complete read set and separate haplotypes (uBAM's containing base modifications only)\r\n- Phased Sniffles2 and/or un-phased cuteSV SV VCF file\r\n- Phased and annotated Sniffles2 and/or un-phased and annotated cuteSV SV VCF file (hg38 only)\r\n\r\n### Duo\r\n\r\n- Aligned, sorted and haplotagged bam\r\n- Alignment depth per chromosome (and per region in the case of targeted sequencing)\r\n- DeepVariant SNP/indel gVCF file\r\n- Joint phased DeepVariant SNP/indel VCF file\r\n- Joint phased and annotated DeepVariant SNP/indel VCF file (hg38 only)\r\n- Bed and bigwig base modification frequencies for complete read set and separate haplotypes (uBAM's containing base modifications only)\r\n- Joint phased Sniffles2 and/or un-phased cuteSV SV VCF file\r\n- Joint phased and annotated Sniffles2 and/or un-phased and annotated cuteSV SV VCF file (hg38 only)\r\n- Joint relatedness and quality control somalier TSV and HTML files\r\n\r\n### Trio\r\n\r\n- Aligned, sorted and haplotagged bam\r\n- Alignment depth per chromosome (and per region in the case of targeted sequencing)\r\n- DeepVariant SNP/indel gVCF file\r\n- Joint phased DeepTrio SNP/indel VCF file\r\n- Joint phased and annotated DeepTrio SNP/indel VCF file (hg38 only)\r\n- Bed and bigwig base modification frequencies for complete read set and separate haplotypes (uBAM's containing base modifications only)\r\n- Joint phased Sniffles2 and/or un-phased cuteSV SV VCF file\r\n- Joint phased and annotated Sniffles2 and/or un-phased and annotated cuteSV SV VCF file (hg38 only)\r\n- Joint relatedness and quality control somalier TSV and HTML files\r\n\r\n> **_Note:_** Running DeepVariant/DeepTrio on ONT data assumes r10 data\r\n\r\n> **_Note:_** Running base modification analyses assumes the input data is in uBAM format and base modifications are present in these data\r\n\r\n## Haploid Aware Mode\r\n\r\n- Enables correct handling of the haploid nature of chrX and chrY for XY samples, along with PAR regions\r\n- Only supported for singletons at the moment\r\n\r\n## Assumptions\r\n\r\n- Running pipeline on Australia's [National Computational Infrastructure (NCI)](https://nci.org.au/)\r\n- Access to if89 project (to access software installs used by pipeface)\r\n- Access to xy86 project (to access variant databases used by pipeface, only required if running variant annotation)\r\n\r\n*[See the list of software and their versions used by this version of pipeface](./docs/software_versions.txt) as well as the [list of variant databases and their versions](./docs/database_versions.txt) if variant annotation is carried out (assuming the default [nextflow_pipeface.config](./config/nextflow_pipeface.config) file is used).*\r\n\r\n## Run it!\r\n\r\nSee a walkthrough for [running pipeface on NCI](./docs/run_on_nci.md).\r\n\r\n## Credit\r\n\r\nThis is a highly collaborative project, with many contributions from the [Deveson Lab](https://www.garvan.org.au/research/labs-groups/deveson-lab). Notably, Dr Andre Reis and Dr Ira Deveson are closely involved in the development of this pipeline. Optimisations involving DeepVariant and DeepTrio have been contributed by Dr Kisaru Liyanage and Dr Matthew Downton from the [National Computational Infrastructure](https://nci.org.au), with support from Australian BioCommons as part of the Workflow Commons project. Haploid-aware mode has been contributed by Dr Hardip Patel & Kirat Alreja from the [National Centre for Indigenous Genomics](https://ncig.anu.edu.au). The installation and hosting of software used in this pipeline has and continues to be supported by the [Australian BioCommons Tools and Workflows project (if89)](https://australianbiocommons.github.io/ables/if89/).\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Computational biology"
        ],
        "filtered_on": "annot* in description",
        "id": "1888",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1888?version=2",
        "name": "Pipeface",
        "number_of_steps": 0,
        "projects": [
            "Deveson Lab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "human genomics",
            "nextflow",
            "ont",
            "workflows",
            "pacbio"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-07",
        "versions": 2
    },
    {
        "create_time": "2025-09-06",
        "creators": [
            "Austyn Trull",
            "Lara Ianov"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-scnanoseq_logo_dark.png\">\n    <img alt=\"nf-core/scnanoseq\" src=\"docs/images/nf-core-scnanoseq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/scnanoseq/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/scnanoseq/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/scnanoseq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/scnanoseq/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/scnanoseq/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.13899279-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.13899279)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/scnanoseq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23scnanoseq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/scnanoseq)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/scnanoseq** is a bioinformatics best-practice analysis pipeline for 10X Genomics single-cell/nuclei RNA-seq data derived from Oxford Nanopore Q20+ chemistry ([R10.4 flow cells (>Q20)](https://nanoporetech.com/about-us/news/oxford-nanopore-announces-technology-updates-nanopore-community-meeting)). Due to the expectation of >Q20 quality, the input data for the pipeline does not depend on Illumina paired data. **Please note `scnanoseq` can also process Oxford data with older chemistry, but we encourage usage of the Q20+ chemistry when possible**.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/scnanoseq/results).\n\n## Pipeline summary\n\n![scnanoseq diagram](assets/scnanoseq_tube_map.png)\n\n1. Raw read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/), [`NanoPlot`](https://github.com/wdecoster/NanoPlot), [`NanoComp`](https://github.com/wdecoster/nanocomp) and [`ToulligQC`](https://github.com/GenomiqueENS/toulligQC))\n2. Unzip and split FASTQ ([`pigz`](https://github.com/madler/pigz))\n   1. Optional: Split FASTQ for faster processing ([`split`](https://linux.die.net/man/1/split))\n3. Trim and filter reads ([`Nanofilt`](https://github.com/wdecoster/nanofilt))\n4. Post trim QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/), [`NanoPlot`](https://github.com/wdecoster/NanoPlot), [`NanoComp`](https://github.com/wdecoster/nanocomp) and [`ToulligQC`](https://github.com/GenomiqueENS/toulligQC))\n5. Barcode detection using a custom whitelist or 10X whitelist. ([`BLAZE`](https://github.com/shimlab/BLAZE))\n6. Extract barcodes. Consists of the following steps:\n   1. Parse FASTQ files into R1 reads containing barcode and UMI and R2 reads containing sequencing without barcode and UMI (custom script `./bin/pre_extract_barcodes.py`)\n   2. Re-zip FASTQs ([`pigz`](https://github.com/madler/pigz))\n7. Barcode correction (custom script `./bin/correct_barcodes.py`)\n8. Post-extraction QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/), [`NanoPlot`](https://github.com/wdecoster/NanoPlot), [`NanoComp`](https://github.com/wdecoster/nanocomp) and [`ToulligQC`](https://github.com/GenomiqueENS/toulligQC))\n9. Alignment to the genome, transcriptome, or both ([`minimap2`](https://github.com/lh3/minimap2))\n10. Post-alignment filtering of mapped reads and gathering mapping QC ([`SAMtools`](http://www.htslib.org/doc/samtools.html))\n11. Post-alignment QC in unfiltered BAM files ([`NanoComp`](https://github.com/wdecoster/nanocomp), [`RSeQC`](https://rseqc.sourceforge.net/))\n12. Barcode (BC) tagging with read quality, BC quality, UMI quality (custom script `./bin/tag_barcodes.py`)\n13. Read deduplication ([`UMI-tools`](https://github.com/CGATOxford/UMI-tools) OR [`Picard MarkDuplicates`](https://broadinstitute.github.io/picard/))\n14. Gene and transcript level matrices generation with [`IsoQuant`](https://github.com/ablab/IsoQuant) and/or transcript level matrices with [`oarfish`](https://github.com/COMBINE-lab/oarfish)\n15. Preliminary matrix QC ([`Seurat`](https://github.com/satijalab/seurat))\n16. Compile QC for raw reads, trimmed reads, pre and post-extracted reads, mapping metrics and preliminary single-cell/nuclei QC ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n```csv title=\"samplesheet.csv\"\nsample,fastq,cell_count\nCONTROL_REP1,AEG588A1_S1.fastq.gz,5000\nCONTROL_REP1,AEG588A1_S2.fastq.gz,5000\nCONTROL_REP2,AEG588A2_S1.fastq.gz,5000\nCONTROL_REP3,AEG588A3_S1.fastq.gz,5000\nCONTROL_REP4,AEG588A4_S1.fastq.gz,5000\nCONTROL_REP4,AEG588A4_S2.fastq.gz,5000\nCONTROL_REP4,AEG588A4_S3.fastq.gz,5000\n```\n\nEach row represents a single-end fastq file. Rows with the same sample identifier are considered technical replicates and will be automatically merged. `cell_count` refers to the expected number of cells you expect.\n\n```bash\nnextflow run nf-core/scnanoseq \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/scnanoseq/usage) and the [parameter documentation](https://nf-co.re/scnanoseq/parameters).\n\n## Pipeline output\n\nThis pipeline produces feature-barcode matrices as the main output. These feature-barcode matrices are able to be ingested directly by most packages used for downstream analyses such as `Seurat`. Additionally, the pipeline produces a number of quality control metrics to ensure that the samples processed meet expected metrics for single-cell/nuclei data.\n\nThe pipeline provides two tools to produce the aforementioned feature-barcode matrices, `IsoQuant` and `oarfish`, and the user is given the ability to choose whether to run both or just one. `IsoQuant` will require a genome fasta to be used as input to the pipeline, and will produce both gene and transcript level matrices. `oarfish` will require a transcriptome fasta to be used as input to the pipeline and will produce only transcript level matrices.\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/scnanoseq/results) tab on the nf-core website pipeline page.\nFor more details about the full set of output files and reports, please refer to the\n[output documentation](https://nf-co.re/scnanoseq/output).\n\n## Troubleshooting\n\nIf you experience any issues, please make sure to reach out on the [#scnanoseq slack channel](https://nfcore.slack.com/archives/C03TUE2K6NS) or [open an issue on our GitHub repository](https://github.com/nf-core/scnanoseq/issues/new/choose). However, some resolutions for common issues will be noted below:\n\n- Due to the nature of the data this pipeline analyzes, some tools may experience increased runtimes. For some of the custom tools made for this pipeline (`preextract_fastq.py` and `correct_barcodes.py`), we have leveraged the splitting done via the `split_amount` parameter to decrease their overall runtimes. The `split_amount` parameter will split the input FASTQs into a number of FASTQ files, each containing a number of lines based on the value used for this parameter. As a result, it is important not to set this parameter to be too low as doing so would cause the creation of a large number of files the pipeline will be processed. While this value can be highly dependent on the data, a good starting point for an analysis would be to set this value to `500000`. If you find that `PREEXTRACT_FASTQ` and `CORRECT_BARCODES` are still taking long amounts of time to run, it would be worth reducing this parameter to `200000` or `100000`, but keeping the value on the order of hundred of thousands or tens of thousands should help with keeping the total number of processes minimal. An example of setting this parameter to be equal to 500000 is shown below:\n\n```yml title=\"params.yml\"\nsplit_amount: 500000\n```\n\n- We have seen a recurrent node failure on slurm clusters that does seem to be related to submission of Nextflow jobs. This issue is not related to this pipeline per se, but rather to Nextflow itself. We are currently working on a resolution. But we have two methods that appear to help overcome should this issue arise:\n  1. Provide a custom config that increases the memory request for the job that failed. This may take a couple attempts to find the correct requests, but we have noted that there does appear to be a memory issue occasionally with these errors.\n  2. Request an interactive session with a decent amount of time and memory and CPUs in order to run the pipeline on the single node. Note that this will take time as there will be minimal parallelization, but this does seem to resolve the issue.\n- We note that umitools dedup can take a large amount of time in order to perform deduplication. One approach we have implemented to assist with speed is to split input files based on chromosome. However for the transcriptome aligned bams, there is some additional work required that involves grouping transcripts into appropriate chromosomes. In order to accomplish this, the pipeline needs to parse the transcript id from the transcriptome FASTA file. The transcript id is often nested in the sequence identifier with additional data and the data is delimited. We have included the delimiters used by reference files obtained from GENCODE, NCBI, and Ensembl. However in case you wish to explicitly control this or if the reference file source uses a different delimiter, you are able to manually set it via the `--fasta_delimiter` parameter.\n- We acknowledge that analyzing PromethION data is a common use case for this pipeline. Currently, the pipeline has been developed with defaults to analyze GridION and average sized PromethION data. For cases, where jobs have fail due for larger PromethION datasets, the defaults can be overwritten by a custom configuation file (provided by the `-c` Nextflow option) where resources can be increased (substantially in some cases). Below are some of the overrides we have used, and while these amounts may not work on every dataset, these will hopefully at least note which processes will need to have their resources increased:\n\n```groovy title=\"custom.config\"\n\nprocess\n{\n    withName: '.*:.*FASTQC.*'\n    {\n        cpus = 20\n    }\n}\n\nprocess\n{\n    withName: '.*:BLAZE'\n    {\n        cpus = 30\n    }\n}\n\nprocess\n{\n    withName: '.*:TAG_BARCODES'\n    {\n        memory = '60.GB'\n    }\n}\n\nprocess\n{\n    withName: '.*:SAMTOOLS_SORT'\n    {\n        cpus = 20\n    }\n}\n\nprocess\n{\n    withName: '.*:MINIMAP2_ALIGN'\n    {\n        cpus = 20\n    }\n}\n\nprocess\n{\n    withName: '.*:ISOQUANT'\n    {\n        cpus = 30\n        memory = '85.GB'\n    }\n}\n```\n\nWe further note that while we encourage the use of `split_amount` as discussed above for larger datasets, the pipeline can be executed without enabling this parameter. When doing this, please consider increasing the time limit to `CORRECT_BARCODES` as it can take hours instead of minutes when `split_amount` is disabled:\n\n```groovy title=\"custom.config\"\n//NOTE: with split_amount disabled, consider increasing the time limit to CORRECT_BARCODES\nprocess\n{\n    withName: '.*:CORRECT_BARCODES'\n    {\n        time = '15.h'\n    }\n}\n```\n\n## Credits\n\nnf-core/scnanoseq was originally written by [Austyn Trull](https://github.com/atrull314), and [Dr. Lara Ianov](https://github.com/lianov).\n\nWe would also like to thank the following people and groups for their support, including financial support:\n\n- Dr. Elizabeth Worthey\n- University of Alabama at Birmingham Biological Data Science Core (U-BDS), RRID:SCR_021766, <https://github.com/U-BDS>\n- Civitan International Research Center\n- Support from: 3P30CA013148-48S8\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#scnanoseq` channel](https://nfcore.slack.com/channels/scnanoseq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/scnanoseq for your analysis, please cite the article as follows:\n\n> **scnanoseq: an nf-core pipeline for Oxford Nanopore single-cell RNA-sequencing**\n>\n> Austyn Trull, nf-core community, Elizabeth A. Worthey, Lara Ianov\n>\n> bioRxiv 2025.04.08.647887; doi: https://doi.org/10.1101/2025.04.08.647887\n\nThe specific pipleine version can be cited using the following doi: [10.5281/zenodo.13899279](https://doi.org/10.5281/zenodo.13899279)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1179",
        "keep": true,
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1179?version=4",
        "name": "nf-core/scnanoseq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "10xgenomics",
            "long-read-sequencing",
            "nanopore",
            "scrna-seq",
            "single-cell"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-06",
        "versions": 4
    },
    {
        "create_time": "2025-07-15",
        "creators": [
            "Lorenz Gruber"
        ],
        "description": "# Description\r\nThe Settlement Delineation and Analysis (SDA) workflows generates a settlement network from geospatial settlement data. It can process _geotiff_ and _shapefile_ inputs and was originally designed to operate on the [World Settlement Footprint](https://geoservice.dlr.de/web/maps/eoc:wsf2019) dataset. Through multiple workflow stages, a settlement network is constructed, contracted (i.e. clustered) and ultimately analysed with centrality measures. The output _shapefile_ stores the aggregated settlement (multi-)polygons and their centrality values in fields. Optionally, the edges of the settlement graph can be visualized in a separate file as well.\r\n# Implementation\r\nThe workflow comprises four main stages, each implemented as a standalone executable, with the interface wrapped in a CWL definition file.\r\n* **Filter**: At first, the raster input is polygonized, and settlements not fulfilling the user-specified filter condition are dropped\r\n* **Neighbours**:  The adjacencies of the settlements are computed according to a user-configurable criterion. The graph is stored in a central graph database.\r\n* **Contraction**: The settlement graph is  contracted with edges fulfilling the contraction criterion being removed from the graph and the incident vertices merged. The connectivity to former neighbors of aggregated vertices is restored afterwards. \r\n* **Analysis**: The final stage computes selected centrality measures on the contracted graph and writes the centrality values in the output shapefile. Optionally, this stage can visualize the graph's edges in a separate output file.\r\n\r\nAdditionally, three orchestrational task are needed:\r\n* **Split**: Splits the input _geotiff_ or _shapefile_ to enable concurrent processing.\r\n* **Components**: Identifies connected components in the settlement graph, which are the unit of parallelization for the **Contraction** and **Analysis** stage.\r\n* **Merge**: Merges the output files of the **Analysis** stage into a single file. \r\n\r\nThe **Job Generator** creates CWL jobs for the aformentionend tasks and models their dependencies in a directed acyclic graph (DAG). The **Scheduler** queues the jobs according to the DAG and submits them to an CWL executor, which runs the job in a separate child process. Both the job generator and scheduler are utilized by the workflow's _main_ file (SettlementDelineation.h/cpp). \r\n\r\n# Deployment\r\nTo run the **Settlement Delineation and Analysis** workflow, the easiest way is to use ```SettlementDelineationAnalysis.py```, which wraps the command line interface of the main binary and executes it in a container using _docker_. Please use the latest version from the [Fishnet Repository](https://gitlab2.informatik.uni-wuerzburg.de/descartes/sos/fishnet)\r\n### Software Requirements\r\n- _Docker_\r\n- _Python 3.x_\r\n### Running the Workflow\r\n```\r\npython3 SettlementDelineationAnalysis.py -i INPUT_FILE -c CONFIG_FILE.json -o OUTPUT_FILE.shp\r\n``` \r\n- **Input**: Input GIS file (*GeoTIFF* | *Shapefile*) on settlement location (e.g. [WSF](https://geoservice.dlr.de/web/maps/eoc:wsf2019))\r\n- **Config**: JSON file containing the config for the workflow run (e.g. [Example Config](https://gitlab2.informatik.uni-wuerzburg.de/descartes/sos/fishnet/-/blob/main/app/sda-workflow/sda-docker.json?ref_type=heads) )\r\n- **Output**: Path of output shapefile  \r\n\r\n\r\n_Please refer to Version 1.1 of the Workflow:_ [https://doi.org/10.48546/WORKFLOWHUB.WORKFLOW.1308.2](https://doi.org/10.48546/WORKFLOWHUB.WORKFLOW.1308.2)\r\n",
        "doi": "10.48546/workflowhub.workflow.1308.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1308",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1308?version=2",
        "name": "Settlement Delineation and Analysis",
        "number_of_steps": 0,
        "projects": [
            "SOS"
        ],
        "source": "WorkflowHub",
        "tags": [
            "c++",
            "cwl",
            "docker",
            "earth observation",
            "graph"
        ],
        "tools": [],
        "type": "Docker",
        "update_time": "2025-09-03",
        "versions": 2
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "James A. Fellows Yates",
            "Sofia Stamouli",
            "Moritz E. Beber",
            "Lauri Mesilaakso",
            "Thomas A. Christensen II",
            "Jianhong Ou",
            "Mahwash Jamy",
            "Maxime Borry",
            "Rafal Stepien",
            "Tanja Normark"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-taxprofiler_logo_custom_dark.png\">\n    <img alt=\"nf-core/taxprofiler\" src=\"docs/images/nf-core-taxprofiler_logo_custom_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/taxprofiler/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/taxprofiler/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/taxprofiler/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/taxprofiler/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/taxprofiler/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7728364-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7728364)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A525.04.2-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/taxprofiler)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23taxprofiler-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/taxprofiler)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n[![Cite Preprint](https://img.shields.io/badge/Cite%20Us!-Cite%20Preprint-orange)](https://doi.org/10.1101/2023.10.20.563221)\n\n## Introduction\n\n**nf-core/taxprofiler** is a bioinformatics best-practice analysis pipeline for taxonomic classification and profiling of shotgun short- and long-read metagenomic data. It allows for in-parallel taxonomic identification of reads or taxonomic abundance estimation with multiple classification and profiling tools against multiple databases, and produces standardised output tables for facilitating results comparison between different tools and databases.\n\n## Pipeline summary\n\n![](docs/images/taxprofiler_tube.png)\n\n1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) or [`falco`](https://github.com/smithlabcode/falco) as an alternative option)\n2. Performs optional read pre-processing\n   - Adapter clipping and merging (short-read: [fastp](https://github.com/OpenGene/fastp), [AdapterRemoval2](https://github.com/MikkelSchubert/adapterremoval); long-read: [porechop](https://github.com/rrwick/Porechop), [Porechop_ABI](https://github.com/bonsai-team/Porechop_ABI))\n   - Low complexity and quality filtering (short-read: [bbduk](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/), [PRINSEQ++](https://github.com/Adrian-Cantu/PRINSEQ-plus-plus); long-read: [Filtlong](https://github.com/rrwick/Filtlong)), [Nanoq](https://github.com/esteinig/nanoq)\n   - Host-read removal (short-read: [BowTie2](http://bowtie-bio.sourceforge.net/bowtie2/); long-read: [Minimap2](https://github.com/lh3/minimap2))\n   - Run merging\n3. Supports statistics metagenome coverage estimation ([Nonpareil](https://nonpareil.readthedocs.io/en/latest/)) and for host-read removal ([Samtools](http://www.htslib.org/))\n4. Performs taxonomic classification and/or profiling using one or more of:\n   - [Kraken2](https://ccb.jhu.edu/software/kraken2/)\n   - [MetaPhlAn](https://huttenhower.sph.harvard.edu/metaphlan/)\n   - [MALT](https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/algorithms-in-bioinformatics/software/malt/)\n   - [DIAMOND](https://github.com/bbuchfink/diamond)\n   - [Centrifuge](https://ccb.jhu.edu/software/centrifuge/)\n   - [Kaiju](https://kaiju.binf.ku.dk/)\n   - [mOTUs](https://motu-tool.org/)\n   - [KrakenUniq](https://github.com/fbreitwieser/krakenuniq)\n   - [KMCP](https://github.com/shenwei356/kmcp)\n   - [ganon](https://pirovc.github.io/ganon/)\n5. Perform optional post-processing with:\n   - [bracken](https://ccb.jhu.edu/software/bracken/)\n6. Standardises output tables ([`Taxpasta`](https://taxpasta.readthedocs.io))\n7. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n8. Plotting Kraken2, Centrifuge, Kaiju and MALT results ([`Krona`](https://hpc.nih.gov/apps/kronatools.html))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n```csv title=\"samplesheet.csv\"\nsample,run_accession,instrument_platform,fastq_1,fastq_2,fasta\n2612,run1,ILLUMINA,2612_run1_R1.fq.gz,,\n2612,run2,ILLUMINA,2612_run2_R1.fq.gz,,\n2612,run3,ILLUMINA,2612_run3_R1.fq.gz,2612_run3_R2.fq.gz,\n```\n\nEach row represents a fastq file (single-end), a pair of fastq files (paired end), or a fasta (with long reads).\n\nAdditionally, you will need a database sheet that looks as follows:\n\n```csv title=\"databases.csv\"\ntool,db_name,db_params,db_path\nkraken2,db2,--quick,/<path>/<to>/kraken2/testdb-kraken2.tar.gz\nmetaphlan,db1,,/<path>/<to>/metaphlan/metaphlan_database/\n```\n\nThat includes directories or `.tar.gz` archives containing databases for the tools you wish to run the pipeline against.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/taxprofiler \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --databases databases.csv \\\n   --outdir <OUTDIR>  \\\n   --run_kraken2 --run_metaphlan\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/taxprofiler/usage) and the [parameter documentation](https://nf-co.re/taxprofiler/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/taxprofiler/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/taxprofiler/output).\n\n## Credits\n\nnf-core/taxprofiler was originally written by James A. Fellows Yates, Sofia Stamouli, Moritz E. Beber, Lili Andersson-Li, and the nf-core/taxprofiler team.\n\n### Team\n\n- [James A. Fellows Yates](https://github.com/jfy133)\n- [Sofia Stamouli](https://github.com/sofstam)\n- [Moritz E. Beber](https://github.com/Midnighter)\n- [Lili Andersson-Li](https://github.com/LilyAnderssonLee)\n\nWe thank the following people for their contributions to the development of this pipeline:\n\n- [Lauri Mesilaakso](https://github.com/ljmesi)\n- [Tanja Normark](https://github.com/talnor)\n- [Maxime Borry](https://github.com/maxibor)\n- [Thomas A. Christensen II](https://github.com/MillironX)\n- [Jianhong Ou](https://github.com/jianhong)\n- [Rafal Stepien](https://github.com/rafalstepien)\n- [Mahwash Jamy](https://github.com/mjamy)\n- [Alex Caswell](https://github.com/AlexHoratio)\n- [Aidan Epstein](https://github.com/epstein6)\n\n### Acknowledgments\n\nWe also are grateful for the feedback and comments from:\n\n- The general [nf-core/community](https://nf-co.re/community)\n\nAnd specifically to\n\n- [Alex H\u00fcbner](https://github.com/alexhbnr)\n\n\u2764\ufe0f also goes to [Zandra Fagern\u00e4s](https://github.com/ZandraFagernas) for the logo.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#taxprofiler` channel](https://nfcore.slack.com/channels/taxprofiler) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/taxprofiler for your analysis, please cite it using the following doi: [10.1101/2023.10.20.563221](https://doi.org/10.1101/2023.10.20.563221).\n\n> Stamouli, S., Beber, M. E., Normark, T., Christensen II, T. A., Andersson-Li, L., Borry, M., Jamy, M., nf-core community, & Fellows Yates, J. A. (2023). nf-core/taxprofiler: Highly parallelised and flexible pipeline for metagenomic taxonomic classification and profiling. In bioRxiv (p. 2023.10.20.563221). https://doi.org/10.1101/2023.10.20.563221\n\nFor the latest version of the code, cite the Zenodo doi: [10.5281/zenodo.7728364](https://doi.org/10.5281/zenodo.7728364)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1025",
        "keep": true,
        "latest_version": 16,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1025?version=16",
        "name": "nf-core/taxprofiler",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "classification",
            "metagenomics",
            "profiling",
            "illumina",
            "long-reads",
            "microbiome",
            "nanopore",
            "pathogen",
            "shotgun",
            "taxonomic-classification",
            "taxonomic-profiling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 16
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "@praveenraj2018 None",
            "@praveenraj2018 None"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-rnavar_logo_dark.png\">\n    <img alt=\"nf-core/rnavar\" src=\"docs/images/nf-core-rnavar_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/rnavar/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/rnavar/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/rnavar/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/rnavar/actions/workflows/linting.yml)\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/rnavar/results)\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.6669636-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.6669636)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/rnavar)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23rnavar-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/rnavar)\n[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)\n[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)\n[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/rnavar** is a bioinformatics pipeline for RNA variant calling analysis following GATK4 best practices.\n\n## Pipeline summary\n\n1. Merge re-sequenced FastQ files ([`cat`](http://www.linfo.org/cat.html))\n2. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n3. (Optionally) Extract UMIs from FASTQ reads ([`UMI-tools`](https://github.com/CGATOxford/UMI-tools))\n4. (Optionally) HLATyping from FASTQ reads ([`Seq2HLA`](https://github.com/TRON-Bioinformatics/seq2HLA))\n5. Align reads to reference genome ([`STAR`](https://github.com/alexdobin/STAR))\n6. Sort and index alignments ([`SAMtools`](https://sourceforge.net/projects/samtools/files/samtools/))\n7. Duplicate read marking ([`Picard MarkDuplicates`](https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard))\n8. Scatter one interval-list into many interval-files ([`GATK4 IntervalListTools`](https://gatk.broadinstitute.org/hc/en-us/articles/4409917392155-IntervalListTools-Picard-))\n9. Splits reads that contain Ns in their cigar string ([`GATK4 SplitNCigarReads`](https://gatk.broadinstitute.org/hc/en-us/articles/4409917482651-SplitNCigarReads))\n10. Estimate and correct systematic bias using base quality score recalibration ([`GATK4 BaseRecalibrator`](https://gatk.broadinstitute.org/hc/en-us/articles/4409897206043-BaseRecalibrator), [`GATK4 ApplyBQSR`](https://gatk.broadinstitute.org/hc/en-us/articles/4409897168667-ApplyBQSR))\n11. Convert a BED file to a Picard Interval List ([`GATK4 BedToIntervalList`](https://gatk.broadinstitute.org/hc/en-us/articles/4409924780827-BedToIntervalList-Picard-))\n12. Call SNPs and indels ([`GATK4 HaplotypeCaller`](https://gatk.broadinstitute.org/hc/en-us/articles/4409897180827-HaplotypeCaller))\n13. Merge multiple VCF files into one VCF ([`GATK4 MergeVCFs`](https://gatk.broadinstitute.org/hc/en-us/articles/4409924817691-MergeVcfs-Picard-))\n14. Index the VCF ([`Tabix`](http://www.htslib.org/doc/tabix.html))\n15. Filter variant calls based on certain criteria ([`GATK4 VariantFiltration`](https://gatk.broadinstitute.org/hc/en-us/articles/4409897204763-VariantFiltration))\n16. Annotate variants ([`BCFtools Annotate`](https://samtools.github.io/bcftools/bcftools.html), [`snpEff`](https://pcingola.github.io/SnpEff/se_introduction/), [Ensembl VEP](https://www.ensembl.org/info/docs/tools/vep/index.html))\n17. Present QC for raw read, alignment, gene biotype, sample similarity, and strand-specificity checks ([`MultiQC`](http://multiqc.info/), [`R`](https://www.r-project.org/))\n\n### Summary of tools and version used in the pipeline\n\n| Tool        | Version |\n| ----------- | ------- |\n| BCFtools    | 1.21    |\n| BEDtools    | 2.31.1  |\n| Ensembl VEP | 114.2   |\n| FastQC      | 0.12.1  |\n| GATK        | 4.6.1.0 |\n| mosdepth    | 0.3.10  |\n| MultiQC     | 1.29    |\n| Picard      | 3.3.0   |\n| Samtools    | 1.21    |\n| Seq2HLA     | 2.3     |\n| SnpEff      | 5.1     |\n| STAR        | 2.7.11b |\n| Tabix       | 1.20    |\n| UMI-tools   | 1.1.5   |\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow.Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\nNow, you can run the pipeline using:\n\n```console\nnextflow run nf-core/rnavar -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> --input samplesheet.csv  --outdir <OUTDIR> --genome GRCh38\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/rnavar/usage) and the [parameter documentation](https://nf-co.re/rnavar/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/rnavar/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/rnavar/output).\n\n## Credits\n\nrnavar was originally written by Praveen Raj and Maxime U Garcia at [The Swedish Childhood Tumor Biobank (Barntum\u00f6rbanken), Karolinska Institutet](https://ki.se/forskning/barntumorbanken).\nNicolas Vannieuwkerke at [CMGG](https://www.cmgg.be/en/) later joined and helped with further development (v 1.1.0 and forward).\n\nMaintenance is now lead by Maxime U Garcia (now at [Seqera](https://seqera.io))\n\nMain developers:\n\n- [Maxime U Garcia](https://github.com/maxulysse)\n- [Nicolas Vannieuwkerke](https://github.com/nvnieuwk)\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Harshil Patel](https://github.com/drpatelh)\n- [Nicol\u00e1s Schcolnicov](https://github.com/nschcolnicov)\n- [\u00d6mer An](https://github.com/bounlu)\n- [Phil Ewels](https://github.com/ewels)\n- [Praveen Raj](https://github.com/praveenraj2018)\n- [Sarah Maman](https://github.com/SarahMaman)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#rnavar` channel](https://nfcore.slack.com/channels/rnavar) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/rnavar for your analysis, please cite it using the following doi: [10.5281/zenodo.6669636](https://doi.org/10.5281/zenodo.6669636)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1019",
        "keep": true,
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1019?version=4",
        "name": "nf-core/rnavar",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gatk4",
            "rnaseq",
            "rna",
            "variant-calling",
            "worflow"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 4
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "Harshil Patel",
            "Phil Ewels",
            "Rickard Hammar\u00e9n"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-rnaseq_logo_dark.png\">\n    <img alt=\"nf-core/rnaseq\" src=\"docs/images/nf-core-rnaseq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/rnaseq/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/rnaseq/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/rnaseq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/rnaseq/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/rnaseq/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.1400710-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.1400710)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/rnaseq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23rnaseq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/rnaseq)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/rnaseq** is a bioinformatics pipeline that can be used to analyse RNA sequencing data obtained from organisms with a reference genome and annotation. It takes a samplesheet and FASTQ files as input, performs quality control (QC), trimming and (pseudo-)alignment, and produces a gene expression matrix and extensive QC report.\n\n![nf-core/rnaseq metro map](docs/images/nf-core-rnaseq_metro_map_grey_animated.svg)\n\n> In case the image above is not loading, please have a look at the [static version](docs/images/nf-core-rnaseq_metro_map_grey.png).\n\n1. Merge re-sequenced FastQ files ([`cat`](http://www.linfo.org/cat.html))\n2. Auto-infer strandedness by subsampling and pseudoalignment ([`fq`](https://github.com/stjude-rust-labs/fq), [`Salmon`](https://combine-lab.github.io/salmon/))\n3. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n4. UMI extraction ([`UMI-tools`](https://github.com/CGATOxford/UMI-tools))\n5. Adapter and quality trimming ([`Trim Galore!`](https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/))\n6. Removal of genome contaminants ([`BBSplit`](http://seqanswers.com/forums/showthread.php?t=41288))\n7. Removal of ribosomal RNA ([`SortMeRNA`](https://github.com/biocore/sortmerna))\n8. Choice of multiple alignment and quantification routes (_For `STAR` the sentieon implementation can be chosen_):\n   1. [`STAR`](https://github.com/alexdobin/STAR) -> [`Salmon`](https://combine-lab.github.io/salmon/)\n   2. [`STAR`](https://github.com/alexdobin/STAR) -> [`RSEM`](https://github.com/deweylab/RSEM)\n   3. [`HiSAT2`](https://ccb.jhu.edu/software/hisat2/index.shtml) -> **NO QUANTIFICATION**\n9. Sort and index alignments ([`SAMtools`](https://sourceforge.net/projects/samtools/files/samtools/))\n10. UMI-based deduplication ([`UMI-tools`](https://github.com/CGATOxford/UMI-tools))\n11. Duplicate read marking ([`picard MarkDuplicates`](https://broadinstitute.github.io/picard/))\n12. Transcript assembly and quantification ([`StringTie`](https://ccb.jhu.edu/software/stringtie/))\n13. Create bigWig coverage files ([`BEDTools`](https://github.com/arq5x/bedtools2/), [`bedGraphToBigWig`](http://hgdownload.soe.ucsc.edu/admin/exe/))\n14. Extensive quality control:\n    1. [`RSeQC`](http://rseqc.sourceforge.net/)\n    2. [`Qualimap`](http://qualimap.bioinfo.cipf.es/)\n    3. [`dupRadar`](https://bioconductor.org/packages/release/bioc/html/dupRadar.html)\n    4. [`Preseq`](http://smithlabresearch.org/software/preseq/)\n    5. [`DESeq2`](https://bioconductor.org/packages/release/bioc/html/DESeq2.html)\n    6. [`Kraken2`](https://ccb.jhu.edu/software/kraken2/) -> [`Bracken`](https://ccb.jhu.edu/software/bracken/) on unaligned sequences; _optional_\n15. Pseudoalignment and quantification ([`Salmon`](https://combine-lab.github.io/salmon/) or ['Kallisto'](https://pachterlab.github.io/kallisto/); _optional_)\n16. Present QC for raw read, alignment, gene biotype, sample similarity, and strand-specificity checks ([`MultiQC`](http://multiqc.info/), [`R`](https://www.r-project.org/))\n\n> **Note**\n> The SRA download functionality has been removed from the pipeline (`>=3.2`) and ported to an independent workflow called [nf-core/fetchngs](https://nf-co.re/fetchngs). You can provide `--nf_core_pipeline rnaseq` when running nf-core/fetchngs to download and auto-create a samplesheet containing publicly available samples that can be accepted directly as input by this pipeline.\n\n> **Warning**\n> Quantification isn't performed if using `--aligner hisat2` due to the lack of an appropriate option to calculate accurate expression estimates from HISAT2 derived genomic alignments. However, you can use this route if you have a preference for the alignment, QC and other types of downstream analysis compatible with the output of HISAT2.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n**samplesheet.csv**:\n\n```csv\nsample,fastq_1,fastq_2,strandedness\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L003_R1_001.fastq.gz,AEG588A1_S1_L003_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L004_R1_001.fastq.gz,AEG588A1_S1_L004_R2_001.fastq.gz,auto\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end). Rows with the same sample identifier are considered technical replicates and merged automatically. The strandedness refers to the library preparation and will be automatically inferred if set to `auto`.\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/rnaseq \\\n    --input <SAMPLESHEET> \\\n    --outdir <OUTDIR> \\\n    --gtf <GTF> \\\n    --fasta <GENOME FASTA> \\\n    -profile <docker/singularity/.../institute>\n```\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/rnaseq/usage) and the [parameter documentation](https://nf-co.re/rnaseq/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/rnaseq/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/rnaseq/output).\n\nThis pipeline quantifies RNA-sequenced reads relative to genes/transcripts in the genome and normalizes the resulting data. It does not compare the samples statistically in order to assign significance in the form of FDR or P-values. For downstream analyses, the output files from this pipeline can be analysed directly in statistical environments like [R](https://www.r-project.org/), [Julia](https://julialang.org/) or via the [nf-core/differentialabundance](https://github.com/nf-core/differentialabundance/) pipeline.\n\n## Online videos\n\nA short talk about the history, current status and functionality on offer in this pipeline was given by Harshil Patel ([@drpatelh](https://github.com/drpatelh)) on [8th February 2022](https://nf-co.re/events/2022/bytesize-32-nf-core-rnaseq) as part of the nf-core/bytesize series.\n\nYou can find numerous talks on the [nf-core events page](https://nf-co.re/events) from various topics including writing pipelines/modules in Nextflow DSL2, using nf-core tooling, running nf-core pipelines as well as more generic content like contributing to Github. Please check them out!\n\n## Credits\n\nThese scripts were originally written for use at the [National Genomics Infrastructure](https://ngisweden.scilifelab.se), part of [SciLifeLab](http://www.scilifelab.se/) in Stockholm, Sweden, by Phil Ewels ([@ewels](https://github.com/ewels)) and Rickard Hammar\u00e9n ([@Hammarn](https://github.com/Hammarn)).\n\nThe pipeline was re-written in Nextflow DSL2 and is primarily maintained by Harshil Patel ([@drpatelh](https://github.com/drpatelh)) from [Seqera Labs, Spain](https://seqera.io/).\n\nThe pipeline workflow diagram was initially designed by Sarah Guinchard ([@G-Sarah](https://github.com/G-Sarah)) and James Fellows Yates ([@jfy133](https://github.com/jfy133)), further modifications where made by Harshil Patel ([@drpatelh](https://github.com/drpatelh)) and Maxime Garcia ([@maxulysse](https://github.com/maxulysse)).\n\nMany thanks to other who have helped out along the way too, including (but not limited to):\n\n- [Alex Peltzer](https://github.com/apeltzer)\n- [Colin Davenport](https://github.com/colindaven)\n- [Denis Moreno](https://github.com/Galithil)\n- [Edmund Miller](https://github.com/edmundmiller)\n- [Gregor Sturm](https://github.com/grst)\n- [Jacki Buros Novik](https://github.com/jburos)\n- [Lorena Pantano](https://github.com/lpantano)\n- [Matthias Zepper](https://github.com/MatthiasZepper)\n- [Maxime Garcia](https://github.com/maxulysse)\n- [Olga Botvinnik](https://github.com/olgabot)\n- [@orzechoj](https://github.com/orzechoj)\n- [Paolo Di Tommaso](https://github.com/pditommaso)\n- [Rob Syme](https://github.com/robsyme)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#rnaseq` channel](https://nfcore.slack.com/channels/rnaseq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/rnaseq for your analysis, please cite it using the following doi: [10.5281/zenodo.1400710](https://doi.org/10.5281/zenodo.1400710)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "44",
        "keep": true,
        "latest_version": 37,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/44?version=37",
        "name": "nf-core/rnaseq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "rna",
            "rna-seq"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 37
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "charles-plessy None",
            "charles-plessy None"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-pairgenomealign_logo_dark.png\">\n    <img alt=\"nf-core/pairgenomealign\" src=\"docs/images/nf-core-pairgenomealign_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/pairgenomealign/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/pairgenomealign/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/pairgenomealign/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/pairgenomealign/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/pairgenomealign/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.13910535-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.13910535)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/pairgenomealign)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23pairgenomealign-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/pairgenomealign)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/pairgenomealign** is a bioinformatics pipeline that aligns one or more _query_ genomes to a _target_ genome, and plots pairwise representations.\n\n![Tubemap workflow summary](docs/images/pairgenomealign-tubemap.png \"Tubemap workflow summary\")\n\nThe main steps of the pipeline are:\n\n1. Genome QC ([`assembly-scan`](https://github.com/rpetit3/assembly-scan)).\n2. Genome indexing ([`lastdb`](https://gitlab.com/mcfrith/last/-/blob/main/doc/lastdb.rst)).\n3. Genome pairwise alignments ([`lastal`](https://gitlab.com/mcfrith/last/-/blob/main/doc/lastal.rst)).\n4. Alignment plotting ([`last-dotplot`](https://gitlab.com/mcfrith/last/-/blob/main/doc/last-dotplot.rst)).\n5. Alignment export to various formats with [`maf-convert`](https://gitlab.com/mcfrith/last/-/blob/main/doc/maf-convert.rst), plus [`Samtools`](https://www.htslib.org/) for SAM/BAM/CRAM.\n\nThe pipeline can generate four kinds of outputs, called _many-to-many_, _many-to-one_, _one-to-many_ and _one-to-one_, depending on whether sequences of one genome are allowed match the other genome multiple times or not.\n\nThese alignments are output in [MAF](https://genome.ucsc.edu/FAQ/FAQformat.html#format5) format, and optional line plot representations are output in PNG format.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fasta\nquery_1,path-to-query-genome-file-one.fasta\nquery_2,path-to-query-genome-file-two.fasta\n```\n\nEach row represents a fasta file, this can also contain multiple rows to accomodate multiple query genomes in fasta format.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/pairgenomealign \\\n   -profile <docker/singularity/.../institute> \\\n   --target sequencefile.fa \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/pairgenomealign/usage) and the [parameter documentation](https://nf-co.re/pairgenomealign/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/pairgenomealign/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/pairgenomealign/output).\n\n## Credits\n\n`nf-core/pairgenomealign` was originally written by [charles-plessy](https://github.com/charles-plessy); the original versions are available at <https://github.com/oist/plessy_pairwiseGenomeComparison>.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Mahdi Mohammed](https://github.com/U13bs1125) ported the original pipeline to _nf-core_ template 2.14.x.\n- [Martin Frith](https://github.com/mcfrith/), the author of LAST, gave us extensive feedback and advices.\n- [Michael Mansfield](https://github.com/mjmansfi) tested the pipeline and provided critical comments.\n- [Aleksandra Bliznina](https://github.com/aleksandrabliznina) contributed to the creation of the initial `last/*` modules.\n- [Jiashun Miao](https://github.com/miaojiashun) and [Huyen Pham](https://github.com/ngochuyenpham) tested the pipeline on vertebrate genomes.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#pairgenomealign` channel](https://nfcore.slack.com/channels/pairgenomealign) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use this pipeline, please cite:\n\n> **Extreme genome scrambling in marine planktonic Oikopleura dioica cryptic species.**\n> Charles Plessy, Michael J. Mansfield, Aleksandra Bliznina, Aki Masunaga, Charlotte West, Yongkai Tan, Andrew W. Liu, Jan Gra\u0161i\u010d, Mar\u00eda Sara del R\u00edo Pisula, Gaspar S\u00e1nchez-Serna, Marc Fabrega-Torrus, Alfonso Ferr\u00e1ndez-Rold\u00e1n, Vittoria Roncalli, Pavla Navratilova, Eric M. Thompson, Takeshi Onuma, Hiroki Nishida, Cristian Ca\u00f1estro, Nicholas M. Luscombe.\n> _Genome Res._ 2024. 34: 426-440; doi: [10.1101/2023.05.09.539028](https://doi.org/10.1101/gr.278295.123). PubMed ID: [38621828](https://pubmed.ncbi.nlm.nih.gov/38621828/)\n\n[OIST research news article](https://www.oist.jp/news-center/news/2024/4/25/oikopleura-who-species-identity-crisis-genome-community)\n\nAnd also please cite the [LAST papers](https://gitlab.com/mcfrith/last/-/blob/main/doc/last-papers.rst).\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1108",
        "keep": true,
        "latest_version": 7,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1108?version=7",
        "name": "nf-core/pairgenomealign",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genomics",
            "comparative-genomics",
            "dot-plot",
            "last",
            "pairwise-alignment",
            "synteny",
            "whole-genome-alignment"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 7
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "Stephen Watts"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-oncoanalyser_logo_dark.png\">\n    <img alt=\"nf-core/oncoanalyser\" src=\"docs/images/nf-core-oncoanalyser_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/oncoanalyser/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/oncoanalyser/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/oncoanalyser/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/oncoanalyser/actions/workflows/linting.yml)\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/oncoanalyser/results)\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.15189386-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.15189386)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/oncoanalyser)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23oncoanalyser-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/oncoanalyser)\n[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)\n[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)\n[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/oncoanalyser** is a Nextflow pipeline for the comprehensive analysis of cancer DNA and RNA sequencing data\nusing the [WiGiTS](https://github.com/hartwigmedical/hmftools) toolkit from the Hartwig Medical Foundation. The pipeline\nsupports a wide range of experimental setups:\n\n- FASTQ, BAM, and / or CRAM input files\n- WGS (whole genome sequencing), WTS (whole transcriptome sequencing), and targeted / panel sequencing<sup>1</sup>\n- Paired tumor / normal and tumor-only samples, and support for donor samples for further normal subtraction\n- Purity estimate for longitudinal samples using genomic features of the primary sample from the same patient<sup>2</sup>\n- UMI (unique molecular identifier) processing supported for DNA sequencing data\n- Most GRCh37 and GRCh38 reference genome builds\n\n<sub><sup>1</sup> built-in support for the [TSO500\npanel](https://www.illumina.com/products/by-type/clinical-research-products/trusight-oncology-500.html) with other\npanels and exomes requiring [creation of custom panel reference\ndata](https://nf-co.re/oncoanalyser/usage#custom-panels)</sub>\n<br />\n<sub><sup>2</sup> for example a primary WGS tissue biospy and longitudinal low-pass WGS ccfDNA sample taken from the\nsame patient</sub>\n\n## Pipeline overview\n\n<p align=\"center\"><img src=\"docs/images/oncoanalyser_pipeline.png\"></p>\n\nThe pipeline mainly uses tools from [WiGiTS](https://github.com/hartwigmedical/hmftools), as well as some other external\ntools. There are [several workflows available](https://nf-co.re/oncoanalyser/usage#introduction) in `oncoanalyser` and\nthe tool information below primarily relates to the `wgts` and `targeted` analysis modes.\n\n> [!NOTE]\n> Due to the limitations of panel data, certain tools (indicated with `*` below) do not run in `targeted` mode.\n\n- Read alignment: [BWA-MEM2](https://github.com/bwa-mem2/bwa-mem2) (DNA), [STAR](https://github.com/alexdobin/STAR) (RNA)\n- Read post-processing: [REDUX](https://github.com/hartwigmedical/hmftools/tree/master/redux) (DNA), [Picard MarkDuplicates](https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard) (RNA)\n- SNV, MNV, INDEL calling: [SAGE](https://github.com/hartwigmedical/hmftools/tree/master/sage), [PAVE](https://github.com/hartwigmedical/hmftools/tree/master/pave)\n- SV calling: [ESVEE](https://github.com/hartwigmedical/hmftools/tree/master/esvee)\n- CNV calling: [AMBER](https://github.com/hartwigmedical/hmftools/tree/master/amber), [COBALT](https://github.com/hartwigmedical/hmftools/tree/master/cobalt), [PURPLE](https://github.com/hartwigmedical/hmftools/tree/master/purple)\n- SV and driver event interpretation: [LINX](https://github.com/hartwigmedical/hmftools/tree/master/linx)\n- RNA transcript analysis: [ISOFOX](https://github.com/hartwigmedical/hmftools/tree/master/isofox)\n- Oncoviral detection: [VIRUSbreakend](https://github.com/PapenfussLab/gridss)\\*, [VirusInterpreter](https://github.com/hartwigmedical/hmftools/tree/master/virus-interpreter)\\*\n- Telomere characterisation: [TEAL](https://github.com/hartwigmedical/hmftools/tree/master/teal)\\*\n- Immune analysis: [LILAC](https://github.com/hartwigmedical/hmftools/tree/master/lilac), [CIDER](https://github.com/hartwigmedical/hmftools/tree/master/cider), [NEO](https://github.com/hartwigmedical/hmftools/tree/master/neo)\\*\n- Mutational signature fitting: [SIGS](https://github.com/hartwigmedical/hmftools/tree/master/sigs)\\*\n- HRD prediction: [CHORD](https://github.com/hartwigmedical/hmftools/tree/master/chord)\\*\n- Tissue of origin prediction: [CUPPA](https://github.com/hartwigmedical/hmftools/tree/master/cuppa)\\*\n- Pharmacogenomics: [PEACH](https://github.com/hartwigmedical/hmftools/tree/master/peach)\n- Summary report: [ORANGE](https://github.com/hartwigmedical/hmftools/tree/master/orange), [linxreport](https://github.com/umccr/linxreport)\n\nFor the `purity_estimate` mode, several of the above tools are run with adjusted configuration in addition to the following.\n\n- Tumor fraction estimation: [WISP](https://github.com/hartwigmedical/hmftools/tree/master/wisp)\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nCreate a samplesheet with your inputs (WGS/WTS BAMs in this example):\n\n```csv\ngroup_id,subject_id,sample_id,sample_type,sequence_type,filetype,filepath\nPATIENT1_WGTS,PATIENT1,PATIENT1-N,normal,dna,bam,/path/to/PATIENT1-N.dna.bam\nPATIENT1_WGTS,PATIENT1,PATIENT1-T,tumor,dna,bam,/path/to/PATIENT1-T.dna.bam\nPATIENT1_WGTS,PATIENT1,PATIENT1-T-RNA,tumor,rna,bam,/path/to/PATIENT1-T.rna.bam\n```\n\nLaunch `oncoanalyser`:\n\n```bash\nnextflow run nf-core/oncoanalyser \\\n  -profile <docker/singularity/.../institute> \\\n  -revision 2.2.0 \\\n  --mode <wgts/targeted> \\\n  --genome <GRCh37_hmf/GRCh38_hmf> \\\n  --input samplesheet.csv \\\n  --outdir output/\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/oncoanalyser/usage) and the [parameter documentation](https://nf-co.re/oncoanalyser/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/oncoanalyser/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/oncoanalyser/output).\n\n## Version information\n\n### Extended support\n\nAs `oncoanalyser` is used in clinical settings and subject to accreditation standards in some instances, there is a need\nfor long-term stability and reliability for feature releases in order to meet operational requirements. This is\naccomplished through long-term support of several nominated feature releases, which all receive bug fixes and security\nfixes during the period of extended support.\n\nEach release that is given extended support is allocated a separate long-lived git branch with the 'stable' prefix, e.g.\n`stable/1.2.x`, `stable/1.5.x`. Feature development otherwise occurs on the `dev` branch with stable releases pushed to\n`master`.\n\nVersions nominated to have current long-term support:\n\n- TBD\n\n## Known issues\n\nPlease refer to [this page](https://github.com/nf-core/oncoanalyser/issues/177) for details regarding any known issues.\n\n## Credits\n\nThe `oncoanalyser` pipeline was written and is maintained by Stephen Watts ([@scwatts](https://github.com/scwatts)) from\nthe [Genomics Platform\nGroup](https://mdhs.unimelb.edu.au/centre-for-cancer-research/our-research/genomics-platform-group) at the [University\nof Melbourne Centre for Cancer Research](https://mdhs.unimelb.edu.au/centre-for-cancer-research).\n\nWe thank the following organisations and people for their extensive assistance in the development of this pipeline,\nlisted in alphabetical order:\n\n- [Hartwig Medical Foundation\n  Australia](https://www.hartwigmedicalfoundation.nl/en/partnerships/hartwig-medical-foundation-australia/)\n- Oliver Hofmann\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#oncoanalyser`\nchannel](https://nfcore.slack.com/channels/oncoanalyser) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nYou can cite the `oncoanalyser` Zenodo record for a specific version using the following DOI:\n[10.5281/zenodo.15189386](https://doi.org/10.5281/zenodo.15189386)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md)\nfile.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia,\n> Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in tags",
        "id": "1006",
        "keep": true,
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1006?version=4",
        "name": "nf-core/oncoanalyser",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dna",
            "ngs",
            "wgs",
            "cancer",
            "clinical",
            "exome",
            "panel",
            "rna",
            "targeted",
            "wigits",
            "wts"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 4
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "Danilo Di Leo",
            "Emelie Nilsson & Daniel Lundin"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-metatdenovo_logo_dark.png\">\n    <img alt=\"nf-core/metatdenovo\" src=\"docs/images/nf-core-metatdenovo_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/metatdenovo/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/metatdenovo/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/metatdenovo/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/metatdenovo/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/metatdenovo/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.10666590-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.10666590)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/metatdenovo)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23metatdenovo-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/metatdenovo)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/metatdenovo** is a bioinformatics best-practice analysis pipeline for assembly and annotation of metatranscriptomic and metagenomic data from prokaryotes, eukaryotes or viruses.\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/metatdenovo/results).\n\n## Usage\n\n![nf-core/metatdenovo metro map](docs/images/metat-metromap.png)\n\n1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n3. Quality trimming and adapter removal for raw reads ([`Trim Galore!`](https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/))\n4. Optional: Filter sequences with [`BBduk`](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/bbduk-guide/)\n5. Optional: Normalize the sequencing depth with [`BBnorm`](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/bbnorm-guide/)\n6. Merge trimmed, pair-end reads ([`Seqtk`](https://github.com/lh3/seqtk))\n7. Choice of de novo assembly programs:\n   1. [`RNAspades`](https://cab.spbu.ru/software/rnaspades/) suggested for both prokaryote and eukaryote assembly\n   2. [`Megahit`](https://github.com/voutcn/megahit) suggested for both prokaryote and eukaryote assembly; requires less resources\n8. Choice of orf caller:\n   1. [`TransDecoder`](https://github.com/TransDecoder/TransDecoder) suggested for eukaryotes; only ORFs\n   2. [`Prokka`](https://github.com/tseemann/prokka) suggested for prokaryotes; ORFs and other features plus functional annotation\n   3. [`Prodigal`](https://github.com/hyattpd/Prodigal) suggested for Prokaryotes; only ORFs\n9. Quantification of genes identified in assemblies:\n   1. Generate index of assembly ([`BBmap index`](https://sourceforge.net/projects/bbmap/))\n   2. Mapping cleaned reads to the assembly for quantification ([`BBmap`](https://sourceforge.net/projects/bbmap/))\n   3. Get raw counts per each gene present in the assembly ([`Featurecounts`](http://subread.sourceforge.net)) -> TSV table with collected featurecounts output\n10. Functional annotation:\n    1. [`Prokka`](https://github.com/tseemann/prokka) feature identification and annotation for prokaryotes\n    2. [`eggNOG-mapper`](https://github.com/eggnogdb/eggnog-mapper)\n    3. [`KofamScan`](https://github.com/takaram/kofam_scan)\n    4. [`HMMER`](https://www.ebi.ac.uk/Tools/hmmer/search/hmmsearch) search ORFs with a set of HMM profiles, and rank results\n11. Taxonomic annotation:\n    1. [`EUKulele`](https://github.com/AlexanderLabWHOI/EUKulele)\n    2. [`Diamond`](https://github.com/bbuchfink/diamond)\n12. Summary statistics.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```\nsample,fastq_1,fastq_2\nsample1,./data/S1_R1_001.fastq.gz,./data/S1_R2_001.fastq.gz\nsample2,./data/S2_fw.fastq.gz,./data/S2_rv.fastq.gz\nsample3,./S4x.fastq.gz,./S4y.fastq.gz\nsample3,./a.fastq.gz,./b.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired-end).\nThe fastq files need to end with `.fq` or `.fastq`, followed by `.gz` if gzipped.\nRead files from multiple rows with the same sample name will be concatenated and treated as a single sample.\nA mix of single-end and paired-end files is allowed, but do not mix single-end and paired-end for the same sample name.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/metatdenovo \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/metatdenovo/usage) and the [parameter documentation](https://nf-co.re/metatdenovo/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/metatdenovo/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/metatdenovo/output).\n\n> [!NOTE]\n> Tables in the `summary_tables` directory under the output directory are made especially for further analysis in tools like R or Python.\n> Their formats are standardized and column names consistent between tables.\n\n## Credits\n\nnf-core/metatdenovo was originally written by Danilo Di Leo (@danilodileo), Emelie Nilsson (@emnilsson) & Daniel Lundin (@erikrikarddaniel).\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#metatdenovo` channel](https://nfcore.slack.com/channels/metatdenovo) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/metatdenovo for your analysis, please cite it using the following doi: [10.5281/zenodo.10666590](https://doi.org/10.5281/zenodo.10666590)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "997",
        "keep": true,
        "latest_version": 6,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/997?version=6",
        "name": "nf-core/metatdenovo",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "eukaryotes",
            "metatranscriptomics",
            "prokaryotes",
            "viruses"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 6
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "Avani Bhojwani and Timothy Little"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-denovotranscript_logo_dark.png\">\n    <img alt=\"nf-core/denovotranscript\" src=\"docs/images/nf-core-denovotranscript_logo_light.png\">\n  </picture>\n</h1>\n[![GitHub Actions CI Status](https://github.com/nf-core/denovotranscript/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/denovotranscript/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/denovotranscript/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/denovotranscript/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/denovotranscript/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.13324371-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.13324371)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/denovotranscript)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23denovotranscript-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/denovotranscript)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/denovotranscript** is a bioinformatics pipeline for de novo transcriptome assembly of paired-end short reads from bulk RNA-seq. It takes a samplesheet and FASTQ files as input, perfoms quality control (QC), trimming, assembly, redundancy reduction, pseudoalignment, and quantification. It outputs a transcriptome assembly FASTA file, a transcript abundance TSV file, and a MultiQC report with assembly quality and read QC metrics.\n\n![nf-core/transfuse metro map](docs/images/denovotranscript_metro_map.drawio.svg)\n\n1. Read QC of raw reads ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n2. Adapter and quality trimming ([`fastp`](https://github.com/OpenGene/fastp))\n3. Read QC of trimmed reads ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n4. Remove rRNA or mitochondrial DNA (optional) ([`SortMeRNA`](https://hpc.nih.gov/apps/sortmeRNA.html))\n5. Transcriptome assembly using any combination of the following:\n\n   - [`Trinity`](https://github.com/trinityrnaseq/trinityrnaseq/wiki) with normalised reads (default=True)\n   - [`Trinity`](https://github.com/trinityrnaseq/trinityrnaseq/wiki) with non-normalised reads\n   - [`rnaSPAdes`](https://ablab.github.io/spades/rna.html) medium filtered transcripts outputted (default=True)\n   - [`rnaSPAdes`](https://ablab.github.io/spades/rna.html) soft filtered transcripts outputted\n   - [`rnaSPAdes`](https://ablab.github.io/spades/rna.html) hard filtered transcripts outputted\n\n6. Redundancy reduction with [`Evidential Gene tr2aacds`](http://arthropods.eugenes.org/EvidentialGene/). A transcript to gene mapping is produced from Evidential Gene's outputs using [`gawk`](https://www.gnu.org/software/gawk/).\n7. Assembly completeness QC ([`BUSCO`](https://busco.ezlab.org/))\n8. Other assembly quality metrics ([`rnaQUAST`](https://github.com/ablab/rnaquast))\n9. Transcriptome quality assessment with [`TransRate`](https://hibberdlab.com/transrate/), including the use of reads for assembly evaluation. This step is not performed if profile is set to `conda` or `mamba`.\n10. Pseudo-alignment and quantification ([`Salmon`](https://combine-lab.github.io/salmon/))\n11. HTML report for raw reads, trimmed reads, BUSCO, and Salmon ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a pair of fastq files (paired end).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/denovotranscript \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/denovotranscript/usage) and the [parameter documentation](https://nf-co.re/denovotranscript/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/denovotranscript/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/denovotranscript/output).\n\n## Credits\n\nnf-core/denovotranscript was written by Avani Bhojwani ([@avani-bhojwani](https://github.com/avani-bhojwani/)) and Timothy Little ([@timslittle](https://github.com/timslittle/)).\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#denovotranscript` channel](https://nfcore.slack.com/channels/denovotranscript) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/denovotranscript for your analysis, please cite it using the following doi: [10.5281/zenodo.13324371](https://doi.org/10.5281/zenodo.13324371)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1100",
        "keep": true,
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1100?version=4",
        "name": "nf-core/denovotranscript",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "denovo-assembly",
            "rna-seq",
            "transcriptome"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 4
    },
    {
        "create_time": "2025-09-02",
        "creators": [
            "Ivna Ivankovic",
            "Todor Gitchev",
            "Zsolt Bal\u00e1zs"
        ],
        "description": "# cfDNA-Flow\r\n\r\n## 1. Overview\r\ncfDNA-Flow facilitates the accurate and reproducible analysis of cfDNA WGS data. It offers various preprocessing options to accommodate different experimental setups and research needs in the field of liquid biopsies.\r\n\r\n## 2. Preprocessing options\r\n### 2.1 Trimming Options\r\ncfDNA-Flow provides the flexibility to either trim or not trim the input reads based on the user's requirements. Trimming removes low-quality bases, which can impact downstream analyses.\r\n\r\n### 2.2 Reference Genome Selection\r\nUsers can choose from the following genome builds: hg19, hs37d5 (hg19decoy), hg38 and hg38 without alternative contigs (hg38noalt). For download links, please refer to the Data Availability section in the accompanying paper.\r\n\r\n### 2.3 Post-Alignment Filtering and GC bias correction\r\nThe pipeline uses the BWA software for alignment, followed by extensive post-alignment filtering steps to ensure reliable alignments. Users can define specific filtering criteria to remove low-quality or ambiguous reads, such as secondary alignments, reads with insertion or deletion, and reads with low mapping qualities.\r\n\r\n## 3. Feature Extraction\r\n\r\n### 3.1 Fragment length features\r\ncfDNA-Flow offers fragment length analysis; calculating the mean, median, and standard deviation values for fragments sized 100 to 220 base pairs (bp), corresponding to the mononucleosomal size range. Additionally, cfDNA-Flow calculates the frequencies of cfDNA fragment sizes ranging from 70 bp to 1000 bp in 10 bp bins.\r\n\r\n### 3.2 Copy number changes\r\ncfDNA-Flow utilizes two copy number analysis tools: [ichorCNA](https://github.com/broadinstitute/ichorCNA) (v0.2.0) and [tMAD](https://github.com/sdchandra/tMAD), to estimate copy number changes and tumor fraction. \r\n\r\n### 3.3 Fragment end motifs\r\ncfDNA-Flow runs [FrEIA](https://github.com/mouliere-lab/FrEIA) to identify differences in fragment end motifs and their diversity between groups.\r\n\r\n### 3.4 Differential coverage analysis over DNase hypersensitivity sites\r\nThese features are calculated using [LIQUORICE](https://github.com/epigen/LIQUORICE/tree/master). You can find more information and access LIQUORICE through the following [link](https://liquorice.readthedocs.io/en/latest/).\r\n\r\n## 4. Usage\r\nTo use the cfDNA-Flow, follow these steps:\r\n\r\n### 4.1 Installation:\r\nClone the cfDNA-Flow repository from GitHub to your local machine.\r\n\r\n        git clone https://github.com/uzh-dqbm-cmi/cfDNA-Flow.git\r\n        cd cfDNA-Flow\r\n\r\nIt is recommended to create a virtual environment to manage the project's dependencies. This ensures that the dependencies do not interfere with other Python projects on your machine. See how to create a Python environment [here](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/).\r\n\r\nOnce the virtual environment is activated, install the required Python dependencies using the `requirements.txt` file.\r\n\r\n        pip install -r requirements.txt\r\n\r\nAdditionally, some R packages are required for the project. Make sure you have R installed (version 4.3). You can install these packages by running the script below:\r\n\r\n        R -f install_packages.R\r\n\r\nAfter following these steps, your environment should be set up with all the necessary dependencies for both Python and R. You are now ready to proceed with using the cfDNA-Flow pipeline. See section 4. Usage. \r\n\r\nOnce you are finished using cfDNA-Flow, deactivate the virtual environment by running:\r\n\r\n        deactivate\r\n\r\n### 4.2 Configuration:\r\nThe configuration file, `test_cfDNA_pipeline.yaml`, is used to specify the input files, reference genome, and desired preprocessing options. User can customize the trimming, alignment, and filtering settings as needed.\r\nSettings for this demo are as follows: reads are trimmed, reference genome is hg38, mapping quality is 30, [SAM flag](https://broadinstitute.github.io/picard/explain-flags.html) is 40, CIGAR string is D. \r\n\r\n### 4.3 Execution/Demo:\r\nTo start preprocessing, execute the following command. Use the -np flag for a dry run to verify everything works correctly.\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_preprocess -np \r\n\r\nIf successful, rerun the command without the -np flag.\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_preprocess \r\n\r\nNext, bed to process BED files, use:\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_bedprocess\r\n\r\nDo global length: \r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_global_length\r\n\r\nDo tMAD:\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_cal_blacklist\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_cal_RefSample\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_cal_t_MAD_forall\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_visualising_t_MAD_forall\r\n\r\nDo ichorCNA:\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_createPoN\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_ichorCNA\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_ichorCNA_results\r\n\r\nDo LIQUORICE:\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_LIQUORICE\r\n\r\nDo FreIA:\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_FrEIA_preprocessing\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_FrEIA\r\n\r\n\r\n### 4.4 Output:\r\nThe pipeline outputs alignment files (BAM files, BED files), and quality control reports. Additionally, it outputs features described above. \r\n\r\n#### BAM files\r\nProcessed BAM files of studied samples, accompanied by their .bai index files, are stored in the `results/BAM/0memhg19False40` folder and have `.sortByCoord.bam` suffix. \r\n\r\n#### BED files\r\nBED files are stored in the `results/BED/0memhg19False40` folder. Those files store information about chromosome number, start and end positions of cfDNA fragments.\r\n\r\n#### Quality control reports\r\nOutput of QC is stored in the `results/QC/0memhg19False40/multiqc_data` folder. Specifically, `multiqc_report.html` file contains multiple the QC metrics: general statistics, Picard metrics (alignment summary, mean read length, mark duplicates, WGS coverage, WGS filtered bases), FastQC (sequence counts, sequence quality histograms and quality scores, per base sequence content, per sequence GC content, per base N content, sequence length distribution, sequence duplication levels, overrepresented sequences, adapter content, status checks).  \r\n\r\n#### Fragment length features\r\nThe output of fragment length features is stored in the `results/feature/0memhg19False40/global_length.tsv` file. Columns store fragment length features for each studied sample (rows).\r\n\r\n#### Coverage features and fragment lengths in 1 Mbp genomic bins\r\nOutputs of features in 1 Mbp genomic bins can be found in the `results/BED/0memhg19False40` folder. Values for all the samples are stored in `mergeddf.csv` file. Values for each individual sample are stored in the files with suffix `binned.csv`.\r\n\r\nAdditional length features for every sample are stored in the folder `results/BED/0memhg19False40` and have the following suffixes:\r\n\r\n`binned_lengths.csv` - each row contains information about the chromosome number, genomic bin number (1 Mbp wide), and the lengths of all cfDNA fragments corresponding to that bin\r\n\r\n`len.csv` - contains a single column listing the lengths of all cfDNA fragments derived from a sample\r\n\r\n`lenuniqcount.csv` - a two-column format representing the histogram of cfDNA fragment lengths along with their frequencies\r\n\r\n#### ichorCNA\r\nResults of ichorCNA analysis can be found in the `results/feature/0memhg19False40/ichorCNA` folder. For detailed ichorCNA output description see this [link](https://github.com/broadinstitute/ichorCNA/wiki/Output). Shortly, ichorCNA outputs tumor fraction estimates based on CNA analysis. Additionally, it outputs CNA plots representing log2 ratio copy number for each bin in the genome.\r\n\r\n#### tMAD\r\nThe outputs of tMAD are stored in the `results/BED/0memhg19False40/tMAD/tMAD_results.tsv` file. This file contains sample names and their corresponding tMAD values. \r\n\r\n#### LIQUORICE\r\nThe outputs of LIQUORICE are stored in the `results/BED/0memhg19False40/LIQUORICE/summary_across_samples_and_ROIS.csv` file. This file contains sample names and their corresponding Dip depth and Dip area values after z-scaling. We recommend using the dip depth values, as we found them most informative. \r\n\r\n#### FrEIA\r\nThe outputs of FrEIA are stored in the `results/BED/0memhg19False40/FrEIA/0memhg19False40_FrEIA_score.csv` file. This file contains sample names and their corresponding tMAD values. \r\n\r\n## 5. Support\r\nWith issues or questions, please contact the maintainers. \r\n",
        "doi": "10.48546/workflowhub.workflow.1900.1",
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics"
        ],
        "filtered_on": "binn* in description",
        "id": "1900",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1900?version=1",
        "name": "cfDNA-Flow",
        "number_of_steps": 0,
        "projects": [
            "KrauthammerLab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "cfdna"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2025-09-19",
        "versions": 1
    },
    {
        "create_time": "2025-09-01",
        "creators": [
            "Azusa Kubota",
            "Atsushi Tajima"
        ],
        "description": "# \ud83e\uddec Click-qPCR \ud83e\uddec\r\n\r\nAn ultra-simple tool for interactive qPCR data analysis developed with R and Shiny.\r\n\r\n[\u65e5\u672c\u8a9e\u7248\u306e\u30e6\u30fc\u30b6\u30fc\u30ac\u30a4\u30c9\u306f\u3053\u3061\u3089 (Read this document in Japanese)](README_jp.md)\r\n\r\n## Overview\r\n\r\nClick-qPCR is a user-friendly Shiny web application designed for the straightforward analysis of real-time quantitative PCR (qPCR) data.\r\n\r\n**This tool is readily accessible via a web browser at <https://kubo-azu.shinyapps.io/Click-qPCR/>, requiring no local installation for end-users.**\r\n\r\nIt allows users to upload their Cq (quantification cycle) values, perform \u0394Cq and \u0394\u0394Cq calculations, visualize results as bar plots with individual data points, and download both the statistical summaries and publication-quality plots.\r\n\r\nFor users who prefer to run or modify the application locally, the source code is also available (see Installation and Usage section below).\r\n\r\nThis tool aims to simplify common qPCR data analysis workflows, making them more accessible to researchers without requiring extensive programming knowledge.\r\n\r\n### <ins>Notice</ins>\r\n\r\nThis repository contains the source code for the Shiny app accompanying the preprint:\r\n\r\nA. Kubota and A. Tajima, *bioRxiv*, (2025). <https://doi.org/10.1101/2025.05.29.656779>.\r\n\r\n**Please cite this paper if you use this app or code in your research.**\r\n\r\n## Features\r\n\r\n-   **Interactive Data Upload:** Easily upload your qPCR data in CSV format. A template is provided to guide data formatting.\r\n\r\n-   **Robust File Upload:** Automatically detects character encoding (e.g., UTF-8, Shift-JIS, etc.) for international compatibility.\r\n\r\n-   **Data Preview:** View the first 10 rows of your uploaded data to ensure it's loaded correctly.\r\n\r\n-   **Tab-Based Analysis:** The user interface is organized into clear tabs for different analyses.\r\n\r\n    -   **Preproceccing and \u0394Cq Analysis:**\r\n        -   Select one or **multiple reference genes**. The \u0394Cq is calculated using the mean Cq of the selected reference genes.\r\n        -   Select one or more target genes.\r\n        -   Set up **multiple group comparisons** simultaneously using an intuitive interface.\r\n        -   Calculates relative expression ($2^{-\\Delta Cq}$).\r\n        -   Performs Welch's *t*-test for statistical significance for each specified pair.\r\n        -   Visualizes all results in a comprehensive bar plot showing mean \u00b1 SD, with individual data points overlaid.\r\n\r\n    -   **\u0394\u0394Cq Analysis:**\r\n        -   Automatically uses the reference gene(s) selected in the \"Preproceccing and \u0394Cq Analysis\" tab.\r\n        -   Select a target gene, a base/control group, and one or more treatment groups.\r\n        -   Calculates fold-change ($2^{-\\Delta\\Delta Cq}$) relative to the base group.\r\n        -   Performs Welch's *t*-test for statistical significance.\r\n        -   Visualizes results in a dedicated bar plot.\r\n\r\n    -   **ANOVA (Dunnett's post-hoc):**\r\n        -   Designed for comparing three or more groups.\r\n        -   Performs a **one-way ANOVA** followed by **Dunnett's post-hoc test** to compare each treatment group against a single control group.\r\n        -   Results are visualized as Relative Expression ($2^{-\\Delta Cq}$) on the **\"\u0394Cq ANOVA (Dunnett's post-hoc)\"** tab.\r\n        -   The same statistical results can be visualized as Fold Change ($2^{-\\Delta\\Delta Cq}$) on the **\"\u0394\u0394Cq ANOVA (Dunnett's post-hoc)\"** tab.\r\n\r\n-   **Advanced Downloading & Plotting:**\r\n    -   **Interactive Plots:** All plots are generated with ggplot2 and can be downloaded in PNG and PDF.\r\n    -   **Custom Plot Dimensions:** Interactively adjust the width and height for downloaded plots using sliders. Resolution (DPI) of PNG downloading is also available to change.\r\n    -   **Fixed Aspect Ratio:** Optionally lock the plot's aspect ratio while resizing.\r\n    -   **Two Download Modes:**\r\n        1.  **Download Plot:** Saves an image using your custom dimension and DPI settings.\r\n        2.  **Save Displayed Size:** Saves an image that is an exact replica of the plot shown on the screen.\r\n    - **Customizable Plot Colors:** Select from several built-in color palettes, including colorblind-friendly and grayscale options, to customize your plot's appearance for presentations or publications.\r\n\r\n        | Palette Name                  | Key Features & Recommendations                                                                                                                 |\r\n        | ----------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\r\n        | **Default (ggplot2)** | The standard, well-recognized `ggplot2` theme.                                          |\r\n        | **Balanced (Set2)** | Provides a set of clear, distinct colors that are easy on the eyes and work well on screen.                     |\r\n        | **Colorblind-Friendly (Viridis)** | Ensures that your plots are accessible to everyone, including those with color vision deficiencies. |\r\n        | **Paired Colors** | Consists of light/dark pairs of colors. Ideal for analyses where you have paired or closely related experimental groups to compare.               |\r\n        | **Pastel (Pastel1)** | A selection of softer, less saturated colors. A great choice for posters or when a less intense visual style is preferred.                      |\r\n        | **Grayscale (for printing)** | Renders the plot in shades of gray. Use this to confirm your figure is interpretable without color.        |\r\n\r\n\r\n-   **Robust & Informative:**\r\n    -   Handles cases with insufficient data or zero variance gracefully without crashing.\r\n    -   Provides clear messages in the results table (e.g., \"Zero variance\") when statistics cannot be calculated.\r\n\r\n-   **Diagnostics Tab:**\r\n    -   This tab provides a self-testing function. When you click the \"Run Diagnostics\" button, the app uses its built-in sample data to automatically test four of its core functions:\r\n        1.  Sample Data Loading\r\n        2.  \u0394Cq Analysis (*t*-test) Validation\r\n        3.  \u0394\u0394Cq Analysis (Fold Change) Validation\r\n        4.  ANOVA and Dunnett's Test Validation\r\n    -   If all tests show \"Passed \u2705\", you can be confident that the app's calculation and statistical capabilities are functioning as intended. When installed locally or modified, the Diagnostics tab is useful for checking the health of the application.\r\n    \r\n## Installation and Usage\r\n\r\nWhile the app is available online, you can also run it locally.\r\n\r\n### Prerequisites\r\n\r\n-   R (version 4.1 or later recommended)\r\n-   The following R packages: `shiny`, `shinyjs`, `readr`, `dplyr`, `ggplot2`, `tidyr`, `DT`, `RColorBrewer`, `fontawesome`, `multcomp`\r\n\r\n### Requirements\r\n\r\n* R (version 4.4.2 or later recommended)\r\n* RStudio (recommended for ease of use, but not required if running from the R console)\r\n* The following R packages (and their dependencies):\r\n* `shiny`\r\n* `shinyjs`\r\n* `readr`\r\n* `dplyr`\r\n* `ggplot2`\r\n* `tidyr`\r\n* `DT`\r\n* `RColorBrewer`\r\n* `fontawesome`\r\n* `multcomp`\r\n\r\nThese packages can be installed in R as follows:\r\n\r\n```R\r\ninstall.packages(c(\"shiny\", \"shinyjs\", \"readr\", \"dplyr\", \"ggplot2\", \"tidyr\", \"DT\", \"RColorBrewer\", \"fontawesome\", \"multcomp\"))\r\n```\r\n\r\n### Running the Application\r\n\r\n<ins>Option 1: Run Directly from GitHub</ins>\r\n\r\nYou can run directly from GitHub using the shiny::runGitHub() function in R or RStudio:\r\n\r\n```R\r\nif (!requireNamespace(\"shiny\", quietly = TRUE)) install.packages(\"shiny\")\r\nshiny::runGitHub(\"kubo-azu/Click-qPCR\")\r\n```\r\n\r\n<ins>Option 2: Clone the repository locally</ins>\r\n\r\n1. Clone this repository to your local machine (your PC):\r\n\r\n```sh\r\ngit clone https://github.com/kubo-azu/Click-qPCR.git\r\n```\r\n\r\n2. Navigate to the cloned directory in R, or open the Click-qPCR.Rproj file in RStudio.\r\n\r\n3. If you are using `renv` (recommended for reproducibility), restore your R environment:\r\n\r\n```R\r\nif (!requireNamespace(\"renv\", quietly = TRUE)) install.packages(\"renv\")\r\nrenv::restore()\r\n```\r\n\r\n4. Run the application:\r\n\r\n```R\r\nshiny::runApp()\r\n```\r\n    \r\n## Data Format\r\n\r\nPrepare your data as a CSV file with the following four columns:\r\n\r\n-   `sample`: Unique identifier for each sample (e.g., Mouse_A, CellLine_1).\r\n-   `group`: The experimental group or condition (e.g., Control, Treatment_X).\r\n-   `gene`: The name of the gene being measured (e.g., Gapdh, Actb).\r\n-   `Cq`: The Quantification Cycle value (numeric). **Note:** This column must be named `Cq`.\r\n\r\nEach row must represent the Cq value of one gene in one sample. If you have technical replicates, please calculate and use their mean value. A template file ([Click-qPCR_template.csv](Click-qPCR_template.csv)) can be downloaded from the application sidebar.\r\n\r\n## How to Use\r\n\r\n1.  **Upload Data:**\r\n    -  On the **\"\u0394Cq Analysis** tab, click \"Upload CSV File\" or \"Use Example Data\". A preview will appear.\r\n\r\n2.  **Perform \u0394Cq Analysis:**\r\n    -   Check \"Enable multiple reference genes\" to select more than one.\r\n    -   Select your \"Reference Gene(s)\".\r\n    -   Select one or more \"Target Gene(s)\".\r\n    -   Under \"Comparison Settings,\" define pairs of groups to compare. Click \"Add\" to create more pairs.\r\n    -   Click **\"Analyze\"**. The plot and statistical table will appear.\r\n\r\n3.  **Perform \u0394\u0394Cq Analysis:**\r\n    -   Click on the **\"\u0394\u0394Cq Analysis\"** tab.\r\n    -   The Reference Gene(s) are automatically inherited.\r\n    -   Select a single \"Target Gene\".\r\n    -   Select the \"Base Group (Control)\".\r\n    -   Select one or more \"Treatment Group(s)\".\r\n    -   Click **\"Run \u0394\u0394Cq Analysis\"**. The fold change plot and table will appear.\r\n\r\n4.  **Perform ANOVA and Dunnett's post-hoc:**\r\n    -   Navigate to the **\"\u0394Cq ANOVA (Dunnett's post-hoc)\"** tab.\r\n    -   Select a single \"Target Gene\".\r\n    -   Select the \"Control Group\".\r\n    -   Select two or more \"Treatment Group(s)\".\r\n    -   Click **\"Run ANOVA\"**. The relative expression plot and a table with ANOVA and Dunnett's test results will appear.\r\n    -   Navigate to the **\"\u0394\u0394Cq ANOVA (Dunnett's post-hoc)\"** tab to see the same results visualized as fold change.\r\n\r\n5.  **Download Results:**\r\n    -   In any tab, use the download buttons to save your results.\r\n    -   Use the **\"Download Plot Settings\"** panel to customize the dimensions and resolution for the \"Download Plot\" button.\r\n    \r\n## Example Analysis with Sample Data\r\n\r\nThis section demonstrates how to use the app's core functions with the built-in sample data.\r\n\r\n### 1. Load Sample Data and Perform \u0394Cq Analysis\r\n\r\nFirst, we'll compare the expression of a single gene between two groups using a Welch's *t*-test.\r\n\r\n* On the **\"Preprocessing and \u0394Cq Analysis\"** tab, click the **\"Use Example Data\"** button.\r\n* Check the box for **\"Enable multiple reference genes\"**.\r\n* For \"Reference Gene(s)\", select both `Gapdh` and `Actb`.\r\n* For \"Target Gene(s)\", ensure only `Hoge` is selected.\r\n* Under \"Comparison Settings,\" set up a comparison between `Control` and `Treatment_X`.\r\n* Click the blue **\"Analyze\"** button.\r\n\r\n#### **Expected Output**\r\n\r\nYou will see a bar chart and a data table summarizing the analysis. The sample data is designed to show that `Hoge` expression is significantly lower in the `Treatment_X` group compared to the `Control` group.\r\n\r\n**Plot:** The chart will display two bars for the `Hoge` gene: one for the `Control` group and one for the `Treatment_X` group. Individual data points will be scattered over the bars, and the bar for `Treatment_X` will be noticeably lower than the `Control` bar. A significance bracket (`***`) will connect the two bars.\r\n\r\n**Statistics Table:** The table below the plot will show the result of the Welch's *t*-test performed on the \u0394Cq values. The p-value will be very small, resulting in a high significance level.\r\n\r\n| gene | group1  | group2      | p_value  | sig |\r\n| :--- | :------ | :---------- | :------- | :-- |\r\n| Hoge | Control | Treatment_X | 1.25e-05 | *** |\r\n\r\n---\r\n\r\n### 2. Perform ANOVA with Dunnett's Post-Hoc Test\r\n\r\nNext, we'll compare one gene across multiple treatment groups against a single control group. The reference genes selected in the first tab (`Gapdh` and `Actb`) will be automatically used.\r\n\r\n* Navigate to the **\"\u0394Cq ANOVA (Dunnett's post-hoc)\"** tab.\r\n* Select `Hoge` as the \"Target Gene\".\r\n* Select `Control` as the \"Control Group\".\r\n* Select `Treatment_X`, `Treatment_Y`, and `Treatment_Z` in the \"Treatment Group(s)\" box.\r\n* Click the blue **\"Run ANOVA\"** button.\r\n\r\n#### **Expected Output**\r\n\r\nThis analysis performs a one-way ANOVA to see if there are any differences among the four groups, followed by Dunnett's test to specifically compare each treatment group to the control. The sample data will show that all treatment groups are significantly different from the control.\r\n\r\n**Plot:** The chart will display four bars for the `Hoge` gene, one for each group (`Control`, `Treatment_X`, `Treatment_Y`, `Treatment_Z`). Significance brackets will be shown comparing each treatment group back to the `Control` bar.\r\n\r\n**Statistics Table:** The table will first display the overall result of the ANOVA *F*-test, which should be highly significant. Below that, it will list the results of Dunnett's test for each treatment-control comparison.\r\n\r\n| group1       | group2                               | p_value  | sig |\r\n| :----------- | :----------------------------------- | :------- | :-- |\r\n| ANOVA F-test | F(3, 12) = 59.39                     | 3.12e-08 |     |\r\n| Control      | Treatment_X - Control = 0            | 2.11e-05 | *** |\r\n| Control      | Treatment_Y - Control = 0            | 1.34e-03 | ** |\r\n| Control      | Treatment_Z - Control = 0            | 2.00e-07 | *** |\r\n\r\n\r\n## Licence\r\n\r\nThe Click-qPCR application and corresponding files are under MIT licence.\r\n\r\n\r\n## Contact\r\n\r\nFeel free to use GitHub Discussions.\r\n",
        "doi": null,
        "edam_operation": [
            "Calculation",
            "Visualisation"
        ],
        "edam_topic": [
            "Copy number variation",
            "Gene expression"
        ],
        "filtered_on": "annot* in description",
        "id": "1725",
        "keep": true,
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1725?version=4",
        "name": "Click-qPCR: An interactive Shiny application for qPCR data analysis",
        "number_of_steps": 0,
        "projects": [
            "Click-qPCR"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "copy number variation",
            "genomics",
            "r",
            "gene expression analysis",
            "qpcr"
        ],
        "tools": [],
        "type": "Unrecognized workflow type",
        "update_time": "2025-09-01",
        "versions": 4
    },
    {
        "create_time": "2025-07-24",
        "creators": [],
        "description": "The Spatial Transcriptomics analysis workflow for Xenium data from the PATH2XNAT project tested on the non-diseased lung dataset from 10X genomics. The analysis workflow written in R and executed in the interactive RStudio environment consists of visualizations, clustering, feature selection and cluster annotation.\r\n\r\nTraining materials elaborating on this analysis workflows can be found in this GitHub repository: https://github.com/HCGB-IGTP/PATH2XNAT/tree/main.\r\n\r\nThis workflow was developed in the PATH2XNAT joint research action within the ISIDORe project. ISIDORe has received funding from the European Union\u2019s Horizon Europe research and innovation programme under grant agreement number 101046133.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1831",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1831?version=1",
        "name": "PATH2XNAT ST workflow Xenium non-diseased lung test data analysis",
        "number_of_steps": 1,
        "projects": [
            "ErasmusMC Clinical Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2025-08-22",
        "versions": 1
    },
    {
        "create_time": "2025-11-04",
        "creators": [
            "Lucille Delisle"
        ],
        "description": "Complete ChIP-seq analysis for single-end sequencing data. Processes raw FASTQ files through adapter removal (cutadapt), alignment to reference genome (Bowtie2), and quality filtering (MAPQ &gt;= 30). Peak calling with MACS2 uses either a fixed extension parameter or built-in model to identify protein-DNA binding sites. Generates alignment files, peak calls, and quality metrics for downstream analysis.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "397",
        "keep": true,
        "latest_version": 16,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/397?version=16",
        "name": "chipseq-sr/main",
        "number_of_steps": 7,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "chip-seq"
        ],
        "tools": [
            "cutadapt",
            "wig_to_bigWig",
            "tp_grep_tool",
            "macs2_callpeak",
            "bowtie2",
            "samtool_filter2",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-11-04",
        "versions": 16
    },
    {
        "create_time": "2025-11-04",
        "creators": [
            "Lucille Delisle"
        ],
        "description": "Complete ChIP-seq analysis for paired-end sequencing data. Processes raw FASTQ files through adapter removal (cutadapt), alignment to reference genome (Bowtie2), and stringent quality filtering (MAPQ &gt;= 30, concordant pairs only). Peak calling with MACS2 optimized for paired-end reads identifies protein-DNA binding sites. Generates alignment files, peak calls, and quality metrics for downstream analysis.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "398",
        "keep": true,
        "latest_version": 17,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/398?version=17",
        "name": "chipseq-pe/main",
        "number_of_steps": 7,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "chip-seq"
        ],
        "tools": [
            "cutadapt",
            "wig_to_bigWig",
            "tp_grep_tool",
            "macs2_callpeak",
            "bowtie2",
            "samtool_filter2",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-11-04",
        "versions": 17
    },
    {
        "create_time": "2025-08-19",
        "creators": [],
        "description": "# PanGIA: A universal framework for identifying association between ncRNAs and diseases\r\n\r\nPanGIA is a deep learning model for predicting ncRNA-disease associations.\r\n## Model Architecture\r\n![](PanGIA/architecture.svg)\r\n\r\n## Installation\r\n```bash\r\nconda create -n pangia python=3.11\r\nconda activate pangia\r\npip install -r requirements.txt\r\n```\r\n\r\n## Prepare Datasets\r\nThe raw data can be downloaded from the following sources:\r\n- **miRNA**: The associations between miRNAs and diseases were obtained from the HMDD v4.0 database, while the sequence information of miRNAs was retrieved from the miRBase database.\r\n\r\n- **LncRNA/circRNA**: This study includes lncRNA and circRNA associations with diseases, with data obtained from LncRNADisease v3.0. The sequence information of circRNAs was retrieved from the circBase database. In contrast, lncRNA sequences were collected from two sources: GENCODE and NONCODE.\r\n\r\n- **piRNA**: The associations between piRNAs and diseases were obtained from the piRDisease v1.0 database, and the sequence information was retrieved from the piRBase and piRNAdb databases.\r\n\r\n- **Disease**: This study utilizes Disease Ontology Identifiers (DOIDs) to construct the disease similarity matrix, with corresponding information obtained from the Disease Ontology database.\r\n\r\nThese data are also organized in the ./data folder.\r\n\r\n## Quick Start\r\n### 1.Data Preprocessing & Cleaning\r\nPrepare the RNA sequence files for each RNA type (`miRNA`, `piRNA`, `lncRNA`, `circRNA`) in CSV format:\r\n\r\n```bash\r\n# Example format (no header):\r\n# RNA_ID,Sequence\r\nmiR0001,AGCUUGGA...\r\nmiR0002,CGAUUAGC...\r\n```\r\nRun the script to perform global alignment of RNA sequences and compute their pairwise similarity:\r\n```bash\r\npython compute_RNA_similarity.py\r\n```\r\nNext, merge the RNA sequence similarity matrices across all RNA types (miRNA, piRNA, lncRNA, circRNA) into a unified format for downstream analysis:\r\n\r\n```bash\r\npython merge_RNA_similarity_matrices.py\r\n```\r\nThis script reads the normalized pairwise similarity matrices generated for each RNA type and combines them into a multi-view or unified similarity representation for further modeling.\r\n\r\nNext, run the script `compute_disease_similarity.py` to generate the disease ontology-based similarity matrix:\r\n\r\n```bash\r\npython compute_disease_similarity.py\r\n```\r\n\r\nThis script calculates pairwise semantic similarities between diseases based on the Disease Ontology (DO) structure, and saves the resulting matrix to:\r\n\r\n```\r\n./data/d2d_do.csv\r\n```\r\nRun the following script to generate the binary association matrix between ncRNAs and diseases:\r\n\r\n```bash\r\npython generate_RD_adj.py\r\n```\r\n\r\nThis script constructs the ncRNA\u2013disease adjacency matrix based on known associations.  \r\nThe output is a matrix where each row represents an ncRNA and each column represents a disease,  \r\nwith entries marked as 1 if an association exists, and 0 otherwise.\r\n\r\nNext, pretrain Word2Vec embeddings for RNA k-mer segments using the following script:\r\n\r\n```bash\r\npython pretrain_RNA_kmer.py\r\n```\r\n\r\nThis script tokenizes RNA sequences into k-mers, performs sliding-window segmentation, pads them to a unified length, and trains Word2Vec embeddings for each RNA type (miRNA, circRNA, lncRNA, piRNA).  \r\nThe output includes:\r\n\r\n- `gensim_feat_<type>_<VECTOR_SIZE>.npy`: A dictionary containing\r\n  - k-mer embedding matrix\r\n  - padded k-mer ID sequences\r\n  - segment-to-sequence mapping\r\n  \r\nRun the programs in the `build dataset` folder sequentially to generate the cross-validation dataset.\r\n\r\n### 2.Model Training\r\nUse the processed similarity matrices and datasets to train the model:\r\n```bash\r\npython main.py\r\n```\r\nDue to the high memory/GPU usage of the network, please pass parameters when running main.py to control the network size according to your own computational resources.\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1878",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1878?version=1",
        "name": "PanGIA",
        "number_of_steps": 0,
        "projects": [
            "PanGIA"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2025-08-19",
        "versions": 1
    },
    {
        "create_time": "2025-08-10",
        "creators": [
            "Nikolay Oskolkov"
        ],
        "description": "# GENome EXogenous (GENEX) sequence detection\r\n\r\nThis is a computational workflow for detecting coordinates of microbial-like or human-like sequences in eukaryotic and procaryotic reference genomes. The workflow accepts a reference genome in FASTA-format and outputs coordinates of microbial-like (human-like) regions in BED-format. The workflow builds a Bowtie2 index of the reference genome and aligns pre-computed microbial (GTDB v.214 or NCBI RefSeq release 213) or human (hg38) pseudo-reads to the reference, then custom scripts are used for detection of the positions of covered regions and quantification of most abundant microbial species, the latter is only when screening for microbial-like sequences in eukaryotic referenes.\r\n\r\nThe workflow was developed by Nikolay Oskolkov, Lund University, Sweden, within the NBIS SciLifeLab long-term support project, PI Tom van der Valk, Centre for Palaeogenetics, Stockholm, Sweden.\r\n\r\nIf you use the workflow for your research, please cite our manuscript:\r\n\r\n    Nikolay Oskolkov, Chenyu Jin, Samantha L\u00f3pez Clinton, Flore Wijnands, Ernst Johnson, \r\n    Benjamin Guinet, Verena Kutschera, Cormac Kinsella, Peter D. Heintzman and Tom van der Valk, \r\n    Disinfecting eukaryotic reference genomes to improve taxonomic inference from environmental \r\n    ancient metagenomic data, https://www.biorxiv.org/content/10.1101/2025.03.19.644176v1, \r\n    https://doi.org/10.1101/2025.03.19.644176\r\n\r\nPlease note that in this gitub reporsitory, we provide a small subset of microbial pseudo-reads for demonstration purposes, the full dataset is available at the SciLifeLab Figshare https://doi.org/10.17044/scilifelab.28491956.\r\n\r\nQuestions regarding the dataset should be sent to nikolay.oskolkov@scilifelab.se\r\n\r\n## Quick start\r\n\r\nPlease clone this repository and install the workflow tools as follows:\r\n\r\n    git clone https://github.com/NikolayOskolkov/MCWorkflow\r\n    cd MCWorkflow\r\n    conda env create -f environment.yaml\r\n    conda activate MCWorkflow\r\n\r\nThen you can run the workflow as:\r\n\r\n    ./micr_cont_detect.sh GCF_002220235.fna.gz data GTDB 4 \\\r\n    GTDB_sliced_seqs_sliding_window.fna.gz 10\r\n\r\nHere, `GCF_002220235.fna.gz` is the eukaryotic reference to be screened for microbial-like sequeneces, `data` is the directory containing the eukaryotic reference, `GTDB` is the type of pseudo-reads to be used for detecting exogenous regions in the eukaryotic reference (can be `GTGB`, `RefSeq` or `human`), `4` is the number of available threads in your computational environment, `GTDB_sliced_seqs_sliding_window.fna.gz` is the pre-computed pseudo-reads (small subset is provided in this github repository, the full datasets can be downloaded from the SciLifeLab Figshare https://doi.org/10.17044/scilifelab.28491956), and `10` is the number of allowed Bowtie2 multi-mappers.\r\n\r\n\r\nPlease also read the very detailed `vignette.html` and follow the preparation steps described there. The vignette `vignette.html` walks you through the explanations of the workflow parameters and interpretation of the output files.\r\n\r\n\r\n\r\n## Nextflow implementation\r\n\r\nAlternatively, you can specify the workflow input files and parameters in the `nextflow.config` and run it using Nextflow:\r\n\r\n    nextflow run main.nf\r\n\r\nThe Nextflow implementation is preferred for scalability and reproducibility purposes. Please place your reference genomes (fasta-files) to be screened for exogenous regions in the `data` folder. An example of the config-file, `nextflow.config`, can look like this:\r\n\r\n    params {\r\n        input_dir = \"data\"                                             // folder with multiple reference genomes (fasta-files)\r\n        type_of_pseudo_reads = \"GTDB\"                                  // type of pseudo-reads to be used for screening the input reference genome, can be \"GTDB\", \"RefSeq\" or \"human\"\r\n        threads = 4                                                    // number of available threads\r\n        input_pseudo_reads = \"GTDB_sliced_seqs_sliding_window.fna.gz\"  // name of pre-computed file with pseudo-reads, can be \"GTDB_sliced_seqs_sliding_window.fna.gz\", \"RefSeq_sliced_seqs_sliding_window.fna.gz\" or \"human_sliced_seqs_sliding_window.fna.gz\"\r\n        n_allowed_multimappers = 10                                    // number of multi-mapping pseudo-reads allowed by Bowtie2, do not change this default number unless you know what you are doing\r\n    }\r\n\r\nPlease modify it to adjust for the number of available threads in your computational environment and the type of analysis, i.e. detecting microbial-like or human-like sequeneces in the reference genome, you would like to perform.\r\n",
        "doi": "10.48546/workflowhub.workflow.1846.1",
        "edam_operation": [
            "Alignment"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "1846",
        "keep": true,
        "latest_version": 1,
        "license": "CC0-1.0",
        "link": "https:/workflowhub.eu/workflows/1846?version=1",
        "name": "GENEX",
        "number_of_steps": 0,
        "projects": [
            "GENEX"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-19",
        "versions": 1
    },
    {
        "create_time": "2025-07-31",
        "creators": [
            "Peter van Heusden"
        ],
        "description": "Find and annotate variants in ampliconic SARS-CoV-2 Illumina sequencing data and classify samples with pangolin and nextclade",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in name",
        "id": "155",
        "keep": true,
        "latest_version": 6,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/155?version=6",
        "name": "sars-cov-2-pe-illumina-artic-ivar-analysis/SARS-COV-2-ILLUMINA-AMPLICON-IVAR-PANGOLIN-NEXTCLADE",
        "number_of_steps": 16,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "virology"
        ],
        "tools": [
            "fastp",
            "samtools_stats",
            "samtools_view",
            "snpeff_sars_cov_2",
            "__FLATTEN__",
            "qualimap_bamqc",
            "tp_sed_tool",
            "ivar_trim",
            "pangolin",
            "ivar_consensus",
            "nextclade",
            "ivar_variants",
            "tp_cat",
            "bwa_mem",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-19",
        "versions": 6
    },
    {
        "create_time": "2021-12-21",
        "creators": [
            "Wolfgang Maier"
        ],
        "description": "COVID-19: variation analysis on WGS PE data\r\n-------------------------------------------\r\n\r\nThis workflows performs paired end read mapping with bwa-mem followed by\r\nsensitive variant calling across a wide range of AFs with lofreq and variant\r\nannotation with snpEff 4.5covid19.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "113",
        "keep": true,
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/113?version=4",
        "name": "sars-cov-2-pe-illumina-wgs-variant-calling/COVID-19-PE-WGS-ILLUMINA",
        "number_of_steps": 11,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "virology",
            "covid-19",
            "covid19.galaxyproject.org",
            "emergen_validated"
        ],
        "tools": [
            "fastp",
            "samtools_stats",
            "samtools_view",
            "lofreq_filter",
            "snpeff_sars_cov_2",
            "lofreq_viterbi",
            "lofreq_call",
            "picard_MarkDuplicates",
            "lofreq_indelqual",
            "bwa_mem",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-19",
        "versions": 4
    },
    {
        "create_time": "2025-08-19",
        "creators": [],
        "description": "# Cloning simulation workflow for sequences present in DB\r\n\r\nRun the GoldenGate cloning simulation for a list of constructs in a CSV file and interact with a database\r\n\r\n## steps:\r\n\r\n_input: csv file (without header)_ : The **CSV** file should contain the constraints line by line in the first column, along with their associated fragments on each line. This data will be passed to the _seq_from_DB_ tool.\r\n\r\n>_workflow_1 Parameter_Maystro_\r\n\r\n1. Distribute workflow parameters on the workflow tools\r\n2. Parameters can be set as a JSON file in input (optional)\r\n3.  Parameters can be manually instate of JSON file\r\n\r\n>_seq_from_DB_\r\n\r\n1. Extract the fragments associated with each constraint from the CSV file.\r\n2. Check if all fragments are present in the database.\r\n\t\t\r\n>_evaluate_manufacturability_\r\n\t\t\r\n1. If any fragment is missing it will serve from the unannotated Genbenk provided to annotate them and add them to the finale gb collection\r\n2. If all fragments are present in the database, GenBank files for each fragment will be passed to the cloning_simulation tool.\r\n\r\n >_clonning_simulation_\r\n\r\n1. GoldenGate cloning simulation based on constraints.\r\n2. Generate GenBank files for each simulated constraint.\r\n\r\n>_seq_to_db_\r\n\r\n1. Constraint GenBank files can be saved to the database, depending on the user's choice.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1877",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1877?version=1",
        "name": "Workflow-1",
        "number_of_steps": 5,
        "projects": [
            "DNA Foundry"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "parameters_maystro_workflow_1",
            "seq_to_db",
            "evaluate_manufacturability",
            "cloning_simulation",
            "seq_form_db"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-19",
        "versions": 1
    },
    {
        "create_time": "2021-12-21",
        "creators": [
            "Wolfgang Maier"
        ],
        "description": "COVID-19: variation analysis on ARTIC ONT data\n----------------------------------------------\n\nThis workflow for ONT-sequenced ARTIC data is modeled after the alignment/variant-calling steps of the [ARTIC pipeline](https://artic.readthedocs.io/en/latest/). It performs, essentially, the same steps as that pipeline\u2019s minion command, i.e. read mapping with minimap2 and variant calling with medaka. Like the Illumina ARTIC workflow it uses ivar for primer trimming. Since ONT-sequenced reads have a much higher error rate than Illumina-sequenced reads and are therefor plagued more by false-positive variant calls, this workflow does make no attempt to handle amplicons affected by potential primer-binding site mutations.\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "111",
        "keep": true,
        "latest_version": 5,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/111?version=5",
        "name": "sars-cov-2-ont-artic-variant-calling/COVID-19-ARTIC-ONT",
        "number_of_steps": 22,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "artic",
            "ont",
            "covid-19",
            "covid19.galaxyproject.org"
        ],
        "tools": [
            "Add_a_column1",
            "qualimap_bamqc",
            "medaka_consensus",
            "fastp",
            "samtools_stats",
            "bamleftalign",
            "datamash_ops",
            "ivar_trim",
            "\n __FILTER_FAILED_DATASETS__",
            "multiqc",
            "lofreq_filter",
            "bedtools_intersectbed",
            "minimap2",
            "tp_find_and_replace",
            "\n __FLATTEN__",
            "bcftools_annotate",
            "samtools_view",
            "snpeff_sars_cov_2",
            "tp_replace_in_column",
            "\n param_value_from_file",
            "medaka_variant"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-19",
        "versions": 5
    },
    {
        "create_time": "2025-10-11",
        "creators": [
            "Viktoria Isabel Schwarz",
            "Wolfgang Maier"
        ],
        "description": "A workflow for the analysis of pox virus genomes sequenced as half-genomes (for ITR resolution) in a tiled-amplicon approach",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in name",
        "id": "439",
        "keep": true,
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/439?version=4",
        "name": "pox-virus-amplicon/main",
        "number_of_steps": 40,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "virology",
            "pox"
        ],
        "tools": [
            "qualimap_bamqc",
            "bwa_mem",
            "split_file_to_collection",
            "fasta_compute_length",
            "fastp",
            "samtools_stats",
            "samtools_merge",
            "datamash_ops",
            "ivar_trim",
            "\n __FILTER_FAILED_DATASETS__",
            "multiqc",
            "\n Cut1",
            "tp_sed_tool",
            "\n Grep1",
            "collection_element_identifiers",
            "ivar_consensus",
            "tp_cat",
            "\n __FLATTEN__",
            "\n __ZIP_COLLECTION__",
            "compose_text_param",
            "samtools_view",
            "\n __SORTLIST__",
            "EMBOSS: maskseq51",
            "\n __APPLY_RULES__",
            "\n param_value_from_file"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-11",
        "versions": 4
    },
    {
        "create_time": "2023-09-22",
        "creators": [
            "Romane Libouban"
        ],
        "description": "# RepeatMasking Workflow\n\nThis workflow uses RepeatModeler and RepeatMasker for genome analysis.\n\n- RepeatModeler is a software package for identifying and modeling de novo families of transposable elements (TEs). At the heart of RepeatModeler are three de novo repeat search programs (RECON, RepeatScout and LtrHarvest/Ltr_retriever) which use complementary computational methods to identify repeat element boundaries and family relationships from sequence data.\n\n- RepeatMasker is a program that analyzes DNA sequences for *interleaved repeats* and *low-complexity* DNA sequences. The result of the program is a detailed annotation of the repeats present in the query sequence, as well as a modified version of the query sequence in which all annotated repeats are present.\n\n## Input dataset for RepeatModeler\n- RepeatModeler requires a single input file, a genome in fasta format.\n\n\n## Outputs dataset for RepeatModeler\n- Two output files are generated:\n    - summary file (.tbl)\n    - fasta file containing alignments in order of appearance in the query sequence\n\n\n## Input dataset for RepeatMasker\n- ReapatMasker requires the fasta file generated by RepeatModeler\n\n## Outputs datasets for RepeatMasker\n- Five output files are generated:\n    - a fasta file\n    - .gff3 file\n    - a table summarizing the repeated content of the sequence analyzed\n    - a file with statistics related to the repeated content of the sequence analyzed\n    - a summary of the mutation sites found and the order of grouping\n    \n",
        "doi": "10.48546/workflowhub.workflow.575.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "575",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/575?version=1",
        "name": "repeatmasking/main",
        "number_of_steps": 2,
        "projects": [
            "Intergalactic Workflow Commission (IWC)",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "repeatmodeler",
            "repeatmasker_wrapper"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2025-07-19",
        "creators": [
            "Romane Libouban"
        ],
        "description": "This workflow allows you to annotate a genome with Helixer and evaluate the quality of the annotation using BUSCO and Genome Annotation statistics. GFFRead is also used to predict protein sequences derived from this annotation, and BUSCO and OMArk are used to assess proteome quality. ",
        "doi": "10.48546/workflowhub.workflow.1255.3",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "1255",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1255?version=3",
        "name": "annotation-helixer/main",
        "number_of_steps": 7,
        "projects": [
            "Intergalactic Workflow Commission (IWC)",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "gffread",
            "jcvi_gff_stats",
            "omark",
            "jbrowse",
            "helixer",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 3
    },
    {
        "create_time": "2025-01-31",
        "creators": [
            "Romane Libouban",
            "Anthony Bretaudeau"
        ],
        "description": "This workflow uses eggNOG mapper and InterProScan for functional annotation of protein sequences.",
        "doi": "10.48546/workflowhub.workflow.1262.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "1262",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1262?version=1",
        "name": "functional-annotation-protein-sequences/main",
        "number_of_steps": 2,
        "projects": [
            "Intergalactic Workflow Commission (IWC)",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "eggnog_mapper",
            "interproscan"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Romane Libouban"
        ],
        "description": "This workflow allows for genome annotation using Maker and evaluates the quality of the annotation.",
        "doi": "10.48546/workflowhub.workflow.1323.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "1323",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1323?version=1",
        "name": "annotation-maker/main",
        "number_of_steps": 8,
        "projects": [
            "Intergalactic Workflow Commission (IWC)",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "maker_map_ids",
            "gffread",
            "jcvi_gff_stats",
            "jbrowse",
            "maker",
            "fasta-stats",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Romane Libouban"
        ],
        "description": "This workflow runs the FEELnc tool to annotate long non-coding RNAs. Before annotating these long non-coding RNAs, StringTie will be used to assemble the RNA-seq alignments into potential trancriptions. The gffread tool provides a genome annotation file in GTF format.",
        "doi": "10.48546/workflowhub.workflow.1324.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "1324",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1324?version=1",
        "name": "lncRNAs-annotation/main",
        "number_of_steps": 5,
        "projects": [
            "Intergalactic Workflow Commission (IWC)",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "gffread",
            "stringtie",
            " The lncRNA annotation is merged with the reference annotation to create a unified genome annotation containing both mRNAs and lncRNAs.\ncat1",
            "map_param_value",
            "feelnc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2025-08-18",
        "creators": [],
        "description": "This WF applies Flexynesis on BRCA data from Metabric for a classification task\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Modeling Breast Cancer Subtypes with Flexynesis](https://training.galaxyproject.org/training-material/topics/statistics/tutorials/flexynesis_classification/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n* Uses [subworkflows](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_subworkflows.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Amirhossein Naghsh Nilchi\n\n**Tutorial Author(s)**: [Amirhossein Naghsh Nilchi](https://training.galaxyproject.org/training-material/hall-of-fame/Nilchia/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1873",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1873?version=1",
        "name": "Final - Modeling Breast Cancer Subtypes + TABPFN",
        "number_of_steps": 19,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "flexynesis",
            "gtn",
            "galaxy",
            "machine_learning",
            "tabpfn"
        ],
        "tools": [
            "",
            "__EXTRACT_DATASET__",
            "flexynesis",
            "tabpfn",
            "flexynesis_plot",
            "pick_value",
            "tp_sort_header_tool"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2025-08-17",
        "creators": [
            "Abao Xing"
        ],
        "description": "**[MRanalysis](https://mranalysis.cn/)** is an interactive R Shiny application designed for Mendelian randomization analysis.\r\n\r\n# MRanalysis\r\n\r\n**Mendelian randomization** (MR) has emerged as a powerful epidemiological method for inferring causal relationships between exposures and outcomes using **genome-wide association study** (GWAS) summary data. By leveraging instrumental variables (IVs), such as single nucleotide polymorphisms (SNPs), MR can revolutionize our understanding of disease etiology, inform public health strategies, and accelerate drug discovery.\r\n\r\nHowever, the widespread adoption of MR is hindered by several challenges, including inconsistent GWAS data formats, lack of standardized workflows, the need for extensive programming expertise, and limitations in data visualization and interpretability. To address these challenges, we introduce **MRanalysis** , a comprehensive and user-friendly, web-based platform that provides the first integrated and standardized MR analysis workflow. This includes GWAS data quality assessment, power/ sample size estimation, MR analysis, SNP-to-gene enrichment analysis, and data visualization. **Built using the R shiny framework, MRanalysis enables users to conduct common MR methods, including univariable, multivariable, and mediation MR analyses through an intuitive, no-code interface** .\r\n\r\nBesides MRanalysis, we developed **GWASkit** , a standalone, and installation-free tool facilitating rapid GWAS dataset preprocessing before MR analyses, including rs ID conversion, format standardization, and data extraction, with significantly lower conversion time and dramatically higher rs ID conversion accuracy than the current tools. Case studies demonstrate the utility, efficiency, and ease of use of our developed platform and GWASkit tool in real-world scenarios. By lowering barriers to investigating causal genetic relationships, our platform represents a significant advance in making MR more accessible, reliable, and efficient. The increased adoption of MR, facilitated by MRanalysis and GWASkit, can accelerate discoveries in genetic epidemiology, strengthen evidence-based public health strategies, and guide the development of targeted clinical interventions.\r\n\r\n# Installation\r\n\r\n## Use Docker (RECOMMENDATION)\r\n\r\nMRanalysis is delivered as Docker images for consistent installations and executions to minimize any potential issues from user environment. **As such, a Docker running environment is required**. For Docker engine installation, user is referred to the Docker web site [https://docs.docker.com/install/](https://docs.docker.com/install/).\r\n\r\nIf a Docker running environment is not already available on the system, it will need to be installed. Docker is available in two editions: Community Edition (CE) and Enterprise Edition (EE). The following is an example for getting and installing Docker CE for Ubuntu/Debian systems. If a Docker running environment is already available on the system, these steps can be skipped and only the MRanalysis docker image would need to be installed.\r\n\r\n### Step 1: Upload Software Repositories\r\n\r\nAs usual, it is a good idea to update the local database of software to make sure you've got access to the latest revisions.\r\n\r\nTherefore, open a terminal window and type:\r\n\r\n```shell\r\nsudo apt-get update\r\n```\r\n\r\nAllow the operation to complete.\r\n\r\n### Step2: Uninstall Old Versions of Docker\r\n\r\nNext, it's recommended to uninstall any old Docker software before proceeding.\r\n\r\nUse the command:\r\n\r\n```shell\r\nsudo apt-get remove docker docker-engine docker.io \r\n```\r\n\r\n### Step 3: Install Docker \r\n\r\nTo install Docker on Ubuntu, in the terminal window enter the command:\r\n\r\n```shell\r\nsudo apt-get install docker.io \r\n```\r\n\r\n### Step 4: Start and Automate Docker\r\n\r\nThe Docker service needs to be set up to run at startup. To do this, type in each command followed by enter:\r\n\r\n```shell\r\nsudo systemctl start docker\r\nsudo systemctl enable docker \r\n```\r\n\r\n### Step 5: Running Docker as a non-root user\r\n\r\nIf you don't want to preface the `docker` command with `sudo`, create a Unix group called docker and add user to it:\r\n\r\n```shell\r\nsudo groupadd docker\r\nsudo usermod -aG docker $USER \r\n```\r\n\r\n### Step 6: Log out and log back in\r\n\r\nAfter logging back in, run Docker as a non-root user.\r\n\r\nAfter the installation of Docker of if you already have Docker environment, follow the steps below to install the MRanalysis docker image.\r\n\r\n```shell\r\n# Download the Docker image\r\nwget https://mranalysis.cn/mranalysis.tar\r\n\r\n# Load the downloaded Docker image\r\ndocker load -i mranalysis.tar\r\n```\r\n\r\n```shell\r\n# Or use docker pull command pull image from docker Hub directly\r\ndocker pull xingabao/mranalysis:latest\r\n```\r\n\r\n### Run MRanalysis Container\r\n\r\nMount your reference data directory to the container. Ensure you have downloaded the required reference datasets (see [References](#References)).\r\n\r\n```shell\r\ndocker run -itd --rm -p 8001:8001 --name mranalysis \\\r\n -v /path/to/your/references:/references \\\r\n -v /path/to/your/.Renviron:/home/shiny/.Renviron \\\r\n xingabao/mranalysis:latest\r\n```\r\n\r\nAccess the platform at [http://127.0.0.1:8001](http://127.0.0.1:8001)\r\n\r\n## Step-by-step manual installation\r\n\r\nFollow these steps to manually install all required system dependencies, R packages, and third-party tools. Test in **Ubuntu:24.04**.\r\n\r\n### Install R Environment\r\n\r\nFirst, install R. On Ubuntu, run:\r\n\r\n```shell\r\nsudo apt-get update\r\nsudo apt-get install -y r-base r-base-dev\r\n```\r\n\r\nFor the latest R version, you may add the [CRAN repository](https://cran.r-project.org/bin/linux/ubuntu/) before installing. See the [official CRAN instructions](https://cran.r-project.org/bin/linux/ubuntu/) for details.\r\n\r\n### Set Up Shiny Server on Ubuntu 24.04\r\n\r\n```shell\r\nsudo apt-get install -y gdebi-core\r\nwget https://download3.rstudio.org/ubuntu-18.04/x86_64/shiny-server-1.5.20.1002-amd64.deb\r\nsudo gdebi shiny-server-1.5.20.1002-amd64.deb\r\n```\r\n\r\n```shell\r\n# Start, stop, or restart Shiny Server.\r\nsudo systemctl start shiny-server\r\nsudo systemctl stop shiny-server\r\nsudo systemctl restart shiny-server\r\n```\r\n\r\nSee details in https://posit.co/download/shiny-server/\r\n\r\n### Install System Dependencies\r\n\r\nOpen a terminal and run:\r\n\r\n```shell\r\nsudo apt-get update\r\nsudo apt-get install -y \\\r\n    cmake libxml2-dev libssl-dev libgmp-dev libharfbuzz-dev \\\r\n    libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev \\\r\n    git bzip2 libgsl-dev libglpk-dev libigraph-dev gcc-9 g++-9\r\n```\r\n\r\nConfigure GCC alternatives:\r\n\r\n```shell\r\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 90 --slave /usr/bin/g++ g++ /usr/bin/g++-9\r\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-13 130 --slave /usr/bin/g++ g++ /usr/bin/g++-13\r\n```\r\n\r\n### Install R Packages\r\n\r\nInstall required CRAN packages:\r\n\r\n```shell\r\nRscript -e 'install.packages(c(\r\n  \"purrr\", \"tibble\", \"reactable\", \"ggplot2\", \"ggthemes\", \"ggdag\",\r\n  \"ggpubr\", \"ggtext\", \"venn\", \"shinymeta\", \"esquisse\", \"openxlsx\",\r\n  \"bs4Dash\", \"shinyalert\", \"shinyjs\", \"stringr\", \"spsComps\", \"colourpicker\",\r\n  \"shinyvalidate\", \"shinyjqui\", \"sysfonts\", \"extrafont\", \"shinydashboard\",\r\n  \"shinycssloaders\", \"formattable\", \"MendelianRandomization\", \"DT\",\r\n  \"devtools\", \"BiocManager\", \"formatR\", \"coloc\", \"gdata\", \"logger\",\r\n  \"Rfast\", \"Rmpfr\", \"pheatmap\"\r\n))'\r\n```\r\n\r\nInstall required Bioconductor packages:\r\n\r\n```shell\r\nRscript -e 'BiocManager::install(c(\r\n  \"clusterProfiler\", \"org.Hs.eg.db\", \"ComplexHeatmap\",\r\n  \"VariantAnnotation\", \"MungeSumstats\"\r\n), update = FALSE, ask = FALSE)'\r\n```\r\n\r\n### Install Additional R Packages from GitHub\r\n\r\nSet up your `GITHUB_PAT` environment variable if necessary.\r\n\r\nInstall specific versions and GitHub packages (you must fill in the actual download URLs):\r\n\r\n```shell\r\n# Install other GitHub packages\r\nRscript -e 'devtools::install_github(\"MRCIEU/CheckSumStats\")'\r\nRscript -e 'devtools::install_github(\"qingyuanzhao/mr.raps\")'\r\nRscript -e 'devtools::install_github(\"mrcieu/gwasglue\")'\r\nRscript -e 'devtools::install_github(\"WSpiller/MVMR\", build_opts = c(\"--no-resave-data\", \"--no-manual\"), build_vignettes = FALSE)'\r\nRscript -e 'devtools::install_github(\"jrs95/geni.plots\", build_vignettes = FALSE)'\r\nRscript -e 'devtools::install_github(\"boxiangliu/locuscomparer\")'\r\nRscript -e 'remotes::install_version(\"RcppEigen\", version = \"0.3.3.9.3\")'\r\nRscript -e 'remotes::install_github(\"jrs95/hyprcoloc\", build_vignettes = FALSE)'\r\nRscript -e 'remotes::install_github(\"xingabao/MRanalysisBase\")'\r\nRscript -e 'remotes::install_github(\"xingabao/GWASkitR\")'\r\n```\r\n\r\n### Install Third-Party Tools\r\n\r\n[PWCoCo](https://github.com/jwr-git/pwcoco), Pair-wise conditional analysis and colocalisation.\r\n\r\n```shell\r\nmkdir -p /tools\r\ncd /tools\r\ngit clone https://github.com/jwr-git/pwcoco.git\r\ncd pwcoco\r\nmkdir build && cd build\r\ncmake ..\r\nmake\r\n```\r\n\r\n[CAVIAR](https://github.com/fhormoz/caviar), a statistical framework that quantifies the probability of each variant to be causal while allowing with arbitrary number of causal variants.\r\n\r\n```shell\r\ncd /tools\r\ngit git clone https://github.com/fhormoz/caviar.git\r\ncd caviar/CAVIAR-C++\r\nsudo update-alternatives --set gcc /usr/bin/gcc-9\r\nmake\r\n```\r\n\r\n[bcftools](https://github.com/samtools/bcftools)\r\n\r\n```shell\r\nwget -O /tmp/bcftools-1.22.tar.bz2 https://github.com/samtools/bcftools/releases/download/1.22/bcftools-1.22.tar.bz2\r\ntar -xvjf /tmp/bcftools-1.22.tar.bz2 -C /tools\r\ncd /tools/bcftools-1.22\r\n./configure --prefix=/tools/bcftools-1.22\r\nsudo update-alternatives --set gcc /usr/bin/gcc-13\r\nmake -j\r\nmake install\r\n```\r\n\r\n[PLINK ](https://www.cog-genomics.org/plink/) is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner.\r\n\r\n```shell\r\nwget -O /tmp/plink_linux_x86_64.zip https://s3.amazonaws.com/plink1-assets/dev/plink_linux_x86_64.zip\r\nmkdir -p /tools/plink\r\nunzip /tmp/plink_linux_x86_64.zip -d /tools/plink\r\n```\r\n\r\n[MAGMA](https://cncr.nl/research/magma/): Generalized Gene-Set Analysis of GWAS Data\r\n\r\n```shell\r\nwget -O /tmp/magma_v1.10.zip https://vu.data.surfsara.nl/index.php/s/zkKbNeNOZAhFXZB/download \r\nmkdir -p /tools/magma_v1.10\r\nunzip /tmp/magma_v1.10.zip -d /tools/magma_v1.10\r\n```\r\n\r\nMost commands require root or sudo privileges.\r\n\r\nFor additional configuration or troubleshooting, please refer to the documentation of each individual tool.\r\n\r\n### Install and Run MRanalysis\r\n\r\n```shell\r\n# 1. Change permissions for the Shiny Server directory\r\nchmod 777 /srv/shiny-server/\r\n\r\n# 2. Move into the Shiny Server web directory\r\ncd /srv/shiny-server/\r\n\r\n# 3. Clone the MRanalysis repository\r\ngit clone https://github.com/xingabao/MRanalysis.git\r\n\r\n# 4. Move all contents (including .git) to /srv/shiny-server root\r\nmv MRanalysis/* .\r\nmv MRanalysis/.git .\r\n\r\n# 5. Copy required extdata files for MRanalysisBase\r\ncp -r /usr/local/lib/R/site-library/MRanalysisBase/extdata /srv/shiny-server/XINGABAO\r\n\r\n# 6. Restart Shiny Server to apply changes\r\nsudo systemctl restart shiny-server\r\n```\r\n\r\nAccess the platform at [http://127.0.0.1:3838](http://127.0.0.1:3838)\r\n\r\n# References\r\n\r\nThe following reference datasets and files are required for the correct functioning of the tools in this pipeline. Please download them manually from their official sources or your institutional repositories.\r\n\r\n## PLINK Reference Panel: **plink/1kg.v3**\r\n\r\n 1000 Genomes Phase 3 reference panel in PLINK binary format (`.bed`, `.bim`, `.fam`). Used for quality control, LD pruning, and as a reference for genetic analyses. Please download from [http://fileserve.mrcieu.ac.uk/ld/1kg.v3.tgz](http://fileserve.mrcieu.ac.uk/ld/1kg.v3.tgz).\r\n\r\nAfter downloading, please extract the archive into the `/references` directory, as shown below. This contains an LD reference panel for each of the 5 super-populations in the 1000 genomes reference dataset. e.g. for the European super population it has the following files: `EUR.bed`, `EUR.bim`, `EUR.fam`.\r\n\r\n## MAGMA Reference Files\r\n\r\nThe **MAGMA reference directory** should contain the following three subdirectories, as shown below. `bfile`:  Contains binary reference panel files in PLINK format (**.bed**, **.bim**, **.fam**). `msigdb`:  Contains gene set files from the Molecular Signatures Database (MSigDB). `reference`:  Contains additional MAGMA reference files, such as gene annotation or location files.\r\n\r\na. `/references/MAGMA/bfile`. Binary reference panel files for MAGMA, typically in PLINK format (`.bed`, `.bim`, `.fam`). Used for gene and gene-set analysis in MAGMA. Please download from [https://cncr.nl/research/magma/](https://cncr.nl/research/magma/).\r\n\r\nb. `/references/MAGMA/reference`. Additional MAGMA reference files, such as annotation and gene location files. Used for mapping SNPs to genes and other MAGMA analyses. Please download from [https://cncr.nl/research/magma/](https://cncr.nl/research/magma/).\r\n\r\nc. `/references/MAGMA/msigdb`. Molecular Signatures Database (MSigDB) gene sets. Used in MAGMA gene-set enrichment analyses. Download from the [MSigDB](https://www.gsea-msigdb.org/gsea/msigdb) website.\r\n\r\n# How to use MRanalysis\r\n\r\n**MRanalysis** is an integrated online platform for Mendelian Randomization (MR) analysis and post-GWAS (Genome-Wide Association Study) workflows. It enables users -- especially those without programming experience -- to easily perform data preprocessing, various MR analyses, and rich result visualizations.\r\n\r\n## Data Preprocessing\r\n\r\n- **Using your own data:**  \r\n  If you have GWAS summary statistics (CSV/TSV/VCF), it is recommended to preprocess the files using [GWASkit](https://github.com/Li-OmicsLab-MPU/GWASkit). GWASkit helps with tasks such as:\r\n  - Format conversion (CSV/TSV \u2194 VCF)\r\n  - rs ID mapping (CHR:POS:REF:ALT \u2194 rsID)\r\n  - Standardizing GWAS data for MRanalysis\r\n\r\n- **Using public datasets:**  \r\n  MRanalysis can directly access and import GWAS summary data from public resources like the IEU OpenGWAS database via API.\r\n\r\n## Quality Control (QC)\r\n\r\nBefore analysis, use the platform's QC tools to ensure data quality:\r\n- **CheckSumStats:** Detect metadata errors and allele frequency inconsistencies.\r\n- **Q-Q Plot and Manhattan Plot:** Visualize P-value distributions and check for significant associations or anomalies.\r\n\r\n## MR Analysis\r\n\r\nMRanalysis supports several MR analysis modes:\r\n- **Univariable MR:** Standard exposure-outcome causal inference.\r\n- **Multivariable MR:** Analyze multiple exposures simultaneously.\r\n- **Mediation (Two-step) MR:** Assess mediation effects and indirect pathways.\r\n\r\nYou can upload your own preprocessed data or use built-in API mode to select public GWAS datasets. All analysis steps are code-free and parameters are user-friendly.\r\n\r\n ## Visualization and Results Interpretation\r\n\r\n- Multiple visualization options available (forest plot, DAG, bar/dot/circos plot, etc.) for intuitive result presentation.\r\n- SNP-to-gene mapping and GO/KEGG enrichment analysis are supported for functional interpretation.\r\n- The platform provides complete code for every analysis, enabling transparency and reproducibility.\r\n\r\n ## Tutorials and Help\r\n\r\n- Each function comes with **vedio** tutorials for step-by-step guidance.\r\n\r\n- Example datasets are available for practice and demonstration.\r\n\r\n##  Access\r\n\r\n- **Online platform:** [https://mranalysis.cn](https://mranalysis.cn)\r\n- **GWASkit tool & documentation:** [https://github.com/Li-OmicsLab-MPU/GWASkit](https://github.com/Li-OmicsLab-MPU/GWASkit)\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.1872.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1872",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1872?version=1",
        "name": "MRanalysis: A Comprehensive Online Platform for Integrated,Multi-MethodMendelian Randomization and Associated Post-GWAS Analyses",
        "number_of_steps": 0,
        "projects": [
            "Li-Omics Lab"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Docker",
        "update_time": "2025-08-17",
        "versions": 1
    },
    {
        "create_time": "2025-08-13",
        "creators": [],
        "description": "# Imputation Workflow h3abionet/chipimputation\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow-%E2%89%A520.04.0-brightgreen.svg)](https://www.nextflow.io/)\r\n[![Docker](https://img.shields.io/badge/docker%20registry-Quay.io-red)](https://quay.io/h3abionet_org/imputation_tools)\r\n[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8B%20%20%E2%97%8B%20%20%E2%97%8B-orange)](https://fair-software.eu)\r\n\r\n## Introduction\r\nImputation is likely to be run in the context of a GWAS, studying population structure, and admixture studies. It is computationally expensive in comparison to other GWAS steps.\r\nThe basic steps of the pipeline is described in the diagram below:\r\n\r\n![chipimputation pipeline workflow diagram](https://www.h3abionet.org/images/workflows/snp_imputation_workflow.png)\r\n\r\n* The workflow is developed using [![Nextflow](https://img.shields.io/badge/nextflow-%E2%89%A520.04.0-brightgreen.svg)](https://www.nextflow.io/) and imputation performed using [Minimac4](https://genome.sph.umich.edu/wiki/Minimac4). \r\n* It identifies regions to be imputed on the basis of an input file in VCF format, split the regions into small chunks, phase each chunk using the phasing tool [Eagle2](https://data.broadinstitute.org/alkesgroup/Eagle/) and produces output in VCF format that can subsequently be used in a [GWAS](https://github.com/h3abionet/h3agwas) workflow.\r\n* It also produce basic plots and reports of the imputation process including the imputation performance report, the imputation accuracy, the allele frequency of the imputed vs of the reference panel and other metrics.    \r\n\r\n**This pipeline comes with docker/singularity containers making installation trivial and results highly reproducible.**\r\n\r\n\r\n\r\n## Getting started\r\n\r\n### Running the pipeline with test dataset\r\nThis pipeline itself needs no installation - NextFlow will automatically fetch it from GitHub.\r\nYou can run the pipeline using test data hosted in github with singularity without have to install or change any parameters.\r\n\r\n```\r\nnextflow run h3abionet/chipimputation/main.nf -profile test,singularity\r\n```\r\n\r\n- `test` profile will download the testdata from [here](https://github.com/h3abionet/chipimputation_test_data/tree/master/testdata_imputation)\r\n- `singularity` profile will download the singularity image from [quay registry](https://quay.io/h3abionet_org/imputation_tools)\r\n\r\nCheck for results in `./output`\r\n\r\n\r\n### Start running your own analysis\r\n\r\nCopy the `test.config` file from the `conf` folder by doing `cp <conf dir>/test.config .` and edit it to suit the path to where your files are stored.\r\n\r\nOnce you have edited the config file, run the command below.\r\n\r\n```bash\r\nnextflow run h3abionet/chipimputation -c \"name of your config file\" -profile singularity\r\n```\r\n\r\n- `singularity` profile will download the singularity image from [quay registry](https://quay.io/h3abionet_org/imputation_tools)\r\n\r\nCheck for results in `./output`\r\n\r\n\r\n## Documentation\r\nThe h3achipimputation pipeline comes with detailed documentation about the pipeline.\r\nThis is found in the `docs/` directory:\r\n\r\n1. [Installation](docs/installation.md)\r\n2. [Pipeline configuration](docs/configuration/config_files.md)  \r\n    2.1. [Configuration files](docs/configs.md)  \r\n    2.2. [Software requirements](docs/soft_requirements.md)  \r\n    2.3. [Other clusters](docs/other_clusters.md)  \r\n3. [Running the pipeline with test data](docs/usage.md)\r\n4. [Running the pipeline with your own config](docs/usage.md)\r\n5. [Running on local machine or cluster](docs/other_clusters.md)\r\n6. [Running docker and singularity](docs/soft_requirements.md)\r\n\r\n\r\n## Support\r\nWe track our open tasks using github's [issues](https://github.com/h3abionet/chipimputation/issues)\r\n\r\n\r\n## Citation\r\nThis  workflow which was developed as part of the H3ABioNet Hackathon held in Pretoria, SA in 2016. Should want to reference it, please use:  \r\n>Baichoo S, Souilmi Y, Panji S, Botha G, Meintjes A, Hazelhurst S, Bendou H, Beste E, Mpangase PT, Souiai O, Alghali M, Yi L, O'Connor BD, Crusoe M, Armstrong D, Aron S, Joubert F, Ahmed AE, Mbiyavanga M, Heusden PV, Magosi LE, Zermeno J, Mainzer LS, Fadlelmola FM, Jongeneel CV, Mulder N. Developing reproducible bioinformatics analysis workflows for heterogeneous computing environments to support African genomics. BMC Bioinformatics. 2018 Nov 29;19(1):457. doi: 10.1186/s12859-018-2446-1. PubMed PMID: 30486782; PubMed Central PMCID: [PMC6264621](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6264621/).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1871",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1871?version=1",
        "name": "chipimputation",
        "number_of_steps": 0,
        "projects": [
            "BioX Fanatics"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-13",
        "versions": 1
    },
    {
        "create_time": "2025-08-13",
        "creators": [
            "Phil Ewels"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-methylseq_logo_dark.png\">\n    <img alt=\"nf-core/methylseq\" src=\"docs/images/nf-core-methylseq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/methylseq/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/methylseq/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/methylseq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/methylseq/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/methylseq/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.1343417-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.1343417)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/methylseq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23methylseq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/methylseq)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/methylseq** is a bioinformatics analysis pipeline used for Methylation (Bisulfite) sequencing data. It pre-processes raw data from FastQ inputs, aligns the reads and performs extensive quality-control on the results.\n\n![nf-core/methylseq metro map](docs/images/4.0.0_metromap.png)\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker / Singularity / Podman / Charliecloud / Apptainer containers making installation trivial and results highly reproducible.\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources.The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/methylseq/results).\n\n> Read more about **Bisulfite Sequencing & Three-Base Aligners** used in this pipeline [here](docs/usage/bs-seq-primer.md)\n\n## Pipeline Summary\n\nThe pipeline allows you to choose between running either [Bismark](https://github.com/FelixKrueger/Bismark) or [bwa-meth](https://github.com/brentp/bwa-meth) / [MethylDackel](https://github.com/dpryan79/methyldackel).\n\nChoose between workflows by using `--aligner bismark` (default, uses bowtie2 for alignment), `--aligner bismark_hisat` or `--aligner bwameth`. For higher performance, the pipeline can leverage the [Parabricks implementation of bwa-meth (fq2bammeth)](https://docs.nvidia.com/clara/parabricks/latest/documentation/tooldocs/man_fq2bam_meth.html), which implements the baseline tool `bwa-meth` in a performant method using fq2bam (BWA-MEM + GATK) as a backend for processing on GPU. To use this option, include the `gpu` profile along with `--aligner bwameth`.\n\nNote: For faster CPU runs with BWA-Meth, enable the BWA-MEM2 algorithm using `--use_mem2`. The GPU pathway (Parabricks) requires `-profile gpu` and a container runtime (Docker, Singularity, or Podman); Conda/Mamba are not supported for the GPU module.\n\n| Step                                         | Bismark workflow         | bwa-meth workflow     |\n| -------------------------------------------- | ------------------------ | --------------------- |\n| Generate Reference Genome Index _(optional)_ | Bismark                  | bwa-meth              |\n| Merge re-sequenced FastQ files               | cat                      | cat                   |\n| Raw data QC                                  | FastQC                   | FastQC                |\n| Adapter sequence trimming                    | Trim Galore!             | Trim Galore!          |\n| Align Reads                                  | Bismark (bowtie2/hisat2) | bwa-meth              |\n| Deduplicate Alignments                       | Bismark                  | Picard MarkDuplicates |\n| Extract methylation calls                    | Bismark                  | MethylDackel          |\n| Sample report                                | Bismark                  | -                     |\n| Summary Report                               | Bismark                  | -                     |\n| Alignment QC                                 | Qualimap _(optional)_    | Qualimap _(optional)_ |\n| Sample complexity                            | Preseq _(optional)_      | Preseq _(optional)_   |\n| Project Report                               | MultiQC                  | MultiQC               |\n\nOptional targeted sequencing analysis is available via `--run_targeted_sequencing` and `--target_regions_file`; see the [usage documentation](https://nf-co.re/methylseq/usage) for details.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2,genome\nSRR389222_sub1,https://github.com/nf-core/test-datasets/raw/methylseq/testdata/SRR389222_sub1.fastq.gz,,\nSRR389222_sub2,https://github.com/nf-core/test-datasets/raw/methylseq/testdata/SRR389222_sub2.fastq.gz,,\nSRR389222_sub3,https://github.com/nf-core/test-datasets/raw/methylseq/testdata/SRR389222_sub3.fastq.gz,,\nEcoli_10K_methylated,https://github.com/nf-core/test-datasets/raw/methylseq/testdata/Ecoli_10K_methylated_R1.fastq.gz,https://github.com/nf-core/test-datasets/raw/methylseq/testdata/Ecoli_10K_methylated_R2.fastq.gz,\n```\n\n> Each row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\nNow, you can run the pipeline using default parameters as:\n\n```bash\nnextflow run nf-core/methylseq --input samplesheet.csv --outdir <OUTDIR> --genome GRCh37 -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/methylseq/usage) and the [parameter documentation](https://nf-co.re/methylseq/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/methylseq/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the [output documentation](https://nf-co.re/methylseq/output).\n\n## Credits\n\nnf-core/methylseq was originally written by Phil Ewels ([@ewels](https://github.com/ewels)), and Sateesh Peri ([@sateeshperi](https://github.com/sateeshperi)) is its active maintainer.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- Felix Krueger ([@FelixKrueger](https://github.com/FelixKrueger))\n- Edmund Miller ([@EMiller88](https://github.com/emiller88))\n- Rickard Hammar\u00e9n ([@Hammarn](https://github.com/Hammarn/))\n- Alexander Peltzer ([@apeltzer](https://github.com/apeltzer/))\n- Patrick H\u00fcther ([@phue](https://github.com/phue/))\n- Maxime U Garcia ([@maxulysse](https://github.com/maxulysse/))\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#methylseq` channel](https://nfcore.slack.com/channels/methylseq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/methylseq for your analysis, please cite it using the following doi: [10.5281/zenodo.1343417](https://doi.org/10.5281/zenodo.1343417)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "998",
        "keep": true,
        "latest_version": 20,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/998?version=20",
        "name": "nf-core/methylseq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "epigenomics",
            "bisulfite-sequencing",
            "dna-methylation",
            "em-seq",
            "epigenome",
            "methyl-seq",
            "pbat",
            "rrbs"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-13",
        "versions": 20
    },
    {
        "create_time": "2025-08-12",
        "creators": [],
        "description": "# VIsoQLR: an interactive tool for the detection, quantification and fine-tuning of isoforms using long-read sequencing\r\n\r\nVIsoQLR is an interactive analyzer, viewer and editor for the semi-automated identification and quantification of known and novel isoforms using long-read sequencing data. VIsoQLR is tailored to thoroughly analyze mRNA expression and maturation in low-throughput splicing assays. This tool takes sequences aligned to a reference, defines consensus splice sites, and quantifies isoforms. Users can edit splice sites through dynamic and interactive graphics and tables as part of their manual curation. Known transcripts, or isoforms detected by other methods, can also be imported as references for comparison.\r\n\r\nA command line version is available at https://github.com/TBLabFJD/Mini-IsoQLR\r\n\r\n#### Cite this article\r\nTo know more about VIsoQLR have a look at the article in Human Genetics: https://link.springer.com/article/10.1007/s00439-023-02539-z\r\n\r\nN\u00fa\u00f1ez-Moreno G, Tamayo A, Ruiz-S\u00e1nchez C, Cort\u00f3n M, M\u00ednguez P. VIsoQLR: an interactive tool for the detection, quantification and fine-tuning of isoforms in selected genes using long-read sequencing. Hum Genet. 2023 Mar 7. doi: 10.1007/s00439-023-02539-z. Epub ahead of print. PMID: 36881176.\r\n\r\n\r\n## Check our video tutorial\r\n[![IMAGE ALT TEXT](http://img.youtube.com/vi/5fBis04A_WA/0.jpg)](http://www.youtube.com/watch?v=5fBis04A_WA \"VIsoQLR: an interactive tool for the detection and quantification of isoforms using long-read seq\")\r\n\r\n## Developers\r\n### Main developers\r\n - Gonzalo N\u00fa\u00f1ez Moreno\r\n\r\n### Contact\r\n - Gonzalo N\u00fa\u00f1ez Moreno (gonzalo.nunezm@quironsalud.es)\r\n\r\n\r\n\r\n## License\r\nMini-IsoQLR source code is provided under the [**Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)**](https://creativecommons.org/licenses/by-nc-sa/4.0/). Mini-IsoQLR includes several third party packages provided under other open source licenses, please check them for additional details.\r\n\r\n\r\n\r\n## Installation\r\nVIsoQLR has been wrapped into a Docker image. To set it up:\r\n\r\n 1. Install Docker following the instructions in https://www.docker.com/. Docker is available for Linux, Windows and Mac users. For windows users, as the image has been developed for Linux kernel, it is necessary to install Linux on Windows with WSL2 (Windows Subsystem for Linux) following the instructions in https://learn.microsoft.com/en-us/windows/wsl/install.\r\n 2. Once installed type on your shell (or Windows PowerShell in Windows once Docker Desktop has been initialized):\r\n    ```\r\n    docker pull tblabfjd/visoqlr:latest\r\n    ```\r\n    This will download the latest version of VIsoQLR image. This image has already all programs, packages and requirements needed to run VIsoQLR\r\n\r\n\r\n\r\n## Run VIsoQLR\r\nTo run VIsoQLR type on your shell (or Windows PowerShell in Windows once Docker Desktop has been initialized):\r\n```\r\ndocker run -it -p 8888:8888 tblabfjd/visoqlr:latest\r\n```\r\nThis will display some text on the shell and once the line `Listening on http://0.0.0.0:8888` appears go to the browser and type `http://0.0.0.0:8888` on Linux systems or `http://localhost:8888` on Windows systems. If successful you will see:\r\n\r\n[![initialized_VIsoQLR](https://github.com/TBLabFJD/VIsoQLR/blob/main/images/initialized_VIsoQLR.png?raw=true)](https://github.com/TBLabFJD/VIsoQLR/tree/main/images/initialized_VIsoQLR.png)\r\n\r\nClick on 'Browse...' button to navigate to your file system to upload your aligner reads in `GFF3`, `BED6`, or `BAM` file format. Once selected the program will automatically run with its default parameters to identify and quantify isoforms. and you will see something like this:\r\n\r\n[![default_analysis_VIsoQLR](https://github.com/TBLabFJD/VIsoQLR/blob/main/images/default_analysis_VIsoQLR.png?raw=true)](https://github.com/TBLabFJD/VIsoQLR/tree/main/images/default_analysis_VIsoQLR.png)\r\n\r\n### Features\r\n - `Analysis bounding`: this panel allows the user to change the analyzed gene and/or the region of study. Mapped regions outside the selected region will be ignored. This can be used to focus the analysis in a section of the gene.\r\n - `Exon coordinates: Automatic detection`: This panel contains three parameters used for the automatic detection of consensus exon coordinates (CECs). The `Read threshold (%)` is used to select candidate CECs above this value based on the proportion exon coordinates among all reads. The `Padding (# of bases)` are the number of bases at each side of a candidate CES where other non-candidate CEC (below range in the frequency filter) are merged. The `Merge close splice sites (# of bases)` is used to merger candidate CESs into the most frequent one if they are closer than the given distance\r\n - `Exon coordinates: Custom coordinates`: This panel allow the user to uploaded known or previously defined splice sites to replace or merge with the existing ones. This accepts as an input the exon coordinate file that can be downloaded from the Exonic starting/ending points panel.\r\n - `Display options`: allows you to modify the aspect and number of elements of the plot\r\n - `Defined isoforms for comparison`: This panel allows you to uploaded transcripts for visual comparison.\r\n - **Figure panel**: the figure is rendered using Plotly which allows the user to zoom in and move through the figure. The color code is used to identify identical exons. Below isoforms, the frequency of start (blue) and end (red) coordinates are shown. The consensus exon coordinates (CECs) are marked with a dot on each bar, and the exact coordinate and frequency are displayed with the cursor over. It is possible to download a screenshot of what is displayed at the moment by clicking the camera icon in the top-right corner. It is also possible to download the whole figure in a dynamic plot (in HTML) and static plot in multiple formats.\r\n - **Exonic starting/ending points panel**: Editable table where the user can add, delete and modify CECs.\r\n - **Isoform information panel**: Table which provides extra isoform information.\r\n - **Exon information panel**: Table which provides extra exon information.\r\n\r\n## Run sequence alignment using VIsoQLR\r\nOne initiated click on \"Mapping\" in the top-left. You should see something like:\r\n[![default_analysis_VIsoQLR](https://github.com/TBLabFJD/VIsoQLR/blob/main/images/mapping_VIsoQLR.png?raw=true)](https://github.com/TBLabFJD/VIsoQLR/tree/main/images/mapping_VIsoQLR.png)\r\nThe screen is divided in 3 panels. The first one is used to build the reference index used by GMAP by uploading the reference sequence(s). Once you upload your sequence(s) the index building will start automatically. Once is complete, a download bottom will appear on the screen. The second panel is dedicated to align the raw reads (in FASTQ format) using GMAP. To do so, upload the ZIP file downloaded from the previous panel as the reference index, the FASTQ file, chose the number of threads and click `Run mapping`. Once the reads alignment is done, a download bottom will appear to retrieve the mapped sequences. In case of selecting the BAM option as an output, two download bottoms will appear: the mapped reads (BAM) and its index (BAI). The BAI file is require, for example, to upload the BAM file in IGV. The last column performs the alignment using Minimap2. In this case the reference must be in FASTA format (the same file used to build the GMAP index in the first panel). Although two major solutions for the alignment are made available, we recommend using GMAP as Minimap2 has problem aligning small exons. This limitation is advised in their GitHub (https://github.com/lh3/minimap2), in the \u201cLimitations\u201d section.\r\n\r\n\r\n\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Data visualisation"
        ],
        "filtered_on": "ITS in description",
        "id": "1869",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1869?version=1",
        "name": "VIsoQLR",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics Unit IIS-FJD"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics-visualization",
            "rna"
        ],
        "tools": [],
        "type": "Docker",
        "update_time": "2025-08-12",
        "versions": 1
    },
    {
        "create_time": "2025-08-12",
        "creators": [
            "Martijn Melissen"
        ],
        "description": "**Workflow for long read quality control, contamination filtering, assembly, variant calling and annotation.**\r\n\r\nSteps:  \r\n- Preprocessing of reference file : https://workflowhub.eu/workflows/1818  \r\n- LongReadSum before and after filtering (read quality control)  \r\n- Filtlong filter on quality and length  \r\n- Flye assembly  \r\n- Minimap2 mapping of reads and assembly  \r\n- Clair3 variant calling of reads  \r\n- Freebayes variant calling of assembly  \r\n- Optional Bakta annotation of genomes with no reference  \r\n- SnpEff building or downloading of a database  \r\n- SnpEff functional annotation  \r\n- Liftoff annotation lift over  \r\n\r\n**All tool CWL files and other workflows can be found here:**\r\n  Tools: https://git.wur.nl/ssb/automated-data-analysis/cwl/-/tree/main/tools\r\n  Workflows: https://git.wur.nl/ssb/automated-data-analysis/cwl/-/tree/main/workflows\r\n",
        "doi": null,
        "edam_operation": [
            "Conversion",
            "Generation"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Genomics",
            "Sequence assembly"
        ],
        "filtered_on": "plasmid* in tags",
        "id": "1868",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1868?version=1",
        "name": "Long Read WGS pipeline",
        "number_of_steps": 31,
        "projects": [
            "Systems and Synthetic Biology"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "assembly",
            "bioinformatics",
            "cwl",
            "galaxy",
            "genomics",
            "python",
            "workflows",
            "plasmid",
            "plasmids"
        ],
        "tools": [
            "Flye",
            "Filtlong",
            "Minimap2",
            "Clair3",
            "FreeBayes",
            "Bakta",
            "snpEff",
            "Liftoff"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-08-12",
        "versions": 1
    },
    {
        "create_time": "2025-07-22",
        "creators": [
            "Martijn Melissen"
        ],
        "description": "**Workflow for preprocessing a reference file. **\r\n\r\nSteps:  \r\n-When a GenBank file is not provided, it is downloaded from NCBI based on a accession number.  \r\n-When multiple plasmid GenBank files are provided, they are merged into one file.  \r\n-When any amount of plasmid GenBank files are provided, the reference is merged with the plasmid GenBank file(s) into one file. A FASTA file is also extracted.  \r\n-When no plasmid Genbank files are provided, a FASTA file is extracted from the reference GenBank file.  \r\n-A GFF3 file is extracted from the final GenBank file.  \r\n-The final step determines the relevant outputs.  \r\n\r\n**All tool CWL files and other workflows can be found here:**  \r\n  Tools: https://git.wur.nl/ssb/automated-data-analysis/cwl/-/tree/main/tools  \r\n  Workflows: https://git.wur.nl/ssb/automated-data-analysis/cwl/-/tree/main/workflows  \r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "plasmid* in tags",
        "id": "1818",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1818?version=1",
        "name": "reference (and plasmid) preprocessing workflow",
        "number_of_steps": 6,
        "projects": [
            "Systems and Synthetic Biology"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "cwl",
            "genbank",
            "gff3",
            "plasmids"
        ],
        "tools": [
            "Determines relevant final outputs.",
            "Merges plasmids when more than one are present.",
            "Merges the plasmid(s) with the reference GenBank file.",
            "Downloads the associated GenBank file from the supplied accession number.",
            "Extracts GFF3 annotation file from the (merged) reference.",
            "Extracts FASTA file from input reference file when no plasmids are provided."
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-08-12",
        "versions": 1
    },
    {
        "create_time": "2025-08-12",
        "creators": [],
        "description": "# GLOWgenes\r\nPrioritization of gene diseases candidates by disease-aware evaluation of heterogeneous evidence networks\r\nVisit www.glowgenes.org for more information\r\n\r\n## Citing\r\nde la Fuente L, Del Pozo-Valero M, Perea-Romero I, Blanco-Kelly F, Fern\u00e1ndez-Caballero L, Cort\u00f3n M, Ayuso C, M\u00ednguez P. Prioritization of New Candidate Genes for Rare Genetic Diseases by a Disease-Aware Evaluation of Heterogeneous Molecular Networks. International Journal of Molecular Sciences. 2023; 24(2):1661. https://doi.org/10.3390/ijms24021661\r\n\r\n## Requirements\r\n\r\nR (tested with version 3.5.0). \r\nR packages: optparse, caret\r\n\r\nPython 2.7 or 3.6\r\n\r\nPython packages: numpy (tested with version 1.11.0), pandas (tested with version 0.19.0), scipy (tested with version 0.18.1), sklearn (tested with version 0.0), networkx (tested with version 3.0)\r\n\r\n## Obtaining network files\r\nDownload network files from: Minguez, Pablo (2022): GLOWgenesNets.zip. figshare. Dataset. https://doi.org/10.6084/m9.figshare.21408393.v1\r\n\r\nYou could also generate your own networks or selected a subset from theose provided by GLOWgenes\r\n\r\n## Editing networks config file\r\nEdit networks_knowledgeCategories.cfg file with your complete directory route to the network files\r\ne.g. substitute PATH by home/pablo/GLOWgenesNets in every line, as in: /PATH/coexpressionCOXPRESdbEXT_HGNCnets.txt\r\n\r\n## Running GLOWgenes\r\n\r\nusage: GLOWgenes.py [-h] -i INPUT -n NETWORKS -o OUTPUT [-t] [-p]\r\n                    [-f FILTERING] [-en EXPNORM] [-co CUTOFF] [-r RATIO]\r\n\r\n\r\npython GLOWgenes.py -i diseaseGenes.txt -n networks.cfg -o outputdir -p\r\n\r\nUse complete paths to avoid errors\r\n\r\n## Parameters\r\n\r\nMandatory parameters:\r\n\r\n**-i --input INPUT**\r\nFile listing known associated disease genes\r\n\r\n**-n --networks NETWORKS**\r\nEvidence network config file. Three tab-separated fields: network path, network name, network category\r\n\r\n[Default network config file](networks_knowledgeCategories.cfg)\r\n\r\nDEFAULT NETWORK CONFIG FILE IS LOCATED AT TEST FOLDER\r\n\r\n**-o --output OUTPUT**\r\nOutput directory\r\n\r\n**-p, --panelapp**       \r\nDisease-associated genes in PanelApp format\r\n\r\nGene Panels from PanelApp can be download from https://panelapp.genomicsengland.co.uk/panels/.\r\n\r\n**-t, --timeprinted**     \r\nKnowledge accumulation approach.\r\n  \r\n**-f FILTERING, --filtering FILTERING**\r\nList of candidate genes. Edges involving genes not listed here are filtered from networks\r\n  \r\n**-en EXPNORM, --expnorm EXPNORM**\r\nExpression levels file. Two tab-separated fields: gene name, expression level               \r\n\r\n\r\n**-co CUTOFF, --cutoff CUTOFF**\r\nMaximum seed initialization value when considering gene expression levels. Range 0-1\r\n  \r\n  \r\n**-r RATIO, --ratio RATIO**\r\nTraining ratio for random training/test splits\r\n\r\n## Running an example\r\nWithin directory example you have full intructions to test GLOWgenes\r\n",
        "doi": null,
        "edam_operation": [
            "Variant prioritisation"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Genomics"
        ],
        "filtered_on": "ITS in description",
        "id": "1866",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-NC-SA-4.0",
        "link": "https:/workflowhub.eu/workflows/1866?version=1",
        "name": "GLOWgenes",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics Unit IIS-FJD"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "gene prioritization",
            "genomics",
            "rare diseases"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2025-08-12",
        "versions": 1
    },
    {
        "create_time": "2025-08-12",
        "creators": [],
        "description": "# WHALE: (W)orkflow for (H)uman-genome (A)nalysis of (L)ong-read (E)xperiments\r\n\r\n## Introduction\r\n\r\n**WHALE** is a bioinformatics pipeline based on Nextflow and nf-core for long-read DNA sequencing analysis. It takes a samplesheet as input and performs quality control, alignment, variant calling and annotation.\r\n\r\n## Pipeline summary\r\n\r\n1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\r\n2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\r\n3. Alignment ([`Minimap2`](https://github.com/lh3/minimap2))\r\n4. Variant calling\r\n    - Single Nucleotide Variant (SNV) calling ([`DeepVariant`](https://github.com/google/deepvariant), [`Clair3`](https://github.com/HKU-BAL/Clair3), [`NanoCaller`](https://github.com/WGLab/NanoCaller))\r\n    - Structural Variant (SV) calling ([`Sniffles2`](https://github.com/fritzsedlazeck/Sniffles), [`CuteSV`](https://github.com/tjiangHIT/cuteSV), [`SVIM`](https://github.com/eldariont/svim))\r\n5. Merge variant calling\r\n6. Annotation\r\n    - SNV annotation ([`VEP`](https://github.com/Ensembl/ensembl-vep))\r\n    - SV annotation ([`AnnotSV`](https://github.com/lgmgeo/AnnotSV))\r\n\r\n## Usage\r\n\r\nFirst, prepare a samplesheet with your input data. Depending on which step of the analysis you want to run, the input data type can be: fastq, bam (and bai), vcf or bed. The samplesheet should look as follows:\r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\nsample,fastq\r\nA123,/path/to/your/input/file/A123.fastq.gz\r\nB456,/path/to/your/input/file/B456.fastq.gz\r\n```\r\n\r\nThere are two types of full analysis:\r\n- SNV analysis: -profile snv_analysis\r\n- SV analysis: -profile sv_analysis\r\n    \r\n  Each full analysis can start with:\r\n  - Alignment: --step mapping (input data type: fastq) (default)\r\n  - Variant calling: --step variant_calling (input data type: bam and bai)\r\n    \r\nA specific step of the analysis can be executed:\r\n- SNV calling (and merge): -profile snv_calling (input data type: bam and bai)\r\n- SV calling (and merge): -profile sv_calling (input data type: bam and bai)\r\n- SNV annotation: -profile snv_annotation (input data type: vcf)\r\n- SV annotation: -profile sv_annotation (input data type: bed)\r\n\r\nProfiles to use in the CCC (UAM):\r\n- -profile uam,singularity,batch\r\n- -profile uam_allcontigs,singularity,batch\r\n\r\nProfiles to use in the server:\r\n- -profile tblabserver,singularity\r\n- -profile tblabserver_allcontigs,singularity\r\n\r\n## Examples\r\n\r\nSNV and SV analysis starting with variant calling in the server:\r\n\r\n```bash\r\nnextflow run WHALE \\\r\n   -profile snv_analysis,sv_analysis,tblabserver,singularity \\\r\n   --input samplesheet.csv \\\r\n   --outdir <OUTDIR> \\\r\n   --step variant_calling\r\n```\r\n\r\nSV calling in the CCC:\r\n\r\n```bash\r\nnextflow run WHALE \\\r\n   -profile sv_calling,uam,singularity,batch \\\r\n   --input samplesheet.csv \\\r\n   --outdir <OUTDIR>\r\n```\r\n\r\n## Pipeline output\r\n\r\n**WHALE** will create the following subdirectories in the output directory:\r\n- alignment\r\n- snv_calling\r\n  - snv_merge\r\n- snv_annotation\r\n- sv_calling\r\n  - sv_merge\r\n- sv_annotation\r\n  - overlapping_sv_samples\r\n- multiqc\r\n- pipeline_info\r\n\r\n## Citations\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nIllustration by [Yolanda Ben\u00edtez](https://github.com/yolandabq)\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Genomics"
        ],
        "filtered_on": "annot* in description",
        "id": "1865",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1865?version=1",
        "name": "WHALE",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics Unit IIS-FJD"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "nextflow",
            "long-read-sequencing"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-12",
        "versions": 1
    },
    {
        "create_time": "2025-08-12",
        "creators": [],
        "description": "# PARROT-FJD\r\nPipeline of Analysis and Research of Rare diseases Optimized in Tblab - Fundaci\u00f3n Jim\u00e9nez D\u00edaz. This is a germline variant calling pipeline implemented in Nextflow which performs mapping, SNV/INDEL calling and annotation, and CNV calling and annotation for targeted sequencing (gene panels and WES) and whole genome sequencing. \r\n\r\n## How to run this pipeline\r\nThe different tasks previously mention are divided into different workflows which are specified usig the `--analysis` flag followed by the corresponding letters:\r\n - D (Download): It downloads the FASTQ files from BaseSpace. If CNV calling or no samples are specified all samples from a project will be downloaded. \r\n - M (Mapping): Specified FASTQ files from a directory (or the ones downloaded) are mapped into analysis-ready BAM files.\r\n - S (SNV/INDEL calling): Specified BAM files (or the ones just mapped) are used for SNV and INDEL calling using GATK by default. Dragen and DeepVariant are also available. The variant caller can be selected with the parameter `--vc_tools`. The options are: \r\n     - `gatk`: Haplotypecaller\r\n     - `dragen`\r\n     - `deepvariant`\r\n     - `all` (equivalent to: gatk,dragen,deepvariant). Using this option, resulting vcfs will be merged into a single-sample VCF (single VCFs from each variant caller are also available)\r\n   More than one tool can be chosen using \",\" (`--vc_tools gatk,dragen`)\r\n - A (Annotation of SNVs and INDELS). Specified VCF files from a directory (or the ones just generated in the SNV/INDEL calling step) are annotated and transformed into a TSV file.\r\n - C (CNV calling and annotation). Specified BAM files (or the ones just mapped) are used for CNV calling using Exomedepth, Convading, Panelcn.mops and GATK, and annotation using AnnotSV. In the case of analycing WGS (`--capture G`) the variant calling used is Manta.\r\n - T (mitochondrial SNV/indel calling + annotation). Specified BAM files (or the ones just mapped) are used for mitochondrial SNV and INDEL calling using Mutect2. The reference chrM used is from hg38. \r\n - H (Expansion Hunter). Specified BAM files (or the ones just mapped) are used for estimating repeat sizes using Expansion Hunter. This analysis can be perforned separately including \"H\" flag in `--analysis`, or it can be included in \"C\" (CNV calling) analysis using the option `--expansion_hunter true`, with or without the flag `--analysis H`. \r\n\r\nMapping and variant calling processes can be parallelized to speed up the analysis. These option can be activated using the parameters `--parallel_mapping true` and `--parallel_calling true`.\r\n\r\n`--parallel_mapping true`: FASTP will be executed to split FASTQ files in three chunks that will be mapped in parallel.   \r\n`--parallel_calling true`: BAM file will be split by chromosomes in smaller BAM files that will be processed in parallel. \r\n\r\nFor using this options, using `--cpus-per-task=44` is recommended. \r\n\r\nYou can generate and keep a cram file out of your bam, when running either MS or just S. The cram is generated inside the out folder: /out/cram/. By default: --keep_cram false\r\n`--keep_cram true`: generate and keep cram file\r\n\r\nYou can use cram file as input using the option `--alignment_file=\"cram\"`. \r\n\r\nYou can generate the mosdepth bed file from your bam, when running either MS or just S. The bed is generated inside the out folder: /out/qc/mosdepth_cov/. By default: --mosdepth_bed false\r\n`--mosdepth_bed true`: generate the mosdepth.bed file needed to update the db of allele frequencies. \r\n\r\nYou need to define with what technique your data was generated: WES, WGS or CES. By default is WES\r\n\t`--technique WES` : My data is/are WES samples\r\n\r\nWhen you use the D (Download option) you need to specify the path to your \"bs\" software is:\r\nbs is the BaseSpace Sequence Hub CLI software.\r\nTo download the software visit the lik and follow instructions: https://developer.basespace.illumina.com/docs/content/documentation/cli/cli-overview\r\nAfter downloading you need to authenticate with your basespace account running the following command: bs auth\r\nAfter authentication you can already run PARROT-FJD with the D (download option) by specifying where your \"bs\" software is stored.\r\nExample: TBLAB -> --baseuser /home/graciela/bin/\r\nExample: UAM -> --baseuser /lustre/home/graciela/\r\n\r\n`--baseuser /lustre/home/graciela/ `: download fastqs from basespace for graciela when running in UAM\r\n\r\nThere are different profiles available depending on the reference release to use, where to run it, and type of contenerization:\r\n\r\nMandatory to choose one:\r\n - hg19: use the reference genome hg19\r\n - hg38: use the reference genome hg38\r\n\r\nMandatory to choose one:\r\n - tblabserver: run pipeline in the server just for the canonical chromosomes\r\n - tblabserver_allcontigs: run pipeline in the server just for all contigs\r\n - uam: run pipeline in the CCC (UAM) just for the canonical chromosomes\r\n - uam_allcontigs: run pipeline in the CCC (UAM) just for all contigs\r\n\r\nMandatory to choose one:\r\n - docker: run pipeline using docker containers\r\n - singularity: run pipeline using singularity containers\r\n\r\nMandatory when running in the CCC (UAM):\r\n - batch: run pipeline in for slurm executor in the CCC (UAM)\r\n\r\n\r\nProfiles to use in the CCC (UAM): \r\n - `-profile hg19,singularity,uam,batch` \r\n - `-profile hg38,singularity,uam,batch`\r\n - `-profile hg19,singularity,uam_allcontigs,batch` \r\n - `-profile hg38,singularity,uam_allcontigs,batch`\r\n\r\nProfiles to use in the server:\r\n - `-profile hg19,singularity,tblabserver` \r\n - `-profile hg38,singularity,tblabserver`\r\n - `-profile hg19,docker,tblabserver` \r\n - `-profile hg38,docker,tblabserver`\r\n - `-profile hg19,singularity,tblabserver_allcontigs` \r\n - `-profile hg38,singularity,tblabserver_allcontigs`\r\n - `-profile hg19,docker,tblabserver_allcontigs` \r\n - `-profile hg38,docker,tblabserver_allcontigs`\r\n\r\nCheck the file nexflow.config to see the description of all arguments.\r\n\r\n## Examples\r\n\r\nPerform a complete analysis from downloading samples from Basespace to SNV/INDEL and CNV calling and annotation.\r\n```\r\nnextflow run /home/gonzalo/nextflowtest/NextVariantFJD/main.nf \\\r\n-profile hg38,singularity,tblabserver --analysis DMSAC \\\r\n--input project_name \\\r\n--output /output/path/ \\\r\n--bed /path/to/captured/regions.bed \\\r\n-with-report report.html\r\n```\r\n\r\n\r\nPerform an analysis from mapping to SNV/INDEL calling and annotation.\r\n```\r\nnextflow run /home/gonzalo/nextflowtest/NextVariantFJD/main.nf \\\r\n-profile hg38,singularity,tblabserver --analysis MSA \\\r\n--input /input/path/to/fastq/ \\\r\n--output /output/path/ \\\r\n-with-report report.html\r\n```\r\n\r\n\r\nPerform a complete analysis from downloading samples from Basespace to SNV/INDEL and CNV calling and annotation. Variant calling analysis is performed to a subset of samples and genes.\r\n```\r\nnextflow run /home/gonzalo/nextflowtest/NextVariantFJD/main.nf \\\r\n-profile hg38,singularity,tblabserver --analysis DMSAC \\\r\n--input project_name \\\r\n--output /output/path/ \\\r\n--bed /path/to/captured/regions.bed \\\r\n--samples /path/to/samplefile.txt \\\r\n--genelist /path/to/genelist.txt \\\r\n-with-report report.html\r\n```\r\n\r\n\r\nPerform a SNV/INDEL calling and annotation analysis. Variant calling analysis is performed to a subset of samples. Genes are prioritized using genelista and glowgenes.\r\n```\r\nnextflow run /home/gonzalo/nextflowtest/NextVariantFJD/main.nf \\\r\n-profile hg38,singularity,tblabserver --analysis SA \\\r\n--input /input/path/to/vcfs/ \\\r\n--output /output/path/ \\\r\n--samples /path/to/samplefile.txt \\\r\n--genelist /path/to/genelist.txt \\\r\n--glowgenes /path/to/glowgenes_result.txt \\\r\n-with-report report.html\r\n```\r\n\r\n## Versions\r\n\r\nGATK 4.4.0.0\r\nVEP release 105\r\ndeepvariant v1.4.0\r\nannotsv 3.1.1\r\nmanta 1.6.0\r\nBWA 0.7.17-r1198-dirty\r\nbcftools 1.15\r\nbedtools 2.27.1\r\nR version 4.2.3\r\npython 3.6\r\n\r\n## License\r\n\r\nPARROT-FJD source code is provided under the [**Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)**](https://creativecommons.org/licenses/by-nc-sa/4.0/). PARROT-FJD includes several third party packages provided under other open source licenses, please check them for additional details.\r\n\r\n[![Licencia de Creative Commons](https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-nc-sa/4.0/)\r\n\r\n## Projects\r\nPI22/00579 \u201cDB4DISCOVERY. Desarrollo de m\u00e9todos de priorizaci\u00f3n y descubrimientos de variantes mediante el reuso de informaci\u00f3n gen\u00f3mica agregada en una base de datos espec\u00edfica de una cohorte.\u201d ISCIII\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Genomics"
        ],
        "filtered_on": "binn* in description",
        "id": "1864",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1864?version=1",
        "name": "PARROT-FJD",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics Unit IIS-FJD"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "nextflow"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-12",
        "versions": 1
    },
    {
        "create_time": "2025-08-12",
        "creators": [],
        "description": "# Introduction\r\n\r\n**nf-CBRA-snvs** (nf-core - CIBERER Bioinformatics for Rare diseases Analysis - Small Nucleotide Variant) is a workflow optimized for the analysis of rare diseases, designed to detect SNVs and INDELs in targeted sequencing data (CES/WES) as well as whole genome sequencing (WGS).\r\n\r\nThis pipeline is developed using Nextflow, a workflow management system that enables an easy execution across various computing environments. It uses Docker or Singularity containers, simplifying setup and ensuring reproducibility of results. The pipeline assigns a container to each process, which simplifies the management and updating of software dependencies. When possible, processes are sourced from nf-core/modules, promoting reusability across all nf-core pipelines and contributing to the broader Nextflow community.\r\n\r\n\r\n# Pipeline summary\r\n\r\nThe pipeline can perform the following steps:\r\n\r\n- **Mapping** of the reads to reference (BWA-MEM)\r\n- Process BAM file (`GATK MarkDuplicates`, `GATK BaseRecalibrator` and `GATK ApplyBQSR`)\r\n- **Variant calling** with the following tools:\r\n\r\n  - GATK4 Haplotypecaller (`run_gatk = true`). This subworkflow includes:\r\n    - **GATK4 Haplotypecaller**.\r\n    - **Hard Filters** and **VarianFiltration** to mark PASS variants. More information [here](docs/variant_calling.md).\r\n    - **Bcftools Filter** to keep PASS variants on chr1-22, X, Y.\r\n    - **Split Multialletic**.\r\n  - Dragen (`run_dragen = true`). This subworkflow includes:\r\n    - **GATK4 Calibratedragstrmodel**\r\n    - **GATK4 Haplotypecaller** with `--dragen-mode`.\r\n    - **VarianFiltration** with `--filter-expression \"QUAL < 10.4139\" --filter-name \"DRAGENHardQUAL\"`to mark PASS variants. More information [here](https://gatk.broadinstitute.org/hc/en-us/articles/4407897446939--How-to-Run-germline-single-sample-short-variant-discovery-in-DRAGEN-mode).\r\n    - **Bcftools Filter** to keep PASS variants on chr1-22, X, Y.\r\n    - **Split Multialletic**.\r\n  - DeepVariant (`run_deepvariant = true`). This subworkflow includes:\r\n    - **DeepVariant makeexamples**: Converts the input alignment file to a tfrecord format suitable for the deep learning model.\r\n    - **DeepVariant callvariants**: Call variants based on input tfrecords. The output is also in tfrecord format, and needs postprocessing to convert it to vcf.\r\n    - **DeepVariant postprocessvariants**: Convert variant calls from callvariants to VCF, and also create GVCF files based on genomic information from makeexamples. More information [here](https://github.com/nf-core/modules/tree/master/modules/nf-core/deepvariant).\r\n    - **Bcftools Filter** to keep PASS variants on chr1-22, X, Y.\r\n    - **Split Multialletic**.\r\n\r\n- **Merge and integration** of the vcfs obtained with the different tools.\r\n- **Annotation** of the variants:\r\n  - Regions of homozygosity (ROHs) with [AUTOMAP](https://github.com/mquinodo/AutoMap)\r\n  - Effect of the variants with [Ensembl VEP](https://www.ensembl.org/info/docs/tools/vep/index.html) using the flag `--everything`, which includes the following options: `--sift b, --polyphen b, --ccds, --hgvs, --symbol, --numbers, --domains, --regulatory, --canonical, --protein, --biotype, --af, --af_1kg, --af_esp, --af_gnomade, --af_gnomadg, --max_af, --pubmed, --uniprot, --mane, --tsl, --appris, --variant_class, --gene_phenotype, --mirna`\r\n  - Postvep format VEP tab demilited output and filter variants by minor allele frequency (`--maf`).\r\n  - You can enhance the annotation by incorporating gene rankings from [GLOWgenes](https://www.translationalbioinformaticslab.es/tblab-home-page/tools/glowgenes), a network-based algorithm developed to prioritize novel candidate genes associated with rare diseases. Precomputed rankings based on PanelApp gene panels are available [here](https://github.com/TBLabFJD/GLOWgenes/blob/master/precomputed_panelAPP/GLOWgenes_precomputed_panelAPP.tsv). To include a specific GLOWgenes ranking, use the option `--glowgenes_panel (path to the panel.txt)`, for example: `--glowgenes_panel https://raw.githubusercontent.com/TBLabFJD/GLOWgenes/refs/heads/master/precomputed_panelAPP/GLOWgenes_prioritization_Neurological_ciliopathies_GA.txt`. Additionally, you can include the Gene-Disease Specificity Score (SGDS) using: `--glowgenes_sgds https://raw.githubusercontent.com/TBLabFJD/GLOWgenes/refs/heads/master/SGDS.csv`. This score ranges from 0 to 1, where 1 indicates a gene ranks highly for only a few specific diseases (high specificity), and 0 indicates the gene consistently ranks highly across many diseases (low specificity).\r\n\r\n\r\n# Usage\r\n\r\nFirst, prepare a samplesheet with your input data:\r\n\r\n```\r\nsample,fastq_1,fastq_2\r\nSAMPLE_PAIRED_END,/path/to/fastq/files/AEG588A1_S1_L002_R1_001.fastq.gz,/path/to/fastq/files/AEG588A1_S1_L002_R2_001.fastq.gz\r\n```\r\n\r\nEach row represents a pair of paired end fastq files. \r\n\r\nYou can run the pipeline using: \r\n\r\n```\r\nnextflow run nf-cbra-snvs/main.nf \\\r\n   -profile <docker/singularity/.../institute> \\\r\n   --input samplesheet.csv \\\r\n   --outdir <OUTDIR>\r\n```\r\n\r\nFor more details and further functionality, please refer to the [usage](docs/usage.md) documentation.\r\n\r\n\r\n# Pipeline output\r\n\r\nFor details about the output files and reports, please refer to the [output](docs/output.md) documentation.\r\n\r\n# Credits\r\n\r\nnf-CBRA-snvs was developed within the framework of a call for intramural cooperative and complementary actions (ACCI) funded by CIBERER (Biomedical Research Network Centre for Rare Diseases).\r\n\r\n**Main Developer**\r\n- [Yolanda Ben\u00edtez Quesada](https://github.com/yolandabq)\r\n\r\n**Coordinator**\r\n- [Carlos Ruiz Arenas](https://github.com/yocra3)\r\n\r\n**Other contributors**\r\n- [Graciela Ur\u00eda Regojo](https://github.com/guriaregojo)\r\n- [Pedro Garrido Rodr\u00edguez](https://github.com/pedro-garridor)\r\n- [Rafa Farias Varona](https://github.com/RafaFariasVarona)\r\n- [Pablo Minguez](https://github.com/pminguez)\r\n- [Daniel Lopez](https://github.com/dlopez-bioinfo)\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Genomics",
            "Rare diseases"
        ],
        "filtered_on": "annot* in description",
        "id": "1862",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1862?version=1",
        "name": "nf-CBRA-snvs",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics Unit IIS-FJD"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "nextflow"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-12",
        "versions": 1
    },
    {
        "create_time": "2025-08-12",
        "creators": [
            "Zhixing Feng"
        ],
        "description": "# NanoFreeLunch\r\n\r\nDetecting DNA modifications quantitatively from Nanopore data without using raw signals.\r\n\r\n# Installation\r\n\r\n1. Install Julia from https://julialang.org/. Do Not use Julia in containers like docker or singularity. \r\n2. Enter the folder of NanoFreeLunch and type `julia setup.jl`.\r\n3. The executable can be found in build/bin/, add the folder to PATH or add softlink of the executable to the folder in your PATH.\r\n\r\n**Warning**: you might experience slow package downloading or get error like `Exception: RequestError: HTTP/1.1 200 Connection established ` if Julia server connection is unstable in your region. You could change your Julia server by typing `export JULIA_PKG_SERVER=https://mirrors.pku.edu.cn/julia` and try again. Here is a list of alternative Julia package servers: \r\n```\r\nhttps://mirrors.pku.edu.cn/julia\r\nhttps://mirrors.sjtug.sjtu.edu.cn/julia\r\nhttps://mirrors.nju.edu.cn/julia\r\nhttps://releases.tongyuan.cc/juliapkg/original\r\n```\r\n\r\n# Demo \r\nA quick demo is available at https://gitee.com/zhixingfeng/nfl-demo/tree/main/demo.\r\n\r\n# Supporting data and code\r\nThe supporting data and code to reproduce the results are available at https://gitee.com/zhixingfeng/nfl-demo. \r\n\r\n# Usage \r\n## Extracting features from aligned reads\r\n\r\n### `prepdata [options] [flags] bamfile reffile locifile outdir`\r\n\r\n### Args\r\n\r\n- `<modelfile>`  \r\n  The model file, which is the output file of `nfl train`.\r\n\r\n- `<Xdata_dir>`  \r\n  The `Xdata` folder in the output directory of `nfl prepdata`.\r\n\r\n- `<outfile>`  \r\n  The output file (predicted modification level for qualitative detection or probability for qualitative detection).\r\n\r\n### Options\r\n\r\n- `-e`, `--esp <0.001>`  \r\n  Precision threshold. Modification level <= `esp` will be set to 0, and modification level >= `1-esp` will be set to 1.\r\n\r\n- `-s`, `--subset <\"\">`  \r\n  Use a subset of features (default: using all the features).\r\n\r\n- `--objective <reg:squarederror>`  \r\n  Default is `reg:squarederror`. Use `reg:logistic` for direct regression instead of logit transformation.\r\n\r\n### Flags\r\n\r\n- `-h`, `--help`  \r\n  Print this help message.\r\n\r\n\r\n# License\r\nThe software is under GPL3 and the data in `test/data` and `test/results` are under the CC0 public domain waiver. \r\n\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.1858.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1858",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1858?version=1",
        "name": "NanoFreeLunch",
        "number_of_steps": 0,
        "projects": [
            "zxfenglab"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Unrecognized workflow type",
        "update_time": "2025-08-20",
        "versions": 1
    },
    {
        "create_time": "2025-08-11",
        "creators": [],
        "description": "Classification and visualization of ITS regions.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [subworkflows](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_subworkflows.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: EMBL's European Bioinformatics Institute, Rand Zoabi, Paul Zierep\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1856",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1856?version=1",
        "name": "MGnify's amplicon pipeline v5.0 - ITS",
        "number_of_steps": 30,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "gtn",
            "galaxy",
            "metagenomics",
            "mgnify_amplicon",
            "name:microgalaxy"
        ],
        "tools": [
            "__FILTER_EMPTY_DATASETS__",
            "",
            "tp_awk_tool",
            "biom_convert",
            "bedtools_maskfastabed",
            "collection_element_identifiers",
            "taxonomy_krona_chart",
            "mapseq",
            "__FILTER_FROM_FILE__"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-11",
        "creators": [],
        "description": "The MAPseq to Ampvis workflow processes MAPseq OTU tables and associated metadata for analysis in Ampvis2. This workflow involves reformatting MAPseq output datasets to produce structured output files suitable for Ampvis2.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Rand Zoabi, Mara Besemer\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1855",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1855?version=1",
        "name": "MAPseq to ampvis2",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "tp_awk_tool",
            "query_tabular",
            "ampvis2_load",
            "collection_column_join",
            "collapse_dataset"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-11",
        "creators": [],
        "description": "Quality control subworkflow for paired-end reads. \n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: EMBL's European Bioinformatics Institute, Rand Zoabi, Paul Zierep\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1854",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1854?version=1",
        "name": "MGnify's amplicon pipeline v5.0 - Quality control PE",
        "number_of_steps": 17,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "gtn",
            "galaxy",
            "metagenomics",
            "mgnify_amplicon",
            "name:microgalaxy"
        ],
        "tools": [
            "fastp",
            "__UNZIP_COLLECTION__",
            "fastq_filter",
            "fastqc",
            "cshl_fasta_formatter",
            "fastq_to_fasta_python",
            "prinseq",
            "tp_find_and_replace",
            "mgnify_seqprep",
            "trimmomatic",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-11",
        "creators": [],
        "description": "MGnify's amplicon pipeline v5.0. Including the Quality control for single-end and paired-end reads, rRNA-prediction, and ITS sub-WFs.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [subworkflows](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_subworkflows.html)\n\n## Thanks to...\n\n**Workflow Author(s)**:  EMBL's European Bioinformatics Institute, Rand Zoabi, Paul Zierep\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1853",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1853?version=1",
        "name": "MGnify's amplicon pipeline v5.0",
        "number_of_steps": 20,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "gtn",
            "galaxy",
            "metagenomics",
            "mgnify_amplicon",
            "name:microgalaxy"
        ],
        "tools": [
            "",
            "tp_awk_tool",
            "CONVERTER_gz_to_uncompressed",
            "CONVERTER_uncompressed_to_gz",
            "__MERGE_COLLECTION__",
            "fastq_dl"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-11",
        "creators": [],
        "description": "This workflow creates taxonomic summary tables out of the amplicon pipeline results. \n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Rand Zoabi\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1851",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1851?version=1",
        "name": "MGnify amplicon summary tables",
        "number_of_steps": 10,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "gtn",
            "galaxy",
            "metagenomics",
            "mgnify_amplicon",
            "name:microgalaxy"
        ],
        "tools": [
            "tp_awk_tool",
            "filter_tabular",
            "query_tabular",
            "collection_column_join",
            "Grouping1"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-11",
        "creators": [],
        "description": "Quality control subworkflow for single-end reads.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: MGnify - EMBL, Rand Zoabi, Paul Zierep\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1850",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1850?version=1",
        "name": "MGnify's amplicon pipeline v5.0 - Quality control SE",
        "number_of_steps": 14,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "gtn",
            "galaxy",
            "metagenomics",
            "mgnify_amplicon",
            "name:microgalaxy"
        ],
        "tools": [
            "fastq_filter",
            "fastqc",
            "cshl_fasta_formatter",
            "fastq_to_fasta_python",
            "prinseq",
            "tp_find_and_replace",
            "trimmomatic",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-08",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-proteinfamilies_logo_dark.png\">\n    <img alt=\"nf-core/proteinfamilies\" src=\"docs/images/nf-core-proteinfamilies_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/proteinfamilies/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/proteinfamilies/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/proteinfamilies/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/proteinfamilies/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/proteinfamilies/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.14881993-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.14881993)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/proteinfamilies)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23proteinfamilies-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/proteinfamilies)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/proteinfamilies** is a bioinformatics pipeline that generates protein families from amino acid sequences and/or updates existing families with new sequences.\nIt takes a protein fasta file as input, clusters the sequences and then generates protein family Hiden Markov Models (HMMs) along with their multiple sequence alignments (MSAs).\nOptionally, paths to existing family HMMs and MSAs can be given (must have matching base filenames one-to-one) in order to update with new sequences in case of matching hits.\n\n<p align=\"center\">\n    <img src=\"docs/images/proteinfamilies_workflow.png\" alt=\"nf-core/proteinfamilies workflow overview\">\n</p>\n\n### Check quality\n\nGenerate input amino acid sequence statistics with ([`SeqKit`](https://github.com/shenwei356/seqkit/))\n\n### Create families\n\n1. Cluster sequences ([`MMseqs2`](https://github.com/soedinglab/MMseqs2/))\n2. Perform multiple sequence alignment (MSA) ([`FAMSA`](https://github.com/refresh-bio/FAMSA/) or [`mafft`](https://github.com/GSLBiotech/mafft/))\n3. Optionally, clip gap parts of the MSA ([`ClipKIT`](https://github.com/JLSteenwyk/ClipKIT/))\n4. Generate family HMMs and fish additional sequences into the family ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\n5. Optionally, remove redundant families by comparing family representative sequences against family models with ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\n6. Optionally, from the remaining families, remove in-family redundant sequences by strictly clustering with ([`MMseqs2`](https://github.com/soedinglab/MMseqs2/)) and keep cluster representatives\n7. Optionally, if in-family redundancy was not removed, reformat the `.sto` full MSAs to `.fas` with ([`HH-suite3`](https://github.com/soedinglab/hh-suite))\n8. Present statistics for remaining/updated family size distributions and representative sequence lengths ([`MultiQC`](http://multiqc.info/))\n\n### Update families\n\n1. Find which families to update by comparing the input sequences against existing family models with ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\n2. For non hit sequences continue with the above: A. Create families. For hit sequences and families continue to: 3\n3. Extract family sequences ([`SeqKit`](https://github.com/shenwei356/seqkit/)) and concatenate with filtered hit sequences of each family\n4. Optionally, remove in-family redundant sequences by strictly clustering with ([`MMseqs2`](https://github.com/soedinglab/MMseqs2/)) and keeping cluster representatives\n5. Perform multiple sequence alignment (MSA) ([`FAMSA`](https://github.com/refresh-bio/FAMSA/) or [`mafft`](https://github.com/GSLBiotech/mafft/))\n6. Optionally, clip gap parts of the MSA ([`ClipKIT`](https://github.com/JLSteenwyk/ClipKIT/))\n7. Update family HMM with ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fasta,existing_hmms_to_update,existing_msas_to_update\nCONTROL_REP1,input/mgnifams_input_small.fa,,\n```\n\nEach row contains a fasta file with amino acid sequences (can be zipped or unzipped).\nOptionally, a row may contain tarball archives (tar.gz) of existing families' HMM and MSA folders, in order to be updated.\nIn this case, the HMM and MSA files must be matching in numbers and in base filenames (not the extension).\nHit families/sequences will be updated, while no hit sequences will create new families.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/proteinfamilies \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/proteinfamilies/usage) and the [parameter documentation](https://nf-co.re/proteinfamilies/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/proteinfamilies/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/proteinfamilies/output).\n\n## Credits\n\nnf-core/proteinfamilies was originally written by Evangelos Karatzas.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Martin Beracochea](https://github.com/mberacochea)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#proteinfamilies` channel](https://nfcore.slack.com/channels/proteinfamilies) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/proteinfamilies for your analysis, please cite it using the following doi: [10.5281/zenodo.14881993](https://doi.org/10.5281/zenodo.14881993).\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1294",
        "keep": true,
        "latest_version": 5,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1294?version=5",
        "name": "nf-core/proteinfamilies",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "proteomics",
            "protein-families"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-08",
        "versions": 5
    },
    {
        "create_time": "2025-08-08",
        "creators": [
            "Jannik Seidel"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-detaxizer_logo_dark.png\">\n    <img alt=\"nf-core/detaxizer\" src=\"docs/images/nf-core-detaxizer_logo_light.png\">\n  </picture>\n</h1>\n\n[![Cite Preprint](https://img.shields.io/badge/Cite%20Us!-Cite%20Publication-important?labelColor=000000)](https://doi.org/10.1101/2025.03.27.645632)\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.10877147-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.10877147)\n\n[![GitHub Actions CI Status](https://github.com/nf-core/detaxizer/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/detaxizer/actions/workflows/nf-test.yml)[![GitHub Actions Linting Status](https://github.com/nf-core/detaxizer/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/detaxizer/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/detaxizer/results)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/detaxizer)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23detaxizer-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/detaxizer)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/detaxizer** is a bioinformatics pipeline that checks for the presence of a specific taxon in (meta)genomic fastq files and to filter out this taxon or taxonomic subtree. The process begins with quality assessment via FastQC and optional preprocessing (adapter trimming, quality cutting and optional length and quality filtering) using fastp, followed by taxonomic classification with kraken2 and/or bbduk, and optionally employs blastn for validation of the reads associated with the identified taxa. Users must provide a samplesheet to indicate the fastq files and, if utilizing bbduk in the classification and/or the validation step, fasta files for usage of bbduk and creating the blastn database to verify the targeted taxon.\n\n![detaxizer metro workflow](docs/images/Detaxizer_metro_workflow.png)\n\n1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n2. Optional pre-processing ([`fastp`](https://github.com/OpenGene/fastp))\n3. Classification of reads ([`Kraken2`](https://ccb.jhu.edu/software/kraken2/), and/or [`bbduk`](https://sourceforge.net/projects/bbmap/))\n4. Optional validation of searched taxon/taxa ([`blastn`](https://blast.ncbi.nlm.nih.gov/Blast.cgi))\n5. Filtering of the searched taxon/taxa from the reads (either from the raw files or the preprocessed reads, using either the output from the classification (kraken2 and/or bbduk) or blastn)\n6. Summary of the processes (how many were classified and optionally how many were validated)\n7. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n```csv title=\"samplesheet.csv\"\nsample,short_reads_fastq_1,short_reads_fastq_2,long_reads_fastq_1\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,AEG588A1_S1_L002_R3_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end). A third fastq file can be provided if long reads are present in your project. For more detailed information about the samplesheet, see the [usage documentation](docs/usage.md).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/detaxizer \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --classification_bbduk \\\n   --classification_kraken2 \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/detaxizer/usage) and the [parameter documentation](https://nf-co.re/detaxizer/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/detaxizer/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/detaxizer/output).\n\nGenerated samplesheets from the directory `/downstream_samplesheets/` can be used for the pipelines:\n\n- [nf-core/mag](https://nf-co.re/mag)\n- [nf-core/taxprofiler](https://nf-co.re/taxprofiler)\n\n## Credits\n\nnf-core/detaxizer was originally written by [Jannik Seidel](https://github.com/jannikseidelQBiC) at the [Quantitative Biology Center (QBiC)](http://qbic.life/).\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Daniel Straub](https://github.com/d4straub)\n\nThis work was initially funded by the German Center for Infection Research (DZIF).\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#detaxizer` channel](https://nfcore.slack.com/channels/detaxizer) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/detaxizer for your analysis, please cite it using the following preprint:\n\n> **nf-core/detaxizer: A Benchmarking Study for Decontamination from Human Sequences**\n>\n> Jannik Seidel, Camill Kaipf, Daniel Straub, Sven Nahnsen\n>\n> bioRxiv 2025.03.27.645632 [doi: 10.1101/2025.03.27.645632](https://doi.org/10.1101/2025.03.27.645632).\n\nAdditionally, the following doi can be cited: [10.5281/zenodo.10877147](https://doi.org/10.5281/zenodo.10877147)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "979",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/979?version=3",
        "name": "nf-core/detaxizer",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "fastq",
            "metabarcoding",
            "metagenomics",
            "de-identification",
            "decontamination",
            "edna",
            "filter",
            "long-reads",
            "microbiome",
            "nanopore",
            "short-reads",
            "shotgun",
            "taxonomic-classification",
            "taxonomic-profiling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-08",
        "versions": 3
    },
    {
        "create_time": "2025-08-06",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# Structural DNA helical parameters from MD trajectory tutorial using BioExcel Building Blocks (biobb)\r\n\r\n**Based on the [NAFlex](https://mmb.irbbarcelona.org/NAFlex) server and in particular in its [Nucleic Acids Analysis section](https://mmb.irbbarcelona.org/NAFlex/help.php?id=tutorialAnalysisNA).**\r\n\r\n***\r\n\r\nThis tutorial aims to illustrate the process of **extracting structural and dynamical properties** from a **DNA MD trajectory helical parameters**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **Drew Dickerson Dodecamer** sequence -CGCGAATTCGCG- (PDB code [1BNA](https://www.rcsb.org/structure/1BNA)). The trajectory used is a  500ns-long MD simulation taken from the [BigNASim](https://mmb.irbbarcelona.org/BIGNASim/) database ([NAFlex_DDD_II](https://mmb.irbbarcelona.org/BIGNASim/getStruc.php?idCode=NAFlex_DDD_II) entry).\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2025 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2025 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.821.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "821",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/821?version=2",
        "name": "Docker Structural DNA helical parameters tutorial",
        "number_of_steps": 0,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Docker",
        "update_time": "2025-08-06",
        "versions": 2
    },
    {
        "create_time": "2025-08-06",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# Structural DNA helical parameters from MD trajectory tutorial using BioExcel Building Blocks (biobb)\r\n\r\n**Based on the [NAFlex](https://mmb.irbbarcelona.org/NAFlex) server and in particular in its [Nucleic Acids Analysis section](https://mmb.irbbarcelona.org/NAFlex/help.php?id=tutorialAnalysisNA).**\r\n\r\n***\r\n\r\nThis tutorial aims to illustrate the process of **extracting structural and dynamical properties** from a **DNA MD trajectory helical parameters**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **Drew Dickerson Dodecamer** sequence -CGCGAATTCGCG- (PDB code [1BNA](https://www.rcsb.org/structure/1BNA)). The trajectory used is a  500ns-long MD simulation taken from the [BigNASim](https://mmb.irbbarcelona.org/BIGNASim/) database ([NAFlex_DDD_II](https://mmb.irbbarcelona.org/BIGNASim/getStruc.php?idCode=NAFlex_DDD_II) entry).\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2025 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2025 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.286.5",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "286",
        "keep": true,
        "latest_version": 5,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/286?version=5",
        "name": "Python Structural DNA helical parameters tutorial",
        "number_of_steps": 0,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2025-08-06",
        "versions": 5
    },
    {
        "create_time": "2025-08-06",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# Structural DNA helical parameters from MD trajectory tutorial using BioExcel Building Blocks (biobb)\r\n\r\n**Based on the [NAFlex](https://mmb.irbbarcelona.org/NAFlex) server and in particular in its [Nucleic Acids Analysis section](https://mmb.irbbarcelona.org/NAFlex/help.php?id=tutorialAnalysisNA).**\r\n\r\n***\r\n\r\nThis tutorial aims to illustrate the process of **extracting structural and dynamical properties** from a **DNA MD trajectory helical parameters**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **Drew Dickerson Dodecamer** sequence -CGCGAATTCGCG- (PDB code [1BNA](https://www.rcsb.org/structure/1BNA)). The trajectory used is a  500ns-long MD simulation taken from the [BigNASim](https://mmb.irbbarcelona.org/BIGNASim/) database ([NAFlex_DDD_II](https://mmb.irbbarcelona.org/BIGNASim/getStruc.php?idCode=NAFlex_DDD_II) entry).\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2025 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2025 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.195.6",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "195",
        "keep": true,
        "latest_version": 6,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/195?version=6",
        "name": "Jupyter Notebook Structural DNA helical parameters tutorial",
        "number_of_steps": 0,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Jupyter",
        "update_time": "2025-08-06",
        "versions": 6
    },
    {
        "create_time": "2025-08-06",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# Macromolecular Coarse-Grained Flexibility (FlexServ) tutorial using BioExcel Building Blocks (biobb)\r\n\r\nThis tutorial aims to illustrate the process of generating protein conformational ensembles from 3D structures and analysing its molecular flexibility, step by step, using the BioExcel Building Blocks library (biobb).\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2025 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2025 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.823.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "823",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/823?version=2",
        "name": "Docker Macromolecular Coarse-Grained Flexibility tutorial",
        "number_of_steps": 0,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Docker",
        "update_time": "2025-08-06",
        "versions": 2
    },
    {
        "create_time": "2025-08-06",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# Macromolecular Coarse-Grained Flexibility (FlexServ) tutorial using BioExcel Building Blocks (biobb)\r\n\r\nThis tutorial aims to illustrate the process of generating protein conformational ensembles from 3D structures and analysing its molecular flexibility, step by step, using the BioExcel Building Blocks library (biobb).\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2025 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2025 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.553.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "553",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/553?version=2",
        "name": "Python Macromolecular Coarse-Grained Flexibility tutorial",
        "number_of_steps": 0,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2025-08-06",
        "versions": 2
    },
    {
        "create_time": "2025-08-06",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# Macromolecular Coarse-Grained Flexibility (FlexServ) tutorial using BioExcel Building Blocks (biobb)\r\n\r\nThis tutorial aims to illustrate the process of generating protein conformational ensembles from 3D structures and analysing its molecular flexibility, step by step, using the BioExcel Building Blocks library (biobb).\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2025 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2025 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.551.3",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "551",
        "keep": true,
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/551?version=3",
        "name": "Jupyter Notebook Macromolecular Coarse-Grained Flexibility tutorial",
        "number_of_steps": 0,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Jupyter",
        "update_time": "2025-08-06",
        "versions": 3
    },
    {
        "create_time": "2025-08-06",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# Protein Conformational ensembles generation\r\n\r\n## Workflow included in the [ELIXIR 3D-Bioinfo](https://elixir-europe.org/communities/3d-bioinfo) Implementation Study:\r\n\r\n### Building on PDBe-KB to chart and characterize the conformation landscape of native proteins\r\n\r\nThis tutorial aims to illustrate the process of generating **protein conformational ensembles** from** 3D structures **and analysing its **molecular flexibility**, step by step, using the **BioExcel Building Blocks library (biobb)**.\r\n\r\n## Conformational landscape of native proteins\r\n**Proteins** are **dynamic** systems that adopt multiple **conformational states**, a property essential for many **biological processes** (e.g. binding other proteins, nucleic acids, small molecule ligands, or switching between functionaly active and inactive states). Characterizing the different **conformational states** of proteins and the transitions between them is therefore critical for gaining insight into their **biological function** and can help explain the effects of genetic variants in **health** and **disease** and the action of drugs.\r\n\r\n**Structural biology** has become increasingly efficient in sampling the different **conformational states** of proteins. The **PDB** has currently archived more than **170,000 individual structures**, but over two thirds of these structures represent **multiple conformations** of the same or related protein, observed in different crystal forms, when interacting with other proteins or other macromolecules, or upon binding small molecule ligands. Charting this conformational diversity across the PDB can therefore be employed to build a useful approximation of the **conformational landscape** of native proteins.\r\n\r\nA number of resources and **tools** describing and characterizing various often complementary aspects of protein **conformational diversity** in known structures have been developed, notably by groups in Europe. These tools include algorithms with varying degree of sophistication, for aligning the 3D structures of individual protein chains or domains, of protein assemblies, and evaluating their degree of **structural similarity**. Using such tools one can **align structures pairwise**, compute the corresponding **similarity matrix**, and identify ensembles of **structures/conformations** with a defined **similarity level** that tend to recur in different PDB entries, an operation typically performed using **clustering** methods. Such workflows are at the basis of resources such as **CATH, Contemplate, or PDBflex** that offer access to **conformational ensembles** comprised of similar **conformations** clustered according to various criteria. Other types of tools focus on differences between **protein conformations**, identifying regions of proteins that undergo large **collective displacements** in different PDB entries, those that act as **hinges or linkers**, or regions that are inherently **flexible**.\r\n\r\nTo build a meaningful approximation of the **conformational landscape** of native proteins, the **conformational ensembles** (and the differences between them), identified on the basis of **structural similarity/dissimilarity** measures alone, need to be **biophysically characterized**. This may be approached at **two different levels**. \r\n- At the **biological level**, it is important to link observed **conformational ensembles**, to their **functional roles** by evaluating the correspondence with **protein family classifications** based on sequence information and **functional annotations** in public databases e.g. Uniprot, PDKe-Knowledge Base (KB). These links should provide valuable mechanistic insights into how the **conformational and dynamic properties** of proteins are exploited by evolution to regulate their **biological function**. <br><br>\r\n\r\n- At the **physical level** one needs to introduce **energetic consideration** to evaluate the likelihood that the identified **conformational ensembles** represent **conformational states** that the protein (or domain under study) samples in isolation. Such evaluation is notoriously **challenging** and can only be roughly approximated by using **computational methods** to evaluate the extent to which the observed **conformational ensembles** can be reproduced by algorithms that simulate the **dynamic behavior** of protein systems. These algorithms include the computationally expensive **classical molecular dynamics (MD) simulations** to sample local thermal fluctuations but also faster more approximate methods such as **Elastic Network Models** and **Normal Node Analysis** (NMA) to model low energy **collective motions**. Alternatively, **enhanced sampling molecular dynamics** can be used to model complex types of **conformational changes** but at a very high computational cost. \r\n\r\nThe **ELIXIR 3D-Bioinfo Implementation Study** *Building on PDBe-KB to chart and characterize the conformation landscape of native proteins* focuses on:\r\n\r\n1. Mapping the **conformational diversity** of proteins and their homologs across the PDB. \r\n2. Characterize the different **flexibility properties** of protein regions, and link this information to sequence and functional annotation.\r\n3. Benchmark **computational methods** that can predict a biophysical description of protein motions.\r\n\r\nThis notebook is part of the third objective, where a list of **computational resources** that are able to predict **protein flexibility** and **conformational ensembles** have been collected, evaluated, and integrated in reproducible and interoperable workflows using the **BioExcel Building Blocks library**. Note that the list is not meant to be exhaustive, it is built following the expertise of the implementation study partners.\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2025 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2025 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.822.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "822",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/822?version=2",
        "name": "Docker Protein conformational ensembles generation",
        "number_of_steps": 0,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Docker",
        "update_time": "2025-08-06",
        "versions": 2
    },
    {
        "create_time": "2025-08-06",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# Protein Conformational ensembles generation\r\n\r\n## Workflow included in the [ELIXIR 3D-Bioinfo](https://elixir-europe.org/communities/3d-bioinfo) Implementation Study:\r\n\r\n### Building on PDBe-KB to chart and characterize the conformation landscape of native proteins\r\n\r\nThis tutorial aims to illustrate the process of generating **protein conformational ensembles** from** 3D structures **and analysing its **molecular flexibility**, step by step, using the **BioExcel Building Blocks library (biobb)**.\r\n\r\n## Conformational landscape of native proteins\r\n**Proteins** are **dynamic** systems that adopt multiple **conformational states**, a property essential for many **biological processes** (e.g. binding other proteins, nucleic acids, small molecule ligands, or switching between functionaly active and inactive states). Characterizing the different **conformational states** of proteins and the transitions between them is therefore critical for gaining insight into their **biological function** and can help explain the effects of genetic variants in **health** and **disease** and the action of drugs.\r\n\r\n**Structural biology** has become increasingly efficient in sampling the different **conformational states** of proteins. The **PDB** has currently archived more than **170,000 individual structures**, but over two thirds of these structures represent **multiple conformations** of the same or related protein, observed in different crystal forms, when interacting with other proteins or other macromolecules, or upon binding small molecule ligands. Charting this conformational diversity across the PDB can therefore be employed to build a useful approximation of the **conformational landscape** of native proteins.\r\n\r\nA number of resources and **tools** describing and characterizing various often complementary aspects of protein **conformational diversity** in known structures have been developed, notably by groups in Europe. These tools include algorithms with varying degree of sophistication, for aligning the 3D structures of individual protein chains or domains, of protein assemblies, and evaluating their degree of **structural similarity**. Using such tools one can **align structures pairwise**, compute the corresponding **similarity matrix**, and identify ensembles of **structures/conformations** with a defined **similarity level** that tend to recur in different PDB entries, an operation typically performed using **clustering** methods. Such workflows are at the basis of resources such as **CATH, Contemplate, or PDBflex** that offer access to **conformational ensembles** comprised of similar **conformations** clustered according to various criteria. Other types of tools focus on differences between **protein conformations**, identifying regions of proteins that undergo large **collective displacements** in different PDB entries, those that act as **hinges or linkers**, or regions that are inherently **flexible**.\r\n\r\nTo build a meaningful approximation of the **conformational landscape** of native proteins, the **conformational ensembles** (and the differences between them), identified on the basis of **structural similarity/dissimilarity** measures alone, need to be **biophysically characterized**. This may be approached at **two different levels**. \r\n- At the **biological level**, it is important to link observed **conformational ensembles**, to their **functional roles** by evaluating the correspondence with **protein family classifications** based on sequence information and **functional annotations** in public databases e.g. Uniprot, PDKe-Knowledge Base (KB). These links should provide valuable mechanistic insights into how the **conformational and dynamic properties** of proteins are exploited by evolution to regulate their **biological function**. <br><br>\r\n\r\n- At the **physical level** one needs to introduce **energetic consideration** to evaluate the likelihood that the identified **conformational ensembles** represent **conformational states** that the protein (or domain under study) samples in isolation. Such evaluation is notoriously **challenging** and can only be roughly approximated by using **computational methods** to evaluate the extent to which the observed **conformational ensembles** can be reproduced by algorithms that simulate the **dynamic behavior** of protein systems. These algorithms include the computationally expensive **classical molecular dynamics (MD) simulations** to sample local thermal fluctuations but also faster more approximate methods such as **Elastic Network Models** and **Normal Node Analysis** (NMA) to model low energy **collective motions**. Alternatively, **enhanced sampling molecular dynamics** can be used to model complex types of **conformational changes** but at a very high computational cost. \r\n\r\nThe **ELIXIR 3D-Bioinfo Implementation Study** *Building on PDBe-KB to chart and characterize the conformation landscape of native proteins* focuses on:\r\n\r\n1. Mapping the **conformational diversity** of proteins and their homologs across the PDB. \r\n2. Characterize the different **flexibility properties** of protein regions, and link this information to sequence and functional annotation.\r\n3. Benchmark **computational methods** that can predict a biophysical description of protein motions.\r\n\r\nThis notebook is part of the third objective, where a list of **computational resources** that are able to predict **protein flexibility** and **conformational ensembles** have been collected, evaluated, and integrated in reproducible and interoperable workflows using the **BioExcel Building Blocks library**. Note that the list is not meant to be exhaustive, it is built following the expertise of the implementation study partners.\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2025 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2025 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.487.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "487",
        "keep": true,
        "latest_version": 2,
        "license": "other-open",
        "link": "https:/workflowhub.eu/workflows/487?version=2",
        "name": "Python Protein conformational ensembles generation",
        "number_of_steps": 0,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2025-08-06",
        "versions": 2
    },
    {
        "create_time": "2025-08-06",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# Protein Conformational ensembles generation\r\n\r\n## Workflow included in the [ELIXIR 3D-Bioinfo](https://elixir-europe.org/communities/3d-bioinfo) Implementation Study:\r\n\r\n### Building on PDBe-KB to chart and characterize the conformation landscape of native proteins\r\n\r\nThis tutorial aims to illustrate the process of generating **protein conformational ensembles** from** 3D structures **and analysing its **molecular flexibility**, step by step, using the **BioExcel Building Blocks library (biobb)**.\r\n\r\n## Conformational landscape of native proteins\r\n**Proteins** are **dynamic** systems that adopt multiple **conformational states**, a property essential for many **biological processes** (e.g. binding other proteins, nucleic acids, small molecule ligands, or switching between functionaly active and inactive states). Characterizing the different **conformational states** of proteins and the transitions between them is therefore critical for gaining insight into their **biological function** and can help explain the effects of genetic variants in **health** and **disease** and the action of drugs.\r\n\r\n**Structural biology** has become increasingly efficient in sampling the different **conformational states** of proteins. The **PDB** has currently archived more than **170,000 individual structures**, but over two thirds of these structures represent **multiple conformations** of the same or related protein, observed in different crystal forms, when interacting with other proteins or other macromolecules, or upon binding small molecule ligands. Charting this conformational diversity across the PDB can therefore be employed to build a useful approximation of the **conformational landscape** of native proteins.\r\n\r\nA number of resources and **tools** describing and characterizing various often complementary aspects of protein **conformational diversity** in known structures have been developed, notably by groups in Europe. These tools include algorithms with varying degree of sophistication, for aligning the 3D structures of individual protein chains or domains, of protein assemblies, and evaluating their degree of **structural similarity**. Using such tools one can **align structures pairwise**, compute the corresponding **similarity matrix**, and identify ensembles of **structures/conformations** with a defined **similarity level** that tend to recur in different PDB entries, an operation typically performed using **clustering** methods. Such workflows are at the basis of resources such as **CATH, Contemplate, or PDBflex** that offer access to **conformational ensembles** comprised of similar **conformations** clustered according to various criteria. Other types of tools focus on differences between **protein conformations**, identifying regions of proteins that undergo large **collective displacements** in different PDB entries, those that act as **hinges or linkers**, or regions that are inherently **flexible**.\r\n\r\nTo build a meaningful approximation of the **conformational landscape** of native proteins, the **conformational ensembles** (and the differences between them), identified on the basis of **structural similarity/dissimilarity** measures alone, need to be **biophysically characterized**. This may be approached at **two different levels**. \r\n- At the **biological level**, it is important to link observed **conformational ensembles**, to their **functional roles** by evaluating the correspondence with **protein family classifications** based on sequence information and **functional annotations** in public databases e.g. Uniprot, PDKe-Knowledge Base (KB). These links should provide valuable mechanistic insights into how the **conformational and dynamic properties** of proteins are exploited by evolution to regulate their **biological function**. <br><br>\r\n\r\n- At the **physical level** one needs to introduce **energetic consideration** to evaluate the likelihood that the identified **conformational ensembles** represent **conformational states** that the protein (or domain under study) samples in isolation. Such evaluation is notoriously **challenging** and can only be roughly approximated by using **computational methods** to evaluate the extent to which the observed **conformational ensembles** can be reproduced by algorithms that simulate the **dynamic behavior** of protein systems. These algorithms include the computationally expensive **classical molecular dynamics (MD) simulations** to sample local thermal fluctuations but also faster more approximate methods such as **Elastic Network Models** and **Normal Node Analysis** (NMA) to model low energy **collective motions**. Alternatively, **enhanced sampling molecular dynamics** can be used to model complex types of **conformational changes** but at a very high computational cost. \r\n\r\nThe **ELIXIR 3D-Bioinfo Implementation Study** *Building on PDBe-KB to chart and characterize the conformation landscape of native proteins* focuses on:\r\n\r\n1. Mapping the **conformational diversity** of proteins and their homologs across the PDB. \r\n2. Characterize the different **flexibility properties** of protein regions, and link this information to sequence and functional annotation.\r\n3. Benchmark **computational methods** that can predict a biophysical description of protein motions.\r\n\r\nThis notebook is part of the third objective, where a list of **computational resources** that are able to predict **protein flexibility** and **conformational ensembles** have been collected, evaluated, and integrated in reproducible and interoperable workflows using the **BioExcel Building Blocks library**. Note that the list is not meant to be exhaustive, it is built following the expertise of the implementation study partners.\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2025 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2025 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.486.4",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "486",
        "keep": true,
        "latest_version": 4,
        "license": "other-open",
        "link": "https:/workflowhub.eu/workflows/486?version=4",
        "name": "Jupyter Notebook Protein conformational ensembles generation",
        "number_of_steps": 0,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Jupyter",
        "update_time": "2025-08-06",
        "versions": 4
    },
    {
        "create_time": "2025-08-06",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# CMIP tutorial using BioExcel Building Blocks (biobb)\r\n\r\n***\r\n\r\nThis tutorial aims to illustrate the process of computing **classical molecular interaction potentials** from **protein structures**, step by step, using the **BioExcel Building Blocks library (biobb)**. Examples shown are **Molecular Interaction Potentials (MIPs) grids, protein-protein/ligand interaction potentials, and protein titration**. The particular structures used are the **Lysozyme** protein (PDB code [1AKI](https://www.rcsb.org/structure/1aki)), and a MD simulation of the complex formed by the **SARS-CoV-2 Receptor Binding Domain and the human Angiotensin Converting Enzyme 2** (PDB code [6VW1](https://www.rcsb.org/structure/6vw1)).\r\n\r\nThe code wrapped is the ***Classical Molecular Interaction Potentials (CMIP)*** code:\r\n\r\n**Classical molecular interaction potentials: Improved setup procedure in molecular dynamics simulations of proteins.**\r\n*Gelp\u00ed, J.L., Kalko, S.G., Barril, X., Cirera, J., de la Cruz, X., Luque, F.J. and Orozco, M. (2001)*\r\n*Proteins, 45: 428-437. [https://doi.org/10.1002/prot.1159](https://doi.org/10.1002/prot.1159)*\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2025 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2025 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.820.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "820",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/820?version=2",
        "name": "Docker Classical Molecular Interaction Potentials",
        "number_of_steps": 0,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Docker",
        "update_time": "2025-08-06",
        "versions": 2
    },
    {
        "create_time": "2025-08-06",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# CMIP tutorial using BioExcel Building Blocks (biobb)\r\n\r\n***\r\n\r\nThis tutorial aims to illustrate the process of computing **classical molecular interaction potentials** from **protein structures**, step by step, using the **BioExcel Building Blocks library (biobb)**. Examples shown are **Molecular Interaction Potentials (MIPs) grids, protein-protein/ligand interaction potentials, and protein titration**. The particular structures used are the **Lysozyme** protein (PDB code [1AKI](https://www.rcsb.org/structure/1aki)), and a MD simulation of the complex formed by the **SARS-CoV-2 Receptor Binding Domain and the human Angiotensin Converting Enzyme 2** (PDB code [6VW1](https://www.rcsb.org/structure/6vw1)).\r\n\r\nThe code wrapped is the ***Classical Molecular Interaction Potentials (CMIP)*** code:\r\n\r\n**Classical molecular interaction potentials: Improved setup procedure in molecular dynamics simulations of proteins.**\r\n*Gelp\u00ed, J.L., Kalko, S.G., Barril, X., Cirera, J., de la Cruz, X., Luque, F.J. and Orozco, M. (2001)*\r\n*Proteins, 45: 428-437. [https://doi.org/10.1002/prot.1159](https://doi.org/10.1002/prot.1159)*\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2025 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2025 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.774.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "774",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/774?version=2",
        "name": "Python Classical Molecular Interaction Potentials",
        "number_of_steps": 0,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2025-08-06",
        "versions": 2
    },
    {
        "create_time": "2025-08-06",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# CMIP tutorial using BioExcel Building Blocks (biobb)\r\n\r\n***\r\n\r\nThis tutorial aims to illustrate the process of computing **classical molecular interaction potentials** from **protein structures**, step by step, using the **BioExcel Building Blocks library (biobb)**. Examples shown are **Molecular Interaction Potentials (MIPs) grids, protein-protein/ligand interaction potentials, and protein titration**. The particular structures used are the **Lysozyme** protein (PDB code [1AKI](https://www.rcsb.org/structure/1aki)), and a MD simulation of the complex formed by the **SARS-CoV-2 Receptor Binding Domain and the human Angiotensin Converting Enzyme 2** (PDB code [6VW1](https://www.rcsb.org/structure/6vw1)).\r\n\r\nThe code wrapped is the ***Classical Molecular Interaction Potentials (CMIP)*** code:\r\n\r\n**Classical molecular interaction potentials: Improved setup procedure in molecular dynamics simulations of proteins.**\r\n*Gelp\u00ed, J.L., Kalko, S.G., Barril, X., Cirera, J., de la Cruz, X., Luque, F.J. and Orozco, M. (2001)*\r\n*Proteins, 45: 428-437. [https://doi.org/10.1002/prot.1159](https://doi.org/10.1002/prot.1159)*\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2025 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2025 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.773.3",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "773",
        "keep": true,
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/773?version=3",
        "name": "Jupyter Notebook Classical Molecular Interaction Potentials",
        "number_of_steps": 0,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Jupyter",
        "update_time": "2025-08-06",
        "versions": 3
    },
    {
        "create_time": "2025-08-04",
        "creators": [],
        "description": "Classification and visualization of SSU, LSU sequences.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [subworkflows](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_subworkflows.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: EMBL's European Bioinformatics Institute, Rand Zoabi, Paul Zierep\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1842",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1842?version=1",
        "name": "MGnify's amplicon pipeline v5.0 - rRNA prediction",
        "number_of_steps": 47,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "gtn",
            "galaxy",
            "metagenomics",
            "mgnify_amplicon",
            "name:microgalaxy"
        ],
        "tools": [
            "__FILTER_EMPTY_DATASETS__",
            "",
            "bedtools_getfastabed",
            "tp_awk_tool",
            "query_tabular",
            "biom_convert",
            "infernal_cmsearch",
            "cshl_fasta_formatter",
            "gops_concat_1",
            "collection_element_identifiers",
            "taxonomy_krona_chart",
            "cmsearch_deoverlap",
            "mapseq",
            "__FILTER_FROM_FILE__"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-02",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-variantbenchmarking_logo_dark.png\">\n    <img alt=\"nf-core/variantbenchmarking\" src=\"docs/images/nf-core-variantbenchmarking_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/variantbenchmarking/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/variantbenchmarking/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/variantbenchmarking/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/variantbenchmarking/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/variantbenchmarking/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.14916661-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.14916661)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/variantbenchmarking)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23variantbenchmarking-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/variantbenchmarking)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/variantbenchmarking** is designed to evaluate and validate the accuracy of variant calling methods in genomic research. Initially, the pipeline is tuned well for available gold standard truth sets (for example, Genome in a Bottle and SEQC2 samples) but it can be used to compare any two variant calling results. The workflow provides benchmarking tools for small variants including SNVs and INDELs, Structural Variants (SVs) and Copy Number Variations (CNVs) for germline and somatic analysis.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\n<p align=\"center\">\n    <img title=\"variantbenchmarking metro map\" src=\"docs/images/variantbenchmarking_metromap.png\" width=100%>\n</p>\n\nThe workflow involves several key processes to ensure reliable and reproducible results as follows:\n\n### Standardization and normalization of variants:\n\nThis initial step ensures consistent formatting and alignment of variants in test and truth VCF files for accurate comparison.\n\n- Subsample if input test vcf is multisample ([bcftools view](https://samtools.github.io/bcftools/bcftools.html#view))\n- Homogenization of multi-allelic variants, MNPs and SVs (including imprecise paired breakends and single breakends) ([variant-extractor](https://github.com/EUCANCan/variant-extractor))\n- Reformatting test VCF files from different SV callers ([svync](https://github.com/nvnieuwk/svync))\n- Rename sample names in test and truth VCF files ([bcftools reheader](https://samtools.github.io/bcftools/bcftools.html#reheader))\n- Splitting multi-allelic variants in test and truth VCF files ([bcftools norm](https://samtools.github.io/bcftools/bcftools.html#norm))\n- Deduplication of variants in test and truth VCF files ([bcftools norm](https://samtools.github.io/bcftools/bcftools.html#norm))\n- Left aligning of variants in test and truth VCF files ([bcftools norm](https://samtools.github.io/bcftools/bcftools.html#norm))\n- Use prepy in order to normalize test files. This option is only applicable for happy benchmarking of germline analysis ([prepy](https://github.com/Illumina/hap.py/tree/master))\n- Split SNVs and indels if the given test VCF contains both. This is only applicable for somatic analysis ([bcftools view](https://samtools.github.io/bcftools/bcftools.html#view))\n\n### Filtering options:\n\nApplying filtering on the process of benchmarking itself might makes it impossible to compare different benchmarking strategies. Therefore, for whom like to compare benchmarking methods this subworkflow aims to provide filtering options for variants.\n\n- Filtration of contigs ([bcftools view](https://samtools.github.io/bcftools/bcftools.html#view))\n- Include or exclude SNVs and INDELs ([bcftools filter](https://samtools.github.io/bcftools/bcftools.html#filter))\n- Size and quality filtering for SVs ([SURVIVOR filter](https://github.com/fritzsedlazeck/SURVIVOR/wiki))\n\n### Liftover of vcfs:\n\nThis sub-workflow provides option to convert genome coordinates of truth VCF and test VCFs and high confidence BED file to a new assembly. Golden standard truth files are build upon specific reference genomes which makes the necessity of lifting over depending on the test VCF in query. Lifting over one or more test VCFs is also possible.\n\n- Create sequence dictionary for the reference ([picard CreateSequenceDictionary](https://gatk.broadinstitute.org/hc/en-us/articles/360037068312-CreateSequenceDictionary-Picard)). This file can be saved and reused.\n- Lifting over VCFs ([picard LiftoverVcf](https://gatk.broadinstitute.org/hc/en-us/articles/360037060932-LiftoverVcf-Picard))\n- Lifting over high confidence coordinates ([UCSC liftover](http://hgdownload.cse.ucsc.edu/admin/exe))\n\n### Statistical inference of input test and truth variants:\n\nThis step provides insights into the distribution of variants before benchmarking by extracting variant statistics:.\n\n- SNVs, INDELs and complex variants ([bcftools stats](https://samtools.github.io/bcftools/bcftools.html#stats))\n- SVs by type ([SURVIVOR stats](https://github.com/fritzsedlazeck/SURVIVOR/wiki))\n\n### Benchmarking of variants:\n\nActual benchmarking of variants are split between SVs and small variants:\n\nAvailable methods for germline and somatic _structural variant (SV)_ benchmarking are:\n\n- Truvari ([truvari bench](https://github.com/acenglish/truvari/wiki/bench))\n- SVanalyzer ([svanalyzer benchmark](https://github.com/nhansen/SVanalyzer/blob/master/docs/svbenchmark.rst))\n- Rtgtools (only for BND) ([rtg bndeval](https://realtimegenomics.com/products/rtg-tools))\n\n> [!NOTE]\n> Please note that there is no somatic specific tool for SV benchmarking in this pipeline.\n\nAvailable methods for germline and somatic _CNVs (copy number variations)_ are:\n\n- Truvari ([truvari bench](https://github.com/acenglish/truvari/wiki/bench))\n- Wittyer ([witty.er](https://github.com/Illumina/witty.er/tree/master))\n- Intersection ([bedtools intersect](https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html))\n\n> [!NOTE]\n> Please note that there is no somatic specific tool for CNV benchmarking in this pipeline.\n\nAvailable methods for *small variants: SNVs and INDEL*s:\n\n- Germline variant benchmarking using ([rtg vcfeval](https://realtimegenomics.com/products/rtg-tools))\n- Germline variant benchmarking using ([hap.py](https://github.com/Illumina/hap.py/blob/master/doc/happy.md))\n- Somatic variant benchmarking using ([rtg vcfeval --squash-ploidy](https://realtimegenomics.com/products/rtg-tools))\n- Somatic variant benchmarking using ([som.py](https://github.com/Illumina/hap.py/tree/master?tab=readme-ov-file#sompy))\n\n> [!NOTE]\n> Please note that using happ.py and som.py with rtgtools as comparison engine is also possible. Check conf/tests/test_ga4gh.config as an example.\n\n### Intersection of benchmark regions:\n\nIntersecting test and truth BED regions produces benchmark metrics. Intersection analysis is especially recommended for _CNV benchmarking_ where result reports may variate per tool.\n\n- Convert SV or CNV VCF file to BED file, if no regions file is provided for test case using ([SVTK vcf2bed](https://github.com/broadinstitute/gatk-sv/blob/main/src/svtk/scripts/svtk))\n- Convert VCF file to BED file, if no regions file is provided for test case using ([Bedops convert2bed](https://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/convert2bed.html#convert2bed))\n- Intersect the regions and gether benchmarking statistics using ([bedtools intersect](https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html))\n\n### Comparison of benchmarking results per TP, FP and FN files\n\nIt is essential to compare benchmarking results in order to infer uniquely or commonly seen TPs, FPs and FNs.\n\n- Merging TP, FP and FN results for happy, rtgtools and sompy ([bcftools merge](https://samtools.github.io/bcftools/bcftools.html#merge))\n- Merging TP, FP and FN results for Truvari and SVanalyzer ([SURVIVOR merge](https://github.com/fritzsedlazeck/SURVIVOR/wiki))\n- Conversion of VCF files to CSV to infer common and unique variants per caller (python script)\n\n### Reporting of benchmark results\n\nThe generation of comprehensive report that consolidates all benchmarking results.\n\n- Merging summary statistics per benchmarking tool (python script)\n- Plotting benchmark metrics per benchmarking tool (R script)\n- Create visual HTML report for the integration of NCBENCH ([datavzrd](https://datavzrd.github.io/docs/index.html))\n- Apply _MultiQC_ to visualize results\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nid,test_vcf,caller\ntest1,test1.vcf.gz,delly\ntest2,test2.vcf,gatk\ntest3,test3.vcf.gz,cnvkit\n```\n\nEach row represents a vcf file (test-query file). For each vcf file and variant calling method (caller) have to be defined.\n\nUser _has to provide truth_vcf and truth_id in config files_.\n\n> [!NOTE]\n> There are publicly available truth sources. For germline analysis, it is common to use [genome in a bottle (GiAB)](https://www.nist.gov/programs-projects/genome-bottle) variants. There are variate type of golden truths and high confidence regions for hg37 and hg38 references. Please select and use carefully.\n> For somatic analysis, [SEQC2 project](https://sites.google.com/view/seqc2/home/data-analysis/high-confidence-somatic-snv-and-indel-v1-2) released SNV, INDEL and CNV regions. One, can select and use those files.\n\nHere you can find example combinations of [truth files](docs/truth.md)\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/variantbenchmarking/usage) and the [parameter documentation](https://nf-co.re/variantbenchmarking/parameters).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/variantbenchmarking \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR> \\\n   --genome GRCh37 \\\n   --analysis germline \\\n   --truth_id HG002 \\\n   --truth_vcf truth.vcf.gz\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n> Conda profile is not available for SVanalyzer (SVBenchmark) tool, if you are planing to use the tool either choose docker or singularity.\n\n### Example usages\n\nThis pipeline enables quite a number of subworkflows suitable for different benchmarking senarios. Please go through [this documentation](docs/testcases.md) to learn some example usages which discusses about the test config files under conf/tests and tests/.\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/variantbenchmarking/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/variantbenchmarking/output).\n\nThis pipeline outputs benchmarking results per method besides to the inferred and compared statistics.\n\n## Credits\n\nnf-core/variantbenchmarking was originally written by K\u00fcbra Narc\u0131 ([@kubranarci](https://github.com/kubranarci)) as a part of benchmarking studies in German Human Genome Phenome Archieve Project ([GHGA](https://www.ghga.de/)).\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- Nicolas Vannieuwkerke ([@nvnienwk](https://github.com/nvnieuwk)),\n- Maxime Garcia ([@maxulysse](https://github.com/maxulysse)),\n- Sameesh Kher ([@khersameesh24](https://github.com/khersameesh24))\n- Florian Heyl ([@heylf](https://github.com/heyl))\n- Kre\u0161imir Be\u0161tak ([@kbestak](https://github.com/kbestak))\n- Elad Herz ([@EladH1](https://github.com/EladH1))\n\n## Acknowledgements\n\n<a href=\"https://www.ghga.de/\">\n  <img src=\"docs/images/GHGA_short_Logo_orange.png\" alt=\"GHGA\" width=\"200\"/>\n</a>\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#variantbenchmarking` channel](https://nfcore.slack.com/channels/variantbenchmarking) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/variantbenchmarking for your analysis, please cite it using the following doi: [110.5281/zenodo.14916661](https://doi.org/10.5281/zenodo.14916661)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1307",
        "keep": true,
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1307?version=4",
        "name": "nf-core/variantbenchmarking",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "benchmark",
            "draft",
            "structural-variants",
            "variant-calling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-02",
        "versions": 4
    },
    {
        "create_time": "2025-07-29",
        "creators": [
            "Tong LI"
        ],
        "description": "PaSTa is a nextflow-based end-to-end image analysis pipeline for decoding image-based spatial transcriptomics data. It performs imaging cycle registration, cell segmentation and transcripts peak decoding. It is currently supports analysis of three types of ST technology:\r\n\r\n- in-situ sequencing-like encoding\r\n- MERFISH-like encoding\r\n- RNAScope-like labelling\r\n\r\nPrerequisites:\r\n1. Nextflow. Installation guide: https://www.nextflow.io/docs/latest/getstarted.html\r\n2. Docker or Singularity. Installation guide: https://docs.docker.com/get-docker/ or https://sylabs.io/guides/3.7/user-guide/quick_start.html\r\n\r\nDemo run with GitPod\r\n1. Clone the repository\r\n```\r\ngit clone https://github.com/cellgeni/Image-ST.git\r\n```\r\n2. Prepare the run.config file *\r\n```\r\nprocess {\r\n        withName: CELLPOSE {\r\n                ext.args = \"--channels [0,0]\"\r\n                storeDir = \"./output/naive_cellpose_segmentation/\"\r\n        }\r\n\r\n        withName: POSTCODE {\r\n                memory = {20.Gb * task.attempt}\r\n                storeDir = \"./output/PoSTcode_decoding_output\"\r\n        }\r\n\r\n        withName: TO_SPATIALDATA {\r\n                memory = {20.Gb * task.attempt}\r\n                ext.args = \"--feature_col 'Name' --expansion_in_pixels 30 --save_label_img False\"\r\n        }\r\n\r\n        withName: MERGE_OUTLINES {\r\n                storeDir = \"./output/merged_cellpose_segmentation/\"\r\n        }\r\n\r\n        withName: BIOINFOTONGLI_MICROALIGNER {\r\n                memory = {50.Gb * task.attempt}\r\n                storeDir = \"./output/registered_stacks\"\r\n        }\r\n\r\n        withName: BIOINFOTONGLI_TILEDSPOTIFLOW {\r\n                memory = {30.Gb * task.attempt}\r\n                storeDir = \"./output/spotiflow_peaks/\"\r\n        }\r\n\r\n        withName: BIOINFOTONGLI_MERGEPEAKS {\r\n                memory = {50.Gb * task.attempt}\r\n                storeDir = \"./output/spotiflow_peaks/\"\r\n        }\r\n\r\n        withName: BIOINFOTONGLI_CONCATENATEWKTS {\r\n                memory = {50.Gb * task.attempt}\r\n                storeDir = \"./output/spotiflow_peaks/\"\r\n        }\r\n\r\n        withName: EXTRACT_PEAK_PROFILE {\r\n                memory = {50.Gb * task.attempt}\r\n                storeDir = \"./output/peak_profiles/\"\r\n        }\r\n}\r\n```\r\n3. Prepare the parameters file (e.g. iss.yaml)\r\n```\r\nimages:\r\n   - ['id': \"test\",\r\n       [\r\n         \"cycle1.ome.tiff\",\r\n         \"cycle2.ome.tiff\",\r\n         \"cycle3.ome.tiff\",\r\n         \"cycle4.ome.tiff\",\r\n         \"cycle5.ome.tiff\",\r\n         \"cycle6.ome.tiff\",\r\n       ]\r\n     ]\r\ncell_diameters: [30]\r\nchs_to_call_peaks: [1,2] // channels to call peaks, can be multiple\r\ncodebook:\r\n  - ['id': \"test\", \"./codebook.csv\", \"./dummy.txt\"] // has to match the meta in `images` variable\r\nsegmentation_method: \"CELLPOSE\" // or DEEPCELL or STARDIST or INSTANSEG\r\n\r\nout_dir: \"./output\"\r\n```\r\n4. Run the pipeline\r\n```\r\nnextflow run ./Image-ST/main.nf -profile lsf,singularity -c run.config -params-file iss.yaml -entry RUN_DECODING -resume\r\n```\r\n5. Check the output in the specified storeDir.\r\n\r\nSpin up Napari with napari-spatialdata plugin installed (https://spatialdata.scverse.org/projects/napari/en/latest/notebooks/spatialdata.html)\r\n\r\nThen use the following command to visualize the output\r\n```\r\nfrom napari_spatialdata import Interactive\r\nimport spatialdata as spd\r\n\r\ndata = spd.read_zarr([path-to-.sdata-folder])\r\nInteractive(data)\r\n```\r\n\r\n*: You may leave the process block empty if you want to use the default parameters.\r\n\r\nFAQ\r\n---\r\n\r\n1. My HOME dir is full when running Singularity image conversion on HPC.\r\n\r\nA quick solution is to manually specify singularity dir by setting:\r\n\r\n\r\n```\r\nsingularity cache clean\r\nexport SINGULARITY_CACHEDIR=./singularity_image_dir\r\nexport NXF_SINGULARITY_CACHEDIR=./singularity_image_dir\r\n```\r\n\r\n2. How do I modify parameters to specific process/step?\r\n\r\nBy following nf-core standard, it is possible to add any parameters to the main script using ext.args=\u201d--[key] [value]\u201d in the run.config file.\r\n\r\nAn example is\r\n\r\nwithName: POSTCODE {\r\n    ext.args = \"--codebook_targer_col L-probe --codebook_code_col code \"\r\n}\r\n\r\n3. Cannot download pretrained model for the deep-learning tools (Spotiflow/CellPose)\r\n\r\n> Exception: URL fetch failure on https://drive.switch.ch/index.php/s/6AoTEgpIAeQMRvX/download: None -- [Errno -3] Temporary failure in name resolution\r\nOr CellPose\r\nurllib.error.URLError: <urlopen error [Errno -3] Temporary failure in name resolution>\r\n\r\nMostly likely you've reached max download (?), wait a bit and try later OR manually download those models and update the configuration file.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1841",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1841?version=1",
        "name": "A nextflow pipeline to run the end-to-end image-based in-situ sequencing decoding and RNAScope-like analysis",
        "number_of_steps": 0,
        "projects": [
            "Euro-BioImaging"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "decoding",
            "image-analysis",
            "spatial",
            "spatial transcriptomics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-07-29",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [
            "Marie Joss\u00e9"
        ],
        "description": "Secondary metabolite biosynthetic gene cluster (SMBGC) Annotation using Neural Networks Trained on Interpro Signatures \r\n\r\n## Associated Tutorial\r\n\r\nThis workflows is part of the tutorial [Marine Omics identifying biosynthetic gene clusters](https://training.galaxyproject.org/training-material/topics/ecology/tutorials/marine_omics_bgc/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\r\n\r\n## Features\r\n\r\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\r\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\r\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\r\n\r\n## Thanks to...\r\n\r\n**Workflow Author(s)**: Marie Joss\u00e9\r\n\r\n**Tutorial Author(s)**: [Marie Josse](https://training.galaxyproject.org/training-material/hall-of-fame/Marie59/)\r\n\r\n**Tutorial Contributor(s)**: [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\r\n\r\n**Grants(s)**: [Fair-Ease](https://training.galaxyproject.org/training-material/hall-of-fame/fairease/), [EuroScienceGateway](https://training.galaxyproject.org/training-material/hall-of-fame/eurosciencegateway/)\r\n\r\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": "10.48546/workflowhub.workflow.1663.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1663",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1663?version=1",
        "name": "Marine Omics identifying biosynthetic gene clusters",
        "number_of_steps": 5,
        "projects": [
            "Galaxy Training Network",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "earth-system",
            "gtn",
            "galaxy",
            "marineomics",
            "ocean"
        ],
        "tools": [
            "interproscan",
            "regex1",
            "sanntis_marine",
            "prodigal"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-28",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [
            "Romane Libouban",
            "Anthony Bretaudeau"
        ],
        "description": "This workflow allows you to annotate a genome with Helixer and evaluate the quality of the annotation using BUSCO and Genome Annotation statistics. GFFRead is also used to predict protein sequences derived from this annotation, and BUSCO and OMArk are used to assess proteome quality. \r\n\r\n## Associated Tutorial\r\n\r\nThis workflows is part of the tutorial [Genome annotation with Helixer](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/helixer/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\r\n\r\n## Features\r\n\r\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\r\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\r\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\r\n\r\n## Thanks to...\r\n\r\n**Workflow Author(s)**: Romane Libouban\r\n\r\n**Tutorial Author(s)**: [Romane LIBOUBAN](https://training.galaxyproject.org/training-material/hall-of-fame/rlibouba/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/)\r\n\r\n**Tutorial Contributor(s)**: [Felicitas Kindel](https://training.galaxyproject.org/training-material/hall-of-fame/felicitas215/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Romane LIBOUBAN](https://training.galaxyproject.org/training-material/hall-of-fame/rlibouba/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\r\n\r\n**Grants(s)**: [EuroScienceGateway](https://training.galaxyproject.org/training-material/hall-of-fame/eurosciencegateway/)\r\n\r\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": "10.48546/workflowhub.workflow.1500.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1500",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1500?version=1",
        "name": "annotation_helixer",
        "number_of_steps": 7,
        "projects": [
            "Galaxy Training Network",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "gffread",
            "jcvi_gff_stats",
            "omark",
            "jbrowse",
            "helixer",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-28",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [
            "Anthony Bretaudeau",
            "Helena Rasche",
            "Nathan Dunn",
            "Mateo Boudet"
        ],
        "description": "Refining Genome Annotations with Apollo\r\n\r\n## Associated Tutorial\r\n\r\nThis workflows is part of the tutorial [Refining Genome Annotations with Apollo (prokaryotes)](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/apollo/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\r\n\r\n\r\n\r\n\r\n\r\n## Thanks to...\r\n\r\n**Tutorial Author(s)**: [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Nathan Dunn](https://training.galaxyproject.org/training-material/hall-of-fame/nathandunn/), [Mateo Boudet](https://training.galaxyproject.org/training-material/hall-of-fame/mboudet/)\r\n\r\n**Tutorial Contributor(s)**: [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/), [Simon Bray](https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/)\r\n\r\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EuroScienceGateway](https://training.galaxyproject.org/training-material/hall-of-fame/eurosciencegateway/)\r\n\r\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": "10.48546/workflowhub.workflow.1554.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1554",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1554?version=1",
        "name": "Apollo Load Test",
        "number_of_steps": 5,
        "projects": [
            "Galaxy Training Network",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "iframe",
            "list_organism",
            "jbrowse",
            "create_account",
            "create_or_update"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-28",
        "versions": 1
    },
    {
        "create_time": "2025-07-28",
        "creators": [
            "Anthony Bretaudeau",
            "Alexandre Cormier",
            "Laura Leroi",
            "Erwan Corre",
            "St\u00e9phanie Robin",
            "Jonathan Kreplak"
        ],
        "description": "Masking repeats in a genome using RepeatMasker\r\n\r\n## Associated Tutorial\r\n\r\nThis workflows is part of the tutorial [Masking repeats with RepeatMasker](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/repeatmasker/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\r\n\r\n## Features\r\n\r\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\r\n\r\n## Thanks to...\r\n\r\n**Workflow Author(s)**: Anthony Bretaudeau\r\n\r\n**Tutorial Author(s)**: [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Alexandre Cormier](https://training.galaxyproject.org/training-material/hall-of-fame/alexcorm/), [Laura Leroi](https://training.galaxyproject.org/training-material/hall-of-fame/lleroi/), [Erwan Corre](https://training.galaxyproject.org/training-material/hall-of-fame/r1corre/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/), [Jonathan Kreplak](https://training.galaxyproject.org/training-material/hall-of-fame/jkreplak/)\r\n\r\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Laura Leroi](https://training.galaxyproject.org/training-material/hall-of-fame/lleroi/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Jonathan Kreplak](https://training.galaxyproject.org/training-material/hall-of-fame/jkreplak/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/), [Simon Bray](https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\r\n\r\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EuroScienceGateway](https://training.galaxyproject.org/training-material/hall-of-fame/eurosciencegateway/)\r\n\r\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": "10.48546/workflowhub.workflow.1538.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1538",
        "keep": true,
        "latest_version": 2,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1538?version=2",
        "name": "RepeatMasker",
        "number_of_steps": 2,
        "projects": [
            "Galaxy Training Network",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "red",
            "repeatmasker_wrapper"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-28",
        "versions": 2
    },
    {
        "create_time": "2025-06-02",
        "creators": [
            "Anthony Bretaudeau"
        ],
        "description": "Structural and functional genome annotation with Funannotate\r\n\r\n## Associated Tutorial\r\n\r\nThis workflows is part of the tutorial [Genome annotation with Funannotate](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/funannotate/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\r\n\r\n## Features\r\n\r\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\r\n\r\n## Thanks to...\r\n\r\n**Workflow Author(s)**: Anthony Bretaudeau\r\n\r\n**Tutorial Author(s)**: [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/)\r\n\r\n**Tutorial Contributor(s)**: [Alexandre Cormier](https://training.galaxyproject.org/training-material/hall-of-fame/alexcorm/), [Laura Leroi](https://training.galaxyproject.org/training-material/hall-of-fame/lleroi/), [Erwan Corre](https://training.galaxyproject.org/training-material/hall-of-fame/r1corre/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Jonathan Kreplak](https://training.galaxyproject.org/training-material/hall-of-fame/jkreplak/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Simon Bray](https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/)\r\n\r\n**Funder(s)**: [ELIXIR Europe](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/)\r\n\r\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EuroScienceGateway](https://training.galaxyproject.org/training-material/hall-of-fame/eurosciencegateway/)\r\n\r\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": "10.48546/workflowhub.workflow.1525.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1525",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1525?version=1",
        "name": "Funannotate",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "interproscan",
            "funannotate_predict",
            "funannotate_annotate",
            "jbrowse",
            "aegean_parseval",
            "rna_star",
            "eggnog_mapper",
            "funannotate_compare",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-28",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [
            "Anthony Bretaudeau"
        ],
        "description": "Functional annotation of protein sequences\r\n\r\n## Associated Tutorial\r\n\r\nThis workflows is part of the tutorial [Functional annotation of protein sequences](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/functional/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\r\n\r\n## Features\r\n\r\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\r\n\r\n## Thanks to...\r\n\r\n**Workflow Author(s)**: Anthony Bretaudeau\r\n\r\n**Tutorial Author(s)**: [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/)\r\n\r\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\r\n\r\n**Funder(s)**: [ELIXIR Europe](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/), [Institut Fran\u00e7ais de Bioinformatique](https://training.galaxyproject.org/training-material/hall-of-fame/ifb/)\r\n\r\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EuroScienceGateway](https://training.galaxyproject.org/training-material/hall-of-fame/eurosciencegateway/)\r\n\r\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": "10.48546/workflowhub.workflow.1521.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1521",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1521?version=1",
        "name": "Functional annotation",
        "number_of_steps": 2,
        "projects": [
            "Galaxy Training Network",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "eggnog_mapper",
            "interproscan"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-28",
        "versions": 1
    },
    {
        "create_time": "2025-06-23",
        "creators": [
            "Romane Libouban"
        ],
        "description": "The idea of this workflow is to compare annotation with two annotation tools that differ in their approach and operation.\r\n\r\n## Associated Tutorial\r\n\r\nThis workflows is part of the tutorial [Comparison of two annotation tools - Helixer and Braker3](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/comparison-braker-helixer-annotation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\r\n\r\n## Features\r\n\r\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\r\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\r\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\r\n\r\n## Thanks to...\r\n\r\n**Workflow Author(s)**: Romane Libouban\r\n\r\n**Tutorial Author(s)**: [Romane LIBOUBAN](https://training.galaxyproject.org/training-material/hall-of-fame/rlibouba/)\r\n\r\n**Grants(s)**: [EuroScienceGateway](https://training.galaxyproject.org/training-material/hall-of-fame/eurosciencegateway/)\r\n\r\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": "10.48546/workflowhub.workflow.1746.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1746",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1746?version=1",
        "name": "Comparison of two annotation tools: Helixer and Braker3",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "busco",
            "braker3",
            "gtn",
            "galaxy",
            "genomeannotation",
            "helixer",
            "jbrowse"
        ],
        "tools": [
            "gffread",
            "omark",
            "jbrowse",
            "braker3",
            "helixer",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-28",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [
            "Romane Libouban",
            "Anthony Bretaudeau"
        ],
        "description": "This workflow uses Braker3 to annotate a genome.\r\n\r\n## Associated Tutorial\r\n\r\nThis workflows is part of the tutorial [Genome annotation with Braker3](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/braker3/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\r\n\r\n## Features\r\n\r\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\r\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\r\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\r\n\r\n## Thanks to...\r\n\r\n**Workflow Author(s)**: Romane Libouban\r\n\r\n**Tutorial Author(s)**: [Romane LIBOUBAN](https://training.galaxyproject.org/training-material/hall-of-fame/rlibouba/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/)\r\n\r\n**Tutorial Contributor(s)**: [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\r\n\r\n**Grants(s)**: [EuroScienceGateway](https://training.galaxyproject.org/training-material/hall-of-fame/eurosciencegateway/)\r\n\r\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": "10.48546/workflowhub.workflow.1509.1",
        "edam_operation": [
            "Gene prediction",
            "Genome annotation",
            "Read mapping",
            "Sequence alignment",
            "Sequence assembly"
        ],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1509",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1509?version=1",
        "name": "Genome annotation with Braker3",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "augustus",
            "busco",
            "braker3",
            "gtn",
            "galaxy",
            "genemark-et",
            "genomeannotation",
            "jbrowse",
            "annotation pipeline",
            "eukaryotic genome",
            "gene prediction",
            "protein evidence",
            "rna-seq",
            "training"
        ],
        "tools": [
            "gffread",
            "jbrowse",
            "busco",
            "fasta-stats",
            "braker3"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-28",
        "versions": 1
    },
    {
        "create_time": "2021-08-18",
        "creators": [
            "Tracy Chew",
            "Cali Willet",
            "Rosemarie Sadsad"
        ],
        "description": "Somatic-ShortV @ NCI-Gadi is a variant calling pipeline that calls somatic short variants (SNPs and indels) from tumour and matched normal BAM files following [GATK's Best Practice Workflow](https://gatk.broadinstitute.org/hc/en-us/articles/360035894731-Somatic-short-variant-discovery-SNVs-Indels-). This workflow is designed for the National Computational Infrastructure's (NCI) Gadi supercompter, leveraging multiple nodes on NCI Gadi to run all stages of the workflow in parallel. \r\n\r\nInfrastructure\\_deployment\\_metadata: Gadi (NCI)",
        "doi": "10.48546/workflowhub.workflow.148.1",
        "edam_operation": [
            "Genome analysis",
            "Genome comparison",
            "Variant calling"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Biomedical science",
            "Genetic variation",
            "Whole genome sequencing"
        ],
        "filtered_on": "metap* in description",
        "id": "148",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/148?version=1",
        "name": "Somatic-ShortV @ NCI-Gadi",
        "number_of_steps": 0,
        "projects": [
            "Sydney Informatics Hub"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gatk4",
            "gadi",
            "indels",
            "mutect2",
            "nci",
            "nci gadi",
            "nci-gadi",
            "snps",
            "somatic",
            "vcf",
            "cancer",
            "scalable",
            "tumour",
            "variant_calling"
        ],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2025-07-25",
        "versions": 1
    },
    {
        "create_time": "2021-08-17",
        "creators": [
            "Tracy Chew",
            "Cali Willet",
            "Georgina Samaha",
            "Rosemarie Sadsad"
        ],
        "description": "Germline-ShortV @ NCI-Gadi is an implementation of the BROAD Institute's best practice workflow for germline short variant discovery. This implementation is optimised for the National Compute Infrastucture's Gadi HPC, utilising scatter-gather parallelism to enable use of multiple nodes with high CPU or memory efficiency. This workflow requires sample BAM files, which can be generated using the [Fastq-to-bam @ NCI-Gadi](https://workflowhub.eu/workflows/146) pipeline. Germline-ShortV can be applied to model and non-model organisms (including non-diploid organisms). \r\n\r\nInfrastructure\\_deployment\\_metadata: Gadi (NCI)",
        "doi": "10.48546/workflowhub.workflow.143.1",
        "edam_operation": [
            "Indel detection",
            "SNP detection",
            "Variant calling",
            "Variant filtering"
        ],
        "edam_topic": [
            "DNA mutation",
            "DNA polymorphism",
            "Genetic variation",
            "Genomics",
            "Population genomics",
            "Whole genome sequencing"
        ],
        "filtered_on": "metap* in description",
        "id": "143",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/143?version=1",
        "name": "Germline-ShortV @ NCI-Gadi",
        "number_of_steps": 0,
        "projects": [
            "Australian BioCommons",
            "Sydney Informatics Hub"
        ],
        "source": "WorkflowHub",
        "tags": [
            "broad",
            "dna",
            "dna-seq",
            "gatk4",
            "genomics",
            "germline",
            "haplotyecaller",
            "indels",
            "snps",
            "wgs",
            "genome",
            "variant_calling"
        ],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2025-07-25",
        "versions": 1
    },
    {
        "create_time": "2021-08-18",
        "creators": [
            "Cali Willet",
            "Tracy Chew"
        ],
        "description": "Bootstrapping-for-BQSR @ NCI-Gadi is a pipeline for bootstrapping a variant resource to enable GATK base quality score recalibration (BQSR) for non-model organisms that lack a publicly available variant resource. This implementation is optimised for the National Compute Infrastucture's Gadi HPC. Multiple rounds of bootstrapping can be performed. Users can use [Fastq-to-bam @ NCI-Gadi](https://workflowhub.eu/workflows/146) and [Germline-ShortV @ NCI-Gadi](https://workflowhub.eu/workflows/143) to produce required input files for Bootstrapping-for-BQSR @ NCI-Gadi. \r\n\r\nInfrastructure\\_deployment\\_metadata: Gadi (NCI)\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.153.1",
        "edam_operation": [
            "Variant calling",
            "Variant filtering"
        ],
        "edam_topic": [
            "Genetic variation",
            "Whole genome sequencing"
        ],
        "filtered_on": "metap* in description",
        "id": "153",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/153?version=1",
        "name": "Bootstrapping-for-BQSR @ NCI-Gadi",
        "number_of_steps": 0,
        "projects": [
            "Sydney Informatics Hub"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "bqsr",
            "bootstrapping",
            "dna",
            "gatk4",
            "genomics",
            "indels",
            "nci",
            "nci-gadi",
            "pbs",
            "snps",
            "wgs",
            "illumina",
            "model",
            "non-model",
            "scalable",
            "variant calling"
        ],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2025-07-25",
        "versions": 1
    },
    {
        "create_time": "2025-07-22",
        "creators": [
            "Yuan Zhang",
            "Cenny Taslim"
        ],
        "description": "# RNA-SeqEZPZ-NF\r\n## Nextflow Pipeline for RNA-SeqEZPZ \r\n### A Point-and-Click Pipeline for Comprehensive Transcriptomics Analysis with Interactive Visualizations\r\n<br />\r\n<br />\r\n\r\nRNA-SeqEZPZ-NF is another implementation of [RNA-SeqEZPZ](https://github.com/cxtaslim/RNA-SeqEZPZ). RNA-SeqEZPZ-NF uses the same user interface as RNA-SeqEZPZ and runs the same pipeline, but runs the pipeline implemented by [Nextflow](https://www.nextflow.io/). This pipeline is currently tested on HPC cluster with SLURM scheduler. Advanced users have the ability to customize the scripts to run with other schedulers.\r\n\r\n<br />\r\n\r\n## Installation\r\nThis pipeline uses Singularity image and Nextflow, which should make it easy to setup. However, the initial setup can sometimes be tricky. Please do not hesitate to create a new issue if you need help with installation or setup.\r\n\r\nIn order to use the pipeline, you will need to have Singularity and Nextflow installed in your HPC. See installation instructions at https://docs.sylabs.io/guides/3.0/user-guide/installation.html and https://www.nextflow.io/docs/latest/install.html\r\n\r\nThe following step-by-step is for a system with SLURM scheduler, Singularity and Nextflow. If you'd like to use the version of the pipeline without Nextflow, please go to [https://github.com/cxtaslim/RNA-SeqEZPZ](https://github.com/cxtaslim/RNA-SeqEZPZ)\r\n\r\n1. Download the code/scripts:\r\n   ```\r\n   git clone https://github.com/yzhang18/RNA-SeqEZPZ-NF.git\r\n   ```\r\n   This step will copy all the required code into your local directory.\r\n2. There are three files that you can make changes to reflect the settings in your local copy of the code.\r\n\r\n   ```\r\n   RNA-SeqEZPZ-NF/main.nf  ## Set up your input, output, genome file, gtf file, etc..\r\n   RNA-SeqEZPZ-NF/scripts/nextflow_config_var.config  ## Set up the variables for your local scheduler.\r\n   RNA-SeqEZPZ-NF/project_ex/nextflow.config  ## Set up the resource limit for processes. This is optional, since most parameters are set up in RNA-Seq-EZPZ-NF/nextflow.config, which is generated automatically by the pipeline. \r\n   ```\r\n\r\n4. Go to the ```RNA-SeqEZPZ-NF``` directory and download the singularity image:\r\n   ```\r\n   # go to RNA-SeqEZPZ-NF directory\r\n   cd RNA-SeqEZPZ-NF\r\n   # download the singularity image and save as rnaseq-pipe-container.sif\r\n   singularity pull --name rnaseq-pipe-container.sif library://cxtaslim/pipelines/rna-seqezpz:latest\r\n   ```\r\n   This step will copy a singularity image.\r\n   Now, you have all the scripts and programs needed to run the entire RNA-Seq pipeline. \r\n\r\n## Downloading hg19 reference files\r\nIn order to run the pipeline, you will need to download reference files.\r\nThese are the steps to get **human hg19** references to run this pipeline. Following these steps will enable you to select hg19 genome in the graphical interface.\r\n1. Go to ```RNA-SeqEZPZ-NF``` directory and create a ```ref/hg19``` directory. **Note**: foldername MUST be ```ref/hg19```\r\n   ```\r\n   # go to RNA-SeqEZPZ-NF directory. Only do this if you haven't done \"cd RNA-SeqEZPZ\" before\r\n   cd RNA-SeqEZPZ-NF\r\n   # create a ref directory inside RNA-SeqEZPZ-NF and a sub-directory called hg19 under ref\r\n   mkdir -p ref/hg19\r\n   ```\r\n3. Go to the directory created in step 1 and download hg19 fasta file to this directory\r\n   ```\r\n   # go to RNA-SeqEZPZ-NF/ref/hg19 directory\r\n   cd ref/hg19\r\n   # download and unzip the fasta file from UCSC genome browser\r\n   wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz | gunzip -c > hg19.fa\r\n   ```\r\n4. Download annotation file (.gtf)\r\n   ```\r\n   # download and unzip the gtf file from UCSC genome browser\r\n   wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/genes/hg19.refGene.gtf.gz | gunzip -c > hg19.refGene.gtf\r\n   ```\r\n5. Optional. Download the chrom.sizes file. You can skip this and the pipeline will generate it for you as long as the ref folder is writable\r\n   ```\r\n   wget https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.chrom.sizes\r\n   ```\r\n7. Now, you should have ```hg19.fa```, ```hg19.refGene.gtf``` and ```hg19.chrom.sizes``` inside ```RNA-SeqEZPZ/ref/hg19```\r\n   ```\r\n   # list the files\r\n   ls -1\r\n   ```\r\n   The above command should show you the fasta, gtf and chrom.sizes files as shown below:\r\n   ```\r\n   ls -1\r\n   hg19.chrom.sizes\r\n   hg19.fa\r\n   hg19.refGene.gtf\r\n   ```\r\n   \r\n## Downloading hg38 reference files\r\nThese are the steps to get **human hg38** references to run this pipeline. Following these steps will enable you to select hg38 genome in the graphical interface.\r\nYou can skip this step if you are not going to use hg38 genome in the graphical interface.\r\n1. Go to ```RNA-SeqEZPZ-NF``` directory and create a ```ref/hg38``` directory. **Note**: foldername MUST be ```ref/hg38```\r\n   ```\r\n   # create RNA-SeqEZPZ-NF/ref/hg38 folder. If you are following the steps above to get hg19 then you'd have to do the\r\n   # following command to create RNA-SeqEZPZ-NF/ref/hg38\r\n   mkdir -p ../hg38\r\n   ```\r\n3. Go to the directory created in step 1 and download hg38 fasta file to this directory\r\n   ```\r\n   # go to RNA-SeqEZPZ-NF/ref/hg38 directory\r\n   cd ../hg38\r\n   # download and unzip the fasta file from UCSC genome browser\r\n   wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz | gunzip -c > hg38.fa\r\n   ```\r\n4. Download annotation file (.gtf)\r\n   ```\r\n   # download and unzip the gtf file from UCSC genome browser\r\n   wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/genes/hg38.refGene.gtf.gz  | gunzip -c > hg38.refGene.gtf\r\n   ```\r\n5. Optional. Download the chrom.sizes file. You can skip this and the pipeline will generate it for you as long as the ref folder is writable\r\n   ```\r\n   wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.chrom.sizes\r\n   ```\r\n7. Now, you should have ```hg38.fa```, ```hg38.refGene.gtf``` and ```hg38.chrom.sizes``` inside ```RNA-SeqEZPZ/ref/hg38```\r\n   ```\r\n   # list the files\r\n   ls -1\r\n   ```\r\n   The above command should show you the fasta, gtf and chrom.sizes files as shown below:\r\n   ```\r\n   ls -1\r\n   hg38.chrom.sizes\r\n   hg38.fa\r\n   hg38.refGene.gtf\r\n   ```\r\n\r\n## Tips on downloading other references\r\n1. Make sure both gtf and fasta files have the same chromosome names.\r\n3. In order for pathway analysis to work, gtf file MUST contains gene symbols.\r\n4. Please place the fasta file inside a folder with ```<genome_name>```.\r\n5. If you don't have ```chrom.sizes``` file for the genome, you need to make the folder ```<genome_name>``` writable.\r\n   On the first run, ```chrom.sizes``` file will be created by the pipeline.\r\n\r\n## Running test dataset with hg19 genome\r\n1. To run the pipeline, if you haven't already, go to the ```RNA-SeqEZPZ-NF``` directory that you cloned on the first step, run run_shiny_analysis.sh with filepath set to ```project_ex```:\r\n```\r\n   # go to RNA-SeqEZPZ-NF folder\r\n   # if you are currently in ref/hg19 folder go up to RNA-SeqEZPZ-NF folder\r\n   cd ../..\r\n   # run the user interface\r\n   bash scripts/run_shiny_analysis.sh filepath=project_ex\r\n   ``` \r\n   <br />\r\n   A Firefox browser will be displayed that will enable you to run the full analysis.\r\n   <br />  \r\n\r\n   ![run_analysis_screenshot](assets/run_analysis_screenshot.png)\r\n\r\n2. In order to run the test dataset, first you will need to select project folder. \r\n   In this case, you would click on ```Select project folder```, a window will appear.\r\n   Please click on ```root``` (make sure it is highlighted with blue background as pictured below) and click on ```Select``` button.\r\n   ![run_example_0](assets/run_example_0.png)\r\n\r\n   **Note**: If you selected the ```project folder``` successfully, under ```Select project folder``` you should see ```Click to load existing samples.txt``` button.\r\n   \r\n3. Next, you will need to fill out the form.\r\n   Test dataset is a down-sampled of the public example dataset described in the manuscript. There are 6 samples:\r\n   | fastq file name | description                                                                                               |\r\n   | ----------------| ----------------------------------------------------------------------------------------------------------|\r\n   | iEF714R1        | cells with knockdown of endogenous EWSR1::FLI1 followed by rescue with a EWSR1::FLI1 construct (714) replicate 1|\r\n   | iEF714R2        | cells with knockdown of endogenous EWSR1::FLI1 followed by rescue with a EWSR1::FLI1 construct (714) replicate 2|\r\n   | iEF563R1        | cells with knockdown of endogenous EWSR1::FLI1 followed by rescue with a EWSR1::ETV4 construct (563) replicate 1|\r\n   | iEF563R2        | cells with knockdown of endogenous EWSR1::FLI1 followed by rescue with a EWSR1::ETV4 construct (563) replicate 2|\r\n   | iEF197R1        | cells with knockdown of endogenous EWSR1::FLI1 followed by rescue with an empty vector (197) replicate 1        |\r\n   | iEF197R2        | cells with knockdown of endogenous EWSR1::FLI1 followed by rescue with an empty vector (197) replicate 2        |\r\n   \r\n   The goal of the analysis is to find genes regulated by EWSR1::FLI1 and genes regulated by EWSR1::ETV4.\r\n   Therefore, we are going to compare iEF714 which we will be our iEF_EF group with iEF_empty as control.\r\n   We will also compare iEF563 which will be our iEF_EE4 group with iEF_empty as control.\r\n   <br> In the form, we will have a total of 6 rows:\r\n   * 2 rows for iEF_EF group since we have two replicates with iEF_empty as control\r\n   * 2 rows for iEF_EE4 group since we have two replicates with iEF_empty as control\r\n   * 2 rows for iEF_empty group since we have two replicates with NA as control. **Note** samples that will be used as control will have control name as NA.\r\n   In each row, we need to select the \\_R1_ and \\_R2_ files for first-pair R1 fastq files and the second-pair R2 fastq files, respectively.\r\n   \r\n   This is what the filled form should look like:\r\n   ![run_example_1](assets/run_example_1.png)\r\n\r\n   You can click on ```Click to load samples.txt``` to automatically fill out the form.\r\n\r\n   **Note**: this step only works because there is an existing samples.txt in the ```project_ex``` directory that was provided for you.\r\n      \r\n4. At this point, you are ready to click on ```Run full analysis``` to run the entire RNA-Seq pipeline steps with the example datasets provided.\r\n\r\n5. After clicking on ```Run full analysis```, you can click on ```Log``` then click on ```Refresh list``` to see the content of ```run_rnaseq_full.out```\r\n   which contains the progress of the pipeline.\r\n  ![run_example_2](assets/run_example_2.png)\r\n   In the screenshot above, the pipeline is currently doing trimming and performing quality control of reads.\r\n   For more information, you can select the log file for a run, e.g. ```trim_fastqc_iEF_EE4_rep2.out``` under ```Choose a log file to view:```\r\n  ![run_example_2_trim](assets/run_example_2_trim.png)\r\n\r\n7. When the entire pipeline is done, you can scroll down on ```run_rnaseq_full.out``` and see similar message as pictured below:\r\n   ![run_example_3](assets/run_example_3.png)\r\n   **Note**: try ```Refresh list``` to view updated file.\r\n\r\n8. Once full analysis is finished, you can click on ```QCs``` tab to see the Quality Control metrics generated.\r\n   ![run_example_4](assets/run_example_4.png)\r\n\r\n9. You can also click on ```Outputs``` tab which contains differential genes analysis calculated by DESeq2 [[2]](#2) and statistical report generated by SARTools [[3]](#3) with modifications.\r\n\r\n10. In the ```Plots``` tab, inserting another comparison group will show the overlap between the two groups of comparisons.\r\n   In this case, it will compare the differential genes in iEF_EF vs iEF_empty with iEF_EE4 vs iEF_empty.\r\n   ![run_example_6](assets/run_example_6.png)\r\n\r\n11. [project_ex/outputs](project_ex/outputs) contains all the outputs automatically generated by the pipeline.\r\n\r\nSince test dataset provided is a small dataset that are provided to quickly test the installation of the pipeline, below we provided screenshots of the ```plots``` tab\r\nwhich were done on the full example dataset to illustrate the analysis that can be done on ```RNA-SeqEZPZ-NF```.\r\n\r\nExample of table feature where you can search by gene name and get its log Fold-Change, mean of count difference, and whether it is significantly up-regulated, down-regulated or not significant (NS).\r\nYou can adjust the significance cut-offs then export the gene list with adjusted significance cut-offs.\r\n\r\n![table_example](assets/table_example.png)\r\n\r\nExample of volcano plot where you can enter the official gene names to make that specific gene be highlighted in the volcano plots.\r\n![volcano_example](assets/volcano_example.png)\r\n\r\nExample of overlaps of genes regulated by EWSR1::FLI1 (iEF_EF vs iEF_empty) with genes regulated by EWSR1::ETV4 (iEF_EE4 vs iEF_empty)\r\n![overlap_example](assets/overlap_example.png)\r\n\r\nExample of upset plot showing overlaps of genes regulated by EWSR1::FLI1 (iEF_EF vs iEF_empty) with genes regulated by EWSR1::ETV4 (iEF_EE4 vs iEF_empty)\r\n![upset_example](assets/upset_example.png)\r\n\r\nExample of pathway analysis genes down-/up-regulated by EWSR1::FLI1 (iEF_EF vs iEF_empty) and genes down-/up-regulated by EWSR1::ETV4 (iEF_EE4 vs iEF_empty)\r\n![pathway_example](assets/pathway_example.png)\r\n\r\n11. The pipeline also provides a \"Nextflow Pipeline Report\" after the pipeline is completed. It provides information about the resource utilization of the whole pipeline and each step. This report is useful for tuning the requested resources for each step. \r\n![Nextflow Pipeline Report Summary](assets/nextflow_pipeline_report.png)\r\n\r\n## References\r\n\r\n<a id=\"2\">[2]</a>\r\nLove, M.I., Huber, W. & Anders, S. Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. Genome Biol 15, 550 (2014). https://doi.org/10.1186/s13059-014-0550-8\r\n\r\n<a id=\"3\">[3]</a>\r\nSARTools: A DESeq2- and EdgeR-Based R Pipeline for Comprehensive Differential Analysis of RNA-Seq Data\r\nVaret H, Brillet-Gu\u00e9guen L, Copp\u00e9e JY, Dillies MA (2016) SARTools: A DESeq2- and EdgeR-Based R Pipeline for Comprehensive Differential Analysis of RNA-Seq Data. PLOS ONE 11(6): e0157022. https://doi.org/10.1371/journal.pone.0157022\r\n\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.1814.2",
        "edam_operation": [
            "RNA-Seq analysis"
        ],
        "edam_topic": [
            "Bioinformatics"
        ],
        "filtered_on": "annot* in description",
        "id": "1814",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-NC-ND-4.0",
        "link": "https:/workflowhub.eu/workflows/1814?version=2",
        "name": "RNA-SeqEZPZ-NF: Nextflow Pipeline for RNA-SeqEZPZ",
        "number_of_steps": 0,
        "projects": [
            "Kendall-Theisen lab pipelines"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-07-23",
        "versions": 2
    },
    {
        "create_time": "2025-07-22",
        "creators": [
            "Cenny Taslim"
        ],
        "description": "# RNA-SeqEZPZ: A Point-and-Click Pipeline for Comprehensive Transcriptomics Analysis with Interactive Visualizations\r\n<br />\r\n<br />\r\n\r\nRNA-SeqEZPZ is a pipeline to run analysis of RNA-Seq experiments from raw FASTQ files all the way to differential genes analysis.\r\nThe pipeline is accessible through a graphical user interface implemented using a Shiny app and features interactive plots.\r\nAdvanced users have the ability to customize the scripts provided with the pipeline.\r\nThis pipeline is designed to run on an HPC cluster.\r\nPlease cite [[1]](#1) if you are using this pipeline for a publication.\r\n\r\n<br />\r\n\r\n## Installation\r\nThis pipeline uses Singularity image, which should make it easy to setup. However, the initial setup can sometimes be tricky. Please do not hesitate to create a new issue if you need help with installation or setup.\r\n\r\nIn order to use the pipeline, you will need to have Singularity installed in your HPC. See installation instruction at https://docs.sylabs.io/guides/3.0/user-guide/installation.html\r\n\r\nThe following step-by-step is for a system with SLURM scheduler and it will run bash scripts. If you need to run it on a different scheduler or if you prefer to use the Nextflow version of the pipeline, please go to https://github.com/yzhang18/RNA-SeqEZPZ-NF\r\n\r\n1. Download the code/scripts:\r\n   ```\r\n   git clone https://github.com/cxtaslim/RNA-SeqEZPZ.git\r\n   ```\r\n   This step will copy all the required code into your local directory.\r\n2. Change the SLURM setting to reflect your HPC settings in your local copy of \r\n\r\n   ```RNA-SeqEZPZ/scripts/slurm_config_var.sh```\r\n\r\n4. Go to the ```RNA-SeqEZPZ``` directory and download the singularity image:\r\n   ```\r\n   # go to RNA-SeqEZPZ directory\r\n   cd RNA-SeqEZPZ\r\n   # download the singularity image and save as rnaseq-pipe-container.sif\r\n   singularity pull --name rnaseq-pipe-container.sif library://cxtaslim/pipelines/rna-seqezpz:latest\r\n   ```\r\n   This step will copy a singularity image.\r\n   Now, you have all the scripts and programs needed to run the entire RNA-Seq pipeline. \r\n\r\n## Downloading hg19 reference files\r\nIn order to run the pipeline, you will need to download reference files.\r\nThese are the steps to get **human hg19** references to run this pipeline. Following these steps will enable you to select hg19 genome in the graphical interface.\r\nThe tutorial below on running test dataset uses human hg19 so you need this to run the test dataset below.\r\n1. Go to ```RNA-SeqEZPZ``` directory and create a ```ref/hg19``` directory. **Note**: foldername MUST be ```ref/hg19```\r\n\r\n   ```\r\n   # if you follow step 4 above, then you are already in RNA-SeqEZPZ directory\r\n   # create a ref directory inside RNA-SeqEZPZ and a sub-directory called hg19 under ref\r\n   mkdir -p ref/hg19\r\n   ```\r\n3. Go to the directory created in step 1 and download hg19 fasta file to this directory\r\n   ```\r\n   # go to RNA-SeqEZPZ/ref/hg19 directory\r\n   cd ref/hg19\r\n   # download and unzip the fasta file from UCSC genome browser\r\n   wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz | gunzip -c > hg19.fa\r\n   ```\r\n4. Download annotation file (.gtf)\r\n   ```\r\n   # download and unzip the gtf file from UCSC genome browser\r\n   wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/genes/hg19.refGene.gtf.gz | gunzip -c > hg19.refGene.gtf\r\n   ```\r\n5. Optional. Download the chrom.sizes file. You can skip this and the pipeline will generate it for you as long as the ref folder is writable\r\n   ```\r\n   wget https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.chrom.sizes\r\n   ```\r\n7. Now, you should have ```hg19.fa```, ```hg19.refGene.gtf``` and ```hg19.chrom.sizes``` inside ```RNA-SeqEZPZ/ref/hg19```\r\n   ```\r\n   # list the files\r\n   ls -1\r\n   ```\r\n   The above command should show you the fasta, gtf and chrom.sizes files as shown below:\r\n   ```\r\n   ls -1\r\n   hg19.chrom.sizes\r\n   hg19.fa\r\n   hg19.refGene.gtf\r\n   ```\r\n  \r\n## Downloading hg38 reference files\r\nThese are the steps to get **human hg38** references to run this pipeline. Following these steps will enable you to select hg38 genome in the graphical interface.\r\nYou can skip this step if you are not going to use hg38 genome in the graphical interface.\r\n1. Go to ```RNA-SeqEZPZ``` directory and create a ```ref/hg38``` directory. **Note**: foldername MUST be ```ref/hg38```\r\n   ```\r\n   # If you are following the steps above to get hg19 then you'd have to do the\r\n   # following command to create RNA-SeqEZPZ/ref/hg38 folder\r\n   mkdir -p ../hg38\r\n   ```\r\n3. Go to the directory created in step 1 and download hg38 fasta file to this directory\r\n   ```\r\n   # go to RNA-SeqEZPZ/ref/hg38 directory\r\n   # if you are following the steps above you do the following command to go to hg38 directory\r\n   cd ../hg38\r\n   # download and unzip the fasta file from UCSC genome browser\r\n   wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz | gunzip -c > hg38.fa\r\n   ```\r\n4. Download annotation file (.gtf)\r\n   ```\r\n   # download and unzip the gtf file from UCSC genome browser\r\n   wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/genes/hg38.refGene.gtf.gz  | gunzip -c > hg38.refGene.gtf\r\n   ```\r\n5. Optional. Download the chrom.sizes file. You can skip this and the pipeline will generate it for you as long as the ref folder is writable\r\n   ```\r\n   wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.chrom.sizes\r\n   ```\r\n7. Now, you should have ```hg38.fa```, ```hg38.refGene.gtf``` and ```hg38.chrom.sizes``` inside ```RNA-SeqEZPZ/ref/hg38```\r\n   ```\r\n   # list the files\r\n   ls -1\r\n   ```\r\n   The above command should show you the fasta, gtf and chrom.sizes files as shown below:\r\n   ```\r\n   ls -1\r\n   hg38.chrom.sizes\r\n   hg38.fa\r\n   hg38.refGene.gtf\r\n   ```\r\n\r\n## Tips on downloading other references\r\n1. Make sure both gtf and fasta files have the same chromosome names.\r\n3. In order for pathway analysis to work, gtf file MUST contains gene symbols.\r\n4. If you don't have ```chrom.sizes``` file for the genome, it will be created for you in the folder where the fasta file is.\r\n\r\n## Running test dataset using hg19\r\n1. To run the pipeline, if you haven't already, go to the ```RNA-SeqEZPZ``` directory that you cloned on the first step, run ```run_shiny_analysis.sh``` with filepath set to ```project_ex```.\r\n   <br/>\r\n   ***Note*** on ```filepath```: since you are using reference file downloaded following the steps above, you only need to set ```filepath``` to a location where FASTQ files and project directory\r\n   are either in the specified path or within its subdirectories. See the section [Running your own dataset using zebrafish danRer11 genome](#running-your-own-dataset-using-zebrafish-danRer11-genome)\r\n   for running the pipeline using genome that's neither hg19 nor hg38. \r\n   ```\r\n   # go to RNA-SeqEZPZ folder\r\n   # if you are currently in ref/hg38 folder following step 7 above, go up to RNA-SeqEZPZ folder\r\n   cd ../..\r\n   # run the user interface\r\n   bash scripts/run_shiny_analysis.sh filepath=project_ex\r\n   ``` \r\n   <br />\r\n   A Firefox browser will be displayed that will enable you to run the full analysis.\r\n   <br />  \r\n\r\n   ![run_analysis_screenshot](assets/run_analysis_screenshot.png)\r\n\r\n3. In order to run the test dataset, first you will need to select project folder. \r\n   In this case, you would click on ```Select project folder```, a window will appear.\r\n   Please click on ```root``` (make sure it is highlighted with blue background as pictured below) and click on ```Select``` button.\r\n   You are clicking on ```root``` because you set your ```filepath``` to ```project_ex``` which is called root in this case.\r\n   ![run_example_0](assets/run_example_0.png)\r\n\r\n   **Note**: If you selected the ```project folder``` successfully, under ```Select project folder``` you should see ```Click to load existing samples.txt``` button.\r\n   \r\n5. Next, you will need to fill out the form.\r\n   <br />\r\n   <a name=\"form\"> How to fill out the form to create samples.txt: </a>\r\n   <br />\r\n   Test dataset is a down-sampled of the public example dataset described in the manuscript. There are 6 samples:\r\n   | fastq file name | description                                                                                               |\r\n   | ----------------| ----------------------------------------------------------------------------------------------------------|\r\n   | iEF714R1        | cells with knockdown of endogenous EWSR1::FLI1 followed by rescue with a EWSR1::FLI1 construct (714) replicate 1|\r\n   | iEF714R2        | cells with knockdown of endogenous EWSR1::FLI1 followed by rescue with a EWSR1::FLI1 construct (714) replicate 2|\r\n   | iEF563R1        | cells with knockdown of endogenous EWSR1::FLI1 followed by rescue with a EWSR1::ETV4 construct (563) replicate 1|\r\n   | iEF563R2        | cells with knockdown of endogenous EWSR1::FLI1 followed by rescue with a EWSR1::ETV4 construct (563) replicate 2|\r\n   | iEF197R1        | cells with knockdown of endogenous EWSR1::FLI1 followed by rescue with an empty vector (197) replicate 1        |\r\n   | iEF197R2        | cells with knockdown of endogenous EWSR1::FLI1 followed by rescue with an empty vector (197) replicate 2        |\r\n   \r\n   The goal of the analysis is to find genes regulated by EWSR1::FLI1 and genes regulated by EWSR1::ETV4.\r\n   Therefore, we are going to compare iEF714 which we will be our iEF_EF group with iEF_empty as control.\r\n   We will also compare iEF563 which will be our iEF_EE4 group with iEF_empty as control.\r\n   <br> In the form, we will have a total of 6 rows:\r\n   * 2 rows for iEF_EF group since we have two replicates with iEF_empty as control\r\n   * 2 rows for iEF_EE4 group since we have two replicates with iEF_empty as control\r\n   * 2 rows for iEF_empty group since we have two replicates with NA as control. **Note** samples that will be used as control will have control name as NA.\r\n   In each row, we need to select the \\_R1_ and \\_R2_ files for first-pair R1 fastq files and the second-pair R2 fastq files, respectively.\r\n   \r\n   This is what the filled form should look like:\r\n   ![run_example_1](assets/run_example_1.png)\r\n\r\n   You can click on ```Click to load samples.txt``` to automatically fill out the form.\r\n\r\n   **Note**: Loading samples.txt only works because there is an existing samples.txt in the ```project_ex``` directory that was provided for you.\r\n      \r\n7. At this point, you are ready to click on ```Run full analysis``` to run the entire RNA-Seq pipeline steps with the example datasets provided.\r\n\r\n8. After clicking on ```Run full analysis```, you can click on ```Log``` then click on ```Refresh list``` to see the content of ```run_rnaseq_full.out```\r\n   which contains the progress of the pipeline.\r\n  ![run_example_2](assets/run_example_2.png)\r\n   In the screenshot above, the pipeline is currently doing trimming and performing quality control of reads.\r\n   For more information, you can select ```run_trim_qc.out``` under ```Choose a log file to view:```\r\n   Please review all the log files to make sure everything is correct.\r\n\r\n9. When the entire pipeline is done, you can scroll down on ```run_rnaseq_full.out``` and see similar message as pictured below:\r\n   ![run_example_3](assets/run_example_3.png)\r\n   **Note**: try ```Refresh list``` to view updated file.\r\n\r\n10. Once full analysis is finished, you can click on ```QCs``` tab to see the Quality Control metrics generated.\r\n   ![run_example_4](assets/run_example_4.png)\r\n\r\n11. You can also click on ```Outputs``` tab which  which contains differential genes analysis calculated by DESeq2 [[2]](#2) and statistical report generated by SARTools [[3]](#3) with modifications.\r\n\r\n12. In the ```Plots``` tab, inserting another comparison group will show the overlap between the two groups of comparisons.\r\n   In this case, it will compare the differential genes in iEF_EF vs iEF_empty with iEF_EE4 vs iEF_empty.\r\n   ![run_example_6](assets/run_example_6.png)\r\n\r\n13. [project_ex_out](project_ex_out) contains all the outputs automatically generated by the pipeline.\r\n    You can run view the result in the interface by going to your ```RNA-SeqEZPZ``` and run ```run_shiny_analysis.sh``` with ```filepath``` set to ```project_ex_out``` as follows:\r\n    ```\r\n    bash scripts/run_shiny_analysis.sh filepath=project_ex_out\r\n    ```\r\n\r\nSince the test dataset provided is a small dataset intended for quickly testing the installation of the pipeline, below we provided screenshots of the ```plots``` tab\r\nwhich were done on the full example dataset to illustrate the analyses that can be done on ```RNA-SeqEZPZ```.\r\n\r\nExample of table feature where you can search by gene name and get its log Fold-Change, mean of count difference, and whether it is significantly up-regulated, down-regulated or not significant (NS).\r\nYou can adjust the significance cut-offs then export the gene list with adjusted significance cut-offs.\r\n\r\n![table_example](assets/table_example.png)\r\n\r\nExample of volcano plot where you can enter the official gene names to make that specific gene be highlighted in the volcano plots.\r\n![volcano_example](assets/volcano_example.png)\r\n\r\nExample of overlaps of genes regulated by EWSR1::FLI1 (iEF_EF vs iEF_empty) with genes regulated by EWSR1::ETV4 (iEF_EE4 vs iEF_empty)\r\n![overlap_example](assets/overlap_example.png)\r\n\r\nExample of upset plot showing overlaps of genes regulated by EWSR1::FLI1 (iEF_EF vs iEF_empty) with genes regulated by EWSR1::ETV4 (iEF_EE4 vs iEF_empty)\r\n![upset_example](assets/upset_example.png)\r\n\r\nExample of pathway analysis genes down-/up-regulated by EWSR1::FLI1 (iEF_EF vs iEF_empty) and genes down-/up-regulated by EWSR1::ETV4 (iEF_EE4 vs iEF_empty)\r\n![pathway_example](assets/pathway_example.png)\r\n\r\n### Running your own dataset using zebrafish danRer11 genome.\r\n***Note*** on ```filepath```: You would need to download zebrafish references and have your FASTQ files and project folder where you would save all the outputs under the same parent directory.\r\nWhen running the pipeline, you would then set the ```filepath``` to a location where FASTQ, references files and project directory are either in the specified path\r\nor within its subdirectories. See example below.\r\n   \r\n1. For example, in the following setting, you will put your references, fastq files and project folder in a folder under ```RNA-SeqEZPZ```\r\n   ```\r\n   # if you follow the steps to run test dataset, you can create ```RNA-SeqEZPZ/ref/danRer11``` with the following command\r\n   mkdir -p ref/danRer11\r\n   # go to danRer11 folder and download the reference files\r\n   cd ref/danRer11\r\n   wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/danRer11/bigZips/danRer11.fa.gz | gunzip -c > danRer11.fa\r\n   wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/danRer11/bigZips/genes/danRer11.refGene.gtf.gz  | gunzip -c > danRer11.refGene.gtf\r\n   wget https://hgdownload.soe.ucsc.edu/goldenPath/danRer11/bigZips/danRer11.chrom.sizes\r\n   ```\r\n2. You would need to put your FASTQ files in the same parent directory as the reference files in step 1 or its subdirectories. In this example, you would put it under ```RNA-SeqEZPZ``` directory.\r\n   For example ```RNA-SeqEZPZ/raw_data/fastq```. That way both reference files and FASTQ files are accessible by setting the filepath to RNA-SeqEZPZ directory.\r\n3. Go to ```RNA-SeqEZPZ``` folder and run ```run_shiny_analysis.sh``` with filepath that contains both FASTQ, reference fasta, gtf files and also where you want to save your analysis.\r\n   \r\n   In this example, since your FASTQ files are inside ```RNA-SeqEZPZ/raw_data/fastq```, your reference are inside ```RNA-SeqEZPZ/ref``` and you want to save your analysis under the ```RNA-SeqEZPZ``` folder, you can set\r\n   filepath to where your RNA-SeqEZPZ is.\r\n   Assuming you are in RNA-SeqEZPZ folder, you can simply specify ```filepath=.```. The dot means setting filepath to the current folder.\r\n   ``` \r\n   # if you are currently in RNA-SeqEZPZ/ref/danRer11 folder following step 1 of running your own dataset,\r\n   # you have to go up twice to go to RNA-SeqEZPZ folder\r\n   cd ../..\r\n   bash scripts/run_shiny_analysis.sh filepath=.\r\n   ```\r\n   **Note**: options for ```run_shiny_analysis.sh```:\r\n      - If needed you can also set the time=DD-HH:MM:SS. (Day-hours:minutes:seconds). This is usually needed when your HPC is reserved for maintenance.\r\n      - If you have more than 50 FASTQ files that you need to analyze, you would need to set max_nsamples=<number-of-FASTQ-files> option.\r\n      - You can also run it with run=debug option to get more messages when trouble shooting.\r\n      - example of command with time set to 1 day 12 hours 30 minutes and 40 seconds, filepath set to current folder, run=debug and maximum number of FASTQ files set to 100:\r\n         ```\r\n         bash scripts/run_shiny_analysis.sh filepath=. time=1-12:30:40 max_nsamples=100 run=debug\r\n         ```\r\n              \r\n      <br />\r\n      A Firefox browser will be displayed that will enable you to run the full analysis.\r\n      <br />  \r\n\r\n      ![run_analysis_screenshot](assets/run_analysis_screenshot.png)\r\n\r\n 4. You will need to select project folder. \r\n      In this case, you would click on ```Select project folder```, a window will appear.\r\n      You can create new folder and specified the folder name in the interface.\r\n      Click on root first then Click on ```Create new folder```, it will allow you to put in a name for the new folder.\r\n      Once you click on the plus sign, it will create the named folder under root which in this case is under RNA-SeqEZPZ.\r\n      In this example, I am creating a folder named ```my_project```\r\n      ![run_create_folder](assets/run_create_folder.png)\r\n      You will need to click on my_project and click ```select``` at the bottom right to select my_project as your project folder.\r\n      After clicking ```select```, you should see my_project under ```Select project folder``` button.\r\n      ![run_my_project](assets/run_my_project.png)\r\n\r\n5. Select your genome. If you are using genome that is neither hg19 or hg38, select ```other```.\r\n6. Type in your genome name. In this case, I'm going to type in ```danRer11```.\r\n   7. Select your genome fasta file and genome GTF file you downloaded in step 1.\r\n   ![run_danrer](assets/run_danrer.png)\r\n   8. Fill out the form. See [step 3](#form) for running test example to fill out the form for your own dataset.\r\n   9. Once you're done filling out the form, you can click on ```Run full analysis``` to run the entire pipeline.\r\n   \r\nFeel free to open an issue for any questions or problems.\r\n\r\n## References\r\n<a id=\"1\">[1]</a>\r\nTaslim, C., Yuan, Z., Kendall, G.C. & Theisen, E.R. RNA-SeqEzPZ: A Point-and-Click Pipeline for Comprehensive Transcriptomics Analysis with Interactive Visualizations. Submitted.\r\n\r\n<a id=\"2\">[2]</a>\r\nLove, M.I., Huber, W. & Anders, S. Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. Genome Biol 15, 550 (2014). https://doi.org/10.1186/s13059-014-0550-8\r\n\r\n<a id=\"3\">[3]</a>\r\nSARTools: A DESeq2- and EdgeR-Based R Pipeline for Comprehensive Differential Analysis of RNA-Seq Data\r\nVaret H, Brillet-Gu\u00e9guen L, Copp\u00e9e JY, Dillies MA (2016) SARTools: A DESeq2- and EdgeR-Based R Pipeline for Comprehensive Differential Analysis of RNA-Seq Data. PLOS ONE 11(6): e0157022. https://doi.org/10.1371/journal.pone.0157022\r\n\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.1813.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1813",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-NC-ND-4.0",
        "link": "https:/workflowhub.eu/workflows/1813?version=2",
        "name": "RNA-SeqEZPZ: A Point-and-Click Pipeline for Comprehensive Transcriptomics Analysis with Interactive Visualizations",
        "number_of_steps": 0,
        "projects": [
            "Kendall-Theisen lab pipelines"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2025-07-23",
        "versions": 2
    },
    {
        "create_time": "2025-07-15",
        "creators": [
            "Jens Krumsieck"
        ],
        "description": "# SciWIn Client Demo\r\nA basic Workflow using [SciWIn Client (`s4n`)](https://github.com/fairagro/m4.4_sciwin_client) can be created with the commands hereafter. This guide assumes the usage of unix based operating systems, however Windows should work, too. If not please [open an issue](https://github.com/fairagro/m4.4_sciwin_client/issues/new).\r\n\r\n## Installation\r\n[![GitHub Release](https://img.shields.io/github/v/release/fairagro/m4.4_sciwin_client)](https://github.com/fairagro/m4.4_sciwin_client/releases/latest)\r\n\r\nThe latest Version of `s4n` can be installed using the following command:\r\n```\r\ncurl --proto '=https' --tlsv1.2 -LsSf https://fairagro.github.io/m4.4_sciwin_client/get_s4n.sh | sh \r\n```\r\n\r\nSpecific Versions can be installed with the following command, by replacing the version tag with a version of choice.\r\n```\r\ncurl --proto '=https' --tlsv1.2 -LsSf https://github.com/fairagro/m4.4_sciwin_client/releases/download/v0.6.0/s4n-installer.sh | sh\r\n```\r\n\r\nThe Installation can be verified using `s4n -V`.\r\nSciWIn Client comes with a lot of commands. In this demo the `init`, `tool`, `workflow` and `execute` commands will be showcased.\r\n```\r\n _____        _  _    _  _____         _____  _  _               _   \r\n/  ___|      (_)| |  | ||_   _|       /  __ \\| |(_)             | |   \r\n\\ `--.   ___  _ | |  | |  | |  _ __   | /  \\/| | _   ___  _ __  | |_  \r\n `--. \\ / __|| || |/\\| |  | | | '_ \\  | |    | || | / _ \\| '_ \\ | __|\r\n/\\__/ /| (__ | |\\  /\\  / _| |_| | | | | \\__/\\| || ||  __/| | | || |_  \r\n\\____/  \\___||_| \\/  \\/  \\___/|_| |_|  \\____/|_||_| \\___||_| |_| \\__|\r\n\r\nClient tool for Scientific Workflow Infrastructure (SciWIn)\r\nDocumentation: https://fairagro.github.io/m4.4_sciwin_client/\r\n\r\nVersion: 0.6.0\r\n\r\nUsage: s4n <COMMAND>\r\n\r\nCommands:\r\n  init         Initializes project folder structure and repository\r\n  tool         Provides commands to create and work with CWL CommandLineTools\r\n  workflow     Provides commands to create and work with CWL Workflows\r\n  annotate     Annotate CWL files\r\n  execute      Execution of CWL Files locally or on remote servers [aliases: ex]\r\n  sync         \r\n  completions  Generate shell completions\r\n  help         Print this message or the help of the given subcommand(s)\r\n\r\nOptions:\r\n  -h, --help     Print help\r\n  -V, --version  Print version\r\n```\r\n\r\n## Demo Repository\r\nThe [Demo Repository](https://github.com/fairagro/sciwin_client_demo) mainly contains two folders `data` and `code`. The result workflow will  download election data, print election results as `barplot`, convert input data into a `geojson` file, and maps it onto the `geojson` data resulting in a `choropleth` graph. See the images for the final outputs.\r\n\r\n![result](https://raw.githubusercontent.com/fairagro/m4.4_sciwin_client_demo/refs/heads/complete_run/election.png)\r\n![result](https://raw.githubusercontent.com/fairagro/m4.4_sciwin_client_demo/refs/heads/complete_run/plot.png)\r\n\r\n## Creating the CommandLineTools\r\nFirst of all, we start, by creating a new `s4n` project.\r\n```\r\ns4n init\r\n```\r\n\r\nCWL mainly describes processes in CommandLineTools which later can be connected into Workflows. CommandLineTools are essentially wrappers for commands that would usually be executed in the command line. CWL uses a special YAML structure to describe those processes.\r\n\r\n```cwl\r\n#!/usr/bin/env cwl-runner\r\n\r\ncwlVersion: v1.2\r\nclass: CommandLineTool\r\n\r\nrequirements:\r\n- class: DockerRequirement\r\n  dockerPull: osgeo/gdal:ubuntu-full-3.6.3\r\n- class: InlineJavascriptRequirement\r\n\r\ninputs:\r\n- id: districts_geojson\r\n  type: string\r\n  default: districts.geojson\r\n  inputBinding:\r\n    position: 0\r\n- id: data_braunschweig\r\n  type: Directory\r\n  default:\r\n    class: Directory\r\n    location: ../../data/braunschweig\r\n  inputBinding:\r\n    position: 1\r\n- id: lco\r\n  type: string\r\n  default: RFC7946=YES\r\n  inputBinding:\r\n    prefix: -lco\r\n\r\noutputs:\r\n- id: districts\r\n  type: File\r\n  outputBinding:\r\n    glob: $(inputs.districts_geojson)\r\n\r\nbaseCommand: ogr2ogr\r\n```\r\n\r\nHowever it may is tedious to write those files by hand. That is where `s4n` comes to the rescue. A Command that would normally happen on the command line just needs to be prefixed with `s4n tool create`. Examples can be found at the [documentation](https://fairagro.github.io/m4.4_sciwin_client/examples/tool-creation/).\r\n\r\nTo create Tools based of the Python scripts in the `code` Directory a virtual environment needs to be created using \r\n```bash\r\npython3 -m venv .venv\r\nsource .venv/bin/activate\r\npip install plotly pandas kaleido matplotlib\r\n```\r\n\r\nThe next step is to download election data using a series of API calls for which luckily already a script exists. The script downloads the data from `votemanager.kdo.de` and writes the `csv` to stdout.\r\nA tool can be created easily be prefixing the python call. However we also need to escape the `>` using a backslash for it to properly work\r\n```bash\r\ns4n tool create python code/download_election_data.py --ags 03101000 --election \"Bundestagswahl 2025\" \\> data.csv\r\n```\r\n\r\nThe written csv file lacks the header information of which party results correspond to which column. Therefore we use the `get_feature_info` script and create a tool as follows:\r\n```bash\r\ns4n tool create python code/get_feature_info.py --data data.csv\r\n```\r\n\r\nWith this information the election plot can be outputted. The script `plot_election` does the job and accepts the json file from `get_feature_info` and the aforementioned csv.\r\n```bash\r\ns4n tool create -c Dockerfile --container-tag pyplot --enable-network python code/plot_election.py --data data.csv --features features.json\r\n```\r\n\r\n## Combining the Tools into a workflow\r\nThe three CommandLineTools now will be combined into an automated pipeline. A barebones workflow can be generated by using the create command\r\n```bash\r\ns4n workflow create demo\r\n```\r\nThe workflow that is being built looks like the graph represented in the following image\r\n\r\n![the resulting workflow](https://raw.githubusercontent.com/fairagro/m4.4_sciwin_client_demo/refs/heads/complete_run/workflow_interim.svg)\r\n\r\nFirst of all a connection between the donwload script and `get_feature_info` as well as `plot_election` is created by\r\n```bash\r\ns4n workflow connect demo --from download_election_data/data --to get_feature_info/data\r\ns4n workflow connect demo --from download_election_data/data --to plot_election/data\r\n```\r\nTo get the correct values for `--from` and `--to` the command `s4n tool ls -a` can be used.\r\n\r\nThe plot tool also needs the feature information, so the next step is to combine both tools:\r\n\r\n```bash\r\ns4n workflow connect demo --from get_feature_info/features --to plot_election/features\r\n```\r\n\r\nTo use the workflow it needs inputs and outputs. In this demo's tools there are a lot of inputs, but some have default values. That means only neccesary connections have to be made. For the creation of inputs the `--from` value neeeds to start with `@inputs`.\r\n```bash\r\ns4n workflow connect demo --from @inputs/election --to download_election_data/election\r\ns4n workflow connect demo --from @inputs/ags --to download_election_data/ags\r\n```\r\n\r\nAdding outputs follows the same logic, however `@outputs` is used in `--to`\r\n```bash\r\ns4n workflow connect demo --from plot_election/election --to @outputs/bar\r\n```\r\n\r\nSaving the workflow is neccessary to have a clean git history for further creating CommandLineTools.\r\n```bash\r\ns4n workflow save demo\r\n```\r\n\r\nDuring the creation `s4n workflow status demo` can always be used to view the connection status.\r\n\r\n## Adding additional steps\r\nThe next tool uses [GDAL](https://de.wikipedia.org/wiki/Geospatial_Data_Abstraction_Library) to convert the shape file in `data/braunschweig` to a `geojson` file. The Command one would typically use would be\r\n```bash\r\nogr2ogr districts.geojson data/braunschweig -lco RFC7946=YES\r\n# s4n command\r\ns4n tool create ogr2ogr districts.geojson data/braunschweig -lco RFC7946=YES\r\n```\r\n\r\nHowever we might not have gdal installed on our machine, so we request `s4n` to not run the command. Therefore `s4n` needs to be told what file will be written with `-o` and for later usage a docker image is specified using `-c`.\r\n```bash\r\ns4n tool create --name shp2geojson --no-run -o districts.geojson -c osgeo/gdal:ubuntu-full-3.6.3 ogr2ogr districts.geojson data/braunschweig -lco RFC7946=YES\r\n```\r\nThis correct creation of the tool can be tested using \r\n```bash\r\ns4n execute local workflows/shp2geojson/shp2geojson.cwl \r\n```\r\n\r\nThe outputted file now needs to be committed to move on\r\n```bash\r\ngit add . && git commit -m \"Execution of shp2geojson\"\r\n```\r\n\r\nIn the last step the plot tool needs to be created. In this tool `plotly` is used to create a `choropleth` graph based on the outputs of the preceeding steps. The packages installed to the virtual environment are needed here. A Dockerfile to use is already in the repo.\r\n```bash\r\ns4n tool create -c Dockerfile --container-tag pyplot --enable-network python code/plot_map.py --geojson districts.geojson --csv data.csv --feature F3 --on gebiet-nr:BEZNUM --output_name plot\r\n```\r\n\r\n## Adding the new tools to Workflow\r\nThe two new tools will now be added to the workflow.\r\n\r\n![the final resulting workflow](https://raw.githubusercontent.com/fairagro/m4.4_sciwin_client_demo/refs/heads/complete_run/workflow_final.svg)\r\n\r\nKnowing that the plot tool needs the geojson, a connection from the geojson output to the corresponding input can be created.\r\n```bash\r\ns4n workflow connect demo --from shp2geojson/districts --to plot_map/geojson\r\n```\r\n\r\nAs the plot step also needs the election data, another connection can be created.\r\n```bash\r\ns4n workflow connect demo --from download_election_data/data --to plot_map/csv\r\n```\r\n\r\nNow we need to wire up the inputs. The input connections for `ags`, `election`, `feature`and `shapes` will be created as follows:\r\n```bash\r\ns4n workflow connect demo --from @inputs/feature --to plot_map/feature\r\ns4n workflow connect demo --from @inputs/shapes --to shp2geojson/data_braunschweig\r\n```\r\n\r\nThe last step is to add the output to the workflow. Only the `png` file is desired.\r\n```bash\r\ns4n workflow connect demo --from plot_map/plot --to @outputs/map\r\n```\r\n\r\nThe final workflow needs to be saved.\r\n```bash\r\ns4n workflow save demo\r\n```\r\n\r\n## Workflow Execution\r\nWe want to clean our workspace by deleting the outputs we created by creating the CommandLineTools. For the execution a parameter file will be created using the `s4n execute make-template` command.\r\n```bash\r\ns4n execute make-template workflows/demo/demo.cwl > inputs.yml\r\n```\r\n\r\nThis needs to be updated using the correct input values:\r\n```yaml\r\nags: \"03101000\"\r\nelection: Bundestagswahl 2025\r\nshapes:\r\n  class: Directory\r\n  location: data/braunschweig\r\nfeature: F3\r\n```\r\n\r\nExecution of the Workflow is done by\r\n```bash\r\ns4n execute local workflows/demo/demo.cwl inputs.yml\r\n```",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1808",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1808?version=1",
        "name": "SciWIn Client Demo",
        "number_of_steps": 5,
        "projects": [
            "FAIRagro M4.4"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2025-07-24",
        "versions": 1
    },
    {
        "create_time": "2025-07-22",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-genomeassembler_logo_dark.png\">\n    <img alt=\"nf-core/genomeassembler\" src=\"docs/images/nf-core-genomeassembler_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/genomeassembler/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/genomeassembler/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/genomeassembler/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/genomeassembler/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/genomeassembler/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.14986998-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.14986998)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/genomeassembler)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23genomeassembler-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/genomeassembler)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23genomeassembler-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/genomeassembler)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/genomeassembler** is a bioinformatics pipeline that carries out genome assembly, polishing and scaffolding from long reads (ONT or pacbio). Assembly can be done via `flye` or `hifiasm`, polishing can be carried out with `medaka` (ONT), or `pilon` (requires short-reads), and scaffolding can be done using `LINKS`, `Longstitch`, or `RagTag` (if a reference is available). Quality control includes `BUSCO`, `QUAST` and `merqury` (requires short-reads).\nCurrently, this pipeline does not implement phasing of polyploid genomes or HiC scaffolding.\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/genomeassembler.dark.png\">\n  <img alt=\"nf-core/genomeassembler\" src=\"docs/images/genomeassembler.light.png\">\n</picture>\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,ontreads,hifireads,ref_fasta,ref_gff,shortread_F,shortread_R,paired\nsampleName,ontreads.fa.gz,hifireads.fa.gz,assembly.fasta.gz,reference.fasta,reference.gff,short_F1.fastq,short_F2.fastq,true\n```\n\nEach row represents one genome to be assembled. `sample` should contain the name of the sample, `ontreads` should contain a path to ONT reads (fastq.gz), `hifireads` a path to HiFi reads (fastq.gz), `ref_fasta` and `ref_gff` contain reference genome fasta and annotations. `shortread_F` and `shortread_R` contain paths to short-read data, `paired` indicates if short-reads are paired. Columns can be omitted if they contain no data, with the exception of `shortread_R`, which needs to be present if `shortread_F` is there, even if it is empty.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/genomeassembler \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/genomeassembler/usage) and the [parameter documentation](https://nf-co.re/genomeassembler/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/genomeassembler/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/genomeassembler/output).\n\n## Credits\n\nnf-core/genomeassembler was originally written by [Niklas Schandry](https://github.com/nschan), of the Faculty of Biology of the Ludwig-Maximilians University (LMU) in Munich, Germany.\n\nI thank the following people for their extensive assistance and constructive reviews during the development of this pipeline:\n\n- [Mahesh Binzer-Panchal](https://github.com/mahesh-panchal)\n- [Matthias H\u00f6rtenhuber](https://github.com/mashehu)\n- [Louis Le N\u00e9zet](https://github.com/LouisLeNezet)\n- [J\u00falia Mir Pedrol](https://github.com/mirpedrol)\n- [Daniel Straub](https://github.com/d4straub)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#genomeassembler` channel](https://nfcore.slack.com/channels/genomeassembler) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/genomeassembler for your analysis, please cite it using the following doi: [10.5281/zenodo.14986998](https://doi.org/10.5281/zenodo.14986998)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1325",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1325?version=3",
        "name": "nf-core/genomeassembler",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genome-assembly"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-07-22",
        "versions": 3
    },
    {
        "create_time": "2025-07-15",
        "creators": [
            "Thomas Barba"
        ],
        "description": "# DUNE: Deep feature extraction by UNet-based Neuroimaging-oriented autoEncoder\r\n\r\nA versatile neuroimaging encoder that captures brain complexity across multiple diseases: cancer, dementia and schizophrenia.\r\n\r\n## Overview\r\n\r\nDUNE (Deep feature extraction by UNet-based Neuroimaging-oriented autoEncoder) is a neuroimaging-oriented deep learning model designed to extract deep features from multisequence brain MRIs, enabling their processing by basic machine learning algorithms. This project provides an end-to-end solution from DICOM conversion to low-dimensional feature extraction that captures clinically relevant patterns across several neurological conditions including cancer, dementia, and schizophrenia.\r\n\r\n## Pipeline Architecture\r\n\r\nThe pipeline consists of the following sequential stages:\r\n\r\n1. **DICOM to NIfTI Conversion**: Transforms medical DICOM images into the NIfTI format for analysis (skipped if NIfTI files are provided directly)\r\n2. **Preprocessing**: Prepares images through brain extraction (optional if already done), bias field correction, and spatial standardization\r\n3. **Feature Extraction**: Uses a UNet-based autoencoder (without skip connections) to extract low-dimensional embeddings\r\n\r\nEach stage can be run independently based on your specific needs.\r\n\r\n## Installation\r\n\r\n### Prerequisites\r\n\r\n- Python 3.8+\r\n- PyTorch 1.7+\r\n- ANTs (Advanced Normalization Tools)\r\n- DICOM2NIFTI\r\n- TorchIO\r\n- Nibabel\r\n- Click\r\n\r\n### ANTs Installation\r\n\r\nDUNE requires ANTs (Advanced Normalization Tools) for brain image preprocessing. ANTs is not installable via pip and requires separate installation:\r\n\r\n- Installation instructions are available on the [ANTs GitHub repository](https://github.com/ANTsX/ANTs)\r\n- Ensure the ANTs binaries are in your PATH before running DUNE\r\n\r\n### Setup\r\n\r\n```bash\r\n# Clone the repository\r\ngit clone https://github.com/gevaertlab/DUNE.git\r\ncd DUNE\r\n\r\n# Install dependencies\r\npip install -r requirements.txt\r\n\r\n# Ensure ANTs binaries are in your PATH\r\n# For example:\r\nexport ANTSPATH=/path/to/ANTs/bin/\r\nexport PATH=${ANTSPATH}:$PATH\r\n```\r\n\r\n## Usage\r\n\r\n### Initialize Workspace\r\n\r\n```bash\r\npython -m src.main init /path/to/workspace\r\n```\r\n\r\n### Process a Case\r\n\r\n```bash\r\n# Process a DICOM folder\r\npython -m src.main process /path/to/dicom/folder /path/to/output\r\n\r\n# Process a NIfTI file with brain extraction\r\npython -m src.main process /path/to/image.nii.gz /path/to/output\r\n\r\n# Process a NIfTI file skipping brain extraction (if already performed)\r\npython -m src.main process /path/to/image.nii.gz /path/to/output --skip-brain-extraction\r\n\r\n# Keep preprocessed files in the output\r\npython -m src.main process /path/to/image.nii.gz /path/to/output --keep-preprocessed\r\n\r\n# Process without creating log files\r\npython -m src.main process /path/to/image.nii.gz /path/to/output --no-logs\r\n\r\n# Process a folder containing multiple NIfTI files\r\npython -m src.main process /path/to/nifti/folder /path/to/output\r\n\r\n# Only perform feature extraction (skip DICOM conversion and preprocessing)\r\npython -m src.main process /path/to/preprocessed.nii.gz /path/to/output --features-only\r\n```\r\n\r\n### Output Structure\r\n\r\nThe pipeline produces a simplified output structure:\r\n\r\n1. **For a single file input (e.g., my_image.nii.gz)**:\r\n   ```\r\n   output_dir/\r\n   \u251c\u2500\u2500 my_image_features.csv              # Extracted features\r\n   \u251c\u2500\u2500 my_image_preprocessed.nii.gz       # (Optional) Preprocessed image if --keep-preprocessed is used\r\n   \u2514\u2500\u2500 logs/                              # Processing logs (unless --no-logs is used)\r\n   ```\r\n\r\n2. **For a directory input (e.g., case_dir/)**:\r\n   ```\r\n   output_dir/\r\n   \u251c\u2500\u2500 case_dir/\r\n   \u2502   \u251c\u2500\u2500 features/\r\n   \u2502   \u2502   \u251c\u2500\u2500 sequence1_features.csv     # Features for sequence 1\r\n   \u2502   \u2502   \u2514\u2500\u2500 sequence2_features.csv     # Features for sequence 2\r\n   \u2502   \u2514\u2500\u2500 preprocessed/                  # (Optional) Only if --keep-preprocessed is used\r\n   \u2502       \u251c\u2500\u2500 sequence1_preprocessed.nii.gz\r\n   \u2502       \u2514\u2500\u2500 sequence2_preprocessed.nii.gz\r\n   \u2514\u2500\u2500 logs/                              # Processing logs (unless --no-logs is used)\r\n   ```\r\n\r\n### Feature Files Structure\r\n\r\nEach generated feature file is a CSV containing 3,072 features extracted from the bottleneck layer of the DUNE autoencoder. The files are structured as follows:\r\n\r\n- The first column contains the sequence identifier\r\n- The remaining columns (3,072) contain the extracted features\r\n\r\nExample of a feature file structure:\r\n\r\n| sequence_id        | feature_0 | feature_1 | feature_2 | ... | feature_3071 |\r\n| ------------------ | --------- | --------- | --------- | --- | ------------ |\r\n| patient001-T1_POST | 0.0821    | -0.1427   | 0.2984    | ... | 0.0193       |\r\n\r\nThese low-dimensional embeddings capture essential brain structure information that can be used for downstream machine learning tasks like classification, regression, or clustering.\r\n\r\n### Configuration\r\n\r\nThe pipeline can be customized through a YAML configuration file:\r\n\r\n```yaml\r\n# Paths to resources\r\npaths:\r\n  scripts:\r\n    preprocessing: \"scripts/preprocessing\"\r\n  templates: \"data/templates\"\r\n  models: \"data/models\"\r\n\r\n# Preprocessing options\r\npreprocessing:\r\n  parameters:\r\n    brain_extraction:\r\n      threshold: 0.5\r\n    bias_correction:\r\n      iterations: 4\r\n  skip_brain_extraction: false\r\n  keep_preprocessed: false\r\n\r\n# Model options\r\nmodel:\r\n  weights_file: \"U-AE.pt\"\r\n  input_size: [256, 256, 256]\r\n\r\n# Output options\r\noutput:\r\n  enable_logs: true\r\n\r\n# Pipeline control options\r\npipeline:\r\n  features_only: false  # If true, only performs feature extraction\r\n```\r\n\r\n## Alternative Model Architectures\r\n\r\nThis repository also contains alternative autoencoder architectures that were evaluated in our paper:\r\n\r\n1. **U-AE** (default): UNet without skip connections - produces the most informative embeddings\r\n2. **UNET**: UNet with skip connections - best for image reconstruction\r\n3. **U-VAE**: Variational UNet without skip connections\r\n4. **VAE**: Fully connected variational autoencoder\r\n\r\nTo use an alternative architecture, specify it in your configuration file:\r\n\r\n```yaml\r\nmodel:\r\n  architecture: \"UNET\"  # Options: \"U-AE\", \"UNET\", \"U-VAE\", \"VAE\"\r\n  weights_path: \"data/models/UNET.pt\"\r\n```\r\nOr when running the feature extraction directly:\r\n\r\n```bash\r\npython src/pipeline/feature_extraction.py input.nii.gz output.csv --architecture UNET\r\n```\r\n\r\n## Command Line Options\r\n\r\n| Option                          | Description                                       |\r\n| ------------------------------- | ------------------------------------------------- |\r\n| `--config`, `-c`                | Path to configuration file                        |\r\n| `--verbose`, `-v`               | Enable verbose output                             |\r\n| `--skip-brain-extraction`, `-s` | Skip brain extraction step                        |\r\n| `--keep-preprocessed`, `-p`     | Keep preprocessed NIfTI files in output           |\r\n| `--no-logs`                     | Disable writing log files                         |\r\n| `--features-only`, `-f`         | Only perform feature extraction                   |\r\n\r\n## Pipeline Flexibility\r\n\r\nDUNE offers flexibility to handle different workflow scenarios:\r\n\r\n1. **Complete Pipeline**: Convert DICOM to NIfTI, preprocess images, and extract features\r\n2. **Partial Processing**: Start with NIfTI files and apply preprocessing and feature extraction\r\n3. **Minimal Preprocessing**: Skip brain extraction if your images are already skull-stripped\r\n4. **Feature Extraction Only**: If you already have fully preprocessed NIfTI files, you can extract features directly\r\n\r\nThis adaptability allows DUNE to fit into existing neuroimaging workflows and accommodate different data preparation stages.\r\n\r\n## Project Structure\r\n\r\n```\r\nDUNE\r\n\u251c\u2500\u2500 data\r\n\u2502   \u251c\u2500\u2500 models           # Pre-trained model weights\r\n\u2502   \u2514\u2500\u2500 templates        # Registration templates (MNI, OASIS)\r\n\u251c\u2500\u2500 scripts\r\n\u2502   \u2514\u2500\u2500 preprocessing    # ANTs-based preprocessing scripts\r\n\u251c\u2500\u2500 src\r\n\u2502   \u251c\u2500\u2500 pipeline         # Core pipeline components\r\n\u2502   \u2502   \u251c\u2500\u2500 dicom.py     # DICOM conversion and NIfTI handling\r\n\u2502   \u2502   \u251c\u2500\u2500 preprocessing.py # Image preprocessing\r\n\u2502   \u2502   \u2514\u2500\u2500 feature_extraction.py # Feature extraction with BrainAE model\r\n\u2502   \u251c\u2500\u2500 utils            # Utility modules\r\n\u2502   \u2502   \u251c\u2500\u2500 file_handling.py # File operations\r\n\u2502   \u2502   \u2514\u2500\u2500 logger.py    # Logging functionality\r\n\u2502   \u2514\u2500\u2500 main.py          # CLI interface and pipeline orchestration\r\n\u2514\u2500\u2500 config.yaml          # Default configuration\r\n```\r\n\r\n## The DUNE Model\r\n\r\nDUNE's feature extraction is powered by a UNet-based autoencoder architecture without skip connections (U-AE):\r\n\r\n- **Encoder**: Compresses the 3D brain volume through multiple convolutional blocks\r\n- **Bottleneck**: Creates a compact latent representation of the brain structure (low-dimensional embeddings)\r\n- **Decoder**: Reconstructs the original image from the latent representation\r\n\r\nThis unsupervised approach learns to capture both obvious and subtle imaging features, creating a numerical \"fingerprint\" of each scan that preserves important structural information while dramatically reducing dimensionality.\r\n\r\n## Citation\r\n\r\nIf you use DUNE in your research, please consider citing:\r\n\r\n```\r\n@article{barba2025dune,\r\n  author = {Barba T, Bagley BA, Steyaert S, Carrillo-Perez F, Sad\u00e9e C, Iv M, Gevaert O},\r\n  title = {DUNE: a versatile neuroimaging encoder captures brain complexity across three major diseases: cancer, dementia and schizophrenia},\r\n  year = {2025},\r\n  url = {https://www.medrxiv.org/content/10.1101/2025.02.24.25322787v1.full}\r\n}\r\n```\r\n\r\n## License\r\nCopyright 2025 - Prof Olivier Gevaert\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.",
        "doi": "10.48546/workflowhub.workflow.1809.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1809",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1809?version=1",
        "name": "DUNE: Deep feature extraction by UNet-based Neuroimaging-oriented autoEncoder",
        "number_of_steps": 0,
        "projects": [
            "Gevaert Lab"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "ANTsX"
        ],
        "type": "Python",
        "update_time": "2025-07-15",
        "versions": 1
    },
    {
        "create_time": "2025-07-14",
        "creators": [],
        "description": "Metatranscriptomics analysis using microbiome RNA-seq data (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metatranscriptomics analysis using microbiome RNA-seq data (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metatranscriptomics-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Pratik Jagtap, Subina Mehta, Ray Sajulga, Emma Leith, Praveen Kumar, Saskia Hiltemann, Paul Zierep\n\n**Tutorial Author(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Ray Sajulga](https://training.galaxyproject.org/training-material/hall-of-fame/jraysajulga/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/)\n\n**Tutorial Contributor(s)**: [Christine Oger](https://training.galaxyproject.org/training-material/hall-of-fame/ogerdfx/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1447",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1447?version=2",
        "name": "Workflow 3: Functional Information (quick)",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "asaim",
            "gtn",
            "galaxy",
            "metagenomics"
        ],
        "tools": [
            "humann_renorm_table",
            "Cut1",
            "humann_split_stratified_table",
            "humann_unpack_pathways",
            "tp_find_and_replace",
            "humann_regroup_table",
            "humann_rename_table",
            "Grep1",
            "combine_metaphlan2_humann2"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-14",
        "versions": 2
    },
    {
        "create_time": "2025-07-13",
        "creators": [
            "Ignacio Garach"
        ],
        "description": "Code and supporting data for the article: \"Exploring the role of normalization and feature selection in microbiome disease classification pipelines.\"\r\n\r\nThe repository contains the following folders:\r\n\r\n* **1. data:** contains OTU/ASV tables and class annotations for the 15 curated datasets considered.\r\n* **2. src:** code writen to perform the analyses from the article and the statistical tests\r\n* **3. results:** tables containing global nested cross validation results\r\n* **4. figures**\r\n\r\n\r\n## License: This project is licensed under GNU GPL 3.0 - check LICENSE file for more details.\r\n\r\n\r\n| Dataset | Samples (Cases, Controls) | Features | IR  | Project ID |\r\n|---------|---------------------------|----------|-----|------------|\r\n| ART     | 114 (86, 28)               | 10733    | 3.07 | PRJNA203810    |\r\n| CDI     | 336 (93, 243)              | 3456     | 2.61 | 10.1128/mbio.01021-14 (DOI)              |\r\n| CRC1    | 490 (229, 261)             | 6920     | 1.14 | PRJNA290926    |\r\n| CRC2    | 102 (46, 56)               | 837      | 1.22 | SRP005150      |\r\n| HIV     | 350 (293, 57)              | 14425    | 5.14 | PRJNA307231    |\r\n| CD1     | 140 (78, 62)               | 3547     | 1.26 | PRJNA237362    |\r\n| CD2     | 160 (68, 92)               | 3547     | 1.35 | PRJNA237362    |\r\n| IBD1    | 91 (67, 24)                | 2742     | 2.79 | PRJNA82109     |\r\n| IBD2    | 114 (68, 46)               | 1496     | 1.48 | 10.1053/j.gastro.2010.08.049 (DOI)              |\r\n| CIR     | 77 (51, 26)                | 3104     | 1.96 | PRJNA174838    |\r\n| MHE     | 77 (26, 51)                | 3104     | 1.96 | PRJNA174838    |\r\n| OB      | 281 (220, 61)              | 6386     | 3.61 | PRJNA32089     |\r\n| PAR1    | 148 (74, 74)               | 10232    | 1.00 | PRJEB4927      |\r\n| PAR2    | 333 (201, 132)             | 6844     | 1.52 | PRJNA601994    |\r\n| PAR3    | 507 (323, 184)             | 12198    | 1.76 | PRJNA601994    |\r\n\r\n*Notes:*  \r\n- ART: Arthritis; CDI: Clostridium difficile Infection; CRC1 and CRC2: Colorectal Cancer; HIV: Human Immunodeficiency Virus; CD1 and CD2: Crohn's Disease; IBD1 and IBD2: Inflammatory Bowel Disease; CIR: Cirrhosis; MHE: Minimal Hepatic Encephalopathy; OB: Obesity; PAR1, PAR2, and PAR3: Parkinson's Disease.  \r\n- CD1 and CD2 were taken from MLRepo, PAR2 and PAR3 were retrieved from their respective article sources, and the remaining datasets were obtained from MicrobiomeHD.\r\n- Project IDs from NCBI for raw data. IBD2 data is only available via MicrobiomeHD repository, CDI raw data is available at mothur (https://mothur.org/CDI_MicrobiomeModeling/)\r\n",
        "doi": "10.48546/workflowhub.workflow.1807.1",
        "edam_operation": [],
        "edam_topic": [
            "Machine learning",
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "1807",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1807?version=1",
        "name": "Exploring the role of normalization and feature selection in microbiome disease classification pipelines",
        "number_of_steps": 0,
        "projects": [
            "Machine Learning Techniques in Microbiome"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "machine learning",
            "metagenomics"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2025-07-13",
        "versions": 1
    },
    {
        "create_time": "2025-07-11",
        "creators": [
            "Patrick Austin",
            "Alexander Belozerov",
            "Subindev Devadasan",
            "Leandro Liborio",
            "Abraham Nieva de la Hidalga",
            "Tom Underwood"
        ],
        "description": "Galaxy workflow for the reproduction of the results published in: Panchal, Monik, Callison, June, Skukauskas, Vainius, Gianolio, Diego, Cibin, Giannantonio, York, Andrew P E, Schuster, Manfred E, Hyde, Timothy I, Collier, Paul, Catlow, C Richard A, Gibson, Emma K (2021) Operando XAFS investigation on the effect of ash deposition on three-way catalyst used in gasoline particulate filters and the effect of the manufacturing process on the catalytic activity DOI: 10.1088/1361-648x/abfe16.\r\n\r\nThis workflow is published as part of the research data submitted for the paper Facilitating Reproducibility in Catalysis Research with Managed Workflows and RO-Crates: A Galaxy Case Study, ChemCatChem, DOI: 10.1002/cctc.202401676.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1805",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1805?version=1",
        "name": "Catalysts in gasoline particulate filters",
        "number_of_steps": 7,
        "projects": [
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2025-07-11",
        "versions": 1
    },
    {
        "create_time": "2025-07-11",
        "creators": [
            "Patrick Austin",
            "Alexander Belozerov",
            "Subindev Devadasan",
            "Leandro Liborio",
            "Abraham Nieva de la Hidalga",
            "Tom Underwood"
        ],
        "description": "Galaxy workflow for the reproduction of the results published in: R. H. Blackmore, M. E. Rivas, G. F. Tierney, K. M. H. .Mohammed, D. Decarolis, S. Hayama, F. Venturini, G. Held, R. Arrigo, M. Amboage, P. Hellier, E. W. Lynch, M. Amri, M. Casavola, T. Eralp Erden, P. Collier, P. P. Wells (2020) The electronic structure, surface properties, and in situ N2O decomposition of mechanochemically synthesised LaMnO3 DOI: 10.1039/d0cp00793e.\r\n\r\nThis workflow is published as part of the research data submitted for the paper Facilitating Reproducibility in Catalysis Research with Managed Workflows and RO-Crates: A Galaxy Case Study, ChemCatChem, DOI: 10.1002/cctc.202401676.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amr in description",
        "id": "1806",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1806?version=1",
        "name": "N2O decomposition on LaMnO3",
        "number_of_steps": 7,
        "projects": [
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2025-07-11",
        "versions": 1
    },
    {
        "create_time": "2025-07-11",
        "creators": [
            "Patrick Austin",
            "Alexander Belozerov",
            "Subindev Devadasan",
            "Leandro Liborio",
            "Abraham Nieva de la Hidalga",
            "Tom Underwood"
        ],
        "description": "Galaxy workflow for the reproduction of the results published in: D. Decarolis, A. H. Clark, T. Pellegrinelli, M. Nachtegaal, E. W. Lynch, C. R. A. .Catlow, E. K. Gibson, A. Goguet, P. P. Wells (2021). Spatial Profiling of a Pd/Al2O3 Catalyst during Selective Ammonia Oxidation DOI: 10.1021/acscatal.0c05356.\r\n\r\nThis workflow is published as part of the research data submitted for the paper Facilitating Reproducibility in Catalysis Research with Managed Workflows and RO-Crates: A Galaxy Case Study, ChemCatChem, DOI: 10.1002/cctc.202401676.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1803",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1803?version=1",
        "name": "Pd-Al2O3 Catalyst for Ammonia",
        "number_of_steps": 33,
        "projects": [
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2025-07-11",
        "versions": 1
    },
    {
        "create_time": "2025-07-11",
        "creators": [
            "Patrick Austin",
            "Alexander Belozerov",
            "Subindev Devadasan",
            "Leandro Liborio",
            "Abraham Nieva de la Hidalga",
            "Tom Underwood"
        ],
        "description": "Galaxy workflow for the reproduction of the results published in: R. H. Blackmore, M. E. Rivas, T. Eralp Erden, T. Dung Tran, H. R. Marchbank, D. Ozkaya, M. Briceno de Gutierrez, A. Wagland, P. Collier, P. P. Wells (2018). Understanding the mechanochemical synthesis of the perovskite LaMnO3 and its catalytic behaviour DOI: 10.1039/C9DT03590G.\r\n\r\nThis workflow is published as part of the research data submitted for the paper Facilitating Reproducibility in Catalysis Research with Managed Workflows and RO-Crates: A Galaxy Case Study, ChemCatChem, DOI: 10.1002/cctc.202401676.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "1802",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1802?version=1",
        "name": "LaMnO3 catalytic behaviour",
        "number_of_steps": 13,
        "projects": [
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2025-07-11",
        "versions": 1
    },
    {
        "create_time": "2025-07-11",
        "creators": [
            "Patrick Austin",
            "Alexander Belozerov",
            "Subindev Devadasan",
            "Leandro Liborio",
            "Abraham Nieva de la Hidalga",
            "Tom Underwood"
        ],
        "description": "Galaxy workflow for with the reproduction of the results published in: C. Stewart, E. K. Gibson, K. Morgan, G. Cibin, A. J. Dent, C. Hardacre, E. V. Kondratenko, V. A. Kondratenko, C. McManus, S. M. Rogers, C. E. Stere, S. Chansai, Y. -C. Wang, S. J. Haigh, P. P. Wells, A. Goguet (2018). Unraveling the H2Promotional Effect on Palladium-Catalyzed CO Oxidation Using a Combination of Temporally and Spatially Resolved Investigations DOI: 10.1021/acscatal.8b01509.\r\n\r\nThis workflow is published as part of the research data submitted for the paper Facilitating Reproducibility in Catalysis Research with Managed Workflows and RO-Crates: A Galaxy Case Study, ChemCatChem, DOI: 10.1002/cctc.202401676.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1801",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1801?version=1",
        "name": "Palladium-Catalyzed CO Oxidation",
        "number_of_steps": 12,
        "projects": [
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2025-07-11",
        "versions": 1
    },
    {
        "create_time": "2025-07-11",
        "creators": [
            "Patrick Austin",
            "Alexander Belozerov",
            "Subindev Devadasan",
            "Leandro Liborio",
            "Abraham Nieva de la Hidalga",
            "Tom Underwood"
        ],
        "description": "Galaxy workflow for the reproduction of the results published in: Antonis M. Messinis, Stephen L. J. Luckham, Peter P. Wells, Diego Gianolio, Emma K. Gibson, Harry M. O\u2019Brien, Hazel A. Sparkes, Sean A. Davis, June Callison, David Elorriaga, Oscar Hernandez-Fajardo & Robin B. Bedford (2019) The highly surprising behaviour of diphosphine ligands in iron-catalysed Negishi cross-coupling Nature Catalysis, 2019, 2, 123-133 DOI: 10.1038/s41929-018-0197-z.\r\n\r\nThis workflow is published as part of the research data submitted for the paper Facilitating Reproducibility in Catalysis Research with Managed Workflows and RO-Crates: A Galaxy Case Study, ChemCatChem, DOI: 10.1002/cctc.202401676.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1799",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1799?version=1",
        "name": "Diphosphine ligands in iron",
        "number_of_steps": 11,
        "projects": [
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2025-07-11",
        "versions": 1
    },
    {
        "create_time": "2025-07-09",
        "creators": [
            "Huihai Wu",
            "Irene Papatheodorou"
        ],
        "description": "[![GitHub Actions CI Status](https://github.com/nf-core/eisca/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/eisca/actions/workflows/ci.yml)\r\n[![GitHub Actions Linting Status](https://github.com/nf-core/eisca/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/eisca/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/eisca/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\r\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/eisca)\r\n\r\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23eisca-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/eisca)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\r\n\r\n## Introduction\r\n\r\n**TGAC/eisca** is a bioinformatics pipeline that perform analysis for single-cell RNA-seq data. The pipeline is built using [Nextflow](https://www.nextflow.io/) and processes (implemented and to be implemented) are as follows:\r\n\r\n- **Primary analysis**\r\n  - FastQC - Raw read QC\r\n  - TrimGalore - Adapter and quality trimming to FastQ files\r\n  - Kallisto & Bustools - Mapping & quantification by Kallisto & Bustools\r\n  - Salmon Alevin - Mapping & quantification by Salmon Alevin\r\n  - STARsolo - Mapping & quantification by STAR\r\n- **Secondary analysis**\r\n  - QC & cell filtering - cell filtering and QC on raw data and filtered data\r\n  - Clustering analysis - single-cell clustering analysis\r\n  - Merging/integration of samples \r\n- **Tertiary analysis**\r\n  - Cell type annotation\r\n  - Differential expression analysis\r\n  - Cell-cell communication analysis\r\n  - Trajectory & pseudotime analysis (to be implemented)\r\n  - Other downstream analyses (to be implemented)\r\n- **Pipeline reporting**\r\n  - Analysis report - Single-ell Analysis Report.\r\n  - MultiQC - Aggregate report describing results and QC for tools registered in nf-core\r\n  - Pipeline information - Report metrics generated during the workflow execution\r\n\r\n\r\n<!-- TODO nf-core:\r\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\r\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\r\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\r\n-->\r\n\r\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\r\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\r\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->\r\n\r\n<!-- 1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\r\n2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/)) -->\r\n\r\n## Usage\r\n\r\n> [!NOTE]\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\r\n\r\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\r\n     Explain what rows and columns represent. For instance (please edit as appropriate):\r\n\r\nFirst, prepare a samplesheet with your input data that looks as follows:\r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\nsample,fastq_1,fastq_2\r\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\r\n```\r\n\r\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\r\n\r\n-->\r\n\r\nFirst, prepare a samplesheet with your input data that looks as follows:\r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\nsample,fastq_1,fastq_2\r\npbmc8k,pbmc8k_S1_L007_R1_001.fastq.gz,pbmc8k_S1_L007_R2_001.fastq.gz\r\npbmc8k,pbmc8k_S1_L008_R1_001.fastq.gz,pbmc8k_S1_L008_R2_001.fastq.gz\r\npbmc5k,pbmc5k_S1_L003_R1_001.fastq.gz,pbmc5k_S1_L003_R2_001.fastq.gz\r\n```\r\n\r\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\r\n\r\n\r\nNow, you can run the pipeline using:\r\n\r\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\r\n\r\n```bash\r\nnextflow run TGAC/eisca \\\r\n   -profile <docker/singularity/.../institute> \\\r\n   --input samplesheet.csv \\\r\n   --genome_fasta GRCm38.p6.genome.chr19.fa \\\r\n   --gtf gencode.vM19.annotation.chr19.gtf \\\r\n   --protocol 10XV2 \\\r\n   --aligner <alevin/kallisto/star/cellranger/universc> \\\r\n   --outdir <OUTDIR>\r\n```\r\n\r\n> [!WARNING]\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\r\n\r\nFor more details and further functionality, please refer to the [usage documentation](https://github.com/TGAC/eisca/blob/master/docs/usage.md).\r\n<!-- (https://nf-co.re/eisca/usage). -->\r\n\r\n## Pipeline output\r\n\r\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/eisca/results) tab on the nf-core website pipeline page.\r\nFor more details about the output files and reports, please refer to the\r\n[output documentation](https://github.com/TGAC/eisca/blob/master/docs/output.md).\r\n<!-- (https://nf-co.re/eisca/output). -->\r\n\r\n## Credits\r\n\r\nnf-core/eisca was originally written by Huihai Wu.\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#eisca` channel](https://nfcore.slack.com/channels/eisca) (you can join with [this invite](https://nf-co.re/join/slack)).\r\n\r\n## Citations\r\n\r\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\r\n<!-- If you use nf-core/eisca for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\r\n\r\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Single-cell sequencing"
        ],
        "filtered_on": "binn* in description",
        "id": "1795",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1795?version=1",
        "name": "EI Single-Cell Analysis pipeline",
        "number_of_steps": 0,
        "projects": [
            "EI Papatheodorou Group"
        ],
        "source": "WorkflowHub",
        "tags": [
            "10x",
            "scrna-seq",
            "single-cell",
            "smart-seq 2"
        ],
        "tools": [
            "kallisto",
            "STAR",
            "Alevin",
            "FastQC",
            "MultiQC",
            "CellChat"
        ],
        "type": "Nextflow",
        "update_time": "2025-07-09",
        "versions": 1
    },
    {
        "create_time": "2025-07-08",
        "creators": [
            "Magdalena Antczak",
            "Cameron Hyde",
            "Lanxi (Daisy) Li",
            "Valentine Murigneux",
            "Sarah Williams",
            "Michael Thang",
            "Bradley Pease",
            "Shaun Bochow",
            "Grace Sun"
        ],
        "description": "<table>\r\n  <tr>\r\n    <td style=\"vertical-align:top; width:50%;\">\r\n      <img src=\"https://raw.githubusercontent.com/qcif/taxodactyl/main/docs/images/taxodactyl_logo.png\" alt=\"taxodactyl-logo\" width=\"100%\"/>\r\n    </td>\r\n    <td style=\"vertical-align:top; width:50%;\">\r\n      <strong>qcif/taxodactyl</strong> is a modular, reproducible Nextflow workflow for the conservative taxonomy assignment to DNA sequences, designed for high-confidence, auditable results in biosecurity and biodiversity contexts. The workflow integrates multiple bioinformatics tools and databases, automates best-practice analysis steps, and produces detailed reports with supporting evidence for each taxonomic assignment.\r\n    </td>\r\n  </tr>\r\n</table>\r\n\r\n<p align=\"center\" style=\"max-width:400px; margin:auto;\">\r\n    <img src=\"https://raw.githubusercontent.com/qcif/taxodactyl/main/docs/images/taxodactyl_diagram.png\" alt=\"taxodactyl_diagram\" width=\"350\"/>\r\n</p>\r\n\r\n### Workflow Overview\r\n\r\nThe pipeline orchestrates a series of analytical steps, each encapsulated in a dedicated module or subworkflow. The main stages are:\r\n\r\n1. **Environment Configuration** Sets up environment variables and paths required for downstream processes, ensuring reproducibility and portability.\r\n\r\n2. **Input Validation** Checks the integrity and compatibility of input files (FASTA sequences, metadata, databases), preventing downstream errors.\r\n\r\n3. **Sequence Search**  \r\n   - **[BLAST Core Nucleotide Database](https://ncbiinsights.ncbi.nlm.nih.gov/2024/07/18/new-blast-core-nucleotide-database/) ([BLASTN](https://blast.ncbi.nlm.nih.gov/Blast.cgi)):** Queries input sequences against the NCBI nucleotide database using BLASTN.\r\n   - **[BOLD v4](https://v4.boldsystems.org/) ([API](https://v4.boldsystems.org/index.php/api_home)):** Queries input sequences against the Barcode of Life Data Systems. Taxonomic lineage included in the results.\r\n\r\n4. **Hit Extraction** Parses BLAST results to extract relevant hits for each query.\r\n\r\n5. **Taxonomic ID Extraction** Retrieves taxonomic IDs for BLAST hits.\r\n\r\n6. **Taxonomic Lineage Extraction** Maps taxonomic IDs to full lineages, enabling downstream filtering and reporting.\r\n\r\n7. **Candidate Extraction** Identifies candidate species for each query, applying user-defined thresholds for identity and coverage.\r\n\r\n8. **Supporting Evidence Evaluation**  \r\n   - **Publications Diversity:** Assesses the diversity of data sources supporting each candidate.\r\n   - **Database Coverage:** Evaluates the representation of candidates in global databases ([GBIF](https://www.gbif.org/), [GenBank](https://www.ncbi.nlm.nih.gov/genbank/), [BOLD](https://v4.boldsystems.org/)).\r\n\r\n9. **Multiple Sequence Alignment ([MAFFT](https://mafft.cbrc.jp/alignment/server/index.html))** Aligns candidate and query sequences to prepare for phylogenetic analysis.\r\n\r\n10. **Phylogenetic Tree Construction ([FastMe](http://www.atgc-montpellier.fr/fastme/))** Builds a phylogenetic tree to visualise relationships among candidates and queries.\r\n\r\n11. **Comprehensive Reporting** Generates detailed HTML and text reports, including sequence alignments, phylogenetic trees, database coverage, and all supporting evidence for each assignment.\r\n",
        "doi": "10.48546/workflowhub.workflow.1782.3",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1782",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1782?version=3",
        "name": "TAXODACTYL - High-confidence, evidence-based taxonomic assignment of DNA sequences",
        "number_of_steps": 0,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-07-08",
        "versions": 3
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "Prediction of HLA binding for verified candidates\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 5a: Predicting HLA Binding](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-predicting-hla-binding/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: galaxyp\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1794",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1794?version=1",
        "name": "GigaScience-RNAseq-Optitype-seq2HLA-to-IEDB-alleles",
        "number_of_steps": 4,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:neoantigen",
            "hla"
        ],
        "tools": [
            "optitype",
            "query_tabular",
            "tp_awk_tool",
            "seq2hla"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-07",
        "versions": 1
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "Merging Fusion and non-normal databases + Discovery peptidomics using FragPipe\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 2: Database merge and FragPipe discovery](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-fragpipe-discovery/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1793",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1793?version=1",
        "name": "GigaScience_Database_merge_FragPipe_STS26T_demonstration",
        "number_of_steps": 7,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:neoantigen"
        ],
        "tools": [
            "query_tabular",
            "validate_fasta_database",
            "Remove beginning1",
            "collapse_dataset",
            "removing anything that matches _HUMAN\nGrep1",
            "fragpipe",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-07",
        "versions": 1
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "Create a protein Fusion database through the Arriba workflow\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 1a: Fusion-Database-Generation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-fusion-database-generation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1792",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1792?version=1",
        "name": "Gigascience_Fusions_demonstration_STS26T-Gent_Workflow",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:neoantigen"
        ],
        "tools": [
            "Uncompressed RNA-Seq forward reads\nCONVERTER_gz_to_uncompressed",
            "Uncompressed RNA-Seq reverse reads\nCONVERTER_gz_to_uncompressed",
            "tp_awk_tool",
            "query_tabular",
            "tab2fasta",
            "rna_star",
            "regex1",
            "arriba_get_filters",
            "arriba"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-07",
        "versions": 1
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "Validate the NeoAntigen Candidates from FragPipe discovery through the PepQuery Novel search\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 3: PepQuery2 Verification](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-peptide-verification/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1791",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1791?version=1",
        "name": "GigaScience_PepQuery2_demonstration_STS26T_neoantigen_candidates_workflow",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:neoantigen"
        ],
        "tools": [
            "query_tabular",
            "tab2fasta",
            "ncbi_blastp_wrapper",
            "pepquery2",
            "msconvert"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-07",
        "versions": 1
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "Annotating the novel peptides\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 4: Variant Annotation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-variant-annotation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "1790",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1790?version=1",
        "name": "GigaScience_Peptide_Annotation_demonstration_STS26T_neoantigen_candidates_workflow",
        "number_of_steps": 11,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:neoantigen"
        ],
        "tools": [
            "regexColumn1",
            "query_tabular",
            "tab2fasta",
            "converting pipes to columns\nConvert characters1",
            "converting colons to columns\nConvert characters1",
            "pep_pointer"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-07",
        "versions": 1
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "Predict binding using IEDB and check novelty peptides with PepQuery\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 5b: IEDB binding PepQuery Validated Neopeptides](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-hla-binding-novel-peptides/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1789",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1789?version=1",
        "name": "GigaScience-IEDB-PepQuery-Neoantigen",
        "number_of_steps": 15,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "iedb",
            "name:neoantigen"
        ],
        "tools": [
            "anything less than 2 and more than 0.5 percentile rank\nFilter1",
            "query_tabular",
            "table_compute",
            "removing header\nRemove beginning1",
            "cutting first column to extract peptides\nCut1",
            "pepquery2",
            "anything less than 0.5 percentile rank\nFilter1",
            "this is to remove header\nRemove beginning1",
            "iedb_api",
            "IEDB-Validated-NeoAntigen_Peptides, filtering if the validation column is yes or no \nFilter1"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-07",
        "versions": 1
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "Generating non-reference protein database for FragPipe discovery\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 1b: Non-Reference-Database-Generation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-non-reference-database-generation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1788",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1788?version=1",
        "name": "Gigascience_Indels_SAV_non-reference_demonstration_STS26T-Gent_Workflow",
        "number_of_steps": 33,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:neoantigen"
        ],
        "tools": [
            "Uncompressed_RNA_Seq_Reads_1\nCONVERTER_gz_to_uncompressed",
            "filter_tabular",
            "regexColumn1",
            "sqlite_to_tabular",
            "query_tabular",
            "tab2fasta",
            "freebayes",
            "stringtie",
            "tp_cat",
            "gffcompare_to_bed",
            "custom_pro_db",
            "Uncompressed_RNA_Seq_Reads_2\nCONVERTER_gz_to_uncompressed",
            "translate_bed",
            "gffcompare",
            "hisat2",
            "fasta2tab",
            "bed_to_protein_map",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-07",
        "versions": 1
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "Pathogens of all samples report generation and visualization\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pathogen detection from (direct Nanopore) sequencing data using Galaxy - Foodborne Edition](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Engy Nasr, B\u00e9r\u00e9nice Batut, Paul Zierep\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [Hans-Rudolf Hotz](https://training.galaxyproject.org/training-material/hall-of-fame/hrhotz/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EOSC-Life](https://training.galaxyproject.org/training-material/hall-of-fame/eosc-life/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in description",
        "id": "1487",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1487?version=2",
        "name": "Pathogen Detection PathoGFAIR Samples Aggregation and Visualisation",
        "number_of_steps": 65,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:iwc",
            "name:microgalaxy",
            "name:pathogfair",
            "name:collection"
        ],
        "tools": [
            "bedtools_getfastabed",
            "tp_split_on_column",
            "tab2fasta",
            "Cut1",
            "Grouping1",
            "Remove beginning1",
            "regex1",
            "fasttree",
            "__FILTER_FAILED_DATASETS__",
            "fasta2tab",
            "regexColumn1",
            "Count1",
            "clustalw",
            "collection_column_join",
            "newick_display",
            "fasta_merge_files_and_filter_unique_sequences",
            "__FILTER_EMPTY_DATASETS__",
            "tp_replace_in_column",
            "barchart_gnuplot",
            "collapse_dataset",
            "tp_multijoin_tool",
            "ggplot2_heatmap",
            "tp_sorted_uniq"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-07",
        "versions": 2
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "Microbiome - QC and Contamination Filtering\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pathogen detection from (direct Nanopore) sequencing data using Galaxy - Foodborne Edition](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Engy Nasr, Paul Zierep\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [Hans-Rudolf Hotz](https://training.galaxyproject.org/training-material/hall-of-fame/hrhotz/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EOSC-Life](https://training.galaxyproject.org/training-material/hall-of-fame/eosc-life/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in description",
        "id": "1492",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1492?version=2",
        "name": "Nanopore Preprocessing",
        "number_of_steps": 25,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:iwc",
            "name:microgalaxy",
            "name:nanopore",
            "name:pathogfair",
            "name:collection"
        ],
        "tools": [
            "nanoplot",
            "fastp",
            "Add_a_column1",
            "regexColumn1",
            "krakentools_extract_kraken_reads",
            "fastqc",
            "minimap2",
            "porechop",
            "Cut1",
            "collection_column_join",
            "samtools_fastx",
            "kraken2",
            "collapse_dataset",
            "__FILTER_FAILED_DATASETS__",
            "bamtools_split_mapped",
            "Grep1",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-07",
        "versions": 2
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "Building an amplicon sequence variant (ASV) table from 16S data using DADA2\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Building an amplicon sequence variant (ASV) table from 16S data using DADA2](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/dada-16S/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: , B\u00e9r\u00e9nice Batut\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Tutorial Contributor(s)**: [Matthias Bernt](https://training.galaxyproject.org/training-material/hall-of-fame/bernt-matthias/), [Clea Siguret](https://training.galaxyproject.org/training-material/hall-of-fame/clsiguret/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Linelle Abueg](https://training.galaxyproject.org/training-material/hall-of-fame/abueg/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Santino Faack](https://training.galaxyproject.org/training-material/hall-of-fame/santamccloud/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1395",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1395?version=2",
        "name": "Building an amplicon sequence variant (ASV) table from 16S data using DADA2",
        "number_of_steps": 21,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "dada2_removeBimeraDenovo",
            "__UNZIP_COLLECTION__",
            "__SORTLIST__",
            "tp_replace_in_line",
            "Add_a_column1",
            "dada2_plotQualityProfile",
            "dada2_dada",
            "cat1",
            "phyloseq_from_dada2",
            "dada2_seqCounts",
            "tp_head_tool",
            "tp_replace_in_column",
            "dada2_mergePairs",
            "collection_element_identifiers",
            "dada2_filterAndTrim",
            "dada2_assignTaxonomyAddspecies",
            "dada2_learnErrors",
            "dada2_makeSequenceTable"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-07",
        "versions": 2
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-drugresponseeval_logo_dark.png\">\n    <img alt=\"nf-core/drugresponseeval\" src=\"docs/images/nf-core-drugresponseeval_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/drugresponseeval/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/drugresponseeval/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/drugresponseeval/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/drugresponseeval/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/drugresponseeval/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.14779984-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.14779984)\n\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.04.2-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.1-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.1)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/drugresponseeval)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23drugresponseeval-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/drugresponseeval)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n# ![drevalpy_summary](assets/dreval_summary.svg)\n\n**DrEval** is a bioinformatics framework that includes a PyPI package (drevalpy) and a Nextflow\npipeline (this repo). DrEval ensures that evaluations are statistically sound, biologically\nmeaningful, and reproducible. DrEval simplifies the implementation of drug response prediction\nmodels, allowing researchers to focus on advancing their modeling innovations by automating\nstandardized evaluation protocols and preprocessing workflows. With DrEval, hyperparameter\ntuning is fair and consistent. With its flexible model interface, DrEval supports any model type,\nranging from statistical models to complex neural networks. By contributing your model to the\nDrEval catalog, you can increase your work's exposure, reusability, and transferability.\n\n1. The response data is loaded\n2. All models are trained and evaluated in a cross-validation setting\n3. For each CV split, the best hyperparameters are determined using a grid search per model\n4. The model is trained on the full training set (train & validation) with the best\n   hyperparameters to predict the test set\n5. If randomization tests are enabled, the model is trained on the full training set with the best\n   hyperparameters to predict the randomized test set\n6. If robustness tests are enabled, the model is trained N times on the full training set with the\n   best hyperparameters\n7. Plots are created summarizing the results\n\nFor baseline models, no randomization or robustness tests are performed.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/drugresponseeval \\\n   -profile <docker/singularity/.../institute> \\\n   --models <RandomForest,model2,...> \\\n   --baselines <NaiveMeanEffectsPredictor,baseline2,...> \\\n   --dataset_name <CTRPv2|CTRPv1|CCLE|GDSC1|GDSC2|custom_dataset>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/drugresponseeval/usage) and the [parameter documentation](https://nf-co.re/drugresponseeval/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/drugresponseeval/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/drugresponseeval/output).\n\n## Credits\n\nnf-core/drugresponseeval was originally written by Judith Bernett (TUM) and Pascal Iversen (FU\nBerlin).\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n## Contributions and Support\n\nContributors to nf-core/drugresponseeval and the drevalpy PyPI package:\n\n- [Judith Bernett](https://github.com/JudithBernett) (TUM)\n- [Pascal Iversen](https://github.com/PascalIversen) (FU Berlin)\n- [Mario Picciani](https://github.com/picciama) (TUM)\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#drugresponseeval` channel](https://nfcore.slack.com/channels/drugresponseeval) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/drugresponseeval for your analysis, please cite it using the following doi: [10.5281/zenodo.14779984](https://doi.org/10.5281/zenodo.14779984)\n\n> Our corresponding publication is at doi [10.1101/2025.05.26.655288](doi.org/10.1101/2025.05.26.655288)\n>\n> Bernett, J., Iversen, P., Picciani, M., Wilhelm, M., Baum, K., & List, M. **From Hype to Health Check: Critical Evaluation of Drug Response Prediction Models with DrEval.**\n>\n> _bioRxiv_, 2025-05.\n\nThe underlying data is available at doi: [10.5281/zenodo.12633909](https://doi.org/10.5281/zenodo.12633909).\n\nThe underlying python package is drevalpy, availably on [PyPI](https://pypi.org/project/drevalpy/) as standalone, for which we also have an extensive [ReadTheDocs Documentation](https://drevalpy.readthedocs.io/en/latest/).\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1266",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1266?version=2",
        "name": "nf-core/drugresponseeval",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cell-lines",
            "cross-validation",
            "deep-learning",
            "drug-response",
            "drug-response-prediction",
            "drugs",
            "fair-principles",
            "generalization",
            "hyperparameter-tuning",
            "machine-learning",
            "randomization-tests",
            "robustness-assessment",
            "training"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-07-07",
        "versions": 2
    },
    {
        "create_time": "2025-06-30",
        "creators": [],
        "description": "# Generate annotated gb for fragments and add them to DB\r\n\r\nAutomatically generate annotated **GenBank** files for your fragments based on your constraints and store them in your database.\r\n\r\n## steps:\r\n\r\n_input_: **GenBank** files : If your fragments are not in the database, you can generate GenBank files for each fragment to be used as input for the annotation workflow and then passed to the _evaluate_manufacturability_ tool.\r\n> evaluate_manufacturability\r\n1. Set the constraints for the annotations.\r\n2. Generate annotated GenBank files and pass them to the _seq_to_db_ tool.\r\n> seq_to_db\r\n1. Annotated GenBank files can be saved to the database, depending on the user's choice.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Biomolecular simulation",
            "Microbiology",
            "Sequence assembly",
            "Synthetic biology"
        ],
        "filtered_on": "annot* in description",
        "id": "1768",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1768?version=1",
        "name": "workflow-0",
        "number_of_steps": 2,
        "projects": [
            "DNA Foundry"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "seq_to_db",
            "evaluate_manufacturability"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-21",
        "versions": 1
    },
    {
        "create_time": "2025-07-01",
        "creators": [],
        "description": "# CWL + RO-Crate Workflow Descriptions\r\n\r\nThis repository stores computational workflows described using the **Common Workflow Language (CWL)** and enriched with metadata using **Research Object Crate (RO-Crate)** conforming to the **Workflow Run RO-Crate** profile.\r\n\r\nEach workflow is contained in its own directory (e.g., `WF5201`, `WF6101`, ...). Inside each workflow directory you will typically find at least:\r\n\r\n- The **CWL workflow definition** (with the same name as the directory, e.g., `WF5201.cwl`).\r\n- The **RO-Crate metadata file** (`ro-crate-metadata.json`).\r\n\r\nAdditional files supporting the workflow may also be included.\r\n\r\n## Overview\r\n\r\nThis document explains how to represent workflows by combining:\r\n\r\n- **CWL (Common Workflow Language):** Used to define the computational steps, data flows, and tools.\r\n- **RO-Crate:** Used to capture associated metadata (e.g., authorship, licenses, software, datasets) for the workflow.\r\n\r\nBy separating the abstract workflow definition from its metadata description, you can leverage existing tools for visualization, editing, and validation of your workflows while maintaining a clear structure.\r\n\r\n## Our Approach\r\n\r\nWe represent workflows using a combination of CWL and RO-Crate:\r\n\r\n- **CWL:** Captures the abstract definition of the workflow, detailing its computational steps, data flows, and the tools utilized. It does not include the implementation details of each operation.\r\n- **RO-Crate:** Provides rich metadata for the overall repository, the workflow file(s), software, and datasets. This metadata allows you to understand the context, provenance, and related details of the workflow components.\r\n\r\nThis separation provides flexibility by keeping the execution details (CWL) distinct from descriptive metadata (RO-Crate), yet they remain tightly connected.\r\n\r\n## Describing a Workflow using CWL + RO-Crate\r\n\r\nTo fully describe a workflow, you must separate the **workflow definition** (using CWL) from the **metadata description** (using RO-Crate).\r\n\r\n### Defining the CWL Workflow\r\n\r\n1. **Identify Global Inputs and Outputs:**  \r\n   Decide on the data that enters the workflow (inputs) and the final results (outputs). Optionally, include intermediate outputs if they are of interest.\r\n\r\n2. **Create the CWL File:**  \r\n   Write a CWL file in YAML format. Start with file metadata such as:\r\n\r\n   ```yaml\r\n   cwlVersion: v1.2\r\n   class: Workflow\r\n\r\n   requirements:\r\n     MultipleInputFeatureRequirement: {}\r\n     SubworkflowFeatureRequirement: {}\r\n   ```\r\n\r\n   > [NOTE]\r\n   > The `requirements` section may vary depending on your workflow. For example, if you use sub-workflows, you must include the `SubworkflowFeatureRequirement`.\r\n\r\n3. **Declare Global Inputs and Outputs:**\r\n\r\n   ```yaml\r\n   inputs:\r\n     DT5210: Directory\r\n     DT5211: Directory\r\n\r\n   outputs:\r\n     DT5208:\r\n       type: Directory\r\n       outputSource: SS5213/DT5208\r\n   ```\r\n\r\n   > [NOTE]\r\n   > Although `Directory` is commonly used to represent a dataset, you might choose a different type. Refer to the CWL documentation for additional types.\r\n\r\n### Defining Workflow Steps\r\n\r\nEach workflow step (or subworkflow) follows a consistent structure:\r\n\r\n```yaml\r\nSS5205:\r\n  in:\r\n    DT5210: DT5210\r\n  run:\r\n    class: Operation\r\n    inputs:\r\n      DT5210: Directory\r\n    outputs:\r\n      DT5201: File\r\n      DT5203: Directory\r\n  out:\r\n    - DT5201\r\n    - DT5203\r\n```\r\n\r\nKey elements are:\r\n\r\n- **`in`:** Defines which data this step requires.\r\n- **`run`:**\r\n  - For operations: Uses the `Operation` class to abstract away the underlying execution details.\r\n  - For subworkflows: Points to another CWL file.\r\n- **`out`:** Lists the output data produced by the step.\r\n\r\n### Connecting Steps via Data Dependencies\r\n\r\nCWL does not require an explicit execution order. Instead, dependencies are determined by connecting outputs to inputs:\r\n\r\n```yaml\r\nST520102:\r\n  in:\r\n    DT5201: ST520101/DT5201\r\n  run: ST520102.cwl\r\n  out:\r\n    - DT5255\r\n```\r\n\r\nThis connection means `ST520102` depends on the output (`DT5201`) of `ST520101` and will execute after it, while still allowing independent steps to run in parallel.\r\n\r\n## Validating Your Workflow and Metadata\r\n\r\n- **CWL Validation:**  \r\n  Use [cwltool](https://github.com/common-workflow-language/cwltool) to check your CWL files for syntax errors and to generate a graphical visualization (using Graphviz `dot` format) for verifying the workflow structure.\r\n\r\n- **RO-Crate Validation:**  \r\n  Validate your `ro-crate-metadata.json` file with tools such as the [RO-Crate Validator](https://github.com/crs4/rocrate-validator) and explore your RO-Crate interactively with [ro-crate-html-js](https://github.com/Language-Research-Technology/ro-crate-html-js).\r\n\r\n---\r\n\r\n## Additional Resources\r\n\r\n- [CWL Official Guide and Getting Started](https://www.commonwl.org/getting-started/)\r\n- [CWL User Guide](https://www.commonwl.org/user_guide/)\r\n- [RO-Crate Official Documentation](https://www.researchobject.org/ro-crate/)\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1786",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1786?version=1",
        "name": "[DTC-V3] WF5301: Lava flow",
        "number_of_steps": 5,
        "projects": [
            "WP5 - Volcanoes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "lava"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2025-07-01",
        "versions": 1
    },
    {
        "create_time": "2025-06-30",
        "creators": [
            "Crist John M. Pastor"
        ],
        "description": "This workflow extracts protein-coding sequences from whole genome sequencing (WGS) data obtained from the European Nucleotide Archive (ENA). It automates the preprocessing, annotation, and selection of relevant protein sequences using tools such as Prokka, FASTA-to-Tabular, and pattern-based selection. The resulting dataset supports downstream analyses including comparative genomics, phylogenetics, and functional annotation.",
        "doi": null,
        "edam_operation": [
            "Feature extraction"
        ],
        "edam_topic": [
            "Biochemistry",
            "Data mining",
            "Drug development",
            "Drug discovery",
            "Immunoinformatics",
            "Immunology",
            "Immunoproteins and antigens",
            "Proteins"
        ],
        "filtered_on": "annot* in description",
        "id": "1767",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1767?version=1",
        "name": "WGS2Protein v3.0.0",
        "number_of_steps": 9,
        "projects": [
            "MTB Bioinformatics Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "prokka",
            "Grep1",
            "fastqc",
            "shovill",
            "trimmomatic",
            "fasta2tab"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-30",
        "versions": 1
    },
    {
        "create_time": "2025-06-29",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-pathogensurveillance_logo_dark.png\">\n    <img alt=\"nf-core/pathogensurveillance\" src=\"docs/images/nf-core-pathogensurveillance_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/pathogensurveillance/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/pathogensurveillance/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/pathogensurveillance/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/pathogensurveillance/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/pathogensurveillance/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.04.2-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.1-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.1)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/pathogensurveillance)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23pathogensurveillance-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/pathogensurveillance)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/pathogensurveillance** is a population genomics pipeline for pathogen identification, variant detection, and biosurveillance.\nThe pipeline accepts paths to raw reads for one or more organisms and creates reports in the form of an interactive HTML document.\nSignificant features include the ability to analyze unidentified eukaryotic and prokaryotic samples, creation of reports for multiple user-defined groupings of samples, automated discovery and downloading of reference assemblies from [NCBI RefSeq](https://www.ncbi.nlm.nih.gov/refseq/), and rapid initial identification based on k-mer sketches followed by a more robust multi gene phylogeny and SNP-based phylogeny.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner.\nIt uses Docker/Singularity/Conda to make installation trivial and results highly reproducible.\nThe [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies.\nWhere appropriate, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure.\nThis ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world data sets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/pathogensurveillance/results).\n\n## Pipeline summary\n\n![](docs/images/pipeline_diagram.png)\n\n## Quick start guide\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nNote that some form of configuration will be needed so that Nextflow knows how to fetch the required software.\nThis is usually done in the form of a config profile.\nYou can chain multiple config profiles in a comma-separated string.\nIn most cases you will include one profile that defines a tool to reproducibly install and use software needed by the pipeline.\nThis is typically one of `docker`, `singularity`, or `conda`.\nIdeally `conda` should not be used unless `docker` or `singularity` cannot be used.\n\nProfiles can also be used to store parameters for the pipeline, such as input data and pipeline options.\nBefore using you own data, consider trying out a small example dataset included with the pipeline as a profile.\nAvailable test dataset profiles include:\n\n- `test`: Test profile of 1 small genome used to run the pipeline as fast as possible for testing purposes.\n- `test_serratia`: Test profile of 10 serratia isolates from Williams et al. 2022 (https://doi.org/10.1038/s41467-022-32929-2)\n- `test_bordetella`: Test profile of 5 Bordetella pertussis isolates sequenced with with Illumina and Nanopore from Wagner et al. 2023\n- `test_salmonella`: Test profile of 5 salmonella isolates from Hawkey et al. 2024 (https://doi.org/10.1038/s41467-024-54418-4)\n- `test_boxwood_blight`: Test profile of 5 samples of the boxwood blight fungus Cylindrocladium buxicola from LeBlanc et al. 2020 (https://doi.org/10.1094/PHYTO-06-20-0219-FI)\n- `test_mycobacteroides`: Test profile of 5 Mycobacteroides abscessus samples from Bronson et al. 2021 (https://doi.org/10.1038/s41467-021-25484-9)\n- `test_bacteria`: Test profile of 10 mixed bacteria from various sources\n- `test_klebsiella`: Test profile of 10 K. pneumoniae and related species from Holt et al. 2015 (https://doi.org/10.1073/pnas.1501049112)\n- `test_small_genomes`: Test profile consisting of 6 samples from species with small genomes from various sources.\n\nAdding `_full` to the end of any of these profiles will run a larger (often much larger) version of these datasets.\n\nFor example, you can run the `test_bacteria` profile with the `docker` profile:\n\n```bash\nnextflow run nf-core/pathogensurveillance -profile docker,test_bacteria -resume --outdir test_output\n```\n\nYou can see the samplesheets used in these profiles here:\n\nhttps://github.com/nf-core/test-datasets/tree/pathogensurveillance\n\nTo run your own input data, prepare a samplesheet as described in the [usage documentation](docs/usage/#samplesheet-input) section below and run the following command:\n\n```bash\nnextflow run nf-core/pathogensurveillance -profile <REPLACE WITH RUN TOOL> -resume --input <REPLACE WITH TSV/CSV> --outdir <REPLACE WITH OUTPUT PATH>\n```\n\nWhere:\n\n- `<REPLACE WITH RUN TOOL>` is one of `docker`, `singularity`, or `conda`\n- `<REPLACE WITH TSV/CSV>` is the path to the input samplesheet\n- `<REPLACE WITH OUTPUT PATH>` is the path to where to save the output\n\n## Documentation\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/pathogensurveillance/usage) and the [parameter documentation](https://nf-co.re/pathogensurveillance/parameters).\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/pathogensurveillance/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the [output documentation](https://nf-co.re/pathogensurveillance/output).\n\n## Credits\n\nThe following people contributed to the pipeline: Zachary S.L. Foster, Martha Sudermann, Camilo Parada-Rojas, Logan K. Blair, Fernanda I. Bocardo, Ricardo Alcal\u00e1-Brise\u00f1o, Hung Phan, Nicholas C. Cauldron, Alexandra J. Weisberg, Je\ufb00 H. Chang, and Niklaus J. Gr\u00fcnwald.\n\n## Funding\n\nThis work was supported by grants from USDA ARS (2072-22000-045-000-D) to NJG, USDA NIFA (2021-67021-34433; 2023-67013-39918) to JHC and NJG, as well as USDAR ARS NPDRS and FNRI and USDA APHIS to NJG.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#pathogensurveillance` channel](https://nfcore.slack.com/channels/pathogensurveillance) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/pathogensurveillance for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\n<picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/combined_logos_dark.png\">\n    <img alt=\"Logos of University of Oregon and USAD\" src=\"docs/images/combined_logos_light.png\">\n</picture>\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1763",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1763?version=1",
        "name": "nf-core/pathogensurveillance",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-29",
        "versions": 1
    },
    {
        "create_time": "2025-06-29",
        "creators": [
            "Christopher Hakkaart"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-demo_logo_dark.png\">\n    <img alt=\"nf-core/demo\" src=\"docs/images/nf-core-demo_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/demo/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/demo/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/demo/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/demo/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/demo/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.12192442-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.12192442)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.04.2-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.1-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.1)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/demo)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23demo-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/demo)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/demo** is a simple nf-core style bioinformatics pipeline for workshops and demonstrations. It was created using the nf-core template and is designed to run quickly using small test data files.\n\n![nf-core/demo metro map](docs/images/nf-core-demo-subway.png)\n\n1. Read QC ([`FASTQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n2. Adapter and quality trimming ([`SEQTK_TRIM`](https://github.com/lh3/seqtk))\n3. Present QC for raw reads ([`MULTIQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nSAMPLE1_PE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R2.fastq.gz\nSAMPLE2_PE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R2.fastq.gz\nSAMPLE3_SE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R1.fastq.gz,\nSAMPLE3_SE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R1.fastq.gz,\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/demo \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/demo/usage) and the [parameter documentation](https://nf-co.re/demo/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/demo/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/demo/output).\n\n## Credits\n\nnf-core/demo was originally written by Chris Hakkaart ([@christopher-hakkaart](https://github.com/christopher-hakkaart)).\n\nThe pipeline is currently being maintained by the Nextflow community team as well as [Geraldine Van der Auwera](https://github.com/vdauwera) and [Florian Wuennemann](https://github.com/FloWuenne).\n\n<!-- We thank the following people for their extensive assistance in the development of this pipeline: -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#demo` channel](https://nfcore.slack.com/channels/demo) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/demo for your analysis, please cite it using the following doi: [10.5281/zenodo.12192442](https://doi.org/10.5281/zenodo.12192442)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in description",
        "id": "1055",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1055?version=3",
        "name": "nf-core/demo",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "tutorial",
            "demo",
            "minimal-example",
            "training"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-29",
        "versions": 3
    },
    {
        "create_time": "2025-06-26",
        "creators": [
            "Clinical Genomics Stockholm"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-raredisease_logo_dark.png\">\n    <img alt=\"nf-core/raredisease\" src=\"docs/images/nf-core-raredisease_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/raredisease/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/raredisease/actions/workflows/ci.yml)\n\n[![GitHub Actions Linting Status](https://github.com/nf-core/raredisease/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/raredisease/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/raredisease/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7995798-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7995798)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n[![GitHub Actions Linting Status](https://github.com/nf-core/raredisease/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/raredisease/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/raredisease/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7995798-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7995798)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.1-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.1)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/raredisease)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23raredisease-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/raredisease)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n#### TOC\n\n- [Introduction](#introduction)\n- [Pipeline summary](#pipeline-summary)\n- [Usage](#usage)\n- [Pipeline output](#pipeline-output)\n- [Credits](#credits)\n- [Contributions and Support](#contributions-and-support)\n- [Citations](#citations)\n\n## Introduction\n\n**nf-core/raredisease** is a best-practice bioinformatic pipeline for calling and scoring variants from WGS/WES data from rare disease patients. This pipeline is heavily inspired by [MIP](https://github.com/Clinical-Genomics/MIP).\n\n> [!NOTE]\n> Right now, we only support paired-end data from Illumina. If you've got other types of data and the pipeline doesn't work for you, just open an issue. We'd be happy to chat about a solution.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/raredisease/results).\n\n## Pipeline summary\n\n  <picture align=\"center\">\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/raredisease_metromap_dark.png\">\n    <img alt=\"nf-core/raredisease workflow\" src=\"docs/images/raredisease_metromap_light.png\">\n  </picture>\n\n**1. Metrics:**\n\n- [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)\n- [Mosdepth](https://github.com/brentp/mosdepth)\n- [MultiQC](http://multiqc.info/)\n- [Picard's CollectMutipleMetrics, CollectHsMetrics, and CollectWgsMetrics](https://broadinstitute.github.io/picard/)\n- [Qualimap](http://qualimap.conesalab.org/)\n- [Sentieon's WgsMetricsAlgo](https://support.sentieon.com/manual/usages/general/)\n- [TIDDIT's cov](https://github.com/J35P312/)\n- [VerifyBamID2](https://github.com/Griffan/VerifyBamID)\n\n**2. Alignment:**\n\n- [Bwa-mem2](https://github.com/bwa-mem2/bwa-mem2)\n- [BWA-MEME](https://github.com/kaist-ina/BWA-MEME)\n- [BWA](https://github.com/lh3/bwa)\n- [Sentieon DNAseq](https://support.sentieon.com/manual/DNAseq_usage/dnaseq/)\n\n**3. Variant calling - SNV:**\n\n- [DeepVariant](https://github.com/google/deepvariant)\n- [Sentieon DNAscope](https://support.sentieon.com/manual/DNAscope_usage/dnascope/)\n\n**4. Variant calling - SV:**\n\n- [Manta](https://github.com/Illumina/manta)\n- [TIDDIT's sv](https://github.com/SciLifeLab/TIDDIT)\n- Copy number variant calling:\n  - [CNVnator](https://github.com/abyzovlab/CNVnator)\n  - [GATK GermlineCNVCaller](https://github.com/broadinstitute/gatk)\n\n**5. Annotation - SNV:**\n\n- [bcftools roh](https://samtools.github.io/bcftools/bcftools.html#roh)\n- [vcfanno](https://github.com/brentp/vcfanno)\n- [CADD](https://cadd.gs.washington.edu/)\n- [VEP](https://www.ensembl.org/info/docs/tools/vep/index.html)\n- [UPD](https://github.com/bjhall/upd)\n- [Chromograph](https://github.com/Clinical-Genomics/chromograph)\n\n**6. Annotation - SV:**\n\n- [SVDB query](https://github.com/J35P312/SVDB#Query)\n- [VEP](https://www.ensembl.org/info/docs/tools/vep/index.html)\n\n**7. Mitochondrial analysis:**\n\n- [Alignment and variant calling - GATK Mitochondrial short variant discovery pipeline ](https://gatk.broadinstitute.org/hc/en-us/articles/4403870837275-Mitochondrial-short-variant-discovery-SNVs-Indels-)\n- [eKLIPse](https://github.com/dooguypapua/eKLIPse/tree/master)\n- Annotation:\n  - [HaploGrep2](https://github.com/seppinho/haplogrep-cmd)\n  - [Hmtnote](https://github.com/robertopreste/HmtNote)\n  - [vcfanno](https://github.com/brentp/vcfanno)\n  - [CADD](https://cadd.gs.washington.edu/)\n  - [VEP](https://www.ensembl.org/info/docs/tools/vep/index.html)\n\n**8. Variant calling - repeat expansions:**\n\n- [Expansion Hunter](https://github.com/Illumina/ExpansionHunter)\n- [Stranger](https://github.com/Clinical-Genomics/stranger)\n\n**9. Variant calling - mobile elements:**\n\n- [RetroSeq](https://github.com/tk2/RetroSeq)\n\n**10. Rank variants - SV and SNV:**\n\n- [GENMOD](https://github.com/Clinical-Genomics/genmod)\n\n**11. Variant evaluation:**\n\n- [RTG Tools](https://github.com/RealTimeGenomics/rtg-tools)\n\nNote that it is possible to include/exclude certain tools or steps.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,lane,fastq_1,fastq_2,sex,phenotype,paternal_id,maternal_id,case_id\nhugelymodelbat,1,reads_1.fastq.gz,reads_2.fastq.gz,1,2,,,justhusky\n```\n\nEach row represents a pair of fastq files (paired end).\n\nSecond, ensure that you have defined the path to reference files and parameters required for the type of analysis that you want to perform. More information about this can be found [here](https://github.com/nf-core/raredisease/blob/dev/docs/usage.md).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/raredisease \\\n   -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/raredisease/usage) and the [parameter documentation](https://nf-co.re/raredisease/parameters).\n\n## Pipeline output\n\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/raredisease/output).\n\n## Credits\n\nnf-core/raredisease was written in a collaboration between the Clinical Genomics nodes in Sweden, with major contributions from [Ramprasad Neethiraj](https://github.com/ramprasadn), [Anders Jemt](https://github.com/jemten), [Lucia Pena Perez](https://github.com/Lucpen), and [Mei Wu](https://github.com/projectoriented) at Clinical Genomics Stockholm.\n\nAdditional contributors were [Sima Rahimi](https://github.com/sima-r), [Gwenna Breton](https://github.com/Gwennid) and [Emma V\u00e4sterviga](https://github.com/EmmaCAndersson) (Clinical Genomics Gothenburg); [Halfdan Rydbeck](https://github.com/hrydbeck) and [Lauri Mesilaakso](https://github.com/ljmesi) (Clinical Genomics Link\u00f6ping); [Subazini Thankaswamy Kosalai](https://github.com/sysbiocoder) (Clinical Genomics \u00d6rebro); [Annick Renevey](https://github.com/rannick), [Peter Pruisscher](https://github.com/peterpru) and [Eva Caceres](https://github.com/fevac) (Clinical Genomics Stockholm); [Ryan Kennedy](https://github.com/ryanjameskennedy) (Clinical Genomics Lund); [Anders Sune Pedersen](https://github.com/asp8200) (Danish National Genome Center) and [Lucas Taniguti](https://github.com/lmtani).\n\nWe thank the nf-core community for their extensive assistance in the development of this pipeline.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#raredisease` channel](https://nfcore.slack.com/channels/raredisease) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/raredisease for your analysis, please cite it using the following doi: [10.5281/zenodo.7995798](https://doi.org/10.5281/zenodo.7995798)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\nYou can read more about MIP's use in healthcare in,\n\n> Stranneheim H, Lagerstedt-Robinson K, Magnusson M, et al. Integration of whole genome sequencing into a healthcare setting: high diagnostic rates across multiple clinical entities in 3219 rare disease patients. Genome Med. 2021;13(1):40. doi:10.1186/s13073-021-00855-5\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1014",
        "keep": true,
        "latest_version": 11,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1014?version=11",
        "name": "nf-core/raredisease",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "wgs",
            "diagnostics",
            "rare-disease",
            "snv",
            "structural-variants",
            "variant-annotation",
            "variant-calling",
            "wes"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-26",
        "versions": 11
    },
    {
        "create_time": "2025-06-25",
        "creators": [
            "Joon-Klaps None",
            "Joon-Klaps None"
        ],
        "description": "A bioinformatics best-practice analysis pipeline for reconstructing consensus genomes and to identify intra-host variants from metagenomic sequencing data or enriched based sequencing data like hybrid capture.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1757",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1757?version=2",
        "name": "Joon-Klaps/viralgenie",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "fastq",
            "ngs",
            "virology",
            "epidemiology",
            "viral-metagenomics",
            "virus-genomes"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-25",
        "versions": 2
    },
    {
        "create_time": "2025-06-25",
        "creators": [
            "Pixelgen Technologies AB"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-pixelator_logo_dark.png\">\n    <img alt=\"nf-core/pixelator\" src=\"docs/images/nf-core-pixelator_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/pixelator/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/pixelator/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/pixelator/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/pixelator/actions/workflows/linting.yml)\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/pixelator/results)\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/pixelator)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23pixelator-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/pixelator)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/pixelator** is a bioinformatics best-practice analysis pipeline for analysis of data from the\nMolecular Pixelation (MPX) and Proximity Network (PNA) assays. It takes a samplesheet as input and will process your data\nusing `pixelator` to produce a PXL file containing single-cell protein abundance and protein interactomics data.\n\n![](./docs/images/nf-core-pixelator-metromap.svg)\n\nDepending on the input data the pipeline will run different steps.\n\nFor PNA data, the pipeline will run the following steps:\n\n1. Do quality control checks of input reads and build amplicons ([`pixelator single-cell-pna amplicon`](https://github.com/PixelgenTechnologies/pixelator))\n2. Create groups of amplicons based on their marker assignments ([`pixelator single-cell-pna demux`](https://github.com/PixelgenTechnologies/pixelator))\n3. Derive original molecules to use as edge list downstream by error correcting, and counting input amplicons ([`pixelator single-cell-pna collapse`](https://github.com/PixelgenTechnologies/pixelator))\n4. Compute the components of the graph from the edge list in order to create putative cells ([`pixelator single-cell-pna graph`](https://github.com/PixelgenTechnologies/pixelator))\n5. Analyze the spatial information in the cell graphs ([`pixelator single-cell-pna analysis`](https://github.com/PixelgenTechnologies/pixelator))\n6. Generate 3D graph layouts for visualization of cells ([`pixelator single-cell-pna layout`](https://github.com/PixelgenTechnologies/pixelator))\n7. Report generation ([`pixelator single-cell-pna report`](https://github.com/PixelgenTechnologies/pixelator))\n\nFor MPX data, the pipeline will run the following steps:\n\n1. Build an amplicons from the input reads ([`pixelator single-cell-mpx amplicon`](https://github.com/PixelgenTechnologies/pixelator))\n2. Read QC and filtering, correctness of the pixel binding sequence sequences ([`pixelator single-cell-mpx preqc | pixelator adapterqc`](https://github.com/PixelgenTechnologies/pixelator))\n3. Assign a marker (barcode) to each read ([`pixelator single-cell-mpx demux`](https://github.com/PixelgenTechnologies/pixelator))\n4. Error correction, duplicate removal, compute read counts ([`pixelator single-cell-mpx collapse`](https://github.com/PixelgenTechnologies/pixelator))\n5. Compute the components of the graph from the edge list in order to create putative cells ([`pixelator single-cell-mpx graph`](https://github.com/PixelgenTechnologies/pixelator))\n6. Call and annotate cells ([`pixelator single-cell-mpx annotate`](https://github.com/PixelgenTechnologies/pixelator))\n7. Analyze the cells for polarization and colocalization ([`pixelator single-cell-mpx analysis`](https://github.com/PixelgenTechnologies/pixelator))\n8. Generate 3D graph layouts for visualization of cells ([`pixelator single-cell-mpx layout`](https://github.com/PixelgenTechnologies/pixelator))\n9. Report generation ([`pixelator single-cell-mpx report`](https://github.com/PixelgenTechnologies/pixelator))\n\n> [!WARNING]\n> Since Nextflow 23.07.0-edge, Nextflow no longer mounts the host's home directory when using Apptainer or Singularity.\n> This causes issues in some dependencies. As a workaround, you can revert to the old behavior by setting the environment variable\n> `NXF_APPTAINER_HOME_MOUNT` or `NXF_SINGULARITY_HOME_MOUNT` to `true` in the machine from which you launch the pipeline.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows (the exact values you need to input depend on the design and panel you are using):\n\n`samplesheet.csv`:\n\n```csv\nsample,design,panel,fastq_1,fastq_2\nsample1,pna-2,proxiome-immuno-155,sample1_R1_001.fastq.gz,sample1_R2_001.fastq.gz\n```\n\nEach row represents a sample and gives the design, a panel file and the input fastq files.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/pixelator \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> This version of the pipeline does not support conda environments, due to issues with upstream dependencies.\n> This means you cannot use the `conda` and `mamba` profiles. Please use `docker` or `singularity` instead.\n> We hope to add support for conda environments in the future.\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/pixelator/usage) and the [parameter documentation](https://nf-co.re/pixelator/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/pixelator/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/pixelator/output).\n\n## Credits\n\nnf-core/pixelator was originally written for [Pixelgen Technologies AB](https://www.pixelgen.com/) by:\n\n- Florian De Temmerman\n- Johan Dahlberg\n- Alvaro Martinez Barrio\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#pixelator` channel](https://nfcore.slack.com/channels/pixelator) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/pixelator for your analysis, please cite it using the following doi: [10.5281/zenodo.10015112](https://doi.org/10.5281/zenodo.10015112)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\nYou can cite the molecular pixelation technology as follows:\n\n> **Molecular pixelation: spatial proteomics of single cells by sequencing.**\n>\n> Filip Karlsson, Tomasz Kallas, Divya Thiagarajan, Max Karlsson, Maud Schweitzer, Jose Fernandez Navarro, Louise Leijonancker, Sylvain Geny, Erik Pettersson, Jan Rhomberg-Kauert, Ludvig Larsson, Hanna van Ooijen, Stefan Petkov, Marcela Gonz\u00e1lez-Granillo, Jessica Bunz, Johan Dahlberg, Michele Simonetti, Prajakta Sathe, Petter Brodin, Alvaro Martinez Barrio & Simon Fredriksson\n>\n> _Nat Methods._ 2024 May 08. doi: [10.1038/s41592-024-02268-9](https://doi.org/10.1038/s41592-024-02268-9)\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1010",
        "keep": true,
        "latest_version": 10,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1010?version=10",
        "name": "nf-core/pixelator",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "molecular-pixelation",
            "pixelator",
            "pixelgen-technologies",
            "proteins",
            "single-cell",
            "single-cell-omics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-25",
        "versions": 10
    },
    {
        "create_time": "2024-09-18",
        "creators": [
            "Diego De Panis"
        ],
        "description": "The workflow takes a (trimmed) Long reads collection, runs Meryl to create a K-mer database, Genomescope2 to estimate genome properties and Smudgeplot to estimate ploidy (optional). The main results are K-mer database and genome profiling plots, tables, and values useful for downstream analysis. Default K-mer length and ploidy for Genomescope are 31 and 2, respectively. ",
        "doi": null,
        "edam_operation": [
            "Sequencing quality control"
        ],
        "edam_topic": [
            "Whole genome sequencing"
        ],
        "filtered_on": "profil* in tags",
        "id": "603",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/603?version=2",
        "name": "ERGA Profiling Long Reads v2505 (WF1)",
        "number_of_steps": 17,
        "projects": [
            "ERGA Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [
            "erga",
            "profiling"
        ],
        "tools": [
            "Add_a_column1",
            "tp_grep_tool",
            "genomescope",
            "Convert characters1",
            "smudgeplot",
            "Cut1",
            "tp_cut_tool",
            "tp_find_and_replace",
            "param_value_from_file",
            "meryl"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-24",
        "versions": 2
    },
    {
        "create_time": "2025-06-23",
        "creators": [],
        "description": "Metatranscriptomics analysis using microbiome RNA-seq data\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metatranscriptomics analysis using microbiome RNA-seq data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metatranscriptomics/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Pratik Jagtap, Subina Mehta, Saskia Hiltemann, Paul Zierep\n\n**Tutorial Author(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Ray Sajulga](https://training.galaxyproject.org/training-material/hall-of-fame/jraysajulga/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Christine Oger](https://training.galaxyproject.org/training-material/hall-of-fame/ogerdfx/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1466",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1466?version=2",
        "name": "Metatranscriptomics analysis using microbiome RNA-seq data",
        "number_of_steps": 33,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "metaphlan",
            "Cut1",
            "humann_split_stratified_table",
            "export2graphlan",
            "Grouping1",
            "humann_regroup_table",
            "graphlan_annotate",
            "graphlan",
            "taxonomy_krona_chart",
            "tp_sort_header_tool",
            "multiqc",
            "combine_metaphlan_humann",
            "fastqc",
            "tp_find_and_replace",
            "humann",
            "bg_sortmerna",
            "humann_renorm_table",
            "cutadapt",
            "fastq_paired_end_interlacer",
            "humann_unpack_pathways",
            "humann_rename_table",
            "Grep1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-23",
        "versions": 2
    },
    {
        "create_time": "2025-06-23",
        "creators": [],
        "description": "Metatranscriptomics analysis using microbiome RNA-seq data (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metatranscriptomics analysis using microbiome RNA-seq data (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metatranscriptomics-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Pratik Jagtap, Subina Mehta, Saskia Hiltemann, Paul Zierep\n\n**Tutorial Author(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Ray Sajulga](https://training.galaxyproject.org/training-material/hall-of-fame/jraysajulga/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/)\n\n**Tutorial Contributor(s)**: [Christine Oger](https://training.galaxyproject.org/training-material/hall-of-fame/ogerdfx/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1451",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1451?version=2",
        "name": "Metatranscriptomics analysis using microbiome RNA-seq data - Workflow 2: Community profile",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "metaphlan",
            "Cut1",
            "export2graphlan",
            "taxonomy_krona_chart",
            "graphlan_annotate",
            "graphlan"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-23",
        "versions": 2
    },
    {
        "create_time": "2025-06-23",
        "creators": [],
        "description": "Metatranscriptomics analysis using microbiome RNA-seq data (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metatranscriptomics analysis using microbiome RNA-seq data (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metatranscriptomics-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Pratik Jagtap, Subina Mehta, Saskia Hiltemann, Paul Zierep\n\n**Tutorial Author(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Ray Sajulga](https://training.galaxyproject.org/training-material/hall-of-fame/jraysajulga/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/)\n\n**Tutorial Contributor(s)**: [Christine Oger](https://training.galaxyproject.org/training-material/hall-of-fame/ogerdfx/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1444",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1444?version=2",
        "name": "Metatranscriptomics analysis using microbiome RNA-seq data - Workflow 1: Preprocessing",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "cutadapt",
            "fastqc",
            "fastq_paired_end_interlacer",
            "multiqc",
            "bg_sortmerna"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-23",
        "versions": 2
    },
    {
        "create_time": "2025-06-23",
        "creators": [],
        "description": "Metatranscriptomics analysis using microbiome RNA-seq data (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metatranscriptomics analysis using microbiome RNA-seq data (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metatranscriptomics-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Pratik Jagtap, Subina Mehta, Saskia Hiltemann, Paul Zierep\n\n**Tutorial Author(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Ray Sajulga](https://training.galaxyproject.org/training-material/hall-of-fame/jraysajulga/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/)\n\n**Tutorial Contributor(s)**: [Christine Oger](https://training.galaxyproject.org/training-material/hall-of-fame/ogerdfx/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1456",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1456?version=2",
        "name": "Metatranscriptomics analysis using microbiome RNA-seq data - Workflow 3: Functional Information",
        "number_of_steps": 18,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "humann_renorm_table",
            "humann_split_stratified_table",
            "humann_unpack_pathways",
            "tp_find_and_replace",
            "humann",
            "humann_regroup_table",
            "humann_rename_table",
            "Grep1",
            "tp_sort_header_tool"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-23",
        "versions": 2
    },
    {
        "create_time": "2025-06-20",
        "creators": [
            "Yichun Feng"
        ],
        "description": "# A Retrieval-Augmented Knowledge Mining Method with Deep Thinking LLMs for Biomedical Research and Clinical Support\r\n\r\n## Introduction\r\nKnowledge graphs and large language models (LLMs) serve as key tools for biomedical knowledge integration and reasoning, facilitating the structured organization of literature and the discovery of deep semantic relationships. However, existing methods still face challenges in knowledge mining and cross-document reasoning: knowledge graph construction is constrained by complex terminology, data heterogeneity, and rapid knowledge evolution, while LLMs exhibit limitations in retrieval and reasoning, making it difficult to efficiently uncover deep cross-document associations and reasoning pathways.\r\nTo address these issues, we propose a pipeline that first utilizes LLMs to construct a biomedical knowledge graph from large-scale literature and then builds a cross-document question-answering dataset (BioCDQA) based on this graph to evaluate latent knowledge retrieval and multi-hop reasoning capabilities. Subsequently, we introduce Integrated and Progressive Retrieval-Augmented Reasoning (IP-RAR) to further enhance retrieval accuracy and knowledge reasoning.\r\nIP-RAR maximizes information recall via Integrated Reasoning-based Retrieval and refines extracted knowledge through Progressive Reasoning-based Generation, which harnesses self-reflection to achieve deep thinking and enable precise contextual understanding.\r\nExperiments show that IP-RAR improves document retrieval F1 by 20\\% and answer generation accuracy by 25\\% over existing methods. This framework helps doctors efficiently integrate treatment evidence for personalized medication plans and enables researchers to analyze advancements and research gaps, accelerating scientific discovery and decision-making.\r\n\r\n<img width=\"1123\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3bb6a95b-88ba-491b-9817-7b50835c8f43\" />\r\nFig. 1. Overview of the proposed framework for biomedical knowledge mining. (A) Biomedical knowledge sources, such as research papers and user queries, are processed through (B) \r\nA knowledge mining pipeline that leverages a LLM alongside the Integrated and Progressive Retrieval-Augmented Reasoning approach, designed to generate knowledge graph and precise answers. (C) The outputs enable diverse applications, including drug synergy/antagonism, drug repurposing, precision medicine, bottleneck analysis, research planning, and knowledge transfer.\r\n\r\n## Framework of IP-RAR\r\n\r\n<img width=\"1171\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b4ba9d05-d950-4033-a723-6cc9177ccb3d\" />\r\n\r\n\r\n\r\nFig. 2. Framework of Integrated and Progressive Retrieval-Augmented Reasoning. (A) Integrated Reasoning-based Retrieval: Performs pre-retrieval reasoning, extracting keywords and generating a virtual answer. Then, a multi-level, multi-granularity retrieval strategy is used to retrieve relevant text chunks, which are ranked based on relevance. (B) Progressive Reasoning-based Generation: Filters out irrelevant text chunks through explanations or self-reflection, then leverages DeepSeek-R1 for deep-thinking-based reasoning on the valid text chunks, generating a precise final response.\r\n\r\n## Biomedical Cross-Document Question Answering Dataset\r\nOur dataset is specifically designed for reasoning-based question answering across multiple documents, consisting of tuples that include the following elements: question, question type, answer, source papers for the answer, and source sentences for the answer. Each tuple contains a natural language question that can be answered using one or more sentences extracted from the answer source papers. The answer source may involve one or more papers, and each answer is composed of several sentences that may originate from a single span or multiple spans across different source papers. The dataset comprises a total of 1,183 question-answer pairs, with a retrieval space of 68,428 papers and over 1.85 million document chunks, providing a rich resource for complex reasoning and retrieval tasks. The construction process is illustrated in Figure 3.\r\n<img width=\"1123\" alt=\"image\" src=\"https://github.com/user-attachments/assets/36cc711a-0e59-4f53-a2b9-79d1256a2959\" />\r\n\r\nFig. 3. Dataset Construction Workflow Diagram.\r\n(A) Data Collection and Processing: The process begins by converting research papers from PDF to markdown (MD) format to facilitate content extraction.\r\n(B) Structured Dataset Generation for Entity-level Knowledge Graph: An LLM is used to extract entities and relationships (Entity1, Relationship, Entity2), which are then standardized to construct an entity-level knowledge graph. This graph supports downstream tasks such as drug repurposing, drug interaction analysis for comorbid conditions, and gene-disease associations.\r\n(C) Structured Dataset Generation for Document-level Knowledge Graph: Summarization is performed using an LLM to extract key aspects such as methods, datasets, and research directions. The resulting document-level knowledge graph facilitates tasks such as research strategic planning and research paper recommendations.\r\n(D) Unstructured Dataset Generation for Multiple Documents: Integration of the entity-level and document-level knowledge graphs produces a comprehensive knowledge graph. This integrated graph enables connections across multiple documents and supports downstream tasks such as content-based factual questioning, research bottleneck analysis, knowledge transfer, trend analysis, and hotspot detection.\r\n\r\n## Environments Setting\r\n1\u3001In a conda env with PyTorch / CUDA available clone and download this repository.\r\n\r\n2\u3001In the top-level directory run:\r\n```bash\r\npip install -r requirements.txt\r\n```\r\n\r\n## Usage\r\n### Data preparation\r\n1\u3001You can obtain the BioCDQA.json from the BioCDQA dataset folder, which is a dataset containing 1183 question-answer pairs. \r\n\r\n2\u3001You can access the 68,428 biomedical papers within the retrieval space through the [all_document.json](https://drive.google.com/file/d/1Q-Va4mfdgJt7x3Y5QiXtoCbtalDbvqZI/view?usp=sharing), where you can download the full text of each paper and view its metadata, including title, abstract, key_topics, publication details, and urls.\r\n\r\n3\u3001You should download the text and convert it into either Markdown or TXT format. We use [Marker](https://github.com/VikParuchuri/marker.git) to convert the text into Markdown format. \r\n\r\n4\u3001Split the text into appropriately sized chunks (e.g., complete sentences with 500\u20131000 tokens per chunk) and save the chunks as all_text_chunks.tsv.\r\n```\r\ntsv_data['id'].append(idx)\r\ntsv_data['text'].append(item['content'])\r\ntsv_data['title'].append(item['pid'])\r\n```\r\n5\u3001The abstract does not need to be downloaded or split. It can be directly obtained from the metadata in all_document.json and converted into all_abstract_chunks.tsv.\r\n\r\n#### Other Dataset\r\nBioASQ: The benchmark dataset for biomedical semantic indexing and question answering at https://participants-area.bioasq.org/datasets/\r\nMASH-QA: A clinical QA dataset with semantic hierarchies at https://github.com/mingzhu0527/MASHQA.git\r\n\r\n### Integrated Reasoning-based Retrieval\r\n#### Download Retrieval Model\r\nWe use [Contriever-MSMARCO](https://huggingface.co/facebook/contriever-msmarco) as our retrieval component.\r\n\r\n#### Pre-Retrieval Reasoning\r\nUse the LLM to generate keywords and a virtual answer based on the query.\r\n```\r\ncd IP-RAR/Integrated_Reasoning-based_Retrieval\r\npython question_analysis.py\r\n```\r\n\r\n#### Generate embeddings for your own data\r\nGenerate the embeddings for the full-text.\r\n```\r\ncd IP-RAR/Integrated_Reasoning-based_Retrieval/retrieval_lm\r\npython generate_passage_embeddings.py \\\r\n    --model_name_or_path contriever-msmarco \\\r\n    --output_dir YOUR_OUTPUT_DIR \\\r\n    --passages all_text_chunks.tsv\r\n```\r\n\r\nSimilarly, generate the embeddings for the abstract.\r\n\r\n\r\n```\r\ncd IP-RAR/Integrated_Reasoning-based_Retrieval/retrieval_lm\r\npython generate_passage_embeddings.py \\\r\n    --model_name_or_path contriever-msmarco \\\r\n    --output_dir YOUR_OUTPUT_DIR \\\r\n    --passages all_abstract_chunks.tsv\r\n```\r\n\r\n#### Multi-Level and Multi-Granularity Retrieval\r\n\r\nPerform retrieval on the text based on the query.\r\n```\r\ncd IP-RAR/Integrated_Reasoning-based_Retrieval/retrieval_lm\r\npython passage_retrieval.py \\\r\n    --model_name_or_path contriever-msmarco\\\r\n    --passages all_text_chunks.tsv \\\r\n    --passages_embeddings passages_00_text_ms \\\r\n    --query BioCDQA.json  \\\r\n    --output_dir YOUR_OUTPUT_FILE \\\r\n    --n_docs 20\r\n```\r\nPerform retrieval on the abstract based on the query.\r\n```\r\npython passage_retrieval.py \\\r\n    --model_name_or_path contriever-msmarco\\\r\n    --passages all_abstract_chunks.tsv \\\r\n    --passages_embeddings passages_00_abstract_ms \\\r\n    --query BioCDQA.json  \\\r\n    --output_dir YOUR_OUTPUT_FILE \\\r\n    --n_docs 10\r\n```\r\nPerform retrieval on the text based on the virtual answer.\r\n```\r\npython passage_retrieval_virtual_answer.py \\\r\n    --model_name_or_path contriever-msmarco\\\r\n    --passages all_text_chunks.tsv \\\r\n    --passages_embeddings passages_00_text_ms \\\r\n    --query question_analysis_output.json\r\n    --output_dir YOUR_OUTPUT_FILE \\\r\n    --n_docs 10\r\n```\r\nPerform retrieval on the text based on the virtual answer.\r\n```\r\npython passage_retrieval_virtual_answer.py \\\r\n    --model_name_or_path contriever-msmarco\\\r\n    --passages all_abstract_chunks.tsv \\\r\n    --passages_embeddings passages_00_abstract_ms \\\r\n    --query question_analysis_output.json  \\\r\n    --output_dir YOUR_OUTPUT_FILE \\\r\n    --n_docs 10\r\n```\r\n\r\nPerform keyword matching based on the text.\r\n```\r\ncd IP-RAR/Integrated_Reasoning-based_Retrieval\r\npython keyword_matching.py\r\n```\r\n\r\n#### Aggregator\r\n```\r\ncd IP-RAR/Integrated_Reasoning-based_Retrieval\r\npython Aggregator.py\r\n```\r\n\r\n### Progressive Reasoning-based Generation\r\nUse the LLM to perform multi-stage validation on the results obtained from the Aggregator and generate the final answer.\r\n```\r\ncd IP-RAR/Progressive_Reasoning-based_Generation\r\npython Generation.py\r\n```\r\n\r\n\r\n### Evaluation\r\n#### Document Retrieval Performance Evaluation\r\nWe employ Mean Precision, Mean Recall, and Mean F-measure to assess the proportion of relevant documents retrieved and the balance between precision and recall. The implementation can be found in Evaluation/Document_Retrieval.py.\r\n#### Answer Accuracy Evaluation\r\nWe utilize GPT-4-based evaluators to score answers on a five-point scale, ensuring semantic accuracy, precision, and relevance, particularly for summary-type questions. The implementation can be found in Evaluation/Answer_Accuracy.py.\r\n\r\n## Knowledge Graph\r\nThe knowledge graph can serve as a valuable resource for researchers, facilitating knowledge discovery, identifying trends, and exploring relationships between biomedical entities and research methods. The knowledge graph comprises 94,962 nodes and 290,403 relationships. You can download the [knowledge graph](https://drive.google.com/file/d/1x5alzBbdigoBI9j2ZX64cLU_Uad_xqfl/view?usp=sharing).\r\n\r\n## License\r\n\r\n### Code\r\nThe code in this repository is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\r\n\r\n### Data\r\nThe data in this repository is licensed under the Creative Commons Public Domain Dedication (CC0 1.0 Universal). See the [DATA_LICENSE](DATA_LICENSE) file for details.\r\n\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.1744.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1744",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1744?version=1",
        "name": "BioCDQA",
        "number_of_steps": 0,
        "projects": [
            "Biomedical_LLM"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Unrecognized workflow type",
        "update_time": "2025-06-20",
        "versions": 1
    },
    {
        "create_time": "2025-06-20",
        "creators": [],
        "description": "# A Retrieval-Augmented Knowledge Mining Method with Deep Thinking LLMs for Biomedical Research and Clinical Support\r\n\r\n## Introduction\r\nKnowledge graphs and large language models (LLMs) serve as key tools for biomedical knowledge integration and reasoning, facilitating the structured organization of literature and the discovery of deep semantic relationships. However, existing methods still face challenges in knowledge mining and cross-document reasoning: knowledge graph construction is constrained by complex terminology, data heterogeneity, and rapid knowledge evolution, while LLMs exhibit limitations in retrieval and reasoning, making it difficult to efficiently uncover deep cross-document associations and reasoning pathways.\r\nTo address these issues, we propose a pipeline that first utilizes LLMs to construct a biomedical knowledge graph from large-scale literature and then builds a cross-document question-answering dataset (BioCDQA) based on this graph to evaluate latent knowledge retrieval and multi-hop reasoning capabilities. Subsequently, we introduce Integrated and Progressive Retrieval-Augmented Reasoning (IP-RAR) to further enhance retrieval accuracy and knowledge reasoning.\r\nIP-RAR maximizes information recall via Integrated Reasoning-based Retrieval and refines extracted knowledge through Progressive Reasoning-based Generation, which harnesses self-reflection to achieve deep thinking and enable precise contextual understanding.\r\nExperiments show that IP-RAR improves document retrieval F1 by 20\\% and answer generation accuracy by 25\\% over existing methods. This framework helps doctors efficiently integrate treatment evidence for personalized medication plans and enables researchers to analyze advancements and research gaps, accelerating scientific discovery and decision-making.\r\n\r\n<img width=\"1123\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3bb6a95b-88ba-491b-9817-7b50835c8f43\" />\r\nFig. 1. Overview of the proposed framework for biomedical knowledge mining. (A) Biomedical knowledge sources, such as research papers and user queries, are processed through (B) \r\nA knowledge mining pipeline that leverages a LLM alongside the Integrated and Progressive Retrieval-Augmented Reasoning approach, designed to generate knowledge graph and precise answers. (C) The outputs enable diverse applications, including drug synergy/antagonism, drug repurposing, precision medicine, bottleneck analysis, research planning, and knowledge transfer.\r\n\r\n## Framework of IP-RAR\r\n\r\n<img width=\"1171\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b4ba9d05-d950-4033-a723-6cc9177ccb3d\" />\r\n\r\n\r\n\r\nFig. 2. Framework of Integrated and Progressive Retrieval-Augmented Reasoning. (A) Integrated Reasoning-based Retrieval: Performs pre-retrieval reasoning, extracting keywords and generating a virtual answer. Then, a multi-level, multi-granularity retrieval strategy is used to retrieve relevant text chunks, which are ranked based on relevance. (B) Progressive Reasoning-based Generation: Filters out irrelevant text chunks through explanations or self-reflection, then leverages DeepSeek-R1 for deep-thinking-based reasoning on the valid text chunks, generating a precise final response.\r\n\r\n## Biomedical Cross-Document Question Answering Dataset\r\nOur dataset is specifically designed for reasoning-based question answering across multiple documents, consisting of tuples that include the following elements: question, question type, answer, source papers for the answer, and source sentences for the answer. Each tuple contains a natural language question that can be answered using one or more sentences extracted from the answer source papers. The answer source may involve one or more papers, and each answer is composed of several sentences that may originate from a single span or multiple spans across different source papers. The dataset comprises a total of 1,183 question-answer pairs, with a retrieval space of 68,428 papers and over 1.85 million document chunks, providing a rich resource for complex reasoning and retrieval tasks. The construction process is illustrated in Figure 3.\r\n<img width=\"1123\" alt=\"image\" src=\"https://github.com/user-attachments/assets/36cc711a-0e59-4f53-a2b9-79d1256a2959\" />\r\n\r\nFig. 3. Dataset Construction Workflow Diagram.\r\n(A) Data Collection and Processing: The process begins by converting research papers from PDF to markdown (MD) format to facilitate content extraction.\r\n(B) Structured Dataset Generation for Entity-level Knowledge Graph: An LLM is used to extract entities and relationships (Entity1, Relationship, Entity2), which are then standardized to construct an entity-level knowledge graph. This graph supports downstream tasks such as drug repurposing, drug interaction analysis for comorbid conditions, and gene-disease associations.\r\n(C) Structured Dataset Generation for Document-level Knowledge Graph: Summarization is performed using an LLM to extract key aspects such as methods, datasets, and research directions. The resulting document-level knowledge graph facilitates tasks such as research strategic planning and research paper recommendations.\r\n(D) Unstructured Dataset Generation for Multiple Documents: Integration of the entity-level and document-level knowledge graphs produces a comprehensive knowledge graph. This integrated graph enables connections across multiple documents and supports downstream tasks such as content-based factual questioning, research bottleneck analysis, knowledge transfer, trend analysis, and hotspot detection.\r\n\r\n## Environments Setting\r\n1\u3001In a conda env with PyTorch / CUDA available clone and download this repository.\r\n\r\n2\u3001In the top-level directory run:\r\n```bash\r\npip install -r requirements.txt\r\n```\r\n\r\n## Usage\r\n### Data preparation\r\n1\u3001You can obtain the BioCDQA.json from the BioCDQA dataset folder, which is a dataset containing 1183 question-answer pairs. \r\n\r\n2\u3001You can access the 68,428 biomedical papers within the retrieval space through the [all_document.json](https://drive.google.com/file/d/1Q-Va4mfdgJt7x3Y5QiXtoCbtalDbvqZI/view?usp=sharing), where you can download the full text of each paper and view its metadata, including title, abstract, key_topics, publication details, and urls.\r\n\r\n3\u3001You should download the text and convert it into either Markdown or TXT format. We use [Marker](https://github.com/VikParuchuri/marker.git) to convert the text into Markdown format. \r\n\r\n4\u3001Split the text into appropriately sized chunks (e.g., complete sentences with 500\u20131000 tokens per chunk) and save the chunks as all_text_chunks.tsv.\r\n```\r\ntsv_data['id'].append(idx)\r\ntsv_data['text'].append(item['content'])\r\ntsv_data['title'].append(item['pid'])\r\n```\r\n5\u3001The abstract does not need to be downloaded or split. It can be directly obtained from the metadata in all_document.json and converted into all_abstract_chunks.tsv.\r\n\r\n#### Other Dataset\r\nBioASQ: The benchmark dataset for biomedical semantic indexing and question answering at https://participants-area.bioasq.org/datasets/\r\nMASH-QA: A clinical QA dataset with semantic hierarchies at https://github.com/mingzhu0527/MASHQA.git\r\n\r\n### Integrated Reasoning-based Retrieval\r\n#### Download Retrieval Model\r\nWe use [Contriever-MSMARCO](https://huggingface.co/facebook/contriever-msmarco) as our retrieval component.\r\n\r\n#### Pre-Retrieval Reasoning\r\nUse the LLM to generate keywords and a virtual answer based on the query.\r\n```\r\ncd IP-RAR/Integrated_Reasoning-based_Retrieval\r\npython question_analysis.py\r\n```\r\n\r\n#### Generate embeddings for your own data\r\nGenerate the embeddings for the full-text.\r\n```\r\ncd IP-RAR/Integrated_Reasoning-based_Retrieval/retrieval_lm\r\npython generate_passage_embeddings.py \\\r\n    --model_name_or_path contriever-msmarco \\\r\n    --output_dir YOUR_OUTPUT_DIR \\\r\n    --passages all_text_chunks.tsv\r\n```\r\n\r\nSimilarly, generate the embeddings for the abstract.\r\n\r\n\r\n```\r\ncd IP-RAR/Integrated_Reasoning-based_Retrieval/retrieval_lm\r\npython generate_passage_embeddings.py \\\r\n    --model_name_or_path contriever-msmarco \\\r\n    --output_dir YOUR_OUTPUT_DIR \\\r\n    --passages all_abstract_chunks.tsv\r\n```\r\n\r\n#### Multi-Level and Multi-Granularity Retrieval\r\n\r\nPerform retrieval on the text based on the query.\r\n```\r\ncd IP-RAR/Integrated_Reasoning-based_Retrieval/retrieval_lm\r\npython passage_retrieval.py \\\r\n    --model_name_or_path contriever-msmarco\\\r\n    --passages all_text_chunks.tsv \\\r\n    --passages_embeddings passages_00_text_ms \\\r\n    --query BioCDQA.json  \\\r\n    --output_dir YOUR_OUTPUT_FILE \\\r\n    --n_docs 20\r\n```\r\nPerform retrieval on the abstract based on the query.\r\n```\r\npython passage_retrieval.py \\\r\n    --model_name_or_path contriever-msmarco\\\r\n    --passages all_abstract_chunks.tsv \\\r\n    --passages_embeddings passages_00_abstract_ms \\\r\n    --query BioCDQA.json  \\\r\n    --output_dir YOUR_OUTPUT_FILE \\\r\n    --n_docs 10\r\n```\r\nPerform retrieval on the text based on the virtual answer.\r\n```\r\npython passage_retrieval_virtual_answer.py \\\r\n    --model_name_or_path contriever-msmarco\\\r\n    --passages all_text_chunks.tsv \\\r\n    --passages_embeddings passages_00_text_ms \\\r\n    --query question_analysis_output.json\r\n    --output_dir YOUR_OUTPUT_FILE \\\r\n    --n_docs 10\r\n```\r\nPerform retrieval on the text based on the virtual answer.\r\n```\r\npython passage_retrieval_virtual_answer.py \\\r\n    --model_name_or_path contriever-msmarco\\\r\n    --passages all_abstract_chunks.tsv \\\r\n    --passages_embeddings passages_00_abstract_ms \\\r\n    --query question_analysis_output.json  \\\r\n    --output_dir YOUR_OUTPUT_FILE \\\r\n    --n_docs 10\r\n```\r\n\r\nPerform keyword matching based on the text.\r\n```\r\ncd IP-RAR/Integrated_Reasoning-based_Retrieval\r\npython keyword_matching.py\r\n```\r\n\r\n#### Aggregator\r\n```\r\ncd IP-RAR/Integrated_Reasoning-based_Retrieval\r\npython Aggregator.py\r\n```\r\n\r\n### Progressive Reasoning-based Generation\r\nUse the LLM to perform multi-stage validation on the results obtained from the Aggregator and generate the final answer.\r\n```\r\ncd IP-RAR/Progressive_Reasoning-based_Generation\r\npython Generation.py\r\n```\r\n\r\n\r\n### Evaluation\r\n#### Document Retrieval Performance Evaluation\r\nWe employ Mean Precision, Mean Recall, and Mean F-measure to assess the proportion of relevant documents retrieved and the balance between precision and recall. The implementation can be found in Evaluation/Document_Retrieval.py.\r\n#### Answer Accuracy Evaluation\r\nWe utilize GPT-4-based evaluators to score answers on a five-point scale, ensuring semantic accuracy, precision, and relevance, particularly for summary-type questions. The implementation can be found in Evaluation/Answer_Accuracy.py.\r\n\r\n## Knowledge Graph\r\nThe knowledge graph can serve as a valuable resource for researchers, facilitating knowledge discovery, identifying trends, and exploring relationships between biomedical entities and research methods. The knowledge graph comprises 94,962 nodes and 290,403 relationships. You can download the [knowledge graph](https://drive.google.com/file/d/1x5alzBbdigoBI9j2ZX64cLU_Uad_xqfl/view?usp=sharing).\r\n\r\n## License\r\n\r\n### Code\r\nThe code in this repository is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.\r\n\r\n### Data\r\nThe data in this repository is licensed under the Creative Commons Public Domain Dedication (CC0 1.0 Universal). See the [DATA_LICENSE](DATA_LICENSE) file for details.\r\n\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1743",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1743?version=1",
        "name": "BioCDQA",
        "number_of_steps": 0,
        "projects": [
            "Biomedical_LLM"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Unrecognized workflow type",
        "update_time": "2025-06-20",
        "versions": 1
    },
    {
        "create_time": "2025-06-20",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-createtaxdb_logo_dark_tax.png\">\n    <img alt=\"nf-core/createtaxdb\" src=\"docs/images/nf-core-createtaxdb_logo_light_tax.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/createtaxdb/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/createtaxdb/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/createtaxdb/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/createtaxdb/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/createtaxdb/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.04.2-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.1-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.1)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/createtaxdb)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23createtaxdb-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/createtaxdb)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/createtaxdb** is a bioinformatics pipeline that constructs custom metagenomic classifier databases for multiple classifiers and profilers from the same input reference genome set in a highly automated and parallelised manner.\nIt supports both nucleotide and protein based classifiers and profilers.\nThe pipeline is designed to be a companion pipeline to [nf-core/taxprofiler](https://nf-co.re/taxprofiler/) for taxonomic profiling of metagenomic data, but can be used for any context.\n\n<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"assets/createtaxdb-metromap-diagram-dark.png\">\n    <img alt=\"nf-core/createtaxdb\" src=\"assets/createtaxdb-metromap-diagram-light.png\">\n  </picture>\n</h1>\n\n1. Prepares input FASTA files for building\n2. Builds databases for:\n   - [Bracken](https://doi.org/10.7717/peerj-cs.104)\n   - [Centrifuge](https://doi.org/10.1101/gr.210641.116)\n   - [DIAMOND](https://doi.org/10.1038/nmeth.3176)\n   - [ganon](https://doi.org/10.1093/bioinformatics/btaa458)\n   - [Kaiju](https://doi.org/10.1038/ncomms11257)\n   - [KMCP](https://doi.org/10.1093/bioinformatics/btac845)\n   - [Kraken2](https://doi.org/10.1186/s13059-019-1891-0)\n   - [KrakenUniq](https://doi.org/10.1186/s13059-018-1568-0)\n   - [MALT](https://doi.org/10.1038/s41559-017-0446-6)\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare an input CSV table with your input reference genomes that looks as follows:\n\n```csv\nid,taxid,fasta_dna,fasta_aa\nHuman_Mitochondrial_genome,9606,chrMT.fna,\nSARS-CoV-2_genome,694009,GCA_011545545.1_ASM1154554v1_genomic.fna.gz,GCA_011545545.1_ASM1154554v1_genomic.faa.gz\nBacteroides_fragilis_genome,817,GCF_016889925.1_ASM1688992v1_genomic.fna.gz,GCF_016889925.1_ASM1688992v1_genomic.faa.gz\nCandidatus_portiera_aleyrodidarum_genome,91844,GCF_000292685.1_ASM29268v1_genomic.fna,GCF_000292685.1_ASM29268v1_genomic.faa\nHaemophilus_influenzae_genome,727,GCF_900478275.1_34211_D02_genomic.fna,GCF_900478275.1_34211_D02_genomic.faa\nStreptococcus_agalactiae_genome,1311,,GCF_002881355.1_ASM288135v1_genomic.faa\n```\n\nEach row contains a human readable name, the taxonomic ID of the organism, and then an (optionally gzipped) Nucleotide and/or Amino Acid FASTA file.\n\nNow, with an appropriate set of taxonomy files you can build databases for multiple profilers - such as Kraken2, ganon, and DIAMOND - in parallel:\n\n```bash\nnextflow run nf-core/createtaxdb \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --accession2taxid /<path>/<to>/taxonomy/nucl_gb.accession2taxid \\\n   --nucl2taxid /<path>/<to>/taxonomy/nucl.accession2taxid.gz \\\n   --prot2taxid /<path>/<to>/taxonomy/prot.accession2taxid.gz \\\n   --nodesdmp /<path>/<to>/taxonomy/nodes.dmp \\\n   --namesdmp /<path>/<to>/taxonomy/names.dmp \\\n   --build_kraken2 \\\n   --kraken2_build_options='--kmer-len 45' \\\n   --build_ganon \\\n   --ganon_build_options='--kmer-size 45' \\\n   --build_diamond \\\n   --diamond_build_options='--no-parse-seqids' \\\n   --outdir <OUTDIR>\n```\n\nThe output directory will contain directories containing the database files for each of the profilers you selected to build.\nOptionally you can also package these as `tar.gz` archives.\n\nYou can also generate pre-prepared input sheets for database specifications of pipelines such as [nf-core/taxprofiler](https://nf-co.re/taxprofiler) using `--generate_downstream_samplesheets`.\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/createtaxdb/usage) and the [parameter documentation](https://nf-co.re/createtaxdb/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/createtaxdb/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/createtaxdb/output).\n\n## Credits\n\nnf-core/createtaxdb was originally written by James A. Fellows Yates, Sam Wilkinson, Alexander Ramos D\u00edaz, Lili Andersson-Li and the nf-core community.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- Zandra Fagern\u00e4s for logo design\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#createtaxdb` channel](https://nfcore.slack.com/channels/createtaxdb) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/createtaxdb for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1742",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1742?version=1",
        "name": "nf-core/createtaxdb",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "profiling",
            "database",
            "database-builder",
            "metagenomic-profiling",
            "taxonomic-profiling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-20",
        "versions": 1
    },
    {
        "create_time": "2025-06-19",
        "creators": [
            "Fabrice Touzain"
        ],
        "description": "Using:\r\n- vadr annotation (**virus model must be selected** in options)\r\n- vardict variant caller\r\n- coverage depth\r\nProvides summarizing files:\r\n- **png** image of **variant calling** with **annotations** and **coverage depths**\r\n- **tsv** file with **all information of significant variants only**",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1741",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1741?version=1",
        "name": "vvv2_align_SE_minimap2_pacbio",
        "number_of_steps": 12,
        "projects": [
            "ANSES-Ploufragan"
        ],
        "source": "WorkflowHub",
        "tags": [
            "virus",
            "bbmap",
            "display",
            "pacbio",
            "vadr",
            "vardict",
            "variant_calling",
            "vvv2_display"
        ],
        "tools": [
            "fastp",
            "samtools_rmdup",
            "vvv2_display",
            "cshl_fasta_formatter",
            "minimap2",
            "bcftools_call",
            "samtools_depth",
            "vadr",
            "vardict_java",
            "bcftools_consensus",
            "bcftools_mpileup"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-19",
        "versions": 1
    },
    {
        "create_time": "2023-06-27",
        "creators": [
            "Fabrice Touzain"
        ],
        "description": "SINGLE-END workflow. \r\nAlign reads on fasta reference/assembly using bwa mem, get a consensus, variants, mutation explanations. \r\n\r\nIMPORTANT: \r\n* For \"bcftools call\" consensus step, the --ploidy file is in \"Donn\u00e9es partag\u00e9es\" (Shared Data) and must be imported in your history to use the worflow by providing this file (tells bcftools to consider haploid variant calling). \r\n* SELECT the mot ADAPTED VADR MODEL for annotation (see vadr parameters).",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "517",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/517?version=1",
        "name": "VVV2_align_SE",
        "number_of_steps": 10,
        "projects": [
            "ANSES-Ploufragan"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "bioinformatics",
            "galaxy",
            "snps",
            "virus",
            "high-throughput_sequencing_analysis",
            "single-end",
            "variant",
            "variant calling",
            "variant_calling"
        ],
        "tools": [
            "fastp",
            "vcfutils_vcf2fq",
            "vvv2_display",
            "cshl_fasta_formatter",
            "bcftools_call",
            "vadr",
            "vardict_java",
            "seqtk_seq",
            "bwa_mem",
            "bcftools_mpileup"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-19",
        "versions": 1
    },
    {
        "create_time": "2023-06-28",
        "creators": [
            "Fabrice Touzain"
        ],
        "description": "PAIRED-END workflow. Align reads on fasta reference/assembly using bwa mem, get a consensus, variants, mutation explanations.\r\n\r\nIMPORTANT: \r\n* For \"bcftools call\" consensus step, the --ploidy file is in \"Donn\u00e9es partag\u00e9es\" (Shared Data) and must be imported in your history to use the worflow by providing this file (tells bcftools to consider haploid variant calling). \r\n* SELECT THE MOST ADAPTED VADR MODEL for annotation (see vadr parameters).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics"
        ],
        "filtered_on": "annot* in tags",
        "id": "518",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/518?version=1",
        "name": "VVV2_align_PE",
        "number_of_steps": 10,
        "projects": [
            "ANSES-Ploufragan"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "annotation",
            "bioinformatics",
            "galaxy",
            "snps",
            "virus",
            "covid-19",
            "paired-end",
            "variant calling",
            "variant_calling",
            "workflow"
        ],
        "tools": [
            "fastp",
            "vcfutils_vcf2fq",
            "vvv2_display",
            "cshl_fasta_formatter",
            "bcftools_call",
            "vadr",
            "vardict_java",
            "seqtk_seq",
            "bwa_mem",
            "bcftools_mpileup"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-19",
        "versions": 1
    },
    {
        "create_time": "2025-06-19",
        "creators": [
            "Fabrice Touzain"
        ],
        "description": "Using:\r\n- vadr annotation (**virus model must be selected** in options)\r\n- vardict variant caller\r\n- coverage depth\r\nProvides summarizing files:\r\n- **png** image of **variant calling** with **annotations** and **coverage depths**\r\n- **tsv** file with **all information of significant variants only**",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1740",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1740?version=1",
        "name": "vvv2_align_SE_bwamem2_nanopore",
        "number_of_steps": 12,
        "projects": [
            "ANSES-Ploufragan"
        ],
        "source": "WorkflowHub",
        "tags": [
            "virus",
            "bbmap",
            "display",
            "nanopore",
            "vadr",
            "vardict",
            "variant_calling",
            "vvv2_display"
        ],
        "tools": [
            "fastp",
            "samtools_rmdup",
            "vvv2_display",
            "cshl_fasta_formatter",
            "bcftools_call",
            "samtools_depth",
            "vadr",
            "vardict_java",
            "bcftools_consensus",
            "bcftools_mpileup",
            "bwa_mem2"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-19",
        "versions": 1
    },
    {
        "create_time": "2025-06-19",
        "creators": [
            "Fabrice Touzain"
        ],
        "description": "Using:\r\n- vadr annotation (**virus model must be selected** in options)\r\n- vardict variant caller\r\n- coverage depth\r\nProvides summarizing files:\r\n- **png** image of **variant calling** with **annotations** and **coverage depths**\r\n- **tsv** file with all **information of significant variants only** (can be opened in Excel/LibreOffice)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1738",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1738?version=1",
        "name": "vvv2_align_PE_bwamem2",
        "number_of_steps": 12,
        "projects": [
            "ANSES-Ploufragan"
        ],
        "source": "WorkflowHub",
        "tags": [
            "virus",
            "bbmap",
            "display",
            "vadr",
            "vardict",
            "variant_calling",
            "vvv2_display"
        ],
        "tools": [
            "fastp",
            "samtools_rmdup",
            "vvv2_display",
            "cshl_fasta_formatter",
            "bcftools_call",
            "samtools_depth",
            "vadr",
            "vardict_java",
            "bcftools_consensus",
            "bcftools_mpileup",
            "bwa_mem2"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-19",
        "versions": 1
    },
    {
        "create_time": "2025-06-19",
        "creators": [
            "Fabrice Touzain"
        ],
        "description": "Using:\r\n- vadr annotation (**virus model must be selected** in options)\r\n- vardict variant caller\r\n- coverage depth\r\nProvides summarizing files:\r\n- **png** image of **variant calling** with **annotations** and **coverage depths**\r\n- **tsv** file with **all information of significant variants only** (can be opened in Excel/LibreOffice)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1739",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1739?version=1",
        "name": "vvv2_align_SE_bwamem2",
        "number_of_steps": 12,
        "projects": [
            "ANSES-Ploufragan"
        ],
        "source": "WorkflowHub",
        "tags": [
            "virus",
            "bbmap",
            "display",
            "vadr",
            "vardict",
            "variant_calling",
            "vvv2_display"
        ],
        "tools": [
            "fastp",
            "samtools_rmdup",
            "vvv2_display",
            "cshl_fasta_formatter",
            "bcftools_call",
            "samtools_depth",
            "vadr",
            "vardict_java",
            "bcftools_consensus",
            "bcftools_mpileup",
            "bwa_mem2"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-19",
        "versions": 1
    },
    {
        "create_time": "2025-06-17",
        "creators": [
            "Alejandra Escobar"
        ],
        "description": "# Introduction\r\n\r\n**ebi-metagenomics/biosiftr** is a bioinformatics pipeline that generates taxonomic and functional profiles for low-yield (shallow shotgun: < 10 M reads) short raw-reads using [`MGnify biome-specific genome catalogues`](https://www.ebi.ac.uk/metagenomics/browse/genomes) as a reference.\r\n\r\nThe biome selection includes all the biomes available in the [`MGnify genome catalogues`](https://www.ebi.ac.uk/metagenomics/browse/genomes).\r\n\r\nThe main sections of the pipeline include the following steps:\r\n\r\n1. Raw-reads quality control ([`fastp`](https://github.com/OpenGene/fastp))\r\n2. HQ reads decontamination versus human, phyX, and host ([`bwa-mem2`](https://github.com/bwa-mem2/bwa-mem2))\r\n3. QC report of decontaminated reads ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\r\n4. Integrated quality report of reads before and after decontamination ([`MultiQC`](http://multiqc.info/))\r\n5. Mapping HQ clean reads using [`Sourmash`](https://github.com/sourmash-bio/sourmash) and bwa-mem2 (optional)\r\n6. Taxonomic profile generation\r\n7. Functional profile inference\r\n\r\nThe final output includes a species relative abundance table, Pfam and KEGG Orthologs (KO) count tables, a KEGG modules completeness table, and DRAM-style visuals (optional). In addition, the shallow-mapping pipeline will integrate the taxonomic and functional tables of all the samples in the input samplesheet.\r\n\r\n## Installation\r\n\r\nThis workflow was built using [Nextflow](https://www.nextflow.io/) and follows [nf-core](https://nf-co.re/) good practices. It is containerised, so users can use either Docker or Apptainer/Singularity to run the pipeline. At the moment, it doesn't support Conda environments.\r\n\r\nThe pipeline requires [Nextflow](https://www.nextflow.io/docs/latest/getstarted.html#installation) and a container technology such as [Apptainer/Singularity](https://github.com/apptainer/singularity/blob/master/INSTALL.md) or [Docker](https://www.docker.com/).\r\n\r\n### Required Reference Databases\r\n\r\nThe first time you run the pipeline, it will download the required MGnify genomes catalogue reference files and the human_phiX BWAMEM2 index. If you select a different host for decontamination, you must provide the index yourself.\r\n\r\nRunning the pipeline using bwamem2 is optional. If you want to run the pipeline with this option set the `--download_bwa true`. This database will occupy considerable storage in your system depending on the biome.\r\n\r\nIn addition, instructions to generate the databases from custom catalogues can be found in the [BioSIFTR paper's repository](https://github.com/EBI-Metagenomics/biosiftr_extended_methods?tab=readme-ov-file#31-processing-custom-genome-catalogues).\r\n\r\n### Usage\r\n\r\nPrepare a samplesheet with your input data that looks as follows:\r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\nsample,fastq_1,fastq_2\r\npaired_sample,/PATH/test_R1.fq.gz,/PATH/test_R2.fq.gz\r\nsingle_sample,/PATH/test.fq.gz\r\n```\r\n\r\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end) where 'sample' is a unique identifier for each dataset, 'fastq_1' is the path to the first FASTQ file, and 'fastq_2' is the path to the second FASTQ file for paired-end data.\r\n\r\nNow, you can run the pipeline using the minimum of arguments:\r\n\r\n```bash\r\nnextflow run ebi-metagenomics/biosiftr \\\r\n   --biome <CATALOGUE_ID> \\\r\n   --input samplesheet.csv \\\r\n   --outdir <PROJECT_NAME> default = `results` \\\r\n   --dbs </path/to/dbs> \\\r\n   --decontamination_indexes </path to folder with bwamem2 indexes>\r\n```\r\n\r\nThe central location for the databases can be set in the config file.\r\n\r\nOptional arguments include:\r\n\r\n```bash\r\n--run_bwa <boolean> default = `false`   # To generate results using bwamem2 besides sourmash\r\n--core_mode <boolean> default = `false` # To use core functions instead of pangenome functions\r\n--run_dram <boolean> default = `false`  # To generate DRAM results\r\n```\r\n\r\nUse `--core_mode true` for large catalogues like the human-gut to avoid over-prediction due to a large number of accessory genes in the pangenome.\r\nNextflow option `-profile` can be used to select a suitable config for your computational resources. You can add profile files to the `config` directory.\r\nNextflow option `-resume` can be used to re-run the pipeline from the last successfully finished step.\r\n\r\n\r\n#### Available biomes\r\n\r\nThis can be any of the MGnify catalogues for which shallow-mapping databases are currently available\r\n\r\n| Biome              | Catalogue Version                                                                    |\r\n| ------------------ | ------------------------------------------------------------------------------------ |\r\n| chicken-gut        | [v1.0.1](https://www.ebi.ac.uk/metagenomics/genome-catalogues/chicken-gut-v1-0-1)    |\r\n| cow-rumen          | [v1.0.1](https://www.ebi.ac.uk/metagenomics/genome-catalogues/cow-rumen-v1-0-1)      |\r\n| human-gut          | [v2.0.2 \u26a0\ufe0f](https://www.ebi.ac.uk/metagenomics/genome-catalogues/human-gut-v2-0-2)   |\r\n| human-oral         | [v1.0.1](https://www.ebi.ac.uk/metagenomics/genome-catalogues/human-oral-v1-0-1)     |\r\n| human-vaginal      | [v1.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/human-vaginal-v1-0)      |\r\n| honeybee-gut       | [v1.0.1](https://www.ebi.ac.uk/metagenomics/genome-catalogues/honeybee-gut-v1-0-1)   |\r\n| marine             | [v2.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/marine-v2-0)             |\r\n| mouse-gut          | [v1.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/mouse-gut-v1-0)          |\r\n| non-model-fish-gut | [v2.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/non-model-fish-gut-v2-0) |\r\n| pig-gut            | [v1.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/pig-gut-v1-0)            |\r\n| sheep-rumen        | [v1.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/sheep-rumen-v1-0)        |\r\n| zebrafish-fecal    | [v1.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/zebrafish-fecal-v1-0)    |\r\n\r\n> **\u26a0\ufe0f Note for human-gut**:\r\n>\r\n> The human-gut shallow-mapping database was created manually by re-running Panaroo to reconstruct the pangenomes. This is likely to have caused discrepancies in the pangenomes, so please bear that in mind.\r\n\r\n## Test\r\n\r\nTo test the installed tool with your downloaded databases, you can run the pipeline using the small test dataset. Even if there are no hits with the biome you are interested in, the pipeline should finish successfully. Add `-profile` if you have set up a config profile for your compute resources.\r\n\r\n```bash\r\ncd biosiftr/tests\r\nnextflow run ../main.nf \\\r\n    --input test_samplesheet.csv \\\r\n    --biome <CATALOGUE_ID> \\\r\n    --dbs </path/to/dbs> \\\r\n    --decontamination_indexes </path to folder with bwamem2 indexes>\r\n```\r\n\r\n## Credits\r\n\r\nebi-metagenomics/biosiftr pipeline was originally written by @Ales-ibt.\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n@mberacochea\r\n",
        "doi": "10.48546/workflowhub.workflow.1735.1",
        "edam_operation": [],
        "edam_topic": [
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "1735",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1735?version=1",
        "name": "BioSIFTR",
        "number_of_steps": 0,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-17",
        "versions": 1
    },
    {
        "create_time": "2025-06-17",
        "creators": [
            "Alejandra Escobar",
            "Martin Beracochea"
        ],
        "description": "# Mobilome Annotation Pipeline (former MoMofy)\r\n\r\nBacteria can acquire genetic material through horizontal gene transfer, allowing them to rapidly adapt to changing environmental conditions. These mobile genetic elements can be classified into three main categories: plasmids, phages, and integrative elements. Plasmids are mostly extrachromosmal; phages can be found extrachromosmal or as temperate phages (prophages); whereas integrons are stable inserted in the chromosome. Autonomous elements are those integrative elements capable of excising themselves from the chromosome and reintegrate elsewhere. They can use a transposase (like insertion sequences and transposons) or an integrase/excisionase (like ICEs and IMEs).\r\n\r\nThe Mobilome Annotation Pipeline is a wrapper that integrates the output of different tools designed for the prediction of plasmids, phages, insertion sequences, and other autonomous integrative mobile genetic elements such as ICEs, IMEs and integrons in prokaryotic genomes and metagenomes. The output is a PROKKA gff file with extra entries for the mobilome.\r\n\r\n## Contents\r\n\r\n- [ Workflow ](#wf)\r\n- [ Setup ](#sp)\r\n- [ Install and dependencies ](#install)\r\n- [ Usage ](#usage)\r\n- [ Inputs ](#in)\r\n- [ Outputs ](#out)\r\n- [ Tests ](#test)\r\n- [ Citation ](#cite)\r\n\r\n<a name=\"wf\"></a>\r\n\r\n## Workflow\r\n\r\nThis workflow has the following main subworkflows:\r\n\r\n- Preprocessing: Rename and filter contigs, and run PROKKA annotation\r\n- Prediction: Run geNomad, ICEfinder, IntegronFinder, and ISEScan\r\n- Annotation: Generate extra-annotation for antimicrobial resistance genes (AMRFinderPlus) and other mobilome-related proteins (MobileOG).\r\n- Integration: Parse and integrate the outputs generated on `Prediction` and `Annotation` subworkflows. In this step optional results of VIRify v3.0.0 can be incorporated. MGEs <500 bp lengh and predictions with no genes are discarded.\r\n- Postprocessing: Write the mobilome fasta file, write a report of the location of AMR genes (either mobilome or chromosome), and generate three new GFF files:\r\n\r\n1. `mobilome_clean.gff`: mobilome + associated CDSs\r\n2. `mobilome_extra.gff`: mobilome + ViPhOGs/mobileOG annotated genes (note that ViPhOG annotation is generated by VIRify)\r\n3. `mobilome_nogenes.gff`: mobilome only\r\n   The output `mobilome_nogenes.gff` is validated in this subworkflow.\r\n\r\n<a name=\"sp\"></a>\r\n\r\n## Setup\r\n\r\nThis workflow is built using [Nextflow](https://www.nextflow.io/). It uses Singularity containers making installation trivial and results highly reproducible.\r\nExplained in this section, there is one manual step required to build the singularity image for [ICEfinder](https://bioinfo-mml.sjtu.edu.cn/ICEfinder/index.php), as we can't distribute that software due to license issues.\r\n\r\n- Install [Nextflow version >=21.10](https://www.nextflow.io/docs/latest/getstarted.html#installation)\r\n- Install [Singularity](https://github.com/apptainer/singularity/blob/master/INSTALL.md)\r\n\r\n<a name=\"install\"></a>\r\n\r\n## Install and dependencies\r\n\r\nTo get a copy of the Mobilome Annotation Pipeline, clone this repo by:\r\n\r\n```bash\r\n$ git clone https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline.git\r\n```\r\n\r\nThe mobileOG-database is required to run an extra step of annotation on the mobilome coding sequences. The first time you run the Mobilome Annotation Pipeline, you will need to download the [Beatrix 1.6 v1](https://mobileogdb.flsi.cloud.vt.edu/entries/database_download) database, move the tarball to `mobilome-annotation-pipeline/databases`, decompress it, and run the script to format the db for diamond:\r\n\r\n```bash\r\n$ mv beatrix-1-6_v1_all.zip /PATH/mobilome-annotation-pipeline/databases\r\n$ cd /PATH/mobilome-annotation-pipeline/databases\r\n$ unzip beatrix-1-6_v1_all.zip\r\n$ nextflow run /PATH/mobilome-annotation-pipeline/format_mobileOG.nf\r\n```\r\n\r\nTwo additional databases need to be manually downloaded and extracted: [AMRFinder plus db](https://ftp.ncbi.nlm.nih.gov/pathogen/Antimicrobial_resistance/AMRFinderPlus/database/latest) and the [geNomad database](https://zenodo.org/records/8339387) databases. Then you can provide the paths to your databases using the `mobileog_db`, the `amrfinder_plus_db` and the `genomad_db` respectively when you run the pipeline.\r\n\r\nMost of the tools are available on [quay.io](https://quay.io) and no install is needed. However, in the case of ICEfinder, you will need to contact the author to get a copy of the software, visit the [ICEfinder website](https://bioinfo-mml.sjtu.edu.cn/ICEfinder/download.html) for more information. Once you have the `ICEfinder_linux.tar.gz` tarball, move it to `mobilome-annotation-pipeline/templates` and build the singularity image using the following command:\r\n\r\n```bash\r\n$ mv ICEfinder_linux.tar.gz /PATH/mobilome-annotation-pipeline/templates/\r\n$ cd /PATH/mobilome-annotation-pipeline/templates/\r\n$ sudo singularity build ../../singularity/icefinder-v1.0-local.sif icefinder-v1.0-local.def\r\n```\r\n\r\nThe path to the ICEfinder image needs to be provided when running the pipeline, unless a custom config file is created.\r\n\r\n\r\n<a name=\"usage\"></a>\r\n\r\n\r\n## Inputs\r\n\r\nTo run the Mobilome Annotation Pipeline on multiple samples, prepare a samplesheet with your input data that looks as in the following example. Note that `virify_gff` is an optional input for this pipeline generated with [VIRify](https://github.com/EBI-Metagenomics/emg-viral-pipeline) v3.0.0 tool. \r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\nsample,assembly,user_proteins_gff,virify_gff\r\nminimal,/PATH/assembly.fasta,,\r\nassembly_proteins,/PATH/assembly.fasta,/PATH/proteins.gff,\r\nassembly_proteins_virify,/PATH/assembly.fasta,/PATH/proteins.gff,/PATH/virify_out.gff\r\n```\r\n\r\nEach row represents a sample. The minimal input is the (meta)genome assembly in fasta format.\r\n\r\nBasic run:\r\n\r\n```bash\r\n$ nextflow run /PATH/mobilome-annotation-pipeline/main.nf --input samplesheet.csv [--icefinder_sif icefinder-v1.0-local.sif]\r\n```\r\n\r\nNote that the final output in gff format is created by adding information to PROKKA output. If you have your own protein prediction files, provide the path the the uncompressed gff file in the samplesheet.csv. This file will be used to generate a `user_mobilome_extra.gff` file containing the mobilome plus any extra annotation generated on the annotation subworkflow.\r\n\r\nIf you want to integrate VIRify results to the final output provide the path to the GFF file generated by VIRify v3.0.0 in your samplesheet.csv.\r\n\r\n\r\n<a name=\"out\"></a>\r\n\r\n## Outputs\r\n\r\nResults will be written by default in the `mobilome_results` directory unless the `--outdir` option is used. There, you will find the following outputs:\r\n\r\n```bash\r\nmobilome_results/\r\n\u251c\u2500\u2500 mobilome.fasta\r\n\u251c\u2500\u2500 mobilome_prokka.gff\r\n\u251c\u2500\u2500 overlapping_integrons.txt\r\n\u251c\u2500\u2500 discarded_mge.txt\r\n\u251c\u2500\u2500 func_annot/\r\n\u251c\u2500\u2500 gff_output_files/\r\n\u251c\u2500\u2500 prediction/\r\n\u2514\u2500\u2500 preprocessing\r\n```\r\n\r\nThe AMRFinderPlus results are generated by default. The `func_annot/amr_location.txt` file contains a summary of the AMR genes annotated and their location (either mobilome or chromosome).\r\n\r\nThe file `discarded_mge.txt` contains a list of predictions that were discarded, along with the reason for their exclusion. Possible reasons include:\r\n\r\n1. 'mge < 500bp' Discarded by length.\r\n2. 'no_cds' If there are no genes encoded in the prediction.\r\n\r\nThe file `overlapping_integrons.txt` is a report of long-MGEs with overlapping coordinates. No predictions are discarded in this case.\r\n\r\nThe main output files containing the mobilome predictions are `mobilome.fasta` containing the nucleotide sequences of every prediction, and `mobilome_prokka.gff` containing the mobilome annotation plus any other feature annotated by PROKKA, mobileOG, or ViPhOG (only when VIRify results are provided).\r\n\r\nThe mobilome prediction IDs are build as follows:\r\n\r\n1. Contig ID\r\n2. MGE type:\r\n   flanking_site\r\n   recombination_site\r\n   prophage\r\n   viral_sequence\r\n   plasmid\r\n   phage_plasmid\r\n   integron\r\n   conjugative_integron\r\n   insertion_sequence\r\n3. Start and end coordinates separated by ':'\r\n\r\nExample:\r\n\r\n```bash\r\n>contig_id|mge_type-start:end\r\n```\r\n\r\nAny CDS with a coverage >= 0.9 in the boundaries of a predicted MGE is considered as part of the mobilome and labelled acordingly in the attributes field under the key `location`.\r\n\r\nThe labels used in the Type column of the gff file corresponds to the following nomenclature according to the [Sequence Ontology resource](http://www.sequenceontology.org/browser/current_svn/term/SO:0000001) when possible:\r\n\r\n| Type in gff file                 | Sequence ontology ID                                                              | Element description                                         | Reporting tool            |\r\n| -------------------------------- | --------------------------------------------------------------------------------- | ----------------------------------------------------------- | ------------------------- |\r\n| insertion_sequence               | [SO:0000973](http://www.sequenceontology.org/browser/current_svn/term/SO:0000973) | Insertion sequence                                          | ISEScan, PaliDIS          |\r\n| terminal_inverted_repeat_element | [SO:0000481](http://www.sequenceontology.org/browser/current_svn/term/SO:0000481) | Terminal Inverted Repeat (TIR) flanking insertion sequences | ISEScan, PaliDIS          |\r\n| integron                         | [SO:0000365](http://www.sequenceontology.org/browser/current_svn/term/SO:0000365) | Integrative mobilizable element                             | IntegronFinder, ICEfinder |\r\n| attC_site                        | [SO:0000950](http://www.sequenceontology.org/browser/current_svn/term/SO:0000950) | Integration site of DNA integron                            | IntegronFinder            |\r\n| conjugative_integron             | [SO:0000371](http://www.sequenceontology.org/browser/current_svn/term/SO:0000371) | Integrative Conjugative Element                             | ICEfinder                 |\r\n| direct_repeat                    | [SO:0000314](http://www.sequenceontology.org/browser/current_svn/term/SO:0000314) | Flanking regions on mobilizable elements                    | ICEfinder                 |\r\n| prophage                         | [SO:0001006](http://www.sequenceontology.org/browser/current_svn/term/SO:0001006) | Temperate phage                                             | geNomad, VIRify           |\r\n| viral_sequence                   | [SO:0001041](http://www.sequenceontology.org/browser/current_svn/term/SO:0001041) | Viral genome fragment                                       | geNomad, VIRify           |\r\n| plasmid                          | [SO:0000155](http://www.sequenceontology.org/browser/current_svn/term/SO:0000155) | Plasmid                                                     | geNomad                   |\r\n\r\n<a name=\"test\"></a>\r\n\r\n## Tests\r\n\r\nNextflow tests are executed with [nf-test](https://github.com/askimed/nf-test). It takes around 3 min in executing.\r\n\r\nRun:\r\n\r\n```bash\r\n$ cd mobilome-annotation-pipeline/\r\n$ nf-test test\r\n```\r\n\r\n<a name=\"cite\"></a>\r\n\r\n## Citation\r\n\r\nThe Mobilome Annotation Pipeline parses and integrates the output of the following tools and DBs sorted alphabetically:\r\n\r\n- AMRFinderPlus v3.11.4 with database v2023-02-23.1 [Feldgarden et al., Sci Rep, 2021](https://doi.org/10.1038/s41598-021-91456-0)\r\n- Diamond v2.0.12 [Buchfink et al., Nature Methods, 2021](https://doi.org/10.1038/s41592-021-01101-x)\r\n- geNomad v1.6.1 [Camargo et al., Nature Biotechnology, 2023](https://doi.org/10.1038/s41587-023-01953-y)\r\n- ICEfinder v1.0 [Liu et al., Nucleic Acids Res, 2019](https://doi.org/10.1093/nar/gky1123)\r\n- IntegronFinder2 v2.0.2 [N\u00e9ron et al., Microorganisms, 2022](https://doi.org/10.3390/microorganisms10040700)\r\n- ISEScan v1.7.2.3 [Xie et al., Bioinformatics, 2017](https://doi.org/10.1093/bioinformatics/btx433)\r\n- MobileOG-DB Beatrix 1.6 v1 [Brown et al., Appl Environ Microbiol, 2022](https://doi.org/10.1128/aem.00991-22)\r\n- PROKKA v1.14.6 [Seemann, Bioinformatics, 2014](https://doi.org/10.1093/bioinformatics/btu153)\r\n- VIRify v3.0.0 [Rangel-Pineros et al., PLoS Comput Biol, 2023](https://doi.org/10.1371/journal.pcbi.1011422)\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Metagenomics",
            "Mobile genetic elements"
        ],
        "filtered_on": "edam",
        "id": "452",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/452?version=2",
        "name": "Mobilome Annotation Pipeline",
        "number_of_steps": 0,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genomics",
            "mge",
            "metagenomics",
            "mobilome",
            "nextflow"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-17",
        "versions": 2
    },
    {
        "create_time": "2025-06-16",
        "creators": [],
        "description": "Assembly of metagenomic sequencing data\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Assembly of metagenomic sequencing data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metagenomics-assembly/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Polina Polunina, B\u00e9r\u00e9nice Batut\n\n**Tutorial Author(s)**: [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Tutorial Contributor(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1390",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1390?version=2",
        "name": "Assembly of metagenomic sequencing data",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "bandage_info",
            "megahit",
            "megahit_contig2fastg",
            "collection_column_join",
            "bandage_image",
            "bowtie2",
            "metaspades",
            "coverm_contig",
            "quast"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-16",
        "versions": 2
    },
    {
        "create_time": "2025-06-14",
        "creators": [
            "Wenlong Ma",
            "Weigang Zheng",
            "Shenghua Qin",
            "Chao Wang",
            "Bowen Lei",
            "Yuwen Liu"
        ],
        "description": "DeepAnnotation can be used to perform genomic selection (GS), which is a promising breeding strategy for agricultural breeding. DeepAnnotation predicts phenotypes from comprehensive multi-omics functional annotations with interpretable deep learning framework. The effectiveness of DeepAnnotation has been demonstrated in predicting three pork production traits (lean meat percentage at 100 kg [LMP], loin muscle depth at 100 kg [LMD], back fat thickness at 100 kg [BF]) on a population of 1940 Duroc boars with 11633164 SNPs. ",
        "doi": "10.48546/workflowhub.workflow.1732.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "1732",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1732?version=1",
        "name": "DeepAnnotation",
        "number_of_steps": 0,
        "projects": [
            "Artificial Design for Intelligent Breeding"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2025-06-15",
        "versions": 1
    },
    {
        "create_time": "2025-06-13",
        "creators": [
            "Jorge Mas-G\u00f3mez",
            "Pedro Mart\u00ednez-Garc\u00eda"
        ],
        "description": "High-throughput phenotyping is addressing the current bottleneck in phenotyping within breeding programs. Imaging tools are becoming the primary resource for improving the efficiency of phenotyping processes and providing large datasets for genomic selection approaches. The advent of AI brings new advantages by enhancing phenotyping methods using imaging, making them more accessible to breeding programs. In this context, we have developed an open Python workflow for analyzing morphology, colour and morphometric traits using AI, which can be applied to fruits and other plant organs. \r\nhttps://www.biorxiv.org/content/10.1101/2025.05.05.652179v1 ",
        "doi": "10.48546/workflowhub.workflow.1731.1",
        "edam_operation": [],
        "edam_topic": [
            "Genotype and phenotype",
            "Imaging"
        ],
        "filtered_on": "ITS in description",
        "id": "1731",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1731?version=1",
        "name": "Open RGB Imaging Workflow for Morphological and Morphometric Analysis",
        "number_of_steps": 0,
        "projects": [
            "AlmondBreedingLab"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2025-06-13",
        "versions": 1
    },
    {
        "create_time": "2025-06-12",
        "creators": [],
        "description": "<div align=\"center\">\r\n\r\n[<img src=\"https://raw.githubusercontent.com/sanjaynagi/AmpSeeker/main/docs/ampseeker-docs/logo.png\" width=\"400\"/>](https://raw.githubusercontent.com/sanjaynagi/AmpSeeker/main/docs/ampseeker-docs/logo.png)   \r\n\r\n\r\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22658.0.0-brightgreen.svg)](https://snakemake.bitbucket.io)\r\n[![GitHub release](https://img.shields.io/github/release/sanjaynagi/AmpSeeker?include_prereleases=&sort=semver&color=blue)](https://github.com/sanjaynagi/AmpSeeker/releases/)\r\n[![License](https://img.shields.io/badge/License-MIT-blue)](#license)\r\n\r\n</div>\r\n\r\n**Documentation**: https://sanjaynagi.github.io/AmpSeeker/ \r\n\r\nAmpSeeker is a snakemake workflow for Amplicon Sequencing data analysis. The pipeline is generic and can work on any data, but is tailored towards insecticide resistance monitoring. It implements:\r\n\r\n- BCL to Fastq conversion\r\n- Genome alignment\r\n- Variant calling\r\n- Quality control\r\n- Coverage\r\n- Visualisation of reads in IGV\r\n- VCF to DataFrame/.xlsx \r\n- Allele frequency calculation\r\n- Population structure\r\n- Geographic sample maps\r\n- Genetic diversity\r\n\r\n- Kdr origin analysis (Ag-vampIR panel)\r\n- Species assignment (Ag-vampIR panel)\r\n\r\nThe workflow uses a combination of papermill and jupyter book, so that users can visually explore the results in a local webpage for convenience.\r\n\r\n## Usage\r\n\r\nPlease see the [documentation](https://sanjaynagi.github.io/AmpSeeker/) for more information on running the workflow.\r\n\r\n## Citation \r\n\r\n**Targeted genomic surveillance of insecticide resistance in African malaria vectors**  \r\nNagi, *et al*., 2025. *bioRxiv*. doi: https://doi.org/10.1101/2025.02.14.637727\r\n\r\n## Testing\r\n\r\nTest cases are in the subfolder `.test`. They are automatically executed via continuous integration with [GitHub Actions](https://github.com/features/actions).\r\n\r\n## Contributing to AmpSeeker\r\n\r\n1. Fork the repository to your own GitHub user account\r\n2. Clone your fork\r\n3. Create a branch to implement the changes or features you would like `git checkout -b my_new_feature-24-03-23`\r\n4. Implement the changes\r\n5. Use `git add FILES`, `git commit -m COMMENT`, and `git push` to push your changes back to the branch\r\n6. Open a Pull request to the main repository \r\n7. Once the pull request is merged, either delete your fork, or switch back to the main branch `git checkout main` and use `git pull upstream main` to incorporate the changes back in your local repo. Prior to `git pull upstream main`, you may need to set sanjaynagi/AmpSeeker as the upstream remote url, with `git remote set-url upstream git@github.com:sanjaynagi/AmpSeeker.git`. \r\n8. At this stage, your local repo should be up to date with the main Ampseeker branch and you are ready to start from #3 if you have more contributions!\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1729",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1729?version=1",
        "name": "AmpSeeker",
        "number_of_steps": 0,
        "projects": [
            "Vector informatics and genomics group"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2025-06-12",
        "versions": 1
    },
    {
        "create_time": "2025-06-11",
        "creators": [
            "Gisela Gabernet",
            "Simon Heumos",
            "Alexander Peltzer"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-airrflow_logo_dark.png\">\n    <img alt=\"nf-core/airrflow\" src=\"docs/images/nf-core-airrflow_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/airrflow/workflows/nf-core%20CI/badge.svg)](https://github.com/nf-core/airrflow/actions?query=workflow%3A%22nf-core+CI%22)\n[![GitHub Actions Linting Status](https://github.com/nf-core/airrflow/workflows/nf-core%20linting/badge.svg)](https://github.com/nf-core/airrflow/actions?query=workflow%3A%22nf-core+linting%22)\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/airrflow/results)\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.2642009-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.2642009)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/airrflow)\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23airrflow-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/airrflow)\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)\n[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)\n[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n[![AIRR compliant](https://img.shields.io/static/v1?label=AIRR-C%20sw-tools%20v1&message=compliant&color=008AFF&labelColor=000000&style=plastic)](https://docs.airr-community.org/en/stable/swtools/airr_swtools_standard.html)\n\n## Introduction\n\n**nf-core/airrflow** is a bioinformatics best-practice pipeline to analyze B-cell or T-cell repertoire sequencing data. The input data can be targeted amplicon bulk sequencing data of the V, D, J and C regions of the B/T-cell receptor with multiplex PCR or 5' RACE protocol, single-cell VDJ sequencing using the 10xGenomics libraries, or assembled reads (bulk or single-cell). It can also extract BCR and TCR sequences from bulk or single-cell untargeted RNAseq data. It makes use of the [Immcantation](https://immcantation.readthedocs.io) toolset as well as other AIRR-seq analysis tools.\n\n![nf-core/airrflow overview](docs/images/airrflow_workflow_overview.png)\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/airrflow/results).\n\n## Pipeline summary\n\nnf-core/airrflow allows the end-to-end processing of BCR and TCR bulk and single cell targeted sequencing data. Several protocols are supported, please see the [usage documentation](https://nf-co.re/airrflow/usage) for more details on the supported protocols. The pipeline has been certified as [AIRR compliant](https://docs.airr-community.org/en/stable/swtools/airr_swtools_compliant.html) by the AIRR community, which means that it is compatible with downstream analysis tools also supporting this format.\n\n![nf-core/airrflow overview](docs/images/metro-map-airrflow.png)\n\n1. QC and sequence assembly\n\n- Bulk\n  - Raw read quality control, adapter trimming and clipping (`Fastp`).\n  - Filter sequences by base quality (`pRESTO FilterSeq`).\n  - Mask amplicon primers (`pRESTO MaskPrimers`).\n  - Pair read mates (`pRESTO PairSeq`).\n  - For UMI-based sequencing:\n    - Cluster sequences according to similarity (optional for insufficient UMI diversity) (`pRESTO ClusterSets`).\n    - Build consensus of sequences with the same UMI barcode (`pRESTO BuildConsensus`).\n  - Assemble R1 and R2 read mates (`pRESTO AssemblePairs`).\n  - Remove and annotate read duplicates (`pRESTO CollapseSeq`).\n  - Filter out sequences that do not have at least 2 duplicates (`pRESTO SplitSeq`).\n- single cell\n  - cellranger vdj\n    - Assemble contigs\n    - Annotate contigs\n    - Call cells\n    - Generate clonotypes\n\n2. V(D)J annotation and filtering (bulk and single-cell)\n\n- Assign gene segments with `IgBlast` using a germline reference (`Change-O AssignGenes`).\n- Annotate alignments in AIRR format (`Change-O MakeDB`)\n- Filter by alignment quality (locus matching v_call chain, min 200 informative positions, max 10% N nucleotides)\n- Filter productive sequences (`Change-O ParseDB split`)\n- Filter junction length multiple of 3\n- Annotate metadata (`EnchantR`)\n\n3. QC filtering (bulk and single-cell)\n\n- Bulk sequencing filtering:\n  - Remove chimeric sequences (optional) (`SHazaM`, `EnchantR`)\n  - Detect cross-contamination (optional) (`EnchantR`)\n  - Collapse duplicates (`Alakazam`, `EnchantR`)\n- Single-cell QC filtering (`EnchantR`)\n  - Remove cells without heavy chains.\n  - Remove cells with multiple heavy chains.\n  - Remove sequences in different samples that share the same `cell_id` and nucleotide sequence.\n  - Modify `cell_id`s to ensure they are unique in the project.\n\n4. Clonal analysis (bulk and single-cell)\n\n- Find threshold for clone definition (`SHazaM`, `EnchantR`).\n- Create germlines and define clones, repertoire analysis (`SCOPer`, `EnchantR`).\n- Build lineage trees (`Dowser`, `IgphyML`, `RAxML`, `EnchantR`).\n\n5. Repertoire analysis and reporting\n\n- Custom repertoire analysis pipeline report (`Alakazam`).\n- Aggregate QC reports (`MultiQC`).\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, ensure that the pipeline tests run on your infrastructure:\n\n```bash\nnextflow run nf-core/airrflow -profile test,<docker/singularity/podman/shifter/charliecloud/conda/institute> --outdir <OUTDIR>\n```\n\nTo run nf-core/airrflow with your data, prepare a tab-separated samplesheet with your input data. Depending on the input data type (bulk or single-cell, raw reads or assembled reads) the input samplesheet will vary. Please follow the [documentation on samplesheets](https://nf-co.re/airrflow/usage#input-samplesheet) for more details. An example samplesheet for running the pipeline on bulk BCR / TCR sequencing data in fastq format looks as follows:\n\n| sample_id | filename_R1                     | filename_R2                     | filename_I1                     | subject_id | species | pcr_target_locus | tissue | sex    | age | biomaterial_provider | single_cell | intervention   | collection_time_point_relative | cell_subset  |\n| --------- | ------------------------------- | ------------------------------- | ------------------------------- | ---------- | ------- | ---------------- | ------ | ------ | --- | -------------------- | ----------- | -------------- | ------------------------------ | ------------ |\n| sample01  | sample1_S8_L001_R1_001.fastq.gz | sample1_S8_L001_R2_001.fastq.gz | sample1_S8_L001_I1_001.fastq.gz | Subject02  | human   | IG               | blood  | NA     | 53  | sequencing_facility  | FALSE       | Drug_treatment | Baseline                       | plasmablasts |\n| sample02  | sample2_S8_L001_R1_001.fastq.gz | sample2_S8_L001_R2_001.fastq.gz | sample2_S8_L001_I1_001.fastq.gz | Subject02  | human   | TR               | blood  | female | 78  | sequencing_facility  | FALSE       | Drug_treatment | Baseline                       | plasmablasts |\n\nEach row represents a sample with fastq files (paired-end).\n\nA typical command to run the pipeline from **bulk raw fastq files** is:\n\n```bash\nnextflow run nf-core/airrflow \\\n-r <release> \\\n-profile <docker/singularity/podman/shifter/charliecloud/conda/institute> \\\n--mode fastq \\\n--input input_samplesheet.tsv \\\n--library_generation_method specific_pcr_umi \\\n--cprimers CPrimers.fasta \\\n--vprimers VPrimers.fasta \\\n--umi_length 12 \\\n--umi_position R1 \\\n--outdir ./results\n```\n\nFor common **bulk sequencing protocols** we provide pre-set profiles that specify primers, UMI length, etc for common commercially available sequencing protocols. Please check the [Supported protocol profiles](#supported-protocol-profiles) for a full list of available profiles. An example command running the NEBNext UMI protocol profile with docker containers is:\n\n```bash\nnextflow run nf-core/airrflow \\\n-profile nebnext_umi,docker \\\n--mode fastq \\\n--input input_samplesheet.tsv \\\n--outdir results\n```\n\nA typical command to run the pipeline from **single cell raw fastq files** (10X genomics) is:\n\n```bash\nnextflow run nf-core/airrflow -r dev \\\n-profile <docker/singularity/podman/shifter/charliecloud/conda/institute> \\\n--mode fastq \\\n--input input_samplesheet.tsv \\\n--library_generation_method sc_10x_genomics \\\n--reference_10x reference/refdata-cellranger-vdj-GRCh38-alts-ensembl-5.0.0.tar.gz \\\n--outdir ./results\n```\n\nA typical command to run the pipeline from **single-cell AIRR rearrangement tables or assembled bulk sequencing fasta** data is:\n\n```bash\nnextflow run nf-core/airrflow \\\n-r <release> \\\n-profile <docker/singularity/podman/shifter/charliecloud/conda/institute> \\\n--input input_samplesheet.tsv \\\n--mode assembled \\\n--outdir results\n```\n\nSee the [usage documentation](https://nf-co.re/airrflow/usage) and the [parameter documentation](https://nf-co.re/airrflow/parameters) for more details on how to use the pipeline and all the available parameters.\n\n:::warning\nPlease provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\nprovided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\nsee [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\n:::\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/airrflow/usage) and the [parameter documentation](https://nf-co.re/airrflow/parameters).\n\n## Pipeline output\n\nTo see the the results of a test run with a full size dataset refer to the [results](https://nf-co.re/airrflow/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/airrflow/output).\n\n## Credits\n\nnf-core/airrflow was originally written by:\n\n- [Gisela Gabernet](https://github.com/ggabernet)\n- [Susanna Marquez](https://github.com/ssnn-airr)\n- [Alexander Peltzer](https://github.com/apeltzer)\n\nWe thank the following people for their extensive assistance in the development of the pipeline:\n\n- [David Ladd](https://github.com/dladd)\n- [Friederike Hanssen](https://github.com/friederikehanssen)\n- [Simon Heumos](https://github.com/subwaystation)\n- [Mark Polster](https://github.com/mapo9)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#airrflow` channel](https://nfcore.slack.com/channels/airrflow) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/airrflow for your analysis, please cite the article as follows:\n\n> **nf-core/airrflow: an adaptive immune receptor repertoire analysis workflow employing the Immcantation framework**\n>\n> Gisela Gabernet, Susanna Marquez, Robert Bjornson, Alexander Peltzer, Hailong Meng, Edel Aron, Noah Y. Lee, Cole G. Jensen, David Ladd, Mark Polster, Friederike Hanssen, Simon Heumos, nf-core community, Gur Yaari, Markus C. Kowarik, Sven Nahnsen, Steven H. Kleinstein. (2024) PLOS Computational Biology, 20(7), e1012265. doi: [https://doi.org/10.1371/journal.pcbi.1012265](https://doi.org/10.1371/journal.pcbi.1012265). Pubmed PMID: 39058741.\n\nThe specific pipeline version using the following DOI: [10.5281/zenodo.2642009](https://doi.org/10.5281/zenodo.2642009)\n\nPlease also cite all the tools that are being used by the pipeline. An extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "963",
        "keep": true,
        "latest_version": 17,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/963?version=17",
        "name": "nf-core/airrflow",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "airr",
            "b-cell",
            "immcantation",
            "immunorepertoire",
            "repseq"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-11",
        "versions": 17
    },
    {
        "create_time": "2025-06-10",
        "creators": [
            "Cyril Noel",
            "Antoine Veron",
            "Fran\u00e7oise Vincent-Hubert",
            "Julien Schaeffer",
            "Marion Desdouits",
            "Soizick Le Guyader"
        ],
        "description": "## Introduction\r\n\r\n**samba-norovirus** is an adaptation of the [**samba workflow**](https://gitlab.ifremer.fr/bioinfo/workflows/samba) for the specific needs in metabarcoding analyses of norovirus. It is a FAIR scalable workflow integrating, into a unique tool, state-of-the-art bioinformatics and statistical methods to conduct reproducible metabarcoding and eDNA analyses using [Nextflow](https://www.nextflow.io) (Di Tommaso *et al.*, 2017). SAMBA performs complete metabarcoding analysis by:\r\n- processing data using commonly used procedure with [QIIME 2](https://qiime2.org/) (version 2024.2 ; Bolyen *et al.*, 2019):\r\n    - remove primers from raw reads and remove reads without detected primer using the QIIME 2 plugin of [cutadapt](http://dx.doi.org/10.14806/ej.17.1.200): `q2-cutadapt`\r\n    - denoise reads and infering ASV using the QIIME 2 plugin of [DADA2](Callahan *et al.*, 2016): `q2-dada2`\r\n    - cluster ASV by small local linking threshold using [swarm](https://github.com/torognes/swarm) (Mah\u00e9 *et al.*, 2022)\r\n    - detect and remove chimeras using [UCHIME](https://doi.org/10.1093/bioinformatics/btr381) (Edgar *et al.*, 2011)\r\n    - assign the ASV taxonomy using the Naive Bayesian classification from the QIIME 2 plugin `q2-feature-classifier`\r\n- post-process ASV table with different opitonal processes:\r\n    - remove contaminant from biological samples using positive and/or negative control samples with the R package: [microDecon](https://github.com/donaldtmcknight/microDecon) (McKnight *et al.*, 2019)\r\n    - removal of ASVs belonging to undesired taxa using the QIIME 2 plugin `q2-taxa` (filter-table & filter-seqs)\r\n    - removal of ASVs based on their frequency, contingency and heir length using the QIIME 2 plugin `q2-feature-table` (filter-table, filter-seqs & filter-features)\r\n- conducting extended statistical and ecological analyses using homemade Rscript\r\n\r\nThe **samba-norovirus** pipeline can run tasks across multiple compute infrastructures in a very portable manner. It comes with singularity containers making installation trivial and results highly reproducible.\r\n\r\n## Requirements\r\n\r\ni. You must have [`Nextflow (\u2265 v24.04.4)`](https://www.nextflow.io/docs/latest/getstarted.html#installation) installed on your computing machine to run the workflow.\r\n\r\nii. You must have [`Singularity (\u2265 v3.6.4)`](https://www.sylabs.io/guides/3.0/user-guide/) installed on your computing machine for full pipeline reproducibility. \r\n\r\niii. If your HPC nodes don't have any internet access. Please download before any workflow run the singularity images available on the [samba-norovirus singularity image repository](https://data-dataref.ifremer.fr/bioinfo/ifremer/sebimer/tools/samba-norovirus/1.0.0/). Then set the `$NXF_SINGULARITY_CACHEDIR` environment variable to the path where you just downloaded the images.\r\n\r\niv. You must download the norovirus database and sequences formatted for the workflow in order to perform the taxonomic assignment of your ASVs and the chimeras detection. You can download all files on the [samba-norovirus database repository](https://data-dataref.ifremer.fr/bioinfo/ifremer/sebimer/sequence-set/samba-norovirus/1.0.0). Then set the `database` and `uchime_ref` parameters in the base.config file with the path where you placed the downloaded files.\r\n\r\n\r\n## Quick Start\r\n\r\ni. Download the pipeline\r\n\r\n```bash\r\ngit clone https://gitlab.ifremer.fr/bioinfo/workflows/samba-norovirus\r\n```\r\n\r\n> To use SAMBA-norovirus on a computing cluster, it is necessary to provide a configuration file for your system. For some institutes, this one already exists and is referenced on [nf-core/configs](https://github.com/nf-core/configs#documentation). If so, you can simply download your institute custom config file and simply use `-c <institute_config_file>` in your command. This will set the appropriate execution settings for your local compute environment.\r\n\r\nii. Start running your own analysis!\r\n\r\nBefore you start analyzing your data, please read the [SAMBA workflow documentation](./docs/usage.md)\r\n\r\n```bash\r\nnextflow run main.nf -profile norovirus,singularity [-c <institute_config_file>]\r\n```\r\n\r\n## Credits\r\n\r\nsamba-norovirus is written by [Cyril No\u00ebl](https://github.com/cnoel-sebimer) from the [SeBiMER](https://sebimer.ifremer.fr/), the Bioinformatics Core Facility of [IFREMER](https://wwz.ifremer.fr/en/). This workflow was developed in close collaboration with members of the Ifremer LSEM lab.\r\n\r\n## Contributions\r\n\r\nWe welcome contributions to the pipeline. If such case you can do one of the following:\r\n* Use issues to submit your questions \r\n* Fork the project, do your developments and submit a pull request\r\n* Contact us (see email below) \r\n",
        "doi": "10.48546/workflowhub.workflow.1727.1",
        "edam_operation": [
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metabarcoding",
            "Workflows"
        ],
        "filtered_on": "edam",
        "id": "1727",
        "keep": true,
        "latest_version": 1,
        "license": "AGPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1727?version=1",
        "name": "SAMBA-norovirus",
        "number_of_steps": 0,
        "projects": [
            "SeBiMER"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metabarcoding",
            "workflows",
            "taxonomic-classification"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-10",
        "versions": 1
    },
    {
        "create_time": "2025-06-05",
        "creators": [
            "Bundit Boonyarit",
            "Matin Kositchutima",
            "Tisorn Na Phattalung",
            "Nattawin Yamprasert",
            "Chanitra Thuwajit",
            "Thanyada Rungrotmongkol",
            "Sarana Nutanong"
        ],
        "description": "# SynProtX\r\n\r\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13285494.svg)](https://doi.org/10.5281/zenodo.13285494)\r\n\r\nAn official implementation of our research paper **\"SynProtX: A Large-Scale Proteomics-Based Deep Learning Model for Predicting Synergistic Anticancer Drug Combinations\"**.\r\n\r\nSynProtX is a deep learning model that integrates large-scale proteomics data, molecular graphs, and chemical fingerprints to predict synergistic effects of anticancer drug combinations. It provides robust performance across tissue-specific and study-specific datasets, enhancing reproducibility and biological relevance in drug synergy prediction.\r\n\r\n## Setting up environment\r\n\r\nWe use [Miniconda](https://docs.anaconda.com/miniconda/) to manage Python dependencies in this project. To reproduce our environment, please run the following script in the terminal:\r\n\r\n```sh\r\nconda env create -f env.yml\r\nconda activate SynProtX\r\n```\r\n\r\n## Downloading raw data\r\n\r\nDatasets, hyperparameters, and model checkpoints can be downloaded through [Zenodo](https://doi.org/10.5281/zenodo.13285494).\r\n\r\n## Generating dataset\r\n\r\nA tarball will be obtained after download. After file extraction, move all nested folders to the root of this project directory. You might need to move all files in `data/export` up to `data` folder. Otherwise, you will run the Jupyter Notebook files to generate mandatory data. Let\u2019s take a look at `ipynb` folder. Run the following files in order if you want to replicate our exported data.\r\n\r\n- `01_drugcomb_clean.ipynb` \u2192 `cleandata_cancer.csv`\r\n- `02_CCLE_gene_expression` \u2192 `CCLE_expression_cleaned.csv`\r\n- `03_omics_preprocess` \u2192 `protein_omics_data_cleaned.csv`\r\n- `04_drugcomb_gene_prot_clean` \u2192 `data_preprocessing_gene.pkl`, `data_drugcomb.pkl`, `data_preprocessing_protein.pkl`\r\n- `05_graph_generate.ipynb` \u2192 `nps_intersected` folder\r\n- `06_smiles_feat_generate.ipynb` \u2192 `smiles_graph_data.pkl`\r\n- `07_to_ecfp6_deepsyn.ipynb` \u2192 `deepsyn_drug_row.npy`, `deepsyn_drug_col.npy`\r\n\r\n> If the console shows an error indicating that SMILES not found, you MUST run the file `06_smiles_feat_generate.ipynb` again to regenerate data.\r\n\r\n## Training and testing\r\n\r\nTo execute a training and testing task for our model, run the following script\r\n\r\n```sh\r\npython synprotx/<model>.py -d <database> -m <mode>\r\n```\r\n\r\nPossible options are listed below.\r\n\r\n- `model` represents the name of the model to run. Must be one of `gat`, `gcn`, `attentivefp` and `gatfp`.\r\n- `--database`/`-d` specifies data source to train the model on. Must be one of `almanac-breast`, `almanac-lung`, `almanac-ovary`, `almanac-skin`, `friedman`, `oneil`.\r\n- `--mode`/`-m` input must be either `clas`, for classification task, or `regr`, for regression task. Default to `clas`\r\n- Flags `--no-feamol`, `--no-feagene`, `--no-feaprot` disable the molecule branch, gene expression branch, and protein expression branch, respectively, when propagate through the model.\r\n\r\n**Note:** There are more options to configure. Execute `python  synprotx/<model>.py -h` for a more detailed description.\r\n",
        "doi": "10.48546/workflowhub.workflow.1726.3",
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Biomedical science",
            "Computational biology",
            "Drug discovery",
            "Machine learning",
            "Translational medicine"
        ],
        "filtered_on": "binn* in description",
        "id": "1726",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1726?version=3",
        "name": "SynProtX",
        "number_of_steps": 0,
        "projects": [
            "BAID Team"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "cheminformatics",
            "machine learning",
            "proteomics"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2025-06-05",
        "versions": 2
    },
    {
        "create_time": "2021-09-10",
        "creators": [
            "Cyril Noel",
            "Alexandre Cormier",
            "Laura Leroi",
            "Patrick Durand"
        ],
        "description": "SAMBA is a FAIR scalable workflow integrating, into a unique tool, state-of-the-art bioinformatics and statistical methods to conduct reproducible eDNA analyses using Nextflow. SAMBA starts processing by verifying integrity of raw reads and metadata. Then all bioinformatics processing is done using commonly used procedure (QIIME 2 and DADA2) but adds new steps relying on dbOTU3 and microDecon to build high quality ASV count tables. Extended statistical analyses are also performed. Finally, SAMBA produces a full dynamic HTML report including resources used, commands executed, intermediate results, statistical analyses and figures.\r\n\r\nThe SAMBA pipeline can run tasks across multiple compute infrastructures in a very portable manner. It comes with singularity containers making installation trivial and results highly reproducible.",
        "doi": "10.48546/workflowhub.workflow.156.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in tags",
        "id": "156",
        "keep": true,
        "latest_version": 1,
        "license": "AGPL-3.0",
        "link": "https:/workflowhub.eu/workflows/156?version=1",
        "name": "SAMBA: Standardized and Automated MetaBarcoding Analyses workflow",
        "number_of_steps": 0,
        "projects": [
            "SeBiMER"
        ],
        "source": "WorkflowHub",
        "tags": [
            "16s",
            "18s",
            "metabarcoding",
            "nextflow",
            "edna"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-05",
        "versions": 1
    },
    {
        "create_time": "2023-09-01",
        "creators": [
            "Veit Schw\u00e4mmle"
        ],
        "description": "## Introduction\r\n\r\n**wombat-p pipelines** is a bioinformatics analysis pipeline that bundles different workflow for the analysis of label-free proteomics data with the purpose of comparison and benchmarking. It allows using files from the [proteomics metadata standard SDRF](https://github.com/bigbio/proteomics-metadata-standard).\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. We used one of the [nf-core](https://nf-co.re/) templates. \r\n\r\n## Pipeline summary\r\n\r\nThis work contains four major different workflows for the analysis or label-free proteomics data, originating from LC-MS experiments.\r\n1. [MaxQuant](https://www.maxquant.org/) + [NormalyzerDE](https://normalyzerde.immunoprot.lth.se/)\r\n2. [SearchGui](http://compomics.github.io/projects/searchgui) + [Proline](https://www.profiproteomics.fr/proline/) + [PolySTest](https://bitbucket.org/veitveit/polystest)\r\n3. [Compomics tools](http://compomics.github.io/) + [FlashLFQ](https://github.com/smith-chem-wisc/FlashLFQ) + [MSqRob](https://github.com/statOmics/MSqRob)\r\n4. Tools from the [Trans-Proteomic Pipeline](http://tools.proteomecenter.org/TPP.php) + [ROTS](https://bioconductor.org/packages/release/bioc/html/ROTS.html)\r\n\r\nInitialization and parameterization of the workflows is based on tools from the [SDRF pipelines](https://github.com/bigbio/sdrf-pipelines), the [ThermoRawFileParser](http://compomics.github.io/projects/ThermoRawFileParser) with our own contributions and additional programs from the wombat-p organizaion [https://github.com/wombat-p/Utilities] as well as our [fork](https://github.com/elixir-proteomics-community/sdrf-pipelines). This includes setting a generalized set of data analysis parameters and the calculation of a multiple benchmarks.\r\n\r\n## Credits\r\n\r\nnf-core/wombat was originally written by the members of the ELIXIR Implementation study  [Comparison, benchmarking and dissemination of proteomics data analysis pipelines](https://elixir-europe.org/internal-projects/commissioned-services/proteomics-pipelines) under the lead of Veit Schw\u00e4mmle and major participation of David Bouyssi\u00e9 and Fredrik Levander.\r\n\r\n## Citations\r\n\r\nManuscript in preparation\r\n\r\n\r\nAs the workflows are using an nf-core template, we refer to the publication:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).",
        "doi": "10.48546/workflowhub.workflow.444.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "444",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/444?version=2",
        "name": "WOMBAT-Pipelines",
        "number_of_steps": 0,
        "projects": [
            "ELIXIR Proteomics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "compomics",
            "maxquant",
            "proline",
            "proteomics",
            "trans-proteomic"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-04",
        "versions": 2
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "roscoff hackathon\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [De novo transcriptome assembly, annotation, and differential expression analysis](https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/full-de-novo/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Gildas Le Corguill\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/lecorguille/), [Erwan Corre](https://training.galaxyproject.org/training-material/hall-of-fame/r1corre/), [Xi Liu](https://training.galaxyproject.org/training-material/hall-of-fame/xiliu/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1716",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1716?version=1",
        "name": "trinity NG",
        "number_of_steps": 21,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "transcriptomics"
        ],
        "tools": [
            "trinity_analyze_diff_expr",
            "trinity_abundance_estimates_to_matrix",
            "trinity_align_and_estimate_abundance",
            "trinity_contig_exn50_statistic",
            "trinity",
            "trinity_define_clusters_by_cutting_tree",
            "fastqc",
            "trinity_filter_low_expr_transcripts",
            "trinity_run_de_analysis",
            "trinity_samples_qccheck",
            "transdecoder",
            "trimmomatic",
            "describe_samples",
            "trinotate",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Metabarcoding/eDNA through Obitools\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metabarcoding/eDNA through Obitools](https://training.galaxyproject.org/training-material/topics/ecology/tutorials/Obitools-metabarcoding/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Coline Royaux](https://training.galaxyproject.org/training-material/hall-of-fame/colineroyaux/), [Olivier Norvez](https://training.galaxyproject.org/training-material/hall-of-fame/onorvez/), [Eric Coissac](https://training.galaxyproject.org/training-material/hall-of-fame/ecoissac/), [Fr\u00e9d\u00e9ric Boyer](https://training.galaxyproject.org/training-material/hall-of-fame/fboyer/), [Yvan Le Bras](https://training.galaxyproject.org/training-material/hall-of-fame/yvanlebras/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Anne Fouilloux](https://training.galaxyproject.org/training-material/hall-of-fame/annefou/), [Yvan Le Bras](https://training.galaxyproject.org/training-material/hall-of-fame/yvanlebras/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [P\u00f4le National de Donn\u00e9es de Biodiversit\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/pndb/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1702",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1702?version=1",
        "name": "Workflow constructed from history 'Tuto Obitools'",
        "number_of_steps": 23,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "ecology",
            "gtn",
            "galaxy"
        ],
        "tools": [
            "obi_grep",
            "obi_ngsfilter",
            "obi_annotate",
            "seq_filter_by_id",
            "obi_illumina_pairend",
            "obi_uniq",
            "ncbi_blastn_wrapper",
            "fastqc",
            "Cut1",
            "obi_tab",
            "obi_stat",
            "fastq_groomer",
            "join1",
            "Filter1",
            "obi_clean",
            "wc_gnu",
            "unzip"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Biodiversity data exploration\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Biodiversity data exploration](https://training.galaxyproject.org/training-material/topics/ecology/tutorials/biodiversity-data-exploration/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Olivier Norvez](https://training.galaxyproject.org/training-material/hall-of-fame/onorvez/), [Marie Josse](https://training.galaxyproject.org/training-material/hall-of-fame/Marie59/), [Coline Royaux](https://training.galaxyproject.org/training-material/hall-of-fame/colineroyaux/), [Yvan Le Bras](https://training.galaxyproject.org/training-material/hall-of-fame/yvanlebras/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Coline Royaux](https://training.galaxyproject.org/training-material/hall-of-fame/colineroyaux/), [Yvan Le Bras](https://training.galaxyproject.org/training-material/hall-of-fame/yvanlebras/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [P\u00f4le National de Donn\u00e9es de Biodiversit\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/pndb/), [P\u00f4le National de Donn\u00e9es de Biodiversit\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/pndb/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [OpenMetaPaper](https://training.galaxyproject.org/training-material/hall-of-fame/fnso2019/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1693",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1693?version=1",
        "name": "Workflow 'Biodiversity data exploration tuto'",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "ecology",
            "gtn",
            "galaxy"
        ],
        "tools": [
            "ecology_link_between_var",
            "tool_anonymization",
            "ecology_stat_presence_abs",
            "ecology_beta_diversity",
            "ecology_homogeneity_normality",
            "ecology_presence_abs_abund"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "High Throughput Molecular Dynamics and Analysis\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [High Throughput Molecular Dynamics and Analysis](https://training.galaxyproject.org/training-material/topics/computational-chemistry/tutorials/htmd-analysis/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Simon Bray](https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/), [Tharindu Senapathi](https://training.galaxyproject.org/training-material/hall-of-fame/tsenapathi/), [Christopher Barnett](https://training.galaxyproject.org/training-material/hall-of-fame/chrisbarnettster/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Nadia Gou\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/nagoue/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Simon Bray](https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/), [Simon Gladman](https://training.galaxyproject.org/training-material/hall-of-fame/slugger70/), [Christopher Barnett](https://training.galaxyproject.org/training-material/hall-of-fame/chrisbarnettster/), [Tharindu Senapathi](https://training.galaxyproject.org/training-material/hall-of-fame/tsenapathi/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Martin \u010cech](https://training.galaxyproject.org/training-material/hall-of-fame/martenson/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Armin Dadras](https://training.galaxyproject.org/training-material/hall-of-fame/dadrasarmin/)\n\n**Funder(s)**: [ELIXIR Europe](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/), [de.NBI](https://training.galaxyproject.org/training-material/hall-of-fame/deNBI/), [University of Freiburg](https://training.galaxyproject.org/training-material/hall-of-fame/uni-freiburg/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1689",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1689?version=1",
        "name": "Workflow constructed from history 'Hsp90-MDAnalysis'",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "computational-chemistry",
            "gtn",
            "galaxy"
        ],
        "tools": [
            "vmd_hbonds",
            "bio3d_pca",
            "md_converter",
            "mdanalysis_cosine_analysis",
            "bio3d_rmsd",
            "bio3d_pca_visualize",
            "bio3d_rmsf",
            "gmx_editconf"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "High Throughput Molecular Dynamics and Analysis\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [High Throughput Molecular Dynamics and Analysis](https://training.galaxyproject.org/training-material/topics/computational-chemistry/tutorials/htmd-analysis/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Simon Bray](https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/), [Tharindu Senapathi](https://training.galaxyproject.org/training-material/hall-of-fame/tsenapathi/), [Christopher Barnett](https://training.galaxyproject.org/training-material/hall-of-fame/chrisbarnettster/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Nadia Gou\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/nagoue/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Simon Bray](https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/), [Simon Gladman](https://training.galaxyproject.org/training-material/hall-of-fame/slugger70/), [Christopher Barnett](https://training.galaxyproject.org/training-material/hall-of-fame/chrisbarnettster/), [Tharindu Senapathi](https://training.galaxyproject.org/training-material/hall-of-fame/tsenapathi/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Martin \u010cech](https://training.galaxyproject.org/training-material/hall-of-fame/martenson/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Armin Dadras](https://training.galaxyproject.org/training-material/hall-of-fame/dadrasarmin/)\n\n**Funder(s)**: [ELIXIR Europe](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/), [de.NBI](https://training.galaxyproject.org/training-material/hall-of-fame/deNBI/), [University of Freiburg](https://training.galaxyproject.org/training-material/hall-of-fame/uni-freiburg/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1686",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1686?version=1",
        "name": "MD protein-ligand workflow (from PDB structure)",
        "number_of_steps": 13,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cheminformatics",
            "computational-chemistry",
            "gtn",
            "galaxy",
            "moleculardynamics"
        ],
        "tools": [
            "gmx_merge_topology_files",
            "ambertools_acpype",
            "gmx_sim",
            "tp_grep_tool",
            "openbabel_compound_convert",
            "gmx_setup",
            "get_pdb",
            "gmx_editconf",
            "gmx_em",
            "gmx_solvate"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "This workflow begins from a set of genome assemblies of different samples, strains, species. The genome is first annotated with Funnanotate. Predicted proteins are furtner annotated with Busco. Next, 'ProteinOrtho' finds orthologs across the samples and makes orthogroups. Orthogroups where all samples are represented are extracted. Orthologs in each orthogroup are aligned with ClustalW. The alignments are cleaned with ClipKIT and the concatenation matrix is built using PhyKit. This can be used for phylogeny reconstruction.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Preparing genomic data for phylogeny reconstruction](https://training.galaxyproject.org/training-material/topics/ecology/tutorials/phylogeny-data-prep/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Miguel Roncoroni\n\n**Tutorial Author(s)**: [Miguel Roncoroni](https://training.galaxyproject.org/training-material/hall-of-fame/roncoronimiguel/), [Brigida Gallone](https://training.galaxyproject.org/training-material/hall-of-fame/brigidagallone/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1656",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1656?version=1",
        "name": "preparing genomic data for phylogeny recostruction (GTN)",
        "number_of_steps": 14,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "ecology",
            "gtn",
            "galaxy"
        ],
        "tools": [
            "proteinortho",
            "tp_replace_in_line",
            "funannotate_predict",
            "repeatmasker_wrapper",
            "clustalw",
            "clipkit",
            "Filter1",
            "collapse_dataset",
            "regex1",
            "proteinortho_grab_proteins",
            "glimmer_gbk_to_orf",
            "phykit_alignment_based",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Genome assembly quality control using PacBio data and with reference genome\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Genome Assembly Quality Control](https://training.galaxyproject.org/training-material/topics/assembly/tutorials/assembly-quality-control/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Erwan Corre, St\u00e9phanie Robin, Anthony Bretaudeau, Alexandre Cormier, Laura Leroi\n\n**Tutorial Author(s)**: [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Alexandre Cormier](https://training.galaxyproject.org/training-material/hall-of-fame/alexcorm/), [Erwan Corre](https://training.galaxyproject.org/training-material/hall-of-fame/r1corre/), [Laura Leroi](https://training.galaxyproject.org/training-material/hall-of-fame/lleroi/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1646",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1646?version=1",
        "name": "Genome Assembly Quality Control",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "gtn",
            "galaxy"
        ],
        "tools": [
            "merqury",
            "quast",
            "chromeister",
            "meryl",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Quality Control\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Quality Control](https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/quality-control/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Maria Doyle](https://training.galaxyproject.org/training-material/hall-of-fame/mblue9/), [Alexandre Cormier](https://training.galaxyproject.org/training-material/hall-of-fame/alexcorm/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Laura Leroi](https://training.galaxyproject.org/training-material/hall-of-fame/lleroi/), [Erwan Corre](https://training.galaxyproject.org/training-material/hall-of-fame/r1corre/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/), [Cameron Hyde](https://training.galaxyproject.org/training-material/hall-of-fame/neoformit/)\n\n**Tutorial Contributor(s)**: [Swathi Nataraj](https://training.galaxyproject.org/training-material/hall-of-fame/Swathi266/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Teresa M\u00fcller](https://training.galaxyproject.org/training-material/hall-of-fame/teresa-m/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Anne Fouilloux](https://training.galaxyproject.org/training-material/hall-of-fame/annefou/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Natalie Kucher](https://training.galaxyproject.org/training-material/hall-of-fame/nakucher/), [Nicola Soranzo](https://training.galaxyproject.org/training-material/hall-of-fame/nsoranzo/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Matthias Bernt](https://training.galaxyproject.org/training-material/hall-of-fame/bernt-matthias/), [Anup Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/anuprulez/), [Anika Erxleben](https://training.galaxyproject.org/training-material/hall-of-fame/erxleben/), [Maria Doyle](https://training.galaxyproject.org/training-material/hall-of-fame/mblue9/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Simon Bray](https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/), [Mateo Boudet](https://training.galaxyproject.org/training-material/hall-of-fame/mboudet/), [Alexandre Cormier](https://training.galaxyproject.org/training-material/hall-of-fame/alexcorm/), [William Durand](https://training.galaxyproject.org/training-material/hall-of-fame/willdurand/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1640",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1640?version=1",
        "name": "GTN - Sequence Analyses - Quality Control (imported from uploaded file)",
        "number_of_steps": 5,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "sequence-analysis"
        ],
        "tools": [
            "cutadapt",
            "fastqc",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "## Associated Tutorial\n\nThis workflows is part of the tutorial [Assembly of metagenomic sequencing data](https://training.galaxyproject.org/training-material/topics/assembly/tutorials/metagenomics-assembly/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in name",
        "id": "1634",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1634?version=1",
        "name": "Metagenomics assembly tutorial workflow",
        "number_of_steps": 8,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy"
        ],
        "tools": [
            "bandage_info",
            "megahit",
            "megahit_contig2fastg",
            "collection_column_join",
            "bandage_image",
            "bowtie2",
            "metaspades",
            "quast"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "From a reference and a primer scheme generate two masked half-genome references for ITR-aware pox virus sequencing data analysis.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pox virus genome analysis from tiled-amplicon sequencing data](https://training.galaxyproject.org/training-material/topics/variant-analysis/tutorials/pox-tiled-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Viktoria Isabel Schwarz, Wolfgang Maier\n\n**Tutorial Author(s)**: [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Tomas Klingstr\u00f6m](https://training.galaxyproject.org/training-material/hall-of-fame/TKlingstrom/)\n\n**Tutorial Contributor(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Anton Nekrutenko](https://training.galaxyproject.org/training-material/hall-of-fame/nekrut/)\n\n**Grants(s)**: [BeYond-COVID](https://training.galaxyproject.org/training-material/hall-of-fame/by-covid/), [ELIXIR-CONVERGE](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-converge/), [Addressing the dual emerging threats of African Swine Fever and Lumpy Skin Disease in Europe](https://training.galaxyproject.org/training-material/hall-of-fame/h2020-defend/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in name",
        "id": "1632",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1632?version=1",
        "name": "pox-virus-tiled-amplicon-ref-masking",
        "number_of_steps": 14,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "variant-analysis"
        ],
        "tools": [
            "fasta_compute_length",
            "compose_text_param",
            "Add_a_column1",
            "Cut1",
            "datamash_ops",
            "EMBOSS: maskseq51",
            "param_value_from_file",
            "Grep1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "## Associated Tutorial\n\nThis workflows is part of the tutorial [Assembly of metagenomic sequencing data](https://training.galaxyproject.org/training-material/topics/assembly/tutorials/metagenomics-assembly/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "1631",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1631?version=1",
        "name": "workflow-generate-dataset-for-assembly-tutorial",
        "number_of_steps": 18,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy"
        ],
        "tools": [
            "cutadapt",
            "random_lines1",
            "bamtools",
            "filter_tabular",
            "ngsutils_bam_filter",
            "fastqc",
            "megahit",
            "bowtie2",
            "bg_uniq",
            "seqtk_subseq",
            "tp_cat"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Genome Assembly using PacBio data and Flye or Hifiasm assembler\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Genome assembly using PacBio data](https://training.galaxyproject.org/training-material/topics/assembly/tutorials/hifi-assembly/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Erwan Corre, St\u00e9phanie Robin, Anthony Bretaudeau, Alexandre Cormier, Solenne Correard\n\n**Tutorial Author(s)**: [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Alexandre Cormier](https://training.galaxyproject.org/training-material/hall-of-fame/alexcorm/), [Erwan Corre](https://training.galaxyproject.org/training-material/hall-of-fame/r1corre/), [Laura Leroi](https://training.galaxyproject.org/training-material/hall-of-fame/lleroi/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/), [Solenne Correard](https://training.galaxyproject.org/training-material/hall-of-fame/scorreard/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Solenne Correard](https://training.galaxyproject.org/training-material/hall-of-fame/scorreard/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1621",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1621?version=1",
        "name": "Genome Assembly using PacBio data",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "gtn",
            "galaxy"
        ],
        "tools": [
            "quast",
            "fasta-stats",
            "flye",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Identification of the binding sites of the Estrogen receptor\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Identification of the binding sites of the Estrogen receptor](https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/estrogen-receptor-binding-site-identification/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Friederike D\u00fcndar](https://training.galaxyproject.org/training-material/hall-of-fame/friedue/), [Anika Erxleben](https://training.galaxyproject.org/training-material/hall-of-fame/erxleben/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Vivek Bhardwaj](https://training.galaxyproject.org/training-material/hall-of-fame/vivekbhr/), [Fidel Ramirez](https://training.galaxyproject.org/training-material/hall-of-fame/fidelram/)\n\n**Tutorial Contributor(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Nicola Soranzo](https://training.galaxyproject.org/training-material/hall-of-fame/nsoranzo/), [Fidel Ramirez](https://training.galaxyproject.org/training-material/hall-of-fame/fidelram/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Niall Beard](https://training.galaxyproject.org/training-material/hall-of-fame/njall/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Maria Doyle](https://training.galaxyproject.org/training-material/hall-of-fame/mblue9/), [Martin \u010cech](https://training.galaxyproject.org/training-material/hall-of-fame/martenson/), [Armin Dadras](https://training.galaxyproject.org/training-material/hall-of-fame/dadrasarmin/)\n\n**Funder(s)**: [de.NBI](https://training.galaxyproject.org/training-material/hall-of-fame/deNBI/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in name",
        "id": "1620",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1620?version=1",
        "name": "Identification Of The Binding Sites Of The Estrogen Receptor - Chip Seq",
        "number_of_steps": 17,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "epigenetics"
        ],
        "tools": [
            "deeptools_bam_compare",
            "macs2_callpeak",
            "deeptools_plot_fingerprint",
            "deeptools_multi_bam_summary",
            "deeptools_compute_gc_bias",
            "deeptools_plot_correlation",
            "deeptools_plot_heatmap",
            "samtools_idxstats",
            "deeptools_compute_matrix",
            "deeptools_bam_coverage"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Identification of the binding sites of the Estrogen receptor\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Identification of the binding sites of the Estrogen receptor](https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/estrogen-receptor-binding-site-identification/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Friederike D\u00fcndar](https://training.galaxyproject.org/training-material/hall-of-fame/friedue/), [Anika Erxleben](https://training.galaxyproject.org/training-material/hall-of-fame/erxleben/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Vivek Bhardwaj](https://training.galaxyproject.org/training-material/hall-of-fame/vivekbhr/), [Fidel Ramirez](https://training.galaxyproject.org/training-material/hall-of-fame/fidelram/)\n\n**Tutorial Contributor(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Nicola Soranzo](https://training.galaxyproject.org/training-material/hall-of-fame/nsoranzo/), [Fidel Ramirez](https://training.galaxyproject.org/training-material/hall-of-fame/fidelram/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Niall Beard](https://training.galaxyproject.org/training-material/hall-of-fame/njall/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Maria Doyle](https://training.galaxyproject.org/training-material/hall-of-fame/mblue9/), [Martin \u010cech](https://training.galaxyproject.org/training-material/hall-of-fame/martenson/), [Armin Dadras](https://training.galaxyproject.org/training-material/hall-of-fame/dadrasarmin/)\n\n**Funder(s)**: [de.NBI](https://training.galaxyproject.org/training-material/hall-of-fame/deNBI/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in name",
        "id": "1619",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1619?version=1",
        "name": "Identification Of The Binding Sites Of The Estrogen Receptor - Qc Mapping",
        "number_of_steps": 4,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "epigenetics"
        ],
        "tools": [
            "fastqc",
            "trim_galore",
            "bowtie2",
            "bam_to_sam"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Assembly\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Chloroplast genome assembly](https://training.galaxyproject.org/training-material/topics/assembly/tutorials/chloroplast-assembly/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Anna Syme](https://training.galaxyproject.org/training-material/hall-of-fame/annasyme/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "1617",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1617?version=1",
        "name": "Chloroplast-genome-assembly-and-annotation",
        "number_of_steps": 13,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "gtn",
            "galaxy"
        ],
        "tools": [
            "bandage_info",
            "nanoplot",
            "prokka",
            "jbrowse",
            "bandage_image",
            "fasta-stats",
            "pilon",
            "flye",
            "bwa_mem"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "## Associated Tutorial\n\nThis workflows is part of the tutorial [Large genome assembly and polishing](https://training.galaxyproject.org/training-material/topics/assembly/tutorials/largegenome/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [subworkflows](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_subworkflows.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Anna Syme\n\n**Tutorial Author(s)**: [Anna Syme](https://training.galaxyproject.org/training-material/hall-of-fame/annasyme/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in name",
        "id": "1611",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1611?version=1",
        "name": "Combined workflows for large genome assembly - upgraded",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "lg-wf"
        ],
        "tools": [
            ""
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Identification of the binding sites of the T-cell acute lymphocytic leukemia protein 1 (TAL1)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Identification of the binding sites of the T-cell acute lymphocytic leukemia protein 1 (TAL1)](https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/tal1-binding-site-identification/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Mallory Freeberg](https://training.galaxyproject.org/training-material/hall-of-fame/malloryfreeberg/), [Mo Heydarian](https://training.galaxyproject.org/training-material/hall-of-fame/moheydarian/), [Vivek Bhardwaj](https://training.galaxyproject.org/training-material/hall-of-fame/vivekbhr/), [Joachim Wolff](https://training.galaxyproject.org/training-material/hall-of-fame/joachimwolff/), [Anika Erxleben](https://training.galaxyproject.org/training-material/hall-of-fame/erxleben/)\n\n**Tutorial Contributor(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Nicola Soranzo](https://training.galaxyproject.org/training-material/hall-of-fame/nsoranzo/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Mallory Freeberg](https://training.galaxyproject.org/training-material/hall-of-fame/malloryfreeberg/), [Friederike D\u00fcndar](https://training.galaxyproject.org/training-material/hall-of-fame/friedue/), [William Durand](https://training.galaxyproject.org/training-material/hall-of-fame/willdurand/), [Fidel Ramirez](https://training.galaxyproject.org/training-material/hall-of-fame/fidelram/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Niall Beard](https://training.galaxyproject.org/training-material/hall-of-fame/njall/), [Maria Doyle](https://training.galaxyproject.org/training-material/hall-of-fame/mblue9/), [Martin \u010cech](https://training.galaxyproject.org/training-material/hall-of-fame/martenson/), [Armin Dadras](https://training.galaxyproject.org/training-material/hall-of-fame/dadrasarmin/)\n\n**Funder(s)**: [de.NBI](https://training.galaxyproject.org/training-material/hall-of-fame/deNBI/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in name",
        "id": "1606",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1606?version=1",
        "name": "Identification of the binding sites of the T-cell acute lymphocytic leukemia protein 1 (TAL1)",
        "number_of_steps": 57,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "epigenetics"
        ],
        "tools": [
            "deeptools_bam_compare",
            "bedtools_intersectbed",
            "macs2_callpeak",
            "fastqc",
            "deeptools_plot_fingerprint",
            "deeptools_multi_bam_summary",
            "bwa",
            "trimmomatic",
            "deeptools_plot_correlation",
            "deeptools_plot_heatmap",
            "samtools_idxstats",
            "deeptools_compute_matrix"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Data Manipulation Olympics\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Data Manipulation Olympics](https://training.galaxyproject.org/training-material/topics/data-science/tutorials/data-manipulation-olympics/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Yongbin Li](https://training.galaxyproject.org/training-material/hall-of-fame/lybCNU/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Scott Cain](https://training.galaxyproject.org/training-material/hall-of-fame/scottcain/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1601",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1601?version=1",
        "name": "GTN Tutorial: Data manipulation Olympics - all steps and exercises",
        "number_of_steps": 59,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "introduction"
        ],
        "tools": [
            "Add_a_column1",
            "regexColumn1",
            "tp_split_on_column",
            "Count1",
            "Show beginning1",
            "Cut1",
            "tabular_to_csv",
            "Filter1",
            "datamash_ops",
            "join1",
            "Remove beginning1",
            "tp_cat",
            "wc_gnu",
            "cat1",
            "tp_sort_header_tool",
            "tp_sorted_uniq"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "This workflow predict in-silico mass spectra using semi-empirical quantum physics method.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Predicting EI+ mass spectra with QCxMS](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/qcxms-predictions/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: RECETOX SpecDat\n\n**Tutorial Author(s)**: [Julia Jakiela](https://training.galaxyproject.org/training-material/hall-of-fame/wee-snufkin/), [Helge Hecht](https://training.galaxyproject.org/training-material/hall-of-fame/hechth/)\n\n**Tutorial Contributor(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Melanie F\u00f6ll](https://training.galaxyproject.org/training-material/hall-of-fame/foellmelanie/), [Julia Jakiela](https://training.galaxyproject.org/training-material/hall-of-fame/wee-snufkin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in tags",
        "id": "1597",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1597?version=1",
        "name": "End-to-end EI+ mass spectra prediction workflow using QCxMS",
        "number_of_steps": 14,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "exposomics",
            "gc-ms",
            "gtn",
            "galaxy",
            "metabolomics",
            "qcxms"
        ],
        "tools": [
            "openbabel_compound_convert",
            "qcxms_production_run",
            "Remove failed runs\n__FILTER_FAILED_DATASETS__",
            "xtb_molecular_optimization",
            "param_value_from_file",
            "qcxms_getres",
            "ctb_im_conformers",
            "tp_cut_tool",
            "tp_cat",
            "split_file_to_collection",
            "qcxms_neutral_run"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Mass spectrometry imaging: Examining the spatial distribution of analytes\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Mass spectrometry imaging: Examining the spatial distribution of analytes](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/msi-analyte-distribution/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Melanie F\u00f6ll](https://training.galaxyproject.org/training-material/hall-of-fame/foellmelanie/), [Maren Stillger](https://training.galaxyproject.org/training-material/hall-of-fame/MarenStillger/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Simon Bray](https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/), [Melanie F\u00f6ll](https://training.galaxyproject.org/training-material/hall-of-fame/foellmelanie/)\n\n**Funder(s)**: [ELIXIR Europe](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/), [University of Freiburg](https://training.galaxyproject.org/training-material/hall-of-fame/uni-freiburg/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in tags",
        "id": "1593",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1593?version=1",
        "name": "MSI Workflow: spatial distribution",
        "number_of_steps": 8,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "metabolomics"
        ],
        "tools": [
            "cardinal_data_exporter",
            "cardinal_filtering",
            "cardinal_spectra_plots",
            "Filter1",
            "cardinal_quality_report",
            "cardinal_mz_images"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "This workflow can be used to assign multi-element molecular formulas to ultrahigh resolution mass spectra.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Molecular formula assignment and mass recalibration with MFAssignR package](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/mfassignr/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: RECETOX, MUNI, Kristina Gomoryova, Helge Hecht\n\n**Tutorial Author(s)**: [Kristina Gomoryova](https://training.galaxyproject.org/training-material/hall-of-fame/KristinaGomoryova/), [Helge Hecht](https://training.galaxyproject.org/training-material/hall-of-fame/hechth/), [Simeon Schum](https://training.galaxyproject.org/training-material/hall-of-fame/skschum/)\n\n**Tutorial Contributor(s)**: [Helge Hecht](https://training.galaxyproject.org/training-material/hall-of-fame/hechth/), [Kristina Gomoryova](https://training.galaxyproject.org/training-material/hall-of-fame/KristinaGomoryova/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in tags",
        "id": "1591",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1591?version=1",
        "name": "Molecular formula assignment and recalibration with MFAssignR package.",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "formulaassignment",
            "gtn",
            "galaxy",
            "metabolomics",
            "noiseestimation",
            "recalibration"
        ],
        "tools": [
            "mfassignr_histnoise",
            "mfassignr_kmdnoise",
            "mfassignr_recallist",
            "mfassignr_mfassign",
            "mfassignr_mfassignCHO",
            "mfassignr_isofiltr",
            "mfassignr_snplot",
            "mfassignr_findRecalSeries",
            "mfassignr_recal"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Mass spectrometry imaging: Finding differential analytes\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Mass spectrometry imaging: Finding differential analytes](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/msi-finding-nglycans/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Melanie F\u00f6ll](https://training.galaxyproject.org/training-material/hall-of-fame/foellmelanie/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in tags",
        "id": "1587",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1587?version=1",
        "name": "MSI Finding Diff Analytes",
        "number_of_steps": 14,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "metabolomics"
        ],
        "tools": [
            "join_files_on_column_fuzzy",
            "cardinal_combine",
            "cardinal_preprocessing",
            "maldi_quant_preprocessing",
            "cardinal_classification",
            "Filter1",
            "cardinal_segmentations",
            "cardinal_quality_report",
            "Summary_Statistics1",
            "cardinal_mz_images",
            "Grep1",
            "maldi_quant_peak_detection"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Mass spectrometry: LC-MS analysis\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Mass spectrometry: LC-MS analysis](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/lcms/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [M\u00e9lanie Petera](https://training.galaxyproject.org/training-material/hall-of-fame/melpetera/), [Gildas Le Corguill\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/lecorguille/), [Jean-Fran\u00e7ois Martin](https://training.galaxyproject.org/training-material/hall-of-fame/jfrancoismartin/), [Yann Guitton](https://training.galaxyproject.org/training-material/hall-of-fame/yguitton/), [Workflow4Metabolomics core team](https://training.galaxyproject.org/training-material/hall-of-fame/workflow4metabolomics/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Gildas Le Corguill\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/lecorguille/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n**Funder(s)**: [ELIXIR Europe](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in tags",
        "id": "1584",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1584?version=1",
        "name": "Workflow Constructed From History 'imported: testpourGCC'",
        "number_of_steps": 17,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "metabolomics"
        ],
        "tools": [
            "msnbase_readmsdata",
            "Univariate",
            "generic_filter",
            "abims_xcms_xcmsSet",
            "abims_CAMERA_annotateDiffreport",
            "quality_metrics",
            "xcms_plot_chromatogram",
            "abims_xcms_retcor",
            "Batch_correction",
            "abims_xcms_fillPeaks",
            "wsdl_hmdb",
            "xcms_merge",
            "abims_xcms_group"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "XCMS and RAMClustR based workflow for data processing and annotation using library matching via matchms.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Mass spectrometry: GC-MS data processing (with XCMS, RAMClustR, RIAssigner, and matchms)](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/gc_ms_with_xcms/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: RECETOX\n\n**Tutorial Author(s)**: [Matej Troj\u00e1k](https://training.galaxyproject.org/training-material/hall-of-fame/xtrojak/), [Helge Hecht](https://training.galaxyproject.org/training-material/hall-of-fame/hechth/), [Maxim Skoryk](https://training.galaxyproject.org/training-material/hall-of-fame/maximskorik/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Matej Troj\u00e1k](https://training.galaxyproject.org/training-material/hall-of-fame/xtrojak/), [M\u00e9lanie Petera](https://training.galaxyproject.org/training-material/hall-of-fame/melpetera/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in tags",
        "id": "1582",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1582?version=1",
        "name": "GC MS using XCMS",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "expospomics",
            "gc-ms",
            "gtn",
            "galaxy",
            "metabolomics"
        ],
        "tools": [
            "msnbase_readmsdata",
            "abims_xcms_xcmsSet",
            "ramclustr",
            "riassigner",
            "abims_xcms_retcor",
            "matchms_similarity",
            "matchms_formatter",
            "msconvert",
            "abims_xcms_fillPeaks",
            "xcms_merge",
            "abims_xcms_group"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Design plasmids encoding predicted pathways by using the BASIC assembly method.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Designing plasmids encoding predicted pathways by using the BASIC assembly method](https://training.galaxyproject.org/training-material/topics/synthetic-biology/tutorials/basic_assembly_analysis/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Kenza Bazi-Kabbaj](https://training.galaxyproject.org/training-material/hall-of-fame/kenza12/), [Thomas Duigou](https://training.galaxyproject.org/training-material/hall-of-fame/tduigou/), [Joan H\u00e9risson](https://training.galaxyproject.org/training-material/hall-of-fame/breakthewall/), [Guillaume Gricourt](https://training.galaxyproject.org/training-material/hall-of-fame/guillaume-gricourt/), [Ioana Popescu](https://training.galaxyproject.org/training-material/hall-of-fame/ioanagry/), [Jean-Loup Faulon](https://training.galaxyproject.org/training-material/hall-of-fame/jfaulon/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "plasmid* in description",
        "id": "1576",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1576?version=1",
        "name": "Genetic Design (BASIC Assembly)",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "synthetic-biology"
        ],
        "tools": [
            "rpbasicdesign",
            "selenzy-wrapper",
            "dnabot"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Starting from the BAM files produced by snippy, generate a table that summarizes the drug-resistance profile for each sample\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Identifying tuberculosis transmission links: from SNPs to transmission clusters](https://training.galaxyproject.org/training-material/topics/evolution/tutorials/mtb_transmission/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Galo A. Goig](https://training.galaxyproject.org/training-material/hall-of-fame/GaloGS/), [Daniela Brites](https://training.galaxyproject.org/training-material/hall-of-fame/dbrites/), [Christoph Stritt](https://training.galaxyproject.org/training-material/hall-of-fame/cstritt/)\n\n**Tutorial Contributor(s)**: [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Galo A. Goig](https://training.galaxyproject.org/training-material/hall-of-fame/GaloGS/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Peter van Heusden](https://training.galaxyproject.org/training-material/hall-of-fame/pvanheus/), [Christoph Stritt](https://training.galaxyproject.org/training-material/hall-of-fame/cstritt/), [Lucille Delisle](https://training.galaxyproject.org/training-material/hall-of-fame/lldelisle/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in name",
        "id": "1564",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1564?version=1",
        "name": "From BAMs to drug resistance prediction with TB-profiler",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "evolution"
        ],
        "tools": [
            "tp_replace_in_line",
            "samtools_view",
            "tp_grep_tool",
            "tp_sed_tool",
            "addName",
            "tb_profiler_profile",
            "Merge single-end and paired-end BAMs in a single collection to be analyzed alltogether\n__MERGE_COLLECTION__",
            "tp_cat"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "## Associated Tutorial\n\nThis workflows is part of the tutorial [Secondary metabolite discovery](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/secondary-metabolite-discovery/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1558",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1558?version=1",
        "name": "Gene Cluster Product Similarity Search",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "sempi3"
        ],
        "tools": [
            "tp_awk_tool",
            "ncbi_acc_download",
            "openbabel_remDuplicates",
            "ctb_np-likeness-calculator",
            "interactive_tool_jupyter_notebook",
            "ctb_chemfp_mol2fps",
            "Remove beginning1",
            "collapse_dataset",
            "antismash",
            "ctb_silicos_qed",
            "ctb_simsearch"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "This workflow generates from only an EBI SCXA reference the metadata for creating an ESet object\n\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Matrix Exchange Format to ESet | Creating a single-cell RNA-seq reference dataset for deconvolution](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/bulk-music-2-preparescref/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Wendi Bacon, Mehmet Tekman\n\n**Tutorial Author(s)**: [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/)\n\n**Tutorial Contributor(s)**: [Marisa Loach](https://training.galaxyproject.org/training-material/hall-of-fame/MarisaJL/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Pablo Moreno](https://training.galaxyproject.org/training-material/hall-of-fame/pcm32/), [Julia Jakiela](https://training.galaxyproject.org/training-material/hall-of-fame/wee-snufkin/), [Pavankumar Videm](https://training.galaxyproject.org/training-material/hall-of-fame/pavanvidem/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in name",
        "id": "1552",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1552?version=1",
        "name": "MuSiC-Deconvolution: Data generation | sc | metadata",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:deconvolution",
            "name:singlecell",
            "name:transcriptomics",
            "name:training"
        ],
        "tools": [
            "add_line_to_file",
            "Cut1",
            "Experimental designs often include extra stuff (likely the barcodes not meeting the EBI pre-processing criteria), so this way we streamline down to only those barcodes in the barcodes file.\njoin1",
            "retrieve_scxa",
            "regex1",
            "tp_cut_tool"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Genome annotation with Maker (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Genome annotation with Maker (short)](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/annotation-with-maker-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1549",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1549?version=1",
        "name": "Genome annotation with Maker (short)",
        "number_of_steps": 8,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "maker_map_ids",
            "gffread",
            "jcvi_gff_stats",
            "jbrowse",
            "maker",
            "fasta-stats",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "This workflow creates an ESet object from scRNA metadata file and EBI SCXA retrieveal\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Matrix Exchange Format to ESet | Creating a single-cell RNA-seq reference dataset for deconvolution](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/bulk-music-2-preparescref/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Uses [subworkflows](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_subworkflows.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Wendi Bacon, Mehmet Tekman\n\n**Tutorial Author(s)**: [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/)\n\n**Tutorial Contributor(s)**: [Marisa Loach](https://training.galaxyproject.org/training-material/hall-of-fame/MarisaJL/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Pablo Moreno](https://training.galaxyproject.org/training-material/hall-of-fame/pcm32/), [Julia Jakiela](https://training.galaxyproject.org/training-material/hall-of-fame/wee-snufkin/), [Pavankumar Videm](https://training.galaxyproject.org/training-material/hall-of-fame/pavanvidem/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1548",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1548?version=1",
        "name": "MuSiC-Deconvolution: Data generation | sc | matrix + ESet",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:singlecell",
            "name:transcriptomics",
            "name:training"
        ],
        "tools": [
            "scanpy_read_10x",
            "music_construct_eset",
            "music_manipulate_eset",
            "anndata_inspect",
            "annotatemyids",
            "retrieve_scxa",
            "171553",
            "datamash_transpose"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Downstream Single-cell RNA analysis with RaceID\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Downstream Single-cell RNA analysis with RaceID](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-raceid/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Mehmet Tekman, Alex Ostrovsky\n\n**Tutorial Author(s)**: [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [Alex Ostrovsky](https://training.galaxyproject.org/training-material/hall-of-fame/astrovsky01/)\n\n**Tutorial Contributor(s)**: [Pavankumar Videm](https://training.galaxyproject.org/training-material/hall-of-fame/pavanvidem/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Simon Bray](https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1547",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1547?version=1",
        "name": "RaceID Workflow",
        "number_of_steps": 8,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "transcriptomics",
            "single-cell"
        ],
        "tools": [
            "raceid_trajectory",
            "raceid_filtnormconf",
            "raceid_inspectclusters",
            "raceid_clustering",
            "raceid_inspecttrajectory"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "A workflow designed to compare chromosomes and study large-scale rearrangements using CHROMEISTER\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [From small to large-scale genome comparison](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/hpc-for-lsgc/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Esteban Perez-Wohlfeil](https://training.galaxyproject.org/training-material/hall-of-fame/estebanpw/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1545",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1545?version=1",
        "name": "CHROMEISTER chromosome comparison",
        "number_of_steps": 1,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "chromeister"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Workflow associated with this training: https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-case_alevin-combine-datasets/tutorial.html\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Combining single cell datasets after pre-processing](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-case_alevin-combine-datasets/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Wendi Bacon\n\n**Tutorial Author(s)**: [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Jonathan Manning](https://training.galaxyproject.org/training-material/hall-of-fame/pinin4fjords/), [Julia Jakiela](https://training.galaxyproject.org/training-material/hall-of-fame/wee-snufkin/), [Pavankumar Videm](https://training.galaxyproject.org/training-material/hall-of-fame/pavanvidem/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Marisa Loach](https://training.galaxyproject.org/training-material/hall-of-fame/MarisaJL/), [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [Pablo Moreno](https://training.galaxyproject.org/training-material/hall-of-fame/pcm32/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in name",
        "id": "1543",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1543?version=1",
        "name": "LOCKED | Combining single cell datasets after pre-processing",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:single-cell",
            "name:training"
        ],
        "tools": [
            "Cut1",
            "tp_replace_in_column",
            "Paste1",
            "anndata_inspect",
            "anndata_manipulate"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "A workflow designed to compare two sequences using GECKO, extract repeats and perform multiple sequence alignment\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [From small to large-scale genome comparison](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/hpc-for-lsgc/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Esteban Perez-Wohlfeil](https://training.galaxyproject.org/training-material/hall-of-fame/estebanpw/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1541",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1541?version=1",
        "name": "GECKO pairwise comparison",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "clustalw",
            "gecko",
            "tp_awk_tool"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "This workflow creates bulk ESet objects from uploaded raw matrix &amp; metadata files\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Bulk matrix to ESet | Creating the bulk RNA-seq dataset for deconvolution](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/bulk-music-3-preparebulk/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Uses [subworkflows](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_subworkflows.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Wendi Bacon, Mehmet Tekman\n\n**Tutorial Author(s)**: [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/)\n\n**Tutorial Contributor(s)**: [Marisa Loach](https://training.galaxyproject.org/training-material/hall-of-fame/MarisaJL/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Pablo Moreno](https://training.galaxyproject.org/training-material/hall-of-fame/pcm32/), [Julia Jakiela](https://training.galaxyproject.org/training-material/hall-of-fame/wee-snufkin/), [Pavankumar Videm](https://training.galaxyproject.org/training-material/hall-of-fame/pavanvidem/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1542",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1542?version=1",
        "name": "MuSiC-Deconvolution: Data generation | bulk | ESet",
        "number_of_steps": 8,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:singlecell",
            "name:transcriptomics",
            "name:training"
        ],
        "tools": [
            "column_remove_by_header",
            "171557",
            "music_construct_eset",
            "music_manipulate_eset",
            "annotatemyids",
            "regex1",
            "tp_cut_tool"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Updated March 2024\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Combining single cell datasets after pre-processing](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-case_alevin-combine-datasets/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Julia Jakiela, Wendi Bacon\n\n**Tutorial Author(s)**: [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Jonathan Manning](https://training.galaxyproject.org/training-material/hall-of-fame/pinin4fjords/), [Julia Jakiela](https://training.galaxyproject.org/training-material/hall-of-fame/wee-snufkin/), [Pavankumar Videm](https://training.galaxyproject.org/training-material/hall-of-fame/pavanvidem/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Marisa Loach](https://training.galaxyproject.org/training-material/hall-of-fame/MarisaJL/), [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [Pablo Moreno](https://training.galaxyproject.org/training-material/hall-of-fame/pcm32/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in name",
        "id": "1539",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1539?version=1",
        "name": "Combining datasets after pre-processing",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:single-cell",
            "name:training"
        ],
        "tools": [
            "anndata_ops",
            "Cut1",
            "tp_replace_in_column",
            "Paste1",
            "anndata_inspect",
            "anndata_manipulate"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "CRISPR screen analysis\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [CRISPR screen analysis](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/crispr-screen/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Maria Doyle](https://training.galaxyproject.org/training-material/hall-of-fame/mblue9/), [Kenji Fujihara](https://training.galaxyproject.org/training-material/hall-of-fame/kenjifujihara/), [Twishi Gulati](https://training.galaxyproject.org/training-material/hall-of-fame/twishigulati/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1534",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1534?version=1",
        "name": "Workflow constructed from history 'CRISPR tutorial Kenji'",
        "number_of_steps": 5,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "cutadapt",
            "mageck_test",
            "fastqc",
            "mageck_count",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Essential genes detection with Transposon insertion sequencing\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Essential genes detection with Transposon insertion sequencing](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/tnseq/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Delphine Lariviere](https://training.galaxyproject.org/training-material/hall-of-fame/delphine-l/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1530",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1530?version=1",
        "name": "Essential genes detection with Transposon insertion sequencing",
        "number_of_steps": 32,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "__EXTRACT_DATASET__",
            "cutadapt",
            "Add_a_column1",
            "bg_find_subsequences",
            "tp_easyjoin_tool",
            "Cut1",
            "bowtie_wrapper",
            "Filter1",
            "transit_gumbel",
            "gff_to_prot",
            "tp_sort_header_tool",
            "deeptools_bam_coverage"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Preparing and filtering gene and cell annotations files and expression matrix to be passed as input for Monocle\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Inferring single cell trajectories with Monocle3](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-case_monocle3-trajectories/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Julia Jakiela](https://training.galaxyproject.org/training-material/hall-of-fame/wee-snufkin/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/), [Pavankumar Videm](https://training.galaxyproject.org/training-material/hall-of-fame/pavanvidem/), [Julia Jakiela](https://training.galaxyproject.org/training-material/hall-of-fame/wee-snufkin/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [Matthias Bernt](https://training.galaxyproject.org/training-material/hall-of-fame/bernt-matthias/), [Pablo Moreno](https://training.galaxyproject.org/training-material/hall-of-fame/pcm32/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n**Grants(s)**: [EPSRC Training Grant DTP 2020-2021 Open University](https://training.galaxyproject.org/training-material/hall-of-fame/epsrc-training-grant/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1520",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1520?version=1",
        "name": "AnnData object to Monocle input files",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:scrna-seq",
            "name:trajectory_analysis",
            "name:transcriptomics"
        ],
        "tools": [
            "regexColumn1",
            "Cut1",
            "join1",
            "datamash_transpose",
            "Double-check the cell_type column number\nFilter1",
            "tp_cut_tool",
            "anndata_inspect"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Workflows for comparison of genes in annotated genomes\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Comparative gene analysis in unannotated genomes](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/gene-centric/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Anton Nekrutenko\n\n**Tutorial Author(s)**: [Anton Nekrutenko](https://training.galaxyproject.org/training-material/hall-of-fame/nekrut/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1518",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1518?version=1",
        "name": "Comparative gene analysis",
        "number_of_steps": 29,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation",
            "vgp"
        ],
        "tools": [
            "Add_a_column1",
            "tp_split_on_column",
            "tab2fasta",
            "Cut1",
            "Get negative strand matches\nFilter1",
            "Remove unnecessary columns\nCut1",
            "Add information about other ORFs in this area. This is done by talking all ORFs in BED format and left joining with coordinates of matched ORFs. As a result we have a sparse table that contains all ORFs surrounding our matches as well as matches themselves. This information is used to generate the final figure.\njoin1",
            "bg_diamond_view",
            "rapidnj",
            "cat1",
            "rbc_mafft",
            "regexColumn1",
            "Filter1",
            "gops_intersect_1",
            "Extract genomic coordinates of matching ORFs \nCut1",
            "Final textual report showing matches, their coordinates and their alignments\nCut1",
            "Get positive strand matches\nFilter1",
            "collapse_dataset",
            "bg_diamond",
            "Set ORF name as the name and frame as score to reestablish BED format\nCut1",
            "Removing unnecessary columns for subsequent processing\nCut1",
            "Join tabular view of alignments with BED description of individual ORFs. This is necessary because to visualize genes we will need genomic coordinates. \njoin1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Trajectory analysis using Monocle3, starting from 3 input files: expression matrix, gene and cell annotations\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Inferring single cell trajectories with Monocle3](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-case_monocle3-trajectories/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Julia Jakiela](https://training.galaxyproject.org/training-material/hall-of-fame/wee-snufkin/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/), [Pavankumar Videm](https://training.galaxyproject.org/training-material/hall-of-fame/pavanvidem/), [Julia Jakiela](https://training.galaxyproject.org/training-material/hall-of-fame/wee-snufkin/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [Matthias Bernt](https://training.galaxyproject.org/training-material/hall-of-fame/bernt-matthias/), [Pablo Moreno](https://training.galaxyproject.org/training-material/hall-of-fame/pcm32/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n**Grants(s)**: [EPSRC Training Grant DTP 2020-2021 Open University](https://training.galaxyproject.org/training-material/hall-of-fame/epsrc-training-grant/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1516",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1516?version=1",
        "name": "Monocle3 workflow",
        "number_of_steps": 17,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:scrna-seq",
            "name:trajectory_analysis",
            "name:transcriptomics"
        ],
        "tools": [
            "monocle3_orderCells",
            "monocle3_preprocess",
            "monocle3_reduceDim",
            "monocle3_partition",
            "monocle3_create",
            "monocle3_topmarkers",
            "monocle3_plotCells",
            "monocle3_diffExp",
            "monocle3_learnGraph"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Bacterial Genome Annotation\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Bacterial Genome Annotation](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/bacterial-genome-annotation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Pierre Marin\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Teresa M\u00fcller](https://training.galaxyproject.org/training-material/hall-of-fame/teresa-m/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n**Funder(s)**: [ABRomics](https://training.galaxyproject.org/training-material/hall-of-fame/abromics/), [ELIXIR Europe](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/), [Institut Fran\u00e7ais de Bioinformatique](https://training.galaxyproject.org/training-material/hall-of-fame/ifb/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1513",
        "keep": true,
        "latest_version": 1,
        "license": "AGPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1513?version=1",
        "name": "Bacterial Genome Annotation",
        "number_of_steps": 10,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "isescan",
            "jbrowse",
            "bakta",
            "tbl2gff3",
            "Grouping1",
            "tp_replace_in_column",
            "integron_finder",
            "plasmidfinder",
            "tp_tail_tool"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Single-cell quality control with scater\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Single-cell quality control with scater](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-scater-qc/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Graham Etherington, Nicola Soranzo, Pavankumar Videm\n\n**Tutorial Author(s)**: [Graham Etherington](https://training.galaxyproject.org/training-material/hall-of-fame/ethering/), [Nicola Soranzo](https://training.galaxyproject.org/training-material/hall-of-fame/nsoranzo/)\n\n**Tutorial Contributor(s)**: [Pavankumar Videm](https://training.galaxyproject.org/training-material/hall-of-fame/pavanvidem/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [Nicola Soranzo](https://training.galaxyproject.org/training-material/hall-of-fame/nsoranzo/), [Graham Etherington](https://training.galaxyproject.org/training-material/hall-of-fame/ethering/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Gildas Le Corguill\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/lecorguille/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Simon Bray](https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1511",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1511?version=1",
        "name": "Single-cell QC with scater",
        "number_of_steps": 5,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "transcriptomics",
            "single-cell"
        ],
        "tools": [
            "scater_filter",
            "scater_create_qcmetric_ready_sce",
            "scater_plot_dist_scatter",
            "scater_plot_pca"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Genome annotation with Maker\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Genome annotation with Maker](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/annotation-with-maker/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Anthony Bretaudeau, French National Institute for Agriculture, Food, and Environment (INRAE)\n\n**Tutorial Author(s)**: [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1504",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1504?version=1",
        "name": "Genome annotation with Maker",
        "number_of_steps": 20,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "maker_map_ids",
            "gffread",
            "jcvi_gff_stats",
            "jbrowse",
            "maker",
            "fasta-stats",
            "snap_training",
            "augustus_training",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Nucleoli segmentation and feature extraction using CellProfiler\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Nucleoli segmentation and feature extraction using CellProfiler](https://training.galaxyproject.org/training-material/topics/imaging/tutorials/tutorial-CP/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Beatriz Serrano-Solano](https://training.galaxyproject.org/training-material/hall-of-fame/beatrizserrano/), [Jean-Karim H\u00e9rich\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/jkh1/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Beatriz Serrano-Solano](https://training.galaxyproject.org/training-material/hall-of-fame/beatrizserrano/), [Anne Fouilloux](https://training.galaxyproject.org/training-material/hall-of-fame/annefou/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in tags",
        "id": "1505",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1505?version=1",
        "name": "CP_pipeline_IDR_training",
        "number_of_steps": 25,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cellprofiler",
            "gtn",
            "galaxy",
            "imaging"
        ],
        "tools": [
            "cp_measure_granularity",
            "cp_measure_object_size_shape",
            "cp_save_images",
            "cp_image_math",
            "cp_mask_image",
            "cp_measure_image_quality",
            "cp_enhance_or_suppress_features",
            "cp_export_to_spreadsheet",
            "cp_relate_objects",
            "cp_measure_texture",
            "cp_convert_objects_to_image",
            "cp_measure_object_intensity",
            "cp_cellprofiler",
            "cp_gray_to_color",
            "cp_measure_image_area_occupied",
            "cp_identify_primary_objects",
            "cp_display_data_on_image",
            "cp_common",
            "idr_download_by_ids",
            "cp_measure_image_intensity"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "example cellprofiler pipeline\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Object tracking using CellProfiler](https://training.galaxyproject.org/training-material/topics/imaging/tutorials/object-tracking-using-cell-profiler/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Yi Sun](https://training.galaxyproject.org/training-material/hall-of-fame/sunyi000/), [Beatriz Serrano-Solano](https://training.galaxyproject.org/training-material/hall-of-fame/beatrizserrano/), [Jean-Karim H\u00e9rich\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/jkh1/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Beatriz Serrano-Solano](https://training.galaxyproject.org/training-material/hall-of-fame/beatrizserrano/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in tags",
        "id": "1503",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1503?version=1",
        "name": "CP_object_tracking_example",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cellprofiler",
            "gtn",
            "galaxy",
            "imaging"
        ],
        "tools": [
            "cp_color_to_gray",
            "cp_identify_primary_objects",
            "cp_common",
            "cp_measure_object_size_shape",
            "cp_tile",
            "cp_overlay_outlines",
            "cp_export_to_spreadsheet",
            "cp_measure_object_intensity",
            "cp_save_images",
            "cp_cellprofiler",
            "unzip",
            "cp_track_objects"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Genome annotation with Prokka\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Genome annotation with Prokka](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/annotation-with-prokka/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Anna Syme, Torsten Seemann, Simon Gladman\n\n**Tutorial Author(s)**: [Anna Syme](https://training.galaxyproject.org/training-material/hall-of-fame/annasyme/), [Torsten Seemann](https://training.galaxyproject.org/training-material/hall-of-fame/tseemann/), [Simon Gladman](https://training.galaxyproject.org/training-material/hall-of-fame/slugger70/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1497",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1497?version=1",
        "name": "Genome Annotation with Prokka",
        "number_of_steps": 2,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "jbrowse",
            "prokka"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Nanopore datasets analysis - Phylogenetic Identification - antibiotic resistance genes detection and contigs building\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pathogen detection from (direct Nanopore) sequencing data using Galaxy - Foodborne Edition](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Engy Nasr, B\u00e9r\u00e9nice Batut, Paul Zierep\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [Hans-Rudolf Hotz](https://training.galaxyproject.org/training-material/hall-of-fame/hrhotz/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EOSC-Life](https://training.galaxyproject.org/training-material/hall-of-fame/eosc-life/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in description",
        "id": "1495",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1495?version=1",
        "name": "Gene-based Pathogen Identification",
        "number_of_steps": 15,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:iwc",
            "name:microgalaxy",
            "name:pathogfair",
            "name:collection"
        ],
        "tools": [
            "compose_text_param",
            "medaka_consensus_pipeline",
            "tab2fasta",
            "bandage_image",
            "abricate",
            "collection_element_identifiers",
            "tp_find_and_replace",
            "flye",
            "param_value_from_file",
            "__BUILD_LIST__",
            "fasta2tab",
            "split_file_to_collection"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Pre-processing of Single-Cell RNA Data\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pre-processing of Single-Cell RNA Data](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-preprocessing/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Anika Erxleben](https://training.galaxyproject.org/training-material/hall-of-fame/erxleben/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Nicola Soranzo](https://training.galaxyproject.org/training-material/hall-of-fame/nsoranzo/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Gildas Le Corguill\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/lecorguille/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Simon Bray](https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/), [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/), [Martin \u010cech](https://training.galaxyproject.org/training-material/hall-of-fame/martenson/), [Armin Dadras](https://training.galaxyproject.org/training-material/hall-of-fame/dadrasarmin/)\n\n**Funder(s)**: [de.NBI](https://training.galaxyproject.org/training-material/hall-of-fame/deNBI/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1494",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1494?version=1",
        "name": "CelSeq2: Single Batch (mm10)",
        "number_of_steps": 5,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "transcriptomics"
        ],
        "tools": [
            "bamFilter",
            "umi_tools_extract",
            "featurecounts",
            "rna_star",
            "umi_tools_count"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Pre-processing of Single-Cell RNA Data\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pre-processing of Single-Cell RNA Data](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-preprocessing/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Uses [subworkflows](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_subworkflows.html)\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Anika Erxleben](https://training.galaxyproject.org/training-material/hall-of-fame/erxleben/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Nicola Soranzo](https://training.galaxyproject.org/training-material/hall-of-fame/nsoranzo/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Mehmet Tekman](https://training.galaxyproject.org/training-material/hall-of-fame/mtekman/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Gildas Le Corguill\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/lecorguille/), [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Simon Bray](https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/), [Wendi Bacon](https://training.galaxyproject.org/training-material/hall-of-fame/nomadscientist/), [Martin \u010cech](https://training.galaxyproject.org/training-material/hall-of-fame/martenson/), [Armin Dadras](https://training.galaxyproject.org/training-material/hall-of-fame/dadrasarmin/)\n\n**Funder(s)**: [de.NBI](https://training.galaxyproject.org/training-material/hall-of-fame/deNBI/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1491",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1491?version=1",
        "name": "CelSeq2: Multi Batch (mm10)",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "transcriptomics"
        ],
        "tools": [
            "__FLATTEN__",
            "collection_column_join",
            "0600e24fe786d72f"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Identification of AMR genes in an assembled bacterial genome\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Identification of AMR genes in an assembled bacterial genome](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/amr-gene-detection/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Bazante Sanders, B\u00e9r\u00e9nice Batut\n\n**Tutorial Author(s)**: [Bazante Sanders](https://training.galaxyproject.org/training-material/hall-of-fame/bazante1/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Bazante Sanders](https://training.galaxyproject.org/training-material/hall-of-fame/bazante1/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Miaomiao Zhou](https://training.galaxyproject.org/training-material/hall-of-fame/miaomiaozhou88/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [pimarin](https://training.galaxyproject.org/training-material/hall-of-fame/pimarin/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n**Funder(s)**: [Avans Hogeschool](https://training.galaxyproject.org/training-material/hall-of-fame/avans-atgm/), [ABRomics](https://training.galaxyproject.org/training-material/hall-of-fame/abromics/), [ELIXIR Europe](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1488",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1488?version=1",
        "name": "mrsa AMR gene detection",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microgalaxy",
            "genome-annotation"
        ],
        "tools": [
            "jbrowse",
            "bakta",
            "tbl2gff3",
            "bowtie2",
            "Grep1",
            "staramr_search"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Data Manipulation Olympics\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Data Manipulation Olympics](https://training.galaxyproject.org/training-material/topics/introduction/tutorials/data-manipulation-olympics/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Yongbin Li](https://training.galaxyproject.org/training-material/hall-of-fame/lybCNU/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Scott Cain](https://training.galaxyproject.org/training-material/hall-of-fame/scottcain/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Yongbin Li](https://training.galaxyproject.org/training-material/hall-of-fame/lybCNU/), [Donny Vrins](https://training.galaxyproject.org/training-material/hall-of-fame/dirowa/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Daniela Schneider](https://training.galaxyproject.org/training-material/hall-of-fame/Sch-Da/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1486",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1486?version=1",
        "name": "GTN Tutorial: Data manipulation Olympics - all steps and exercises",
        "number_of_steps": 59,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "introduction"
        ],
        "tools": [
            "Add_a_column1",
            "regexColumn1",
            "tp_split_on_column",
            "Count1",
            "Show beginning1",
            "Cut1",
            "tabular_to_csv",
            "Filter1",
            "datamash_ops",
            "join1",
            "Remove beginning1",
            "tp_cat",
            "wc_gnu",
            "cat1",
            "tp_sort_header_tool",
            "tp_sorted_uniq"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Long non-coding RNAs (lncRNAs) annotation with FEELnc\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Long non-coding RNAs (lncRNAs) annotation with FEELnc](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/lncrna/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: St\u00e9phanie Robin\n\n**Tutorial Author(s)**: [St\u00e9phanie Robin](https://training.galaxyproject.org/training-material/hall-of-fame/stephanierobin/)\n\n**Tutorial Contributor(s)**: [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Teresa M\u00fcller](https://training.galaxyproject.org/training-material/hall-of-fame/teresa-m/), [Romane LIBOUBAN](https://training.galaxyproject.org/training-material/hall-of-fame/rlibouba/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1484",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1484?version=1",
        "name": "Long non-coding RNAs (lncRNAs) annotation with FEELnc",
        "number_of_steps": 4,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "genome-annotation"
        ],
        "tools": [
            "feelnc",
            "stringtie",
            "gffread",
            "tp_cat"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Microbiome - Taxonomy Profiling\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pathogen detection from (direct Nanopore) sequencing data using Galaxy - Foodborne Edition](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Engy Nasr, B\u00e9r\u00e9nice Batut, Paul Zierep\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [Hans-Rudolf Hotz](https://training.galaxyproject.org/training-material/hall-of-fame/hrhotz/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EOSC-Life](https://training.galaxyproject.org/training-material/hall-of-fame/eosc-life/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in name",
        "id": "1483",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1483?version=1",
        "name": "Taxonomy Profiling and Visualization with Krona",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:iwc",
            "name:microgalaxy",
            "name:pathogfair",
            "name:collection"
        ],
        "tools": [
            "taxonomy_krona_chart",
            "kraken2",
            "krakentools_kreport2krona"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Interpreting MaxQuant data using MSstats involves applying a rigorous statistical framework to glean meaningful insights from quantitative proteomic datasets\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 5: Data Interpretation](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/clinical-mp-5-data-interpretation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1482",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1482?version=1",
        "name": "WF5_Data_Interpretation_Worklow",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "msstatstmt",
            "unipept",
            "Grep1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "## Associated Tutorial\n\nThis workflows is part of the tutorial [Assembly of metagenomic sequencing data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metagenomics-assembly/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Tutorial Contributor(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "1480",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1480?version=1",
        "name": "workflow-generate-dataset-for-assembly-tutorial",
        "number_of_steps": 18,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy"
        ],
        "tools": [
            "cutadapt",
            "random_lines1",
            "bamtools",
            "filter_tabular",
            "ngsutils_bam_filter",
            "fastqc",
            "megahit",
            "bowtie2",
            "bg_uniq",
            "seqtk_subseq",
            "tp_cat"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Microbiome - Variant calling and Consensus Building\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pathogen detection from (direct Nanopore) sequencing data using Galaxy - Foodborne Edition](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Engy Nasr, B\u00e9r\u00e9nice Batut, Paul Zierep\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [Hans-Rudolf Hotz](https://training.galaxyproject.org/training-material/hall-of-fame/hrhotz/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EOSC-Life](https://training.galaxyproject.org/training-material/hall-of-fame/eosc-life/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in description",
        "id": "1479",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1479?version=1",
        "name": "Allele-based Pathogen Identification",
        "number_of_steps": 23,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:iwc",
            "name:microgalaxy",
            "name:pathogfair",
            "name:collection"
        ],
        "tools": [
            "snpSift_filter",
            "regexColumn1",
            "Count1",
            "minimap2",
            "Cut1",
            "CONVERTER_gz_to_uncompressed",
            "tp_head_tool",
            "bcftools_norm",
            "samtools_depth",
            "samtools_coverage",
            "Remove beginning1",
            "table_compute",
            "collapse_dataset",
            "Paste1",
            "clair3",
            "tp_cut_tool",
            "bcftools_consensus",
            "snpSift_extractFields"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Antibiotic resistance detection\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Antibiotic resistance detection](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/plasmid-metagenomics-nanopore/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Willem de Koning](https://training.galaxyproject.org/training-material/hall-of-fame/willemdek11/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1477",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1477?version=1",
        "name": "Copy Of GTN Training - Antibiotic Resistance Detection",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "nanoplot",
            "unicycler",
            "racon",
            "gfa_to_fa",
            "minimap2",
            "miniasm",
            "bandage_image",
            "PlasFlow",
            "staramr_search"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Analyses of metagenomics data - The global picture\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Analyses of metagenomics data - The global picture](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/general-tutorial/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1476",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1476?version=1",
        "name": "Amplicon Tutorial",
        "number_of_steps": 17,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_screen_seqs",
            "mothur_filter_seqs",
            "mothur_make_shared",
            "mothur_classify_otu",
            "mothur_make_biom",
            "mothur_summary_seqs",
            "mothur_cluster_split",
            "krona-text",
            "mothur_align_seqs",
            "mothur_count_seqs",
            "mothur_make_group",
            "mothur_merge_files",
            "mothur_pre_cluster",
            "mothur_unique_seqs",
            "mothur_classify_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Generating a large database and then reducing it to a compact database using Metanovo\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 1: Database-Generation](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/clinical-mp-1-database-generation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1474",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1474?version=1",
        "name": "WF1_Database_Generation_Workflow",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "metanovo",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S rRNA analysis with Nanopore reads\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial analysis with Nanopore data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/nanopore-16S-metagenomics/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1473",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1473?version=1",
        "name": "Training: 16S rRNA Analysis with Nanopore Sequencing Reads",
        "number_of_steps": 11,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "fastp",
            "tp_replace_in_line",
            "datamash_reverse",
            "fastqc",
            "porechop",
            "Remove beginning1",
            "taxonomy_krona_chart",
            "kraken2",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Analyses of metagenomics data - The global picture\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Analyses of metagenomics data - The global picture](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/general-tutorial/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1472",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1472?version=1",
        "name": "WGS Part In \"Analyses Of Metagenomics Data - The Global Picture\"",
        "number_of_steps": 7,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "metaphlan2",
            "humann2_regroup_table",
            "taxonomy_krona_chart",
            "humann2",
            "metaphlan2krona",
            "humann2_renorm_table"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "encyclopedia- DIA Metaproteomics\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [EncyclopeDIA](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/encyclopedia/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/), [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1471",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1471?version=1",
        "name": "EncyclopeDIA-GTN",
        "number_of_steps": 4,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "proteomics"
        ],
        "tools": [
            "msconvert",
            "encyclopedia_quantify",
            "encyclopedia_searchtolib"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "This workflow performs taxonomic profiling of metagenomic data and visualizes microbial community composition using Kraken2 and Bracken as well as MetaPhlAn.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Taxonomic Profiling and Visualization of Metagenomic Data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/taxonomic-profiling/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Tarnima Omara\n\n**Tutorial Author(s)**: [Sophia Hampe](https://training.galaxyproject.org/training-material/hall-of-fame/sophia120199/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [Tarnima Omara](https://training.galaxyproject.org/training-material/hall-of-fame/Tarnima-Omara/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Teresa M\u00fcller](https://training.galaxyproject.org/training-material/hall-of-fame/teresa-m/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1470",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1470?version=1",
        "name": "Taxonomic Profiling and Visualization of Metagenomic Data",
        "number_of_steps": 10,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "__UNZIP_COLLECTION__",
            "krakentools_kreport2krona",
            "metaphlan",
            "interactive_tool_phinch",
            "taxonomy_krona_chart",
            "kraken2",
            "interactive_tool_pavian",
            "kraken_biom",
            "est_abundance"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Quantification using the MaxQuant tool\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 4: Quantitation](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/clinical-mp-4-quantitation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1468",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1468?version=1",
        "name": "WF4_Quantitation_Workflow",
        "number_of_steps": 7,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "maxquant",
            "Grep1",
            "Cut1",
            "Grouping1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "This workflow query metagenomic raw data against a metaplasmidome database to identify plasmids and annotate them with genes, KO, PFAM\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Query an annotated mobile genetic element database to identify and annotate genetic elements (e.g. plasmids) in metagenomics data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metaplasmidome_query/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Nadia Gou\u00e9\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Nadia Gou\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/nagoue/), [Didier Debroas](https://training.galaxyproject.org/training-material/hall-of-fame/debroas/)\n\n**Tutorial Contributor(s)**: [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1469",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1469?version=1",
        "name": "Query a metaplasmidome database to identify and annotate plasmids in metagenomes",
        "number_of_steps": 47,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "metagenomics",
            "metaplasmidome",
            "name:microgalaxy"
        ],
        "tools": [
            "CONVERTER_fasta_to_tabular",
            "Add_a_column1",
            "tab2fasta",
            "minimap2",
            "Cut1",
            "Filter1",
            "join1",
            "Grouping1",
            "histogram_rpy",
            "add_column_headers",
            "MQoutputfilter",
            "count_gff_features",
            "tp_replace_in_column",
            "tp_tail_tool",
            "cat1",
            "sort1",
            "tp_sorted_uniq",
            "ggplot2_histogram"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Proteogenomics 3: Novel peptide analysis\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Proteogenomics 3: Novel peptide analysis](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/proteogenomics-novel-peptide-analysis/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Ray Sajulga](https://training.galaxyproject.org/training-material/hall-of-fame/jraysajulga/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1467",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1467?version=1",
        "name": "GTN Proteogemics3 Novel Peptide Analysis",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "proteomics"
        ],
        "tools": [
            "ncbi_blastp_wrapper",
            "pep_pointer",
            "peptide_genomic_coordinate",
            "query_tabular"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (extended)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (extended)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1465",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1465?version=1",
        "name": "Training: 16S rRNA Sequencing With Mothur: Main Tutorial",
        "number_of_steps": 38,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_summary_single",
            "mothur_filter_seqs",
            "mothur_make_biom",
            "mothur_seq_error",
            "mothur_cluster",
            "mothur_classify_otu",
            "mothur_cluster_split",
            "mothur_screen_seqs",
            "mothur_chimera_vsearch",
            "mothur_remove_lineage",
            "mothur_rarefaction_single",
            "mothur_venn",
            "mothur_heatmap_sim",
            "mothur_sub_sample",
            "mothur_make_contigs",
            "taxonomy_krona_chart",
            "mothur_pre_cluster",
            "mothur_taxonomy_to_krona",
            "mothur_unique_seqs",
            "mothur_count_groups",
            "mothur_align_seqs",
            "mothur_dist_shared",
            "mothur_summary_seqs",
            "newick_display",
            "mothur_make_shared",
            "mothur_classify_seqs",
            "mothur_remove_seqs",
            "mothur_dist_seqs",
            "mothur_count_seqs",
            "mothur_tree_shared",
            "mothur_remove_groups",
            "mothur_get_groups",
            "XY_Plot_1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "WF3- Peptide verification/validaiton workflow\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 3: Verification](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/clinical-mp-3-verification/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1464",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1464?version=1",
        "name": "WF3_VERIFICATION_WORKFLOW",
        "number_of_steps": 19,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "dbbuilder",
            "uniprotxml_downloader",
            "query_tabular",
            "Cut1",
            "Filter1",
            "Remove beginning1",
            "Grouping1",
            "collapse_dataset",
            "pepquery2",
            "tp_cat",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Generating a large database and then reducing it to a compact database using Metanovo\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 1: Database-Generation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-1-database-generation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1461",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1461?version=1",
        "name": "WF1_Database_Generation_Workflow",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "metanovo",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Workflow for running LotuS2 tool on fungal ITS paired-end sequencing data, to identify the fungi present in the samples\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Identifying Mycorrhizal Fungi from ITS2 sequencing using LotuS2](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/lotus2-identifying-fungi/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Society for the Protection of Underground Networks, Sujai Kumar, Bethan Manley\n\n**Tutorial Author(s)**: [Sujai Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/sujaikumar/)\n\n**Tutorial Contributor(s)**: [Bethan Manley](https://training.galaxyproject.org/training-material/hall-of-fame/bethanmanley/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Sujai Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/sujaikumar/)\n\n**Funder(s)**: [Society for the Protection of Underground Networks](https://training.galaxyproject.org/training-material/hall-of-fame/societyprotectionundergroundnetworks/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1460",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1460?version=1",
        "name": "Workflow for Identifying MF from ITS2 sequencing using LotuS2 - tutorial example run'",
        "number_of_steps": 1,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "ecology",
            "fungi",
            "gtn",
            "galaxy",
            "lotus2",
            "metagenomics"
        ],
        "tools": [
            "lotus2"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "metaquantome-taxonomy\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [metaQuantome 3: Taxonomy](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/metaquantome-taxonomy/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Marie Crane](https://training.galaxyproject.org/training-material/hall-of-fame/mariecrane/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in name",
        "id": "1459",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1459?version=1",
        "name": "metaquantome-taxonomy-workflow",
        "number_of_steps": 10,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "proteomics"
        ],
        "tools": [
            "metaquantome_sample",
            "metaquantome_viz",
            "metaquantome_db",
            "metaquantome_filter",
            "metaquantome_expand",
            "metaquantome_stat"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Annotating the novel peptides\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 5: Variant Annotation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-5-variant-annotation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "1457",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1457?version=1",
        "name": "GigaScience_Peptide_Annotation_demonstration_STS26T_neoantigen_candidates_workflow",
        "number_of_steps": 11,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:neoantigen"
        ],
        "tools": [
            "regexColumn1",
            "query_tabular",
            "tab2fasta",
            "converting pipes to columns\nConvert characters1",
            "converting colons to columns\nConvert characters1",
            "pep_pointer"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "metaquantome-function\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [metaQuantome 2: Function](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/metaquantome-function/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Marie Crane](https://training.galaxyproject.org/training-material/hall-of-fame/mariecrane/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in name",
        "id": "1453",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1453?version=1",
        "name": "metaquantome-function-worklow",
        "number_of_steps": 10,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "proteomics"
        ],
        "tools": [
            "metaquantome_sample",
            "metaquantome_viz",
            "metaquantome_db",
            "metaquantome_filter",
            "metaquantome_expand",
            "metaquantome_stat"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "metaquantome-data-creation\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [metaQuantome 1: Data creation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/metaquantome-data-creation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Marie Crane](https://training.galaxyproject.org/training-material/hall-of-fame/mariecrane/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in name",
        "id": "1450",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1450?version=1",
        "name": "metaQuantome_datacreation_workflow",
        "number_of_steps": 16,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "proteomics"
        ],
        "tools": [
            "tp_replace_in_line",
            "search_gui",
            "query_tabular",
            "flashlfq",
            "Cut1",
            "tp_replace_in_column",
            "Remove beginning1",
            "Filter1",
            "unipept",
            "peptide_shaker",
            "msconvert",
            "regex1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Metaproteomics tutorial\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metaproteomics tutorial](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/metaproteomics/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/), [Clemens Blank](https://training.galaxyproject.org/training-material/hall-of-fame/blankclemens/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in name",
        "id": "1443",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1443?version=1",
        "name": "Metaproteomics_GTN",
        "number_of_steps": 10,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "proteomics"
        ],
        "tools": [
            "search_gui",
            "query_tabular",
            "unipept",
            "peptide_shaker",
            "sqlite_to_tabular"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Create a protein Fusion database through the Arriba workflow\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 1: Fusion-Database-Generation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-1-fusion-database-generation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1442",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1442?version=1",
        "name": "Gigascience_Fusions_demonstration_STS26T-Gent_Workflow",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:neoantigen"
        ],
        "tools": [
            "Uncompressed RNA-Seq forward reads\nCONVERTER_gz_to_uncompressed",
            "Uncompressed RNA-Seq reverse reads\nCONVERTER_gz_to_uncompressed",
            "tp_awk_tool",
            "query_tabular",
            "tab2fasta",
            "rna_star",
            "regex1",
            "arriba_get_filters",
            "arriba"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Identification of the micro-organisms in a beer using Nanopore sequencing\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Identification of the micro-organisms in a beer using Nanopore sequencing](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/beer-data-analysis/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Teresa M\u00fcller, Polina Polunina\n\n**Tutorial Author(s)**: [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [Siyu Chen](https://training.galaxyproject.org/training-material/hall-of-fame/chensy96/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Teresa M\u00fcller](https://training.galaxyproject.org/training-material/hall-of-fame/teresa-m/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Teresa M\u00fcller](https://training.galaxyproject.org/training-material/hall-of-fame/teresa-m/), [Siyu Chen](https://training.galaxyproject.org/training-material/hall-of-fame/chensy96/), [Nuwan Goonasekera](https://training.galaxyproject.org/training-material/hall-of-fame/nuwang/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1439",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1439?version=1",
        "name": "Identification of the micro-organisms in a beer using Nanopore sequencing",
        "number_of_steps": 8,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "fastp",
            "krakentools_kreport2krona",
            "fastqc",
            "porechop",
            "Filter1",
            "taxonomy_krona_chart",
            "kraken2"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Generating non-reference protein database for FragPipe discovery\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 2: Non-Reference-Database-Generation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-2-non-reference-database-generation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1437",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1437?version=1",
        "name": "Gigascience_Indels_SAV_non-reference_demonstration_STS26T-Gent_Workflow",
        "number_of_steps": 33,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:neoantigen"
        ],
        "tools": [
            "Uncompressed_RNA_Seq_Reads_1\nCONVERTER_gz_to_uncompressed",
            "filter_tabular",
            "regexColumn1",
            "sqlite_to_tabular",
            "query_tabular",
            "tab2fasta",
            "freebayes",
            "stringtie",
            "tp_cat",
            "gffcompare_to_bed",
            "custom_pro_db",
            "Uncompressed_RNA_Seq_Reads_2\nCONVERTER_gz_to_uncompressed",
            "translate_bed",
            "gffcompare",
            "hisat2",
            "fasta2tab",
            "bed_to_protein_map",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Discovery workflow with SG/PS and MaxQuant to generate microbial peptides\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 2: Discovery](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/clinical-mp-2-discovery/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1435",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1435?version=1",
        "name": "WF2_Discovery-Workflow",
        "number_of_steps": 24,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "fasta_merge_files_and_filter_unique_sequences",
            "filter_tabular",
            "search_gui",
            "query_tabular",
            "Grep1",
            "maxquant",
            "Cut1",
            "peptide_shaker",
            "Filter1",
            "Remove beginning1",
            "Grouping1",
            "msconvert",
            "tp_cat",
            "ident_params",
            "fasta_cli",
            "fasta2tab",
            "dbbuilder"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Interpreting MaxQuant data using MSstats involves applying a rigorous statistical framework to glean meaningful insights from quantitative proteomic datasets\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 5: Data Interpretation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-5-data-interpretation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1433",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1433?version=1",
        "name": "WF5_Data_Interpretation_Worklow",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "msstatstmt",
            "unipept",
            "Grep1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Calculating diversity from bracken output\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Calculating \u03b1 and \u03b2 diversity from microbiome taxonomic data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/diversity/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Paul Zierep, Sophia Hampe, B\u00e9r\u00e9nice Batut\n\n**Tutorial Author(s)**: [Sophia Hampe](https://training.galaxyproject.org/training-material/hall-of-fame/sophia120199/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Teresa M\u00fcller](https://training.galaxyproject.org/training-material/hall-of-fame/teresa-m/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Sophia Hampe](https://training.galaxyproject.org/training-material/hall-of-fame/sophia120199/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in name",
        "id": "1431",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1431?version=1",
        "name": "Calculating diversity from microbiome taxonomic data",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:gtn"
        ],
        "tools": [
            "krakentools_beta_diversity",
            "krakentools_alpha_diversity"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Prediction of HLA binding for verified candidates\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 6: Predicting HLA Binding](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-6-predicting-hla-binding/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: galaxyp\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1430",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1430?version=1",
        "name": "GigaScience-RNAseq-Optitype-seq2HLA-to-IEDB-alleles",
        "number_of_steps": 4,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:neoantigen",
            "hla"
        ],
        "tools": [
            "optitype",
            "query_tabular",
            "tp_awk_tool",
            "seq2hla"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "## Associated Tutorial\n\nThis workflows is part of the tutorial [Proteogenomics 2: Database Search](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/proteogenomics-dbsearch/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Ray Sajulga](https://training.galaxyproject.org/training-material/hall-of-fame/jraysajulga/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/), [Delphine Lariviere](https://training.galaxyproject.org/training-material/hall-of-fame/delphine-l/)\n\n**Tutorial Contributor(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Nicola Soranzo](https://training.galaxyproject.org/training-material/hall-of-fame/nsoranzo/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Melanie F\u00f6ll](https://training.galaxyproject.org/training-material/hall-of-fame/foellmelanie/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1429",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1429?version=1",
        "name": "Proteogenomics 2: Database Search",
        "number_of_steps": 7,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy"
        ],
        "tools": [
            "search_gui",
            "query_tabular",
            "tab2fasta",
            "ident_params",
            "peptide_shaker",
            "mz_to_sqlite"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1428",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1428?version=1",
        "name": "Workflow 3: Classification [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_summary_seqs",
            "mothur_remove_lineage",
            "mothur_classify_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "WF3- Peptide verification/validaiton workflow\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 3: Verification](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-3-verification/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1425",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1425?version=1",
        "name": "WF3_VERIFICATION_WORKFLOW",
        "number_of_steps": 19,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "dbbuilder",
            "uniprotxml_downloader",
            "query_tabular",
            "Cut1",
            "Filter1",
            "Remove beginning1",
            "Grouping1",
            "collapse_dataset",
            "pepquery2",
            "tp_cat",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1422",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1422?version=1",
        "name": "Workflow 1: Quality Control [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 5,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_summary_seqs",
            "mothur_screen_seqs",
            "mothur_count_seqs",
            "mothur_unique_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Quantification using the MaxQuant tool\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 4: Quantitation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-4-quantitation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1420",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1420?version=1",
        "name": "WF4_Quantitation_Workflow",
        "number_of_steps": 7,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "maxquant",
            "Grep1",
            "Cut1",
            "Grouping1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "This workflow will create a simple plot of a microbial sized genome (e.g. E. coli) using a couple of datasets like sequencing depth (bigwigs), gff3 formatted annotations, and some variants.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Ploting a Microbial Genome with Circos](https://training.galaxyproject.org/training-material/topics/visualisation/tutorials/circos-microbial/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Helena Rasche\n\n**Tutorial Author(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Tutorial Contributor(s)**: [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1419",
        "keep": true,
        "latest_version": 1,
        "license": "AGPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1419?version=1",
        "name": "Circos for E. Coli",
        "number_of_steps": 10,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:circos",
            "name:viz"
        ],
        "tools": [
            "Cut1",
            "circos_interval_to_text",
            "circos_gc_skew",
            "circos_interval_to_tile",
            "gff2bed1",
            "circos",
            "circos_wiggle_to_scatter"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1418",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1418?version=1",
        "name": "Workflow7: Beta Diversity [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_venn",
            "mothur_heatmap_sim",
            "mothur_dist_shared",
            "collapse_dataset",
            "mothur_tree_shared",
            "newick_display"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "version 1.0, 160318, published at https://github.com/Stortebecker/secretome_prediction\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Secretome Prediction](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/secretome-prediction/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Florian Christoph Sigloch](https://training.galaxyproject.org/training-material/hall-of-fame/stortebecker/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n**Tutorial Contributor(s)**: [Nicola Soranzo](https://training.galaxyproject.org/training-material/hall-of-fame/nsoranzo/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Melanie F\u00f6ll](https://training.galaxyproject.org/training-material/hall-of-fame/foellmelanie/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [William Durand](https://training.galaxyproject.org/training-material/hall-of-fame/willdurand/), [Clemens Blank](https://training.galaxyproject.org/training-material/hall-of-fame/blankclemens/), [Florian Christoph Sigloch](https://training.galaxyproject.org/training-material/hall-of-fame/stortebecker/), [Niall Beard](https://training.galaxyproject.org/training-material/hall-of-fame/njall/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Martin \u010cech](https://training.galaxyproject.org/training-material/hall-of-fame/martenson/), [Armin Dadras](https://training.galaxyproject.org/training-material/hall-of-fame/dadrasarmin/)\n\n**Funder(s)**: [ELIXIR Europe](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/), [de.NBI](https://training.galaxyproject.org/training-material/hall-of-fame/deNBI/), [University of Freiburg](https://training.galaxyproject.org/training-material/hall-of-fame/uni-freiburg/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "1417",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1417?version=1",
        "name": "Secreted Proteins Via GO Annotation And WoLF PSORT For shCTSB Paper",
        "number_of_steps": 28,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "proteomics"
        ],
        "tools": [
            "tp_easyjoin_tool",
            "uniprot",
            "GO:0005764 \"lysosome\"\nget_subontology_from",
            "tp_cut_tool",
            "Filters all proteins that are predicted to reside in lysosomes.\nFilter1",
            "GO:0043230 \"extracellular organelle\"\nget_subontology_from",
            "Extracts uniprot accessions from all proteins in the input list that are predicted by WolfPsort to be lysosomal and/or extracellular.\nConvert characters1",
            "Removes comment lines at the head of the Uniprot Go database file.\nGrep1",
            "wolf_psort",
            "GO:0009986 \"cell surface\"\nterm_id_vs_term_name",
            "GO:0005576 \"extracellular region\"\nterm_id_vs_term_name",
            "GO:0005887 \"integral component of plasma membrane\"\nget_subontology_from",
            "GO:0005887 \"integral component of plasma membrane\"\nterm_id_vs_term_name",
            "bg_uniq",
            "GO:0005764 \"lysosome\"\nterm_id_vs_term_name",
            "GO:0009986 \"cell surface\"\nget_subontology_from",
            "GO:0005576 \"extracellular region\"\nget_subontology_from",
            "GO:0043230 \"extracellular organelle\"\nterm_id_vs_term_name",
            "filters out all proteins in the input protein list of interest that are annotated as \"extracellular region\" and \"lysosome\" and \"plasma membrane\" excluding \"extracellular organelle\" and \"cytoplasmic side of plasma membrane\"\ncomp1",
            "Filters all proteins that are predicted to reside extracellularly.\nFilter1",
            "tp_sorted_uniq"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Validate the NeoAntigen Candidates from FragPipe discovery through the PepQuery Novel search\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 4: PepQuery2 Verification](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-4-peptide-verification/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1416",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1416?version=1",
        "name": "GigaScience_PepQuery2_demonstration_STS26T_neoantigen_candidates_workflow",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:neoantigen"
        ],
        "tools": [
            "query_tabular",
            "tab2fasta",
            "ncbi_blastp_wrapper",
            "pepquery2",
            "msconvert"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Annotating a protein list identified by LC-MS/MS experiments\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Annotating a protein list identified by LC-MS/MS experiments](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/proteome_annotation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Valentin Loux](https://training.galaxyproject.org/training-material/hall-of-fame/vloux/), [Florence Combes](https://training.galaxyproject.org/training-material/hall-of-fame/combesf/), [David Christiany](https://training.galaxyproject.org/training-material/hall-of-fame/davidchristiany/), [Yves Vandenbrouck](https://training.galaxyproject.org/training-material/hall-of-fame/yvandenb/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "1414",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1414?version=1",
        "name": "'Proteome Annotation'",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "proteomics"
        ],
        "tools": [
            "Jvenn",
            "IDconverter",
            "rna_abbased_data",
            "retrieve_from_hpa",
            "cluter_profiler",
            "MQoutputfilter",
            "reactome_analysis"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1412",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1412?version=1",
        "name": "Workflow 6: Alpha Diversity [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_summary_single",
            "mothur_rarefaction_single",
            "XY_Plot_1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Merging Fusion and non-normal databases + Discovery peptidomics using FragPipe\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 3: Database merge and FragPipe discovery](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-3-fragpipe-discovery/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1410",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1410?version=1",
        "name": "GigaScience_Database_merge_FragPipe_STS26T_demonstration",
        "number_of_steps": 7,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:neoantigen"
        ],
        "tools": [
            "query_tabular",
            "validate_fasta_database",
            "Remove beginning1",
            "collapse_dataset",
            "removing anything that matches _HUMAN\nGrep1",
            "fragpipe",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Annotating a protein list identified by LC-MS/MS experiments\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Annotating a protein list identified by LC-MS/MS experiments](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/proteome_annotation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Valentin Loux](https://training.galaxyproject.org/training-material/hall-of-fame/vloux/), [Florence Combes](https://training.galaxyproject.org/training-material/hall-of-fame/combesf/), [David Christiany](https://training.galaxyproject.org/training-material/hall-of-fame/davidchristiany/), [Yves Vandenbrouck](https://training.galaxyproject.org/training-material/hall-of-fame/yvandenb/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "1411",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1411?version=1",
        "name": "ProteoRE ProteomeAnnotation Tutorial (release 2.0)",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "proteomics"
        ],
        "tools": [
            "Jvenn",
            "IDconverter",
            "rna_abbased_data",
            "retrieve_from_hpa",
            "reactome_analysis",
            "MQoutputfilter",
            "cluter_profiler"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1408",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1408?version=1",
        "name": "Workflow 4: Mock OTU Clustering [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 4,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_dist_seqs",
            "mothur_rarefaction_single",
            "mothur_make_shared",
            "mothur_cluster"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "## Associated Tutorial\n\nThis workflows is part of the tutorial [Proteogenomics 1: Database Creation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/proteogenomics-dbcreation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Ray Sajulga](https://training.galaxyproject.org/training-material/hall-of-fame/jraysajulga/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1407",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1407?version=1",
        "name": "Proteogenomics 1: Database Creation",
        "number_of_steps": 25,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy"
        ],
        "tools": [
            "fasta2tab",
            "cat_multi_datasets",
            "filter_tabular",
            "regexColumn1",
            "query_tabular",
            "tab2fasta",
            "freebayes",
            "stringtie",
            "tp_replace_in_column",
            "custom_pro_db",
            "gffcompare_to_bed",
            "translate_bed",
            "gffcompare",
            "hisat2",
            "sqlite_to_tabular",
            "bed_to_protein_map",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Predict binding using IEDB and check novelty peptides with PepQuery\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Neoantigen 7: IEDB binding PepQuery Validated Neopeptides](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/neoantigen-7-hla-binding-novel-peptides/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1406",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1406?version=1",
        "name": "GigaScience-IEDB-PepQuery-Neoantigen",
        "number_of_steps": 15,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "iedb",
            "name:neoantigen"
        ],
        "tools": [
            "anything less than 2 and more than 0.5 percentile rank\nFilter1",
            "query_tabular",
            "table_compute",
            "removing header\nRemove beginning1",
            "cutting first column to extract peptides\nCut1",
            "pepquery2",
            "anything less than 0.5 percentile rank\nFilter1",
            "this is to remove header\nRemove beginning1",
            "iedb_api",
            "IEDB-Validated-NeoAntigen_Peptides, filtering if the validation column is yes or no \nFilter1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1404",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1404?version=1",
        "name": "Workflow 5: OTU Clustering [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_sub_sample",
            "mothur_count_groups",
            "mothur_remove_groups",
            "mothur_classify_otu",
            "mothur_cluster_split",
            "mothur_make_shared"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "1400",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1400?version=1",
        "name": "Workflow 2: Data Cleaning And Chimera Removal [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_screen_seqs",
            "mothur_filter_seqs",
            "mothur_chimera_vsearch",
            "mothur_remove_seqs",
            "mothur_summary_seqs",
            "mothur_pre_cluster",
            "mothur_unique_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Peptide and Protein ID using SearchGUI and PeptideShaker\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Peptide and Protein ID using SearchGUI and PeptideShaker](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/protein-id-sg-ps/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Florian Christoph Sigloch](https://training.galaxyproject.org/training-material/hall-of-fame/stortebecker/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n**Tutorial Contributor(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Nicola Soranzo](https://training.galaxyproject.org/training-material/hall-of-fame/nsoranzo/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Melanie F\u00f6ll](https://training.galaxyproject.org/training-material/hall-of-fame/foellmelanie/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [William Durand](https://training.galaxyproject.org/training-material/hall-of-fame/willdurand/), [Clemens Blank](https://training.galaxyproject.org/training-material/hall-of-fame/blankclemens/), [Florian Christoph Sigloch](https://training.galaxyproject.org/training-material/hall-of-fame/stortebecker/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Niall Beard](https://training.galaxyproject.org/training-material/hall-of-fame/njall/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Martin \u010cech](https://training.galaxyproject.org/training-material/hall-of-fame/martenson/), [Armin Dadras](https://training.galaxyproject.org/training-material/hall-of-fame/dadrasarmin/)\n\n**Funder(s)**: [ELIXIR Europe](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/), [de.NBI](https://training.galaxyproject.org/training-material/hall-of-fame/deNBI/), [University of Freiburg](https://training.galaxyproject.org/training-material/hall-of-fame/uni-freiburg/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1399",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1399?version=1",
        "name": "ProteinID SG PS Tutorial WF datasetCollection",
        "number_of_steps": 7,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "proteomics"
        ],
        "tools": [
            "search_gui",
            "Output: all identified non-contaminant proteins\nGrep1",
            "Output: identified non-contaminant proteins, validated to be \"Confident\".\nGrep1",
            "FileConverter",
            "peptide_shaker",
            "Output: all identified contaminant proteins\nGrep1",
            "PeakPickerHiRes"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Peptide and Protein ID using SearchGUI and PeptideShaker\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Peptide and Protein ID using SearchGUI and PeptideShaker](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/protein-id-sg-ps/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Florian Christoph Sigloch](https://training.galaxyproject.org/training-material/hall-of-fame/stortebecker/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n**Tutorial Contributor(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Nicola Soranzo](https://training.galaxyproject.org/training-material/hall-of-fame/nsoranzo/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Melanie F\u00f6ll](https://training.galaxyproject.org/training-material/hall-of-fame/foellmelanie/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [William Durand](https://training.galaxyproject.org/training-material/hall-of-fame/willdurand/), [Clemens Blank](https://training.galaxyproject.org/training-material/hall-of-fame/blankclemens/), [Florian Christoph Sigloch](https://training.galaxyproject.org/training-material/hall-of-fame/stortebecker/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Niall Beard](https://training.galaxyproject.org/training-material/hall-of-fame/njall/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Martin \u010cech](https://training.galaxyproject.org/training-material/hall-of-fame/martenson/), [Armin Dadras](https://training.galaxyproject.org/training-material/hall-of-fame/dadrasarmin/)\n\n**Funder(s)**: [ELIXIR Europe](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/), [de.NBI](https://training.galaxyproject.org/training-material/hall-of-fame/deNBI/), [University of Freiburg](https://training.galaxyproject.org/training-material/hall-of-fame/uni-freiburg/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1394",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1394?version=1",
        "name": "Peptide And Protein ID Tutorial",
        "number_of_steps": 7,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "proteomics"
        ],
        "tools": [
            "search_gui",
            "Output: only those non-contaminant proteins not evaluated to be \"Doubtful\".\nGrep1",
            "Output: all identified proteins without common contaminants. CAVE: some proteins may be both!\nGrep1",
            "peptide_shaker",
            "FileConverter",
            "Output: all identified contaminants.\nGrep1",
            "PeakPickerHiRes"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Discovery workflow with SG/PS and MaxQuant to generate microbial peptides\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 2: Discovery](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-2-discovery/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1396",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1396?version=1",
        "name": "WF2_Discovery-Workflow",
        "number_of_steps": 24,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "fasta_merge_files_and_filter_unique_sequences",
            "filter_tabular",
            "search_gui",
            "query_tabular",
            "Grep1",
            "maxquant",
            "Cut1",
            "peptide_shaker",
            "Filter1",
            "Remove beginning1",
            "Grouping1",
            "msconvert",
            "tp_cat",
            "ident_params",
            "fasta_cli",
            "fasta2tab",
            "dbbuilder"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-05-29",
        "creators": [
            "Yichao Hua"
        ],
        "description": "# SeuratExtend: An Enhanced Toolkit for scRNA-seq Analysis\r\n\r\n## Overview\r\n\r\n`SeuratExtend` is an R package designed to provide an improved and easy-to-use toolkit for scRNA-seq analysis and visualization, built upon the Seurat object. While `Seurat` is a widely-used tool in the R community that offers a foundational framework for scRNA-seq analysis, it has limitations when it comes to more advanced analysis and customized visualization. `SeuratExtend` expands upon `Seurat` by offering an array of enhanced visualization tools, an integrated functional and pathway analysis pipeline, seamless integration with popular Python tools, and a suite of utility functions for data manipulation and presentation. Designed to be user-friendly even for beginners, the package retains a level of professionalism that ensures rigorous analysis.\r\n\r\n**Key Features**:\r\n\r\n- **Enhanced Data Visualization**: Includes heatmaps, violin plots, dimensional reduction (UMAP) plots, waterfall plots, dot plots, proportion bars, volcano plots, and GSEA plots.\r\n- **Integrated Functional and Pathway Analysis**: Supports GO and Reactome databases, with the option to use custom databases.\r\n- **Python Tool Integration**: Easily apply tools like scVelo, SCENIC, and Palantir within R using the Seurat object.\r\n- **Utility Functions**: Assorted functions for calculations and color selections to streamline your scRNA-seq analysis.\r\n\r\n## Resources\r\n\r\n- **GitHub Repository**: Access the source code and contribute to SeuratExtend on [GitHub](https://github.com/huayc09/SeuratExtend).\r\n- **Online Tutorial**: For a comprehensive guide on using SeuratExtend, visit our [tutorial website](https://huayc09.github.io/SeuratExtend/).\r\n- **SeuratExtend Chatbot**: Try our AI-powered assistant (beta version, powered by ChatGPT) for help with scRNA-seq analysis: [scRNA-seq Assistant](https://chatgpt.com/g/g-8scQjmzkd-scrna-seq-assistant).\r\n\r\n## Citation\r\n\r\nIf you use SeuratExtend in your research, please cite:\r\n\r\nHua, Y., Weng, L., Zhao, F., and Rambow, F. (2024). SeuratExtend: Streamlining Single-Cell RNA-Seq Analysis Through an Integrated and Intuitive Framework. bioRxiv, 2024.08.01.606144. https://doi.org/10.1101/2024.08.01.606144\r\n\r\n## Installation\r\n\r\nInstall `SeuratExtend` directly from GitHub:\r\n\r\n```R\r\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\r\n    install.packages(\"remotes\")\r\n}\r\nremotes::install_github(\"huayc09/SeuratExtend\")\r\n```\r\n\r\n## Vignettes and Tutorials\r\n\r\n### [Quick Start-Up Guide](#quick-start-up-guide-1)\r\n\r\n### [What's New in v1.2.0](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/News.md)\r\n\r\n### [Enhanced Visualization](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Visualization.md)\r\n- [Create an Enhanced Dimensional Reduction Plot](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Visualization.md#create-an-enhanced-dimensional-reduction-plot) `DimPlot2` `FeaturePlot3` `FeaturePlot3.grid` `theme_umap_arrows`\r\n- [Generate a Heatmap Plot](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Visualization.md#generate-a-heatmap-plot) `Heatmap`\r\n- [Create Enhanced Dot Plots](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Visualization.md#create-enhanced-dot-plots-new-in-v110) **(New in v1.1.0)** `DotPlot2`\r\n- [Create an Enhanced Violin Plot](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Visualization.md#create-an-enhanced-violin-plot) `VlnPlot2`\r\n- [Visualize Cluster Distribution in Samples](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Visualization.md#visualize-cluster-distribution-in-samples) `ClusterDistrBar`\r\n- [Generate a Waterfall Plot](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Visualization.md#generate-a-waterfall-plot) `WaterfallPlot`\r\n- [Create Volcano Plots](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Visualization.md#create-volcano-plots-new-in-v110) **(New in v1.1.0)** `VolcanoPlot`\r\n- [Explore Color Functions](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Visualization.md#explore-color-functions) `color_pro` `color_iwh` `ryb2rgb` `save_colors`\r\n\r\n### [Geneset Enrichment Analysis (GSEA)](vignettes/GSEA.md)\r\n- [Conduct GSEA using the GO or Reactome database](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/GSEA.md#conduct-gsea-using-the-go-or-reactome-database) `GeneSetAnalysisGO` `GeneSetAnalysisReactome`\r\n- [Perform GSEA using customized genesets](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/GSEA.md#perform-gsea-using-customized-genesets) `GeneSetAnalysis`\r\n- [Find pathways in the GO/Reactome database or customized genesets](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/GSEA.md#find-pathways-in-the-goreactome-database-or-customized-genesets) `SearchDatabase` `SearchPathways`\r\n- [Convert GO/Reactome pathway IDs to pathway names](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/GSEA.md#convert-goreactome-pathway-ids-to-pathway-names) `RenameGO` `RenameReactome`\r\n- [Filter the GO/Reactome pathway list based on certain criteria](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/GSEA.md#filter-the-goreactome-pathway-list-based-on-certain-criteria) `FilterGOTerms` `FilterReactomeTerms`\r\n- [Create a GSEA plot emulating the Broad Institute analysis](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/GSEA.md#create-a-gsea-plot-emulating-the-broad-institute-analysis) `GSEAplot`\r\n\r\n### [Trajectory and Pseudotime Analysis](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Trajectory.md)\r\n-  [scVelo Tutorial for Trajectory Analysis](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Trajectory.md#analyzing-single-cell-trajectories-with-scvelo) `scVelo.SeuratToAnndata` `scVelo.Plot` \r\n-  [Palantir Tutorial for Trajectory and Pseudotime Analysis](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Trajectory.md#palantir-tutorial-for-trajectory-and-pseudotime-analysis) `Palantir.RunDM` `Palantir.Pseudotime`\r\n-  [MAGIC for Denoising and Smoothing Gene Expression](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Trajectory.md#magic-for-denoising-and-smoothing-gene-expression) `Palantir.Magic`\r\n-  [CellRank Tutorial for Trajectory Analysis](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Trajectory.md#cellrank-tutorial-for-trajectory-analysis) `Cellrank.Compute` `Cellrank.Plot`\r\n-  [Gene Expression Dynamics Along Differentiation Trajectories](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Trajectory.md#gene-expression-dynamics-along-differentiation-trajectories) `GeneTrendCurve.Palantir` `GeneTrendHeatmap.Palantir` `GeneTrendCurve.Slingshot` `GeneTrendHeatmap.Slingshot`\r\n-  [Slingshot Tutorial for Pseudotime Analysis](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Trajectory.md#slingshot-tutorial-for-pseudotime-analysis) `RunSlingshot` \r\n-  [Integration of Seurat with Python Tools](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Trajectory.md#integration-of-seurat-with-python-tools) `create_condaenv_seuratextend` `Seu2Adata` `Seu2Loom` `adata.LoadLoom` `adata.AddDR` `adata.AddMetadata` `adata.Save` `adata.Load`\r\n\r\n### [SCENIC for Gene Regulatory Networks Analysis](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/SCENIC.md)\r\n- [Importing SCENIC Loom Files into Seurat](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/SCENIC.md#importing-scenic-loom-files-into-seurat) `ImportPyscenicLoom`\r\n- [Visualizing SCENIC Results](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/SCENIC.md#visualizing-scenic-results) \r\n\r\n### [Utility Tools and Functions](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Utilities.md)\r\n- [Facilitate Gene Naming Conversions](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Utilities.md#facilitate-gene-naming-conversions) `HumanToMouseGenesymbol` `MouseToHumanGenesymbol` `EnsemblToGenesymbol` `GenesymbolToEnsembl` `UniprotToGenesymbol`\r\n- [Compute Statistics Grouped by Clusters](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Utilities.md#compute-statistics-grouped-by-clusters) `CalcStats`\r\n- [Assess Proportion of Positive Cells in Clusters](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Utilities.md#assess-proportion-of-positive-cells-in-clusters) `feature_percent`\r\n- [Run Standard Seurat Pipeline](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/Utilities.md#run-standard-seurat-pipeline) `RunBasicSeurat`\r\n\r\n### [Single-Cell RNA-seq Analysis Course](#single-cell-rna-seq-analysis-course-new-in-v110-1) **(New in v1.1.0)**\r\n\r\n### [FAQ](https://github.com/huayc09/SeuratExtend/blob/master/vignettes/FAQ.md)\r\n\r\n## Quick Start-Up Guide\r\n\r\nThis quick start-up guide provides an overview of the most frequently\r\nused functions in single-cell RNA sequencing (scRNA-seq) analysis. After\r\nrunning the standard Seurat pipeline (refer to this [Seurat pbmc3k\r\ntutorial](https://satijalab.org/seurat/articles/pbmc3k_tutorial)), you\r\nshould have a Seurat object ready for further analysis. Below, we\r\nillustrate the use of a subset of the pbmc dataset as an example to\r\ndemonstrate various functionalities of the `SeuratExtend` package.\r\n\r\n### Visualizing Clusters\r\n\r\n``` r\r\nlibrary(Seurat)\r\nlibrary(SeuratExtend)\r\n\r\n# Visualizing cell clusters using DimPlot2\r\nDimPlot2(pbmc, theme = theme_umap_arrows())\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-1-1.png)<!-- -->\r\n\r\n### Analyzing Cluster Distribution\r\n\r\nTo check the percentage of each cluster within different samples:\r\n\r\n``` r\r\n# Cluster distribution bar plot\r\nClusterDistrBar(pbmc$orig.ident, pbmc$cluster)\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->\r\n\r\n### Marker Gene Analysis with Heatmap\r\n\r\nTo examine the marker genes of each cluster and visualize them using a\r\nheatmap:\r\n\r\n``` r\r\n# Calculating z-scores for variable features\r\ngenes.zscore <- CalcStats(\r\n  pbmc,\r\n  features = VariableFeatures(pbmc),\r\n  group.by = \"cluster\",\r\n  order = \"p\",\r\n  n = 4)\r\n  \r\n# Displaying heatmap\r\nHeatmap(genes.zscore, lab_fill = \"zscore\")\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->\r\n\r\n### Enhanced Dot Plots (New in v1.1.0)\r\n\r\n``` r\r\n# Create grouped features\r\ngrouped_features <- list(\r\n  \"B_cell_markers\" = c(\"MS4A1\", \"CD79A\"),\r\n  \"T_cell_markers\" = c(\"CD3D\", \"CD8A\", \"IL7R\"),\r\n  \"Myeloid_markers\" = c(\"CD14\", \"FCGR3A\", \"S100A8\")\r\n)\r\n\r\nDotPlot2(pbmc, features = grouped_features)\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->\r\n\r\n### Enhanced Visualization of Marker Genes\r\n\r\nFor visualizing specific markers via a violin plot that incorporates box\r\nplots, median lines, and performs statistical testing:\r\n\r\n``` r\r\n# Specifying genes and cells of interest\r\ngenes <- c(\"CD3D\", \"CD14\", \"CD79A\")\r\ncells <- WhichCells(pbmc, idents = c(\"B cell\", \"CD8 T cell\", \"Mono CD14\"))\r\n\r\n# Violin plot with statistical analysis\r\nVlnPlot2(\r\n  pbmc,\r\n  features = genes,\r\n  group.by = \"cluster\",\r\n  cells = cells,\r\n  stat.method = \"wilcox.test\")\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->\r\n\r\n### Visualizing Multiple Markers on UMAP\r\n\r\nDisplaying three markers on a single UMAP, using RYB coloring for each\r\nmarker:\r\n\r\n``` r\r\nFeaturePlot3(pbmc, feature.1 = \"CD3D\", feature.2 = \"CD14\", feature.3 = \"CD79A\", pt.size = 1)\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->\r\n\r\n### Create Volcano Plots (New in v1.1.0)\r\n\r\nCreate a basic volcano plot comparing two cell types:\r\n\r\n``` r\r\nVolcanoPlot(pbmc, \r\n            ident.1 = \"B cell\",\r\n            ident.2 = \"CD8 T cell\")\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->\r\n\r\n### Conducting Geneset Enrichment Analysis (GSEA)\r\n\r\nExamining all the pathways of the immune process in the Gene Ontology\r\n(GO) database, and visualizing by a heatmap that displays the top\r\npathways of each cluster across multiple cell types:\r\n\r\n``` r\r\noptions(spe = \"human\")\r\npbmc <- GeneSetAnalysisGO(pbmc, parent = \"immune_system_process\", n.min = 5)\r\nmatr <- RenameGO(pbmc@misc$AUCell$GO$immune_system_process)\r\ngo_zscore <- CalcStats(\r\n  matr,\r\n  f = pbmc$cluster,\r\n  order = \"p\",\r\n  n = 3)\r\nHeatmap(go_zscore, lab_fill = \"zscore\")\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->\r\n\r\n### Detailed Comparison of Two Cell Types\r\n\r\nUsing a GSEA plot to focus on a specific pathway for deeper comparative\r\nanalysis:\r\n\r\n``` r\r\nGSEAplot(\r\n  pbmc,\r\n  ident.1 = \"B cell\",\r\n  ident.2 = \"CD8 T cell\",\r\n  title = \"GO:0042113 B cell activation (335g)\",\r\n  geneset = GO_Data$human$GO2Gene[[\"GO:0042113\"]])\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->\r\n\r\n### Importing and Visualizing SCENIC Analysis\r\n\r\nAfter conducting Gene Regulatory Networks Analysis using pySCENIC,\r\nimport the output and visualize various aspects within Seurat:\r\n\r\n``` r\r\n# Downloading a pre-computed SCENIC loom file\r\nscenic_loom_path <- file.path(tempdir(), \"pyscenic_integrated-output.loom\")\r\ndownload.file(\"https://zenodo.org/records/10944066/files/pbmc3k_small_pyscenic_integrated-output.loom\", scenic_loom_path, mode = \"wb\")\r\n\r\n# Importing SCENIC Loom Files into Seurat\r\npbmc <- ImportPyscenicLoom(scenic_loom_path, seu = pbmc)\r\n\r\n# Visualizing variables such as cluster, gene expression, and SCENIC regulon activity with customized colors\r\nDimPlot2(\r\n  pbmc,\r\n  features = c(\"cluster\", \"orig.ident\", \"CEBPA\", \"tf_CEBPA\"),\r\n  cols = list(\"tf_CEBPA\" = \"OrRd\"),\r\n  theme = NoAxes()\r\n) + theme_umap_arrows()\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->\r\n\r\n``` r\r\n# Creating a waterfall plot to compare regulon activity between cell types\r\nDefaultAssay(pbmc) <- \"TF\"\r\nWaterfallPlot(\r\n  pbmc,\r\n  features = rownames(pbmc),\r\n  ident.1 = \"Mono CD14\",\r\n  ident.2 = \"CD8 T cell\",\r\n  exp.transform = FALSE,\r\n  top.n = 20)\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->\r\n\r\n### Trajectory Analysis with Palantir in R\r\n\r\nTrajectory analysis helps identify developmental pathways and\r\ntransitions between different cell states. In this section, we\r\ndemonstrate how to perform trajectory analysis using the Palantir\r\nalgorithm on a subset of myeloid cells, integrating everything within\r\nthe R environment.\r\n\r\n#### Download and Prepare the Data\r\n\r\nFirst, we download a small subset of myeloid cells to illustrate the\r\nanalysis:\r\n\r\n``` r\r\n# Download the example Seurat Object with myeloid cells\r\nmye_small <- readRDS(url(\"https://zenodo.org/records/10944066/files/pbmc10k_mye_small_velocyto.rds\", \"rb\"))\r\n```\r\n\r\n#### Diffusion Map Calculation\r\n\r\nPalantir uses diffusion maps for dimensionality reduction to infer\r\ntrajectories. Here\u2019s how to compute and visualize them:\r\n\r\n``` r\r\n# Compute diffusion map\r\nmye_small <- Palantir.RunDM(mye_small)\r\n```\r\n\r\n    ## Determing nearest neighbor graph...\r\n\r\n``` r\r\n# Visualize the first two diffusion map dimensions\r\nDimPlot2(mye_small, reduction = \"ms\")\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-13-1.png)<!-- -->\r\n\r\n#### Pseudotime Calculation\r\n\r\nPseudotime ordering assigns each cell a time point in a trajectory,\r\nindicating its progression along a developmental path:\r\n\r\n``` r\r\n# Calculate pseudotime with a specified start cell\r\nmye_small <- Palantir.Pseudotime(mye_small, start_cell = \"sample1_GAGAGGTAGCAGTACG-1\")\r\n```\r\n\r\n    ## Sampling and flocking waypoints...\r\n    ## Time for determining waypoints: 0.00112607479095459 minutes\r\n    ## Determining pseudotime...\r\n    ## Shortest path distances using 30-nearest neighbor graph...\r\n    ## Time for shortest paths: 0.014574062824249268 minutes\r\n    ## Iteratively refining the pseudotime...\r\n    ## Correlation at iteration 1: 1.0000\r\n    ## Entropy and branch probabilities...\r\n    ## Markov chain construction...\r\n    ## Identification of terminal states...\r\n    ## Computing fundamental matrix and absorption probabilities...\r\n    ## Project results to all cells...\r\n\r\n``` r\r\n# Store pseudotime results in meta.data for easy plotting\r\nps <- mye_small@misc$Palantir$Pseudotime\r\ncolnames(ps)[3:4] <- c(\"fate1\", \"fate2\")\r\nmye_small@meta.data[,colnames(ps)] <- ps\r\n\r\n# Visualize pseudotime and cell fates\r\nDimPlot2(\r\n  mye_small,\r\n  features = colnames(ps),\r\n  reduction = \"ms\",\r\n  cols = list(continuous = \"A\", Entropy = \"D\"),\r\n  theme = NoAxes())\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-14-1.png)<!-- -->\r\n\r\n#### Visualization Along Trajectories\r\n\r\nVisualizing gene expression or regulon activity along calculated\r\ntrajectories can provide insights into dynamic changes:\r\n\r\n``` r\r\n# Create smoothed gene expression curves along trajectory\r\nGeneTrendCurve.Palantir(\r\n  mye_small,\r\n  pseudotime.data = ps,\r\n  features = c(\"CD14\", \"FCGR3A\")\r\n)\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-15-1.png)<!-- -->\r\n\r\n``` r\r\n# Create a gene trend heatmap for different fates\r\nGeneTrendHeatmap.Palantir(\r\n  mye_small,\r\n  features = VariableFeatures(mye_small)[1:10],\r\n  pseudotime.data = ps,\r\n  lineage = \"fate1\"\r\n)\r\n```\r\n\r\n![](vignettes/quick_start_files/figure-gfm/unnamed-chunk-16-1.png)<!-- -->\r\n\r\n### scVelo Analysis\r\n\r\nscVelo is a Python tool used for RNA velocity analysis. We demonstrate\r\nhow to integrate and analyze velocyto-generated data within the Seurat\r\nworkflow using scVelo.\r\n\r\n#### Preparing for scVelo\r\n\r\nFirst, download the pre-calculated velocyto loom file:\r\n\r\n``` r\r\n# Download velocyto loom file\r\nloom_path <- file.path(tempdir(), \"pbmc10k_mye_small.loom\")\r\ndownload.file(\"https://zenodo.org/records/10944066/files/pbmc10k_mye_small.loom\", \r\n              loom_path,\r\n              mode = \"wb\")  # Use binary mode for Windows compatibility\r\n\r\n# Set up the path for saving the AnnData object in the HDF5 (h5ad) format\r\nif (.Platform$OS.type == \"windows\") {\r\n    adata_path <- normalizePath(file.path(tempdir(), \"mye_small.h5ad\"), winslash = \"/\")\r\n} else {\r\n    adata_path <- file.path(tempdir(), \"mye_small.h5ad\")\r\n}\r\n\r\n# Integrate Seurat Object and velocyto loom into an AnnData object\r\nscVelo.SeuratToAnndata(\r\n  mye_small,\r\n  filename = adata_path,\r\n  velocyto.loompath = loom_path,\r\n  prefix = \"sample1_\",\r\n  postfix = \"-1\"\r\n)\r\n```\r\n\r\n    ## scVelo version: 0.3.0\r\n    ## Filtered out 10891 genes that are detected 20 counts (shared).\r\n    ## Normalized count data: X, spliced, unspliced.\r\n    ## Extracted 2000 highly variable genes.\r\n    ## Logarithmized X.\r\n    ## computing neighbors\r\n    ##     finished (0:00:00) --> added \r\n    ##     'distances' and 'connectivities', weighted adjacency matrices (adata.obsp)\r\n    ## computing moments based on connectivities\r\n    ##     finished (0:00:00) --> added \r\n    ##     'Ms' and 'Mu', moments of un/spliced abundances (adata.layers)\r\n    ## computing velocities\r\n    ##     finished (0:00:00) --> added \r\n    ##     'velocity', velocity vectors for each individual cell (adata.layers)\r\n    ## computing velocity graph (using 1/28 cores)\r\n    ## WARNING: Unable to create progress bar. Consider installing `tqdm` as `pip install tqdm` and `ipywidgets` as `pip install ipywidgets`,\r\n    ## or disable the progress bar using `show_progress_bar=False`.\r\n    ##     finished (0:00:01) --> added \r\n    ##     'velocity_graph', sparse matrix with cosine correlations (adata.uns)\r\n\r\n    ## NULL\r\n\r\n#### Plotting scVelo Results\r\n\r\nOnce the data is processed, visualize the RNA velocity:\r\n\r\n``` r\r\n# Plot RNA velocity\r\nscVelo.Plot(color = \"cluster\", basis = \"ms_cell_embeddings\", \r\n            save = \"quick_start_scvelo.png\", figsize = c(5,4))\r\n```\r\n\r\n<img src=\"vignettes/figures/scvelo_quick_start_scvelo.png\" width=\"700\" />\r\n\r\nFor detailed usage of the functions and more advanced analysis, please refer to the vignettes and tutorials.\r\n\r\n## Single-Cell RNA-seq Analysis Course (New in v1.1.0)\r\n\r\nA comprehensive 6-lesson course originally presented at the [Institute for AI in Medicine (IKIM)](https://www.ikim.uk-essen.de/institute), University Hospital Essen on October 8, 2024, organized by the [Department of Applied Computational Cancer Research](https://www.ikim.uk-essen.de/groups/accr). The course materials have been updated for SeuratExtend v1.1.0 and are now freely available online. Starting with fundamentals of R and Seurat, the course progressively builds to cover enhanced visualization, functional analysis, quality control, and cutting-edge methods including trajectory analysis, regulatory networks, and cell-cell communication. Perfect for beginners while providing depth needed for advanced applications.\r\n\r\n### [Lesson 1: Introduction to R Programming](https://huayc09.github.io/SeuratExtend/articles/single-cell-course/1.basic-R.html)\r\nEssential R programming fundamentals tailored for scRNA-seq analysis. Covers basic data types, data structures (vectors, matrices, data frames), file operations, and package management. Perfect for beginners starting their journey in bioinformatics.\r\n\r\n### [Lesson 2: Basic Single-Cell Analysis with Seurat](https://huayc09.github.io/SeuratExtend/articles/single-cell-course/2.Seurat.html)\r\nComprehensive walkthrough of the standard Seurat workflow, from raw count matrix to cell type annotation. Learn about data normalization, dimensionality reduction, clustering, and visualization through hands-on analysis of PBMC data.\r\n\r\n### [Lesson 3: Advanced Visualization with SeuratExtend](https://huayc09.github.io/SeuratExtend/articles/single-cell-course/3.Visualization.html)\r\nMaster advanced visualization techniques using SeuratExtend's enhanced plotting functions. Explore DimPlot2, FeaturePlot3, Heatmap, and other tools to create publication-ready figures. Includes practical examples of customizing plots and color schemes.\r\n\r\n### [Lesson 4: Gene Set Enrichment Analysis and Utilities](https://huayc09.github.io/SeuratExtend/articles/single-cell-course/4.GSEA.html)\r\nMaster functional enrichment analysis using GO and Reactome databases through SeuratExtend's integrated GSEA pipeline. Learn to perform custom gene set analysis, interpret enrichment scores, and utilize helpful utility functions for gene naming conversions and cell proportions.\r\n\r\n### [Lesson 5: Core Workflow Enhancements](https://huayc09.github.io/SeuratExtend/articles/single-cell-course/5.Core-Enhancement.html)\r\nElevate your scRNA-seq analysis with advanced quality control, doublet removal, data integration using Harmony, cell cycle analysis, and alternative normalization methods like SCTransform. Understand key considerations for processing and analyzing multi-sample datasets.\r\n\r\n### [Lesson 6: Advanced Analytical Methods (Part 1)](https://huayc09.github.io/SeuratExtend/articles/single-cell-course/6.Advanced.html) [(Part 2)](https://huayc09.github.io/SeuratExtend/articles/single-cell-course/6.Advanced-2.html)\r\nExplore cutting-edge techniques including trajectory analysis with scVelo/Palantir, cell-cell communication using CellChat/NicheNet, regulatory network inference with SCENIC, and specialized analyses for TCR/BCR data and copy number variations.\r\n\r\n## License\r\n\r\nThe SeuratExtend R package code is licensed under GPL-3.0.\r\n\r\nThe data files (*.rda files in the 'data' folder) are released under CC0 1.0 Universal (CC0 1.0) Public Domain Dedication, meaning they are in the public domain and can be used without any restrictions.\r\n\r\n## Publications Using SeuratExtend\r\n\r\n1. Hua, Y., Vella, G., Rambow, F., et al. (2022). Cancer immunotherapies transition endothelial cells into HEVs that generate TCF1+ T lymphocyte niches through a feed-forward loop. **Cancer Cell** 40, 1600-1618. https://doi.org/10.1016/j.ccell.2022.11.002\r\n2. Hua, Y., Wu, N., Miao, J., Shen, M. (2023). Single-cell transcriptomic analysis in two patients with rare systemic autoinflammatory diseases treated with anti-TNF therapy. **Front. Immunol.** 14. https://doi.org/10.3389/fimmu.2023.1091336\r\n3. Verhoeven, J., Jacobs, K.A., Rizzollo, F., Lodi, F., Hua, Y., Po\u017aniak, J., Narayanan Srinivasan, A., Houbaert, D., Shankar, G., More, S., et al. (2023). Tumor endothelial cell autophagy is a key vascular-immune checkpoint in melanoma. **EMBO Mol. Med.** 15, e18028. https://doi.org/10.15252/emmm.202318028\r\n4. Dobersalske, C., Rauschenbach, L., Hua, Y., Berliner, C., Steinbach, A., Gr\u00fcneboom, A., Kokkaliaris, K.D., Heiland, D.H., Berger, P., Langer, S., et al. (2024). Cranioencephalic functional lymphoid units in glioblastoma. **Nat. Med.** https://doi.org/10.1038/s41591-024-03152-x\r\n",
        "doi": "10.48546/workflowhub.workflow.1385.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1385",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1385?version=1",
        "name": "SeuratExtend",
        "number_of_steps": 0,
        "projects": [
            "Applied Computational Cancer Research"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "r-package",
            "single-cell-rna-seq",
            "visualization"
        ],
        "tools": [],
        "type": "Unrecognized workflow type",
        "update_time": "2025-05-30",
        "versions": 1
    },
    {
        "create_time": "2025-05-29",
        "creators": [
            "Luisa Santus",
            "Jose Espinosa Carrasco"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-multiplesequencealign_logo_dark.png\">\n    <img alt=\"nf-core/multiplesequencealign\" src=\"docs/images/nf-core-multiplesequencealign_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/multiplesequencealign/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/multiplesequencealign/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/multiplesequencealign/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/multiplesequencealign/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/multiplesequencealign/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.13889386-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.13889386)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A525.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/multiplesequencealign)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23multiplesequencealign-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/multiplesequencealign)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\nUse **nf-core/multiplesequencealign** to:\n\n1. **Deploy** one (or many) of the most popular Multiple Sequence Alignment (MSA) tools.\n2. **Benchmark** MSA tools (and their inputs) using various metrics.\n\nMain steps:\n\n  <details>\n      <summary><strong>Inputs summary</strong> (Optional)</summary>\n      <p>Computation of summary statistics on the input files (e.g., average sequence similarity across the input sequences, their length, pLDDT extraction if available).</p>\n  </details>\n\n  <details>\n      <summary><strong>Guide Tree</strong> (Optional)</summary>\n      <p>Renders a guide tree with a chosen tool (list available in <a href=\"https://nf-co.re/multiplesequencealign/usage#2-guide-trees\">usage</a>). Some aligners use guide trees to define the order in which the sequences are aligned.</p>\n  </details>\n\n  <details>\n      <summary><strong>Align</strong> (Required)</summary>\n      <p>Aligns the sequences with a chosen tool (list available in <a href=\"https://nf-co.re/multiplesequencealign/usage#3-align\">usage</a>).</p>\n  </details>\n\n  <details>\n      <summary><strong>Evaluate</strong> (Optional)</summary>\n      <p>Evaluates the generated alignments with different metrics: Sum Of Pairs (SoP), Total Column score (TC), iRMSD, Total Consistency Score (TCS), etc.</p>\n  </details>\n\n  <details>\n      <summary><strong>Report</strong>(Optional)</summary>\n      <p>Reports the collected information of the runs in a Shiny app and a summary table in MultiQC. Optionally, it can also render the <a href=\"https://github.com/steineggerlab/foldmason\">Foldmason</a> MSA visualization in HTML format.</p>\n  </details>\n\n<br>\n\nMore introductory material: [bytesize talk](https://youtu.be/iRY-Y1p5gtc), [nextflow summit talk](https://www.youtube.com/watch?v=suNulysHIN0) from the nextlow summit, [poster](https://github.com/nf-core/multiplesequencealign/blob/dev/docs/images/poster-nf-msa.pdf).\n\n![Alt text](docs/images/nf-core-msa_metro_map.png?raw=true \"nf-core-msa metro map\")\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n### Quick start - test run\n\nTo get a feeling of what the pipeline does, run:\n\n(You don't need to download or provide any file, try it!)\n\n```\nnextflow run nf-core/multiplesequencealign \\\n   -profile test_tiny,docker \\\n   --outdir results\n```\n\nand if you want to see how a more complete run looks like, you can try:\n\n```\nnextflow run nf-core/multiplesequencealign \\\n   -profile test,docker \\\n   --outdir results\n```\n\n## How to set up an easy run:\n\n> [!NOTE]\n> We have a lot more of use cases examples under [FAQs](\"https://nf-co.re/multiplesequencealign/usage/FAQs)\n\n### Input data\n\nYou can provide either (or both) a **fasta** file or a set of **protein structures**.\n\nAlternatively, you can provide a [samplesheet](https://nf-co.re/multiplesequencealign/usage/#samplesheet-input) and a [toolsheet](https://nf-co.re/multiplesequencealign/usage/#toolsheet-input).\n\nSee below how to provide them.\n\n> Find some example input data [here](https://github.com/nf-core/test-datasets/tree/multiplesequencealign)\n\n### CASE 1: One input dataset, one tool.\n\nIf you only have one dataset and want to align it using one specific MSA tool (e.g. FAMSA or FOLDMASON), you can run the pipeline with one single command.\n\nIs your input a fasta file ([example](https://github.com/nf-core/test-datasets/blob/multiplesequencealign/testdata/setoxin-ref.fa))? Then:\n\n```bash\nnextflow run nf-core/multiplesequencealign \\\n   -profile easy_deploy,docker \\\n   --seqs <YOUR_FASTA.fa> \\\n   --aligner FAMSA \\\n   --outdir outdir\n```\n\nIs your input a directory where your PDB files are stored ([example](https://github.com/nf-core/test-datasets/blob/multiplesequencealign/testdata/af2_structures/seatoxin-ref.tar.gz))? Then:\n\n```bash\nnextflow run nf-core/multiplesequencealign \\\n   -profile easy_deploy,docker \\\n   --pdbs_dir <PATH_TO_YOUR_PDB_DIR> \\\n   --aligner FOLDMASON \\\n   --outdir outdir\n```\n\n<details>\n  <summary> FAQ: Which are the available tools I can use?</summary>\n  Check the list here: <a href=\"https://nf-co.re/multiplesequencealign/usage/#3-align\"> available tools</a>.\n</details>\n\n<details>\n  <summary> FAQ: Can I use both <em>--seqs</em> and <em>--pdbs_dir</em>?</summary>\n  Yes, go for it! This might be useful if you want a structural evaluation of a sequence-based aligner for instance.\n</details>\n\n<details>\n  <summary> FAQ: Can I specify also which guidetree to use? </summary>\n  Yes, use the <code>--tree</code> flag. More info: <a href=\"https://nf-co.re/multiplesequencealign/usage\">usage</a> and <a href=\"https://nf-co.re/multiplesequencealign/parameters\">parameters</a>.\n</details>\n\n<details>\n  <summary> FAQ: Can I specify the arguments of the tools (tree and aligner)? </summary>\n  Yes, use the <code>--args_tree</code> and <code>--args_aligner</code> flags. More info: <a href=\"https://nf-co.re/multiplesequencealign/usage\">usage</a> and <a href=\"https://nf-co.re/multiplesequencealign/parameters\">parameters</a>.\n</details>\n\n### CASE 2: Multiple datasets, multiple tools.\n\n```bash\nnextflow run nf-core/multiplesequencealign \\\n   -profile test,docker \\\n   --input <samplesheet.csv> \\\n   --tools <toolsheet.csv> \\\n   --outdir outdir\n```\n\nYou need **2 input files**:\n\n- **samplesheet** (your datasets)\n- **toolsheet** (which tools you want to use).\n\n<details>\n  <summary> What is a samplesheet?</summary>\n  The sample sheet defines the <b>input datasets</b> (sequences, structures, etc.) that the pipeline will process.\n\nA minimal version:\n\n```csv\nid,fasta\nseatoxin,seatoxin.fa\ntoxin,toxin.fa\n```\n\nA more complete one:\n\n```csv\nid,fasta,reference,optional_data\nseatoxin,seatoxin.fa,seatoxin-ref.fa,seatoxin_structures\ntoxin,toxin.fa,toxin-ref.fa,toxin_structures\n```\n\nEach row represents a set of sequences (in this case the seatoxin and toxin protein families) to be aligned and the associated (if available) reference alignments and dependency files (this can be anything from protein structure or any other information you would want to use in your favourite MSA tool).\n\nPlease check: <a href=\"https://nf-co.re/multiplesequencealign/usage/#samplesheet-input\">usage</a>.\n\n> [!NOTE]\n> The only required input is the id column and either fasta or optional_data.\n\n</details>\n\n<details>\n  <summary> What is a toolsheet?</summary>\n  The toolsheet specifies <em>which combination of tools will be deployed and benchmarked in the pipeline</em>.\n\nEach line defines a combination of guide tree and multiple sequence aligner to run with the respective arguments to be used.\n\nThe only required field is `aligner`. The fields `tree`, `args_tree` and `args_aligner` are optional and can be left empty.\n\nA minimal version:\n\n```csv\ntree,args_tree,aligner,args_aligner\n,,FAMSA,\n```\n\nThis will run the FAMSA aligner.\n\nA more complex one:\n\n```csv\ntree,args_tree,aligner,args_aligner\nFAMSA, -gt upgma -medoidtree, FAMSA,\n, ,TCOFFEE,\nFAMSA,,REGRESSIVE,\n```\n\nThis will run, in parallel:\n\n- the FAMSA guidetree with the arguments <em>-gt upgma -medoidtree</em>. This guidetree is then used as input for the FAMSA aligner.\n- the TCOFFEE aligner\n- the FAMSA guidetree with default arguments. This guidetree is then used as input for the REGRESSIVE aligner.\n\nPlease check: <a href=\"https://nf-co.re/multiplesequencealign/usage/#toolsheet-input\">usage</a>.\n\n> [!NOTE]\n> The only required input is `aligner`.\n\n</details>\n\nFor more details on more advanced runs: [usage documentation](https://nf-co.re/multiplesequencealign/usage) and the [parameter documentation](https://nf-co.re/multiplesequencealign/parameters).\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\n## Pipeline resources\n\nWhich resources is the pipeline using? You can find the default resources used in [base.config](conf/base.config).\n\nIf you are using specific profiles, e.g. [test](conf/test.config), these will overwrite the defaults.\n\nIf you want to modify the needed resources, please refer [usage](https://nf-co.re/multiplesequencealign/docs/usage/#custom-configuration).\n\n## Pipeline output\n\nExample results: [results](https://nf-co.re/multiplesequencealign/results) tab on the nf-core website pipeline page.\nFor more details: [output documentation](https://nf-co.re/multiplesequencealign/output).\n\n## Extending the pipeline\n\nFor details on how to add your favourite guide tree, MSA or evaluation step in nf-core/multiplesequencealign please refer to the [extending documentation](https://nf-co.re/multiplesequencealign/usage/adding_a_tool).\n\n## Credits\n\nnf-core/multiplesequencealign was originally written by Luisa Santus ([@luisas](https://github.com/luisas)) and Jose Espinosa-Carrasco ([@JoseEspinosa](https://github.com/JoseEspinosa)) from The Comparative Bioinformatics Group at The Centre for Genomic Regulation, Spain.\n\nThe following people have significantly contributed to the development of the pipeline and its modules: Leon Rauschning ([@lrauschning](https://github.com/lrauschning)), Alessio Vignoli ([@alessiovignoli](https://github.com/alessiovignoli)), Igor Trujnara ([@itrujnara](https://github.com/itrujnara)) and Leila Mansouri ([@l-mansouri](https://github.com/l-mansouri)).\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#multiplesequencealign` channel](https://nfcore.slack.com/channels/multiplesequencealign) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/multiplesequencealign for your analysis, please cite it using the following doi: [10.5281/zenodo.13889386](https://doi.org/10.5281/zenodo.13889386)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1178",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1178?version=3",
        "name": "nf-core/multiplesequencealign",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-29",
        "versions": 3
    },
    {
        "create_time": "2025-05-29",
        "creators": [
            "workflow4metabolomics"
        ],
        "description": "This workflow is composed with the XCMS tool R package (Smith, C.A. 2006) able to extract, filter, align and fill gapand the possibility to annotate isotopes, adducts and fragments using the CAMERA R package (Kuhl, C 2012).\n\n\nhttps://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/lcms-preprocessing/tutorial.html ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "677",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/677?version=2",
        "name": "lcms-preprocessing/main",
        "number_of_steps": 12,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "msnbase_readmsdata",
            "abims_xcms_xcmsSet",
            "xcms_plot_chromatogram",
            "abims_xcms_retcor",
            "intens_check",
            "abims_CAMERA_annotateDiffreport",
            "checkFormat",
            "abims_xcms_fillPeaks",
            "xcms_merge",
            "abims_xcms_group"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-04-17",
        "creators": [
            "Dan Parsons",
            "Ben Price"
        ],
        "description": "<div align=\"center\">\r\n    <img src=\"./Gene_fetch_logo.svg\" width=\"300\" alt=\"Gene Fetch Logo\">\r\n</div>\r\n\r\n\r\n# Gene_fetch \r\nThis tool fetches gene sequences from NCBI databases based on taxonomy IDs (taxids) or taxonomic information. It can retrieve both protein and nucleotide sequences for various genes, including protein-coding genes (e.g., cox1, cytb, rbcl, matk) and rRNA genes (e.g., 16S, 18S).\r\n\r\n\r\n## Feature highlight\r\n- Fetch protein and/or nucleotide sequences from NCBI GenBank database.\r\n- Handles both direct nucleotide sequences and protein-linked nucleotide searches (CDS extraction includes fallback mechanisms for atypical annotation formats).\r\n.\r\n- Support for both protein-coding and rDNA genes.\r\n- Single-taxid mode (-s/--single) for retrieving a specified number of target sequences for a particular taxon (default length thresholds are reduced (protein: 50aa, nucleotide: 100bp)).\r\n- Customisable length filtering thresholds for protein and nucleotide sequences.\r\n- Automatic taxonomy traversal: Uses fetched NCBI taxonomic lineage for a given taxid when sequences are not found at the input taxonomic level. I.e., Search at given taxid level (e.g., species), if no sequences are found, escalate species->phylum until a suitable sequence is found.\r\n- Validates fetched sequence using higher taxonomy, avoiding potential taxonomic homonyms.\r\n- Robust error handling, error and progress logging, and NCBI API rate limits (10 requests/second).\r\n- Handles complex sequence features (e.g., complement strands, joined sequences, WGS entries) in addition to 'simple' cds extaction (if --type nucleotide/both). The tool avoids \"unverified\" sequences and WGS entries not containing sequence data (i.e. master records).\r\n- 'Checkpointing': if a run fails/crashes, the script can be rerun using the same arguments and it will resume from where it stopped.\r\n- When more than 50 matching sequences are found for a sample, the tool fetches summary information for all matches (using NCBI esummary API), orders them by length, and processes the top 10 longest sequences.\r\n\r\n## Contents\r\n - [Installation](#installation)\r\n - [Usage](#usage)\r\n - [Examples](#Examples)\r\n - [Input](#input)\r\n - [Output](#output)\r\n - [Cluster](#running-gene_fetch-on-a-cluster)\r\n - [Supported targets](#supported-targets)\r\n - [Notes](#notes)\r\n - [Benchmarking](#benchmarking)\r\n - [Future developments](#future-developments)\r\n - [Contributions and citation](#contributions-and-citations)\r\n\r\n\r\n## Installation\r\nFirst, clone the Gene Fetch GitHub repository to your current path, and enter the Gene Fetch installation directory \r\n```bash\r\ngit clone https://github.com/bge-barcoding/gene_fetch\r\n\r\ncd gene_fetch\r\n```\r\nRun the commands below to install the necessary dependencies and activate the Conda environment. [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) must be installed.\r\n```bash\r\nconda env create -n fetch -f fetch.yaml\r\n\r\nconda activate fetch\r\n```\r\nAlternatively, you can install the dependencies below directly or in your own Conda environment\r\n```\r\nconda install python>=3.9 pip\r\npip install ratelimit>=2.2.1\r\npip install biopython>=1.80\r\n```\r\n\r\n## Usage\r\n```bash\r\npython gene_fetch.py -g/--gene <gene_name> --type <sequence_type> -i/--in <samples.csv> -o/--out <output_directory> \r\n```\r\n* `--h/--help`: Show help and exit.\r\n### Required arguments\r\n* `-g/--gene`: Name of gene to search for in NCBI GenBank database (e.g., cox1/16s/rbcl).\r\n* `--type`: Sequence type to fetch; 'protein', 'nucleotide', or 'both' ('both' will initially search and fetch a protein sequence, and then fetches the corresponding nucleotide CDS for that protein sequence).\r\n* `-i/--in`: Path to input CSV file containing sample IDs and TaxIDs (see [Input](#input) section below).\r\n* `i2/--in2`: Path to alternative input CSV file containing sample IDs and taxonomic information for each sample (see [Input](#input) section below).\r\n* `o/--out`: Path to output directory. The directory will be created if it does not exist.\r\n* `e/--email` and `-k/--api-key`: Email address and associated API key for NCBI account. An NCBI account is required to run this tool (due to otherwise strict API limitations) - information on how to create an NCBI account and find your API key can be found [here](https://support.nlm.nih.gov/kbArticle/?pn=KA-05317).\r\n####= Optional arguments\r\n* `--protein_size`: Minimum protein sequence length filter. Applicable to mode 'normal' and 'single-taxid' search modes (default: 500).\r\n* `--nucleotide_size`: Minimum nucleotide sequence length filter. Applicable to mode 'normal' and 'single-taxid' search modes (default: 1500).\r\n* `s/--single`: Taxonomic ID for 'single-taxid' sequence search mode (`-i` and `-i2` ignored when run with `-s` mode). 'Single-taxid' mode will fetch all target gene or protein sequences on GenBank for a specific taxonomic ID.\r\n* `--max-sequences`: Maximum number of sequences to fetch for a specific taxonomic ID (only applies when run in 'single-taxid' mode).\r\n\r\n\r\n## Examples\r\nFetch both protein and nucleotide sequences for COI with default sequence length thresholds.\r\n```\r\npython gene_fetch.py -e your.email@domain.com -k your_api_key \\\r\n                    -g cox1 -o ./output_dir -i ./samples.csv \\\r\n                    --type both\r\n```\r\n\r\nFetch rbcL nucleotide sequences using sample taxonomic information, applying a minimum nucleotide sequence length of 1000bp\r\n```\r\npython gene_fetch.py -e your.email@domain.com -k your_api_key \\\r\n                    -g rbcl -o ./output_dir -i2 ./taxonomy.csv \\\r\n                    --type nucleotide --nucleotide_size 1000\r\n```\r\n\r\nRetrieve 1000 available matK protein sequences >400aa for _Arabidopsis thaliana_ (taxid: 3702).\r\n```\r\npython gene_fetch.py -e your.email@domain.com -k your_api_key \\\r\n                    -g matk -o ./output_dir -s 3702 \\\r\n                    --type protein --protein_size 400 --max-sequences 1000\r\n```\r\n\r\n\r\n## Input\r\n**Example 'samples.csv' input file (-i/--in)**\r\n| ID | taxid |\r\n| --- | --- |\r\n| sample-1  | 177658 |\r\n| sample-2 | 177627 |\r\n| sample-3 | 3084599 |\r\n\r\n**Example 'samples_taxonomy.csv' input file (-i2/--in2)**\r\n| ID | phylum | class | order | family | genus | species |\r\n| --- | --- | --- | --- | --- | --- | --- |\r\n| sample-1  | Arthropoda | Insecta | Diptera | Acroceridae | Astomella | Astomella hispaniae |\r\n| sample-2 | Arthropoda | Insecta | Hemiptera | Cicadellidae | Psammotettix | Psammotettix sabulicola |\r\n| sample-3 | Arthropoda | Insecta | Trichoptera | Limnephilidae | Dicosmoecus | Dicosmoecus palatus |\r\n\r\n\r\n## Output\r\n### 'Normal' mode\r\n```\r\noutput_dir/\r\n\u251c\u2500\u2500 nucleotide/                 # Nucleotide sequences. Only populated if '--type nucleotide/both' utilised.\r\n\u2502   \u251c\u2500\u2500 sample-1_dna.fasta   \r\n\u2502   \u251c\u2500\u2500 sample-2_dna.fasta\r\n\u2502   \u2514\u2500\u2500 ...\r\n\u251c\u2500\u2500 sample-1.fasta              # Protein sequences.\r\n\u251c\u2500\u2500 sample-2.fasta\r\n\u251c\u2500\u2500 sequence_references.csv     # Sequence metadata.\r\n\u251c\u2500\u2500 failed_searches.csv         # Failed search attempts (if any).\r\n\u2514\u2500\u2500 gene_fetch.log              # Log.\r\n```\r\n\r\n**sequence_references.csv output example**\r\n| ID | taxid | protein_accession | protein_length | nucleotide_accession | nucleotide_length | matched_rank | ncbi_taxonomy | reference_name | protein_reference_path | nucleotide_reference_path |\r\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\r\n| sample-1 | 177658 | AHF21732.1 | 510 | KF756944.1 | 1530 | genus:Apatania | Eukaryota; ...; Apataniinae; Apatania | sample-1 | abs/path/to/protein_references/sample-1.fasta | abs/path/to/protein_references/sample-1_dna.fasta |\r\n| sample-2 | 2719103 | QNE85983.1 | 518 | MT410852.1 | 1557 | species:Isoptena serricornis | Eukaryota; ...; Chloroperlinae; Isoptena | sample-2 | abs/path/to/protein_references/sample-2.fasta | abs/path/to/protein_references/sample-2_dna.fasta |\r\n| sample-3 | 1876143 | YP_009526503.1 | 512 | NC_039659.1 | 1539 | genus:Triaenodes | Eukaryota; ...; Triaenodini; Triaenodes | sample-3 | abs/path/to/protein_references/sample-3.fasta | abs/path/to/protein_references/sample-3_dna.fasta |\r\n\r\n\r\n### 'Single-taxid' mode\r\n```\r\noutput_dir/\r\n\u251c\u2500\u2500 nucleotide/                      # Nucleotide sequences. Only populated if '--type nucleotide/both' utilised.\r\n\u2502   \u251c\u2500\u2500 ACCESSION1_dna.fasta   \r\n\u2502   \u251c\u2500\u2500 ACCESSION2_dna.fasta\r\n\u2502   \u2514\u2500\u2500 ...\r\n\u251c\u2500\u2500 ACCESSION1.fasta                 # Protein sequences.\r\n\u251c\u2500\u2500 ACCESSION2.fasta\r\n\u251c\u2500\u2500 fetched_nucleotide_sequences.csv # Only populated if '--type nucleotide/both' utilised. Sequence metadata.\r\n\u251c\u2500\u2500 fetched_protein_sequences.csv    # Only populated if '--type protein/both' utilised. Sequence metadata.\r\n\u251c\u2500\u2500 failed_searches.csv              # Failed search attempts (if any).\r\n\u2514\u2500\u2500 gene_fetch.log                   # Log.\r\n```\r\n\r\n**fetched_protein|nucleotide_sequences.csv output example**\r\n| ID | taxid | Description |\r\n| --- | --- | --- |\r\n| PQ645072.1 | 1501 | Ochlerotatus nigripes isolate Pool11 cytochrome c oxidase subunit I (COX1) gene, partial cds; mitochondrial |\r\n| PQ645071.1 | 1537 | Ochlerotatus nigripes isolate Pool10 cytochrome c oxidase subunit I (COX1) gene, partial cds; mitochondrial |\r\n| PQ645070.1 | 1501 | Ochlerotatus impiger isolate Pool2 cytochrome c oxidase subunit I (COX1) gene, partial cds; mitochondrial |\r\n| PQ645069.1 | 1518\t| Ochlerotatus impiger isolate Pool1 cytochrome c oxidase subunit I (COX1) gene, partial cds; mitochondrial |\r\n| PP355486.1 | 581 | Aedes scutellaris isolate NC.033 cytochrome c oxidase subunit I (COX1) gene, partial cds; mitochondrial |\r\n\r\n\r\n## Running gene_fetch on a cluster\r\n- See '1_gene_fetch.sh' for running gene_fetch.py on a HPC cluster (SLURM job schedular). \r\n- Edit 'mem' and/or 'cpus-per-task' to set memory and CPU/threads allocation.\r\n- Change paths and variables as needed.\r\n- Run '1_gene_fetch.sh' with:\r\n```\r\nsbatch 1_gene_fetch.sh\r\n```\r\n\r\n## Supported targets\r\nGene Fetch does function with other targets than those listed below, but it has hard-coded name variations and 'smarter' searching for the below targets. More targets can be added into script (see 'class config').\r\n- cox1/COI/cytochrome c oxidase subunit I\r\n- cox2/COII/cytochrome c oxidase subunit II\r\n- cox3/COIIIcytochrome c oxidase subunit III\r\n- cytb/cob/cytochrome b\r\n- nd1/NAD1/NADH dehydrogenase subunit 1\r\n- nd2/NAD2/NADH dehydrogenase subunit 2\r\n- rbcL/RuBisCO/ribulose-1,5-bisphosphate carboxylase/oxygenase large subunit\r\n- matK/maturase K/maturase type II intron splicing factor\r\n- 16S ribosomal RNA/16s\r\n- SSU/18s\r\n- LSU/28s\r\n- 12S ribosomal RNA/12s\r\n- ITS (ITS1-5.8S-ITS2)\r\n- ITS1/internal transcribed spacer 1\r\n- ITS2/internal transcribed spacer 2\r\n- tRNA-Leucine/trnL\r\n\r\n\r\n## Benchmarking\r\n| Sample Description | Run Mode | Target | Input File | Data Type | Memory | CPUs | Run Time |\r\n|--------------------|----------|--------|------------|-----------|--------|------|----------|\r\n| 570 Arthropod samples | Normal | COX1 | taxonomy.csv | Both | 10G | 18 | 02:51:06 |\r\n| 570 Arthropod samples | Normal | COX1 | samples.csv | Nucleotide | 5G | 4 | 02:04:01 |\r\n| 570 Arthropod samples | Normal | COX1 | samples.csv | Protein | 5G | 4 | 01:50:31 |\r\n| 570 Arthropod samples | Normal | 18S | samples.csv | Nucleotide | 10G | 8 | 01:38:16 |\r\n| 570 Arthropod samples | Normal | ND1 | samples.csv | Nucleotide | 10G | 4 | 01:58:35 |\r\n| All (159) _A. thaliana_ sequences >300aa | Single-taxid | rbcL | N/A | Protein | 5G | 1 | 00:02:39 |\r\n| 1000 Culicidae sequences >500bp | Single-taxid | COX1 | N/A | nucleotide | 20G | 16 | 00:30:36 |\r\n| 1000 _M. tubercolisis_ sequences | Single-taxid | 16S | N/A | nucleotide | 20G | 16 | 00:10:33 |\r\n\r\n## Future Development\r\n- Add optional alignment of retrieved sequences\r\n- Add support for direct GenBank submission format output\r\n- Enhance LRU caching for taxonomy lookups to reduce API calls\r\n- Further improve efficiency of record searching and selecting the longest sequence\r\n- Add support for additional genetic markers beyond the currently supported set\r\n\r\n\r\n## Contributions and citations\r\nGeneFetch was written by Dan Parsons & Ben Price @ NHMUK (2024).\r\n\r\nIf you use GeneFetch, please cite our publication: **XYZ()**\r\n\r\nIf you have any questions or suggested improvements, please do get in touch in the issues!\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1342",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1342?version=2",
        "name": "Gene Fetch",
        "number_of_steps": 0,
        "projects": [
            "iBOL Europe Museum Skimming",
            "Biodiversity Genomics Europe (general)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bge",
            "bioinformatics"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2025-06-01",
        "versions": 2
    },
    {
        "create_time": "2025-05-28",
        "creators": [
            "Anne Fouilloux"
        ],
        "description": "# Annual percentage Change for population in Germany (1950 - 2025)\r\n\r\nThis workflow run was executed on Galaxy (Workflow Rerun Information)\r\n\r\n**Workflow:** Climate Stripes\r\n\r\n**Execution Status:** scheduled\r\n\r\n**Executed:** 2025-05-27 14:14:07.240149\r\n\r\n\r\n## Workflow Inputs\r\n\r\n### Formal Input Definitions\r\n\r\n- **Germany-Population-Annual--Change-2025-05-27-15-17.csv** (File)\r\n\r\n- **Column name to use for plotting** (Text)\r\n\r\n- **Plot Title** (Text)\r\n\r\n- **nxsplit** (Integer)\r\n  - Description: Number of values per intervals\r\n\r\n- **xname (column name for the x-axis)** (Text)\r\n  - Description: The column name to use for the x-axis.\r\n\r\n- **Date/time format for the x-axis column** (Text)\r\n  - Description: Date/time format to use when reading the column to be used for the x-axis.\r\n\r\n- **dates format for xlabels** (Text)\r\n  - Description: Format for plotting dates on the x-axis\r\n\r\n- **Matplotlib colormap** (Text)\r\n  - Description: Parameter 'colormap': valid options: winter,PRGn,hsv,gist_ncar,RdYlGn,summer,BrBG,Pastel1,autumn,PuBuGn,seismic,YlOrRd,Purples,Wistia,YlOrBr,tab10,tab20c,gist_heat,bone,gist_yarg,ocean,flag,RdGy,gist_earth,coolwarm,spring,PuBu,cool,gist_stern,gray,Reds,Greens,Accent,BuGn,RdGy_r,Set3,Pastel2,pink,OrRd,gist_rainbow,Blues,binary,gist_gray,PuOr,Set2,rainbow,copper,RdBu_r,Oranges,Set1,afmhot,BuPu,gnuplot2,brg,terrain,YlGnBu,tab20,Greys,bwr,RdPu,PuRd,tab20b,PiYG,hot,gnuplot,YlGn,Dark2,prism,Spectral,Paired,RdPu_r,RdBu,RdYlBu,GnBu,cubehelix,CMRmap,jet,nipy_spectral) Using default: 'RdBu_r'.\r\n\r\n### Actual Input Files Used\r\n\r\n- **Germany-Population-Annual--Change-2025-05-27-15-17.csv**\r\n  - Format: `text/plain`\r\n  - Path: `datasets/Germany-Population-Annual--Change-2025-05-27-15-17.csv_31e7840b5aedca43e53def1f4fdf0064.csv`\r\n\r\n\r\n## Workflow Parameters\r\n\r\n- **input:**\r\n  - __class__: `NoReplacement`\r\n\r\n- **adv:**\r\n  - colormap: `{'__class__': 'ConnectedValue'}`\r\n  - format_date: `{'__class__': 'ConnectedValue'}`\r\n  - format_plot: `{'__class__': 'ConnectedValue'}`\r\n  - nxsplit: `{'__class__': 'ConnectedValue'}`\r\n  - xname: `{'__class__': 'ConnectedValue'}`\r\n\r\n- **ifilename:**\r\n  - __class__: `ConnectedValue`\r\n\r\n- **title:**\r\n  - __class__: `ConnectedValue`\r\n\r\n- **variable:**\r\n  - __class__: `ConnectedValue`\r\n\r\n\r\n## Workflow Outputs\r\n\r\n### Formal Output Definitions\r\n\r\n- **stripes.png** (File)\r\n\r\n### Actual Output Files Generated\r\n\r\n- **stripes.png**\r\n  - Format: `application/octet-stream`\r\n  - Path: `datasets/stripes.png_31e7840b5aedca437fdd404c00a60dc0.png`\r\n\r\n\r\n## Rerun Template\r\n\r\nTo rerun this workflow:\r\n\r\n1. **Workflow:** Climate Stripes\r\n\r\n2. **Required inputs:**\r\n   - Germany-Population-Annual--Change-2025-05-27-15-17.csv (type: `File`)\r\n   - Column name to use for plotting (type: `Text`)\r\n   - Plot Title (type: `Text`)\r\n   - nxsplit (type: `Integer`)\r\n   - xname (column name for the x-axis) (type: `Text`)\r\n   - Date/time format for the x-axis column (type: `Text`)\r\n   - dates format for xlabels (type: `Text`)\r\n   - Matplotlib colormap (type: `Text`)\r\n\r\n3. **Parameters to set:**\r\n   - input:\r\n     - __class__: `NoReplacement`\r\n   - adv:\r\n     - colormap: `{'__class__': 'ConnectedValue'}`\r\n     - format_date: `{'__class__': 'ConnectedValue'}`\r\n     - format_plot: `{'__class__': 'ConnectedValue'}`\r\n     - nxsplit: `{'__class__': 'ConnectedValue'}`\r\n     - xname: `{'__class__': 'ConnectedValue'}`\r\n   - ifilename:\r\n     - __class__: `ConnectedValue`\r\n   - title:\r\n     - __class__: `ConnectedValue`\r\n   - variable:\r\n     - __class__: `ConnectedValue`\r\n\r\n4. **Expected outputs:**\r\n   - stripes.png (type: `File`)",
        "doi": "10.48546/workflowhub.workflow.1382.1",
        "edam_operation": [
            "Visualisation"
        ],
        "edam_topic": [
            "Environmental sciences"
        ],
        "filtered_on": "binn* in description",
        "id": "1382",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1382?version=1",
        "name": "Annual percentage Change for population in Germany (1950 - 2025)",
        "number_of_steps": 1,
        "projects": [
            "Galaxy Climate"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Generate stripe from tabular file with at least one column. User needs to specify the column name in the tool. "
        ],
        "type": "Galaxy",
        "update_time": "2025-05-28",
        "versions": 1
    },
    {
        "create_time": "2025-05-24",
        "creators": [
            "Leon Bichmann"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-mhcquant_logo_dark.png\">\n    <img alt=\"nf-core/mhcquant\" src=\"docs/images/nf-core-mhcquant_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/mhcquant/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/mhcquant/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/mhcquant/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/mhcquant/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/mhcquant/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.8427707-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.8427707)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/mhcquant)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23mhcquant-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/mhcquant)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/mhcquant** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/mhcquant \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/mhcquant/usage) and the [parameter documentation](https://nf-co.re/mhcquant/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/mhcquant/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/mhcquant/output).\n\n## Credits\n\nnf-core/mhcquant was originally written by Jonas Scheid, Steffen Lemke, Leon Bichmann, Marissa Dubbelaar.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#mhcquant` channel](https://nfcore.slack.com/channels/mhcquant) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/mhcquant for your analysis, please cite it using the following doi: [10.5281/zenodo.8427707](https://doi.org/10.5281/zenodo.8427707) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "999",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/999?version=24",
        "name": "nf-core/mhcquant",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dda",
            "immunopeptidomics",
            "mass-spectrometry",
            "mhc",
            "openms",
            "peptides"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-24",
        "versions": 24
    },
    {
        "create_time": "2025-05-24",
        "creators": [
            "Friederike Hanssen"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-bamtofastq_logo_dark.png\">\n    <img alt=\"nf-core/bamtofastq\" src=\"docs/images/nf-core-bamtofastq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/bamtofastq/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/bamtofastq/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/bamtofastq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/bamtofastq/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/bamtofastq/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.4710628-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.4710628)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/bamtofastq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23bamtofastq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/bamtofastq)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/bamtofastq** is a bioinformatics best-practice analysis pipeline that converts (un)mapped `.bam` or `.cram` files into `fq.gz` files. Initially, it auto-detects, whether the input file contains single-end or paired-end reads. Following this step, the reads are sorted using `samtools collate` and extracted with `samtools fastq`. Furthermore, for mapped bam/cram files it is possible to only convert reads mapping to a specific region or chromosome. The obtained FastQ files can then be used to further process with other pipelines.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources.The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/bamtofastq/results).\n\n## Pipeline summary\n\nBy default, the pipeline currently performs the following steps:\n\n1. Quality control (QC) of input (bam/cram) files ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)).\n2. Check if input files are single- or paired-end ([`Samtools`](https://www.htslib.org/)).\n3. Compute statistics on input files ([`Samtools`](https://www.htslib.org/)).\n4. Convert to fastq reads ([`Samtools`](https://www.htslib.org/)).\n5. QC of converted fastq reads ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)).\n6. Checking whether the produced fastq files are valid ([fastq_utils](https://github.com/nunofonseca/fastq_utils)).\n7. Summarize QC and statistics before and after format conversion ([`MultiQC`](http://multiqc.info/)).\n\n<p align=\"center\">\n    <img title=\"Bamtofastq Workflow\" src=\"docs/images/nf-core-bamtofastq-subway.png\" width=60%>\n</p>\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nDownload the pipeline and test it on a minimal dataset with a single command:\n\n```bash\nnextflow run nf-core/bamtofastq -profile test,<docker/singularity/.../institute> --outdir './results'\n```\n\nTo run your own analysis, start by preparing a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample_id,mapped,index,file_type\ntest,test1.bam,test1.bam.bai,bam\ntest2,test2.bam,test2.bam.bai,bam\n```\n\nEach row represents a bam/cram file with or without indices.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/bamtofastq \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/bamtofastq/usage) and the [parameter documentation](https://nf-co.re/bamtofastq/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/bamtofastq/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/bamtofastq/output).\n\n## Credits\n\nnf-core/bamtofastq was originally written by Friederike Hanssen. It was ported to DSL2 by Susanne Jodoin.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Gisela Gabernet](https://github.com/ggabernet)\n- [Matilda \u00c5slin](https://github.com/matrulda)\n- [Bruno Grande](https://github.com/BrunoGrandePhd)\n\n### Resources\n\nThe individual steps of this pipeline are based of on the following tutorials and resources:\n\n1.  [Extracting paired FASTQ read data from a BAM mapping file](http://darencard.net/blog/2017-09-07-extract-fastq-bam/)\n2.  [Check if BAM is derived from pair-end or single-end reads](https://www.biostars.org/p/178730/)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#bamtofastq` channel](https://nfcore.slack.com/channels/bamtofastq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/bamtofastq for your analysis, please cite it using the following doi: [10.5281/zenodo.4710628](https://doi.org/10.5281/zenodo.4710628)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "968",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/968?version=7",
        "name": "nf-core/bamtofastq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "conversion",
            "bamtofastq",
            "cramtofastq"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-24",
        "versions": 7
    },
    {
        "create_time": "2025-05-23",
        "creators": [
            "No author provided",
            "Hadrien Gourl\u00e9",
            "Daniel Straub",
            "Sabrina Krakau"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/mag_logo_mascot_dark.png\">\n    <img alt=\"nf-core/mag\" src=\"docs/images/mag_logo_mascot_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/mag/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/mag/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/mag/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/mag/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/mag/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.3589527-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.3589527)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n[![Cite Publication](https://img.shields.io/badge/Cite%20Us!-Cite%20Publication-orange)](https://doi.org/10.1093/nargab/lqac007)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.10.0-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/mag)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23mag-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/mag)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/mag** is a bioinformatics best-practise analysis pipeline for assembly, binning and annotation of metagenomes.\n\n<p align=\"center\">\n    <img src=\"docs/images/mag_workflow.png\" alt=\"nf-core/mag workflow overview\" width=\"90%\">\n</p>\n\n## Pipeline summary\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nBy default, the pipeline currently performs the following: it supports both short and long reads, quality trims the reads and adapters with [fastp](https://github.com/OpenGene/fastp), [AdapterRemoval](https://github.com/MikkelSchubert/adapterremoval), or [trimmomatic](https://github.com/usadellab/Trimmomatic) and [Porechop](https://github.com/rrwick/Porechop), and performs basic QC with [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/), and merges multiple sequencing runs.\n\nThe pipeline then:\n\n- assigns taxonomy to reads using [Centrifuge](https://ccb.jhu.edu/software/centrifuge/) and/or [Kraken2](https://github.com/DerrickWood/kraken2/wiki)\n- performs assembly using [MEGAHIT](https://github.com/voutcn/megahit) and [SPAdes](http://cab.spbu.ru/software/spades/), and checks their quality using [Quast](http://quast.sourceforge.net/quast)\n- (optionally) performs ancient DNA assembly validation using [PyDamage](https://github.com/maxibor/pydamage) and contig consensus sequence recalling with [Freebayes](https://github.com/freebayes/freebayes) and [BCFtools](http://samtools.github.io/bcftools/bcftools.html)\n- predicts protein-coding genes for the assemblies using [Prodigal](https://github.com/hyattpd/Prodigal), and bins with [Prokka](https://github.com/tseemann/prokka) and optionally [MetaEuk](https://www.google.com/search?channel=fs&client=ubuntu-sn&q=MetaEuk)\n- performs metagenome binning using [MetaBAT2](https://bitbucket.org/berkeleylab/metabat/src/master/), [MaxBin2](https://sourceforge.net/projects/maxbin2/), and/or with [CONCOCT](https://github.com/BinPro/CONCOCT), and checks the quality of the genome bins using [Busco](https://busco.ezlab.org/), [CheckM](https://ecogenomics.github.io/CheckM/), or [CheckM2](https://github.com/chklovski/CheckM2) and optionally [GUNC](https://grp-bork.embl-community.io/gunc/).\n- Performs ancient DNA validation and repair with [pyDamage](https://github.com/maxibor/pydamage) and [freebayes](https://github.com/freebayes/freebayes)\n- optionally refines bins with [DAS Tool](https://github.com/cmks/DAS_Tool)\n- assigns taxonomy to bins using [GTDB-Tk](https://github.com/Ecogenomics/GTDBTk) and/or [CAT](https://github.com/dutilh/CAT) and optionally identifies viruses in assemblies using [geNomad](https://github.com/apcamargo/genomad), or Eukaryotes with [Tiara](https://github.com/ibe-uw/tiara)\n\nFurthermore, the pipeline creates various reports in the results directory specified, including a [MultiQC](https://multiqc.info/) report summarizing some of the findings and software versions.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n```bash\nnextflow run nf-core/mag -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> --input '*_R{1,2}.fastq.gz' --outdir <OUTDIR>\n```\n\nor\n\n```bash\nnextflow run nf-core/mag -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> --input samplesheet.csv --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/mag/usage) and the [parameter documentation](https://nf-co.re/mag/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/mag/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/mag/output).\n\n### Group-wise co-assembly and co-abundance computation\n\nEach sample has an associated group ID (see [input specifications](https://nf-co.re/mag/usage#input_specifications)). This group information can be used for group-wise co-assembly with `MEGAHIT` or `SPAdes` and/or to compute co-abundances for the binning step with `MetaBAT2`. By default, group-wise co-assembly is disabled, while the computation of group-wise co-abundances is enabled. For more information about how this group information can be used see the documentation for the parameters [`--coassemble_group`](https://nf-co.re/mag/parameters#coassemble_group) and [`--binning_map_mode`](https://nf-co.re/mag/parameters#binning_map_mode).\n\nWhen group-wise co-assembly is enabled, `SPAdes` is run on accordingly pooled read files, since `metaSPAdes` does not yet allow the input of multiple samples or libraries. In contrast, `MEGAHIT` is run for each group while supplying lists of the individual readfiles.\n\n## Credits\n\nnf-core/mag was written by [Hadrien Gourl\u00e9](https://hadriengourle.com) at [SLU](https://slu.se), [Daniel Straub](https://github.com/d4straub) and [Sabrina Krakau](https://github.com/skrakau) at the [Quantitative Biology Center (QBiC)](http://qbic.life). [James A. Fellows Yates](https://github.com/jfy133) and [Maxime Borry](https://github.com/maxibor) at the [Max Planck Institute for Evolutionary Anthropology](https://www.eva.mpg.de) joined in version 2.2.0.\n\nOther code contributors include:\n\n- [Antonia Schuster](https://github.com/AntoniaSchuster)\n- [Alexander Ramos](https://github.com/alxndrdiaz)\n- [Carson Miller](https://github.com/CarsonJM)\n- [Daniel Lundin](https://github.com/erikrikarddaniel)\n- [Danielle Callan](https://github.com/d-callan)\n- [Gregory Sprenger](https://github.com/gregorysprenger)\n- [Jim Downie](https://github.com/prototaxites)\n- [Phil Palmer](https://github.com/PhilPalmer)\n- [@willros](https://github.com/willros)\n- [Adam Rosenbaum](https://github.com/muabnezor)\n- [Diego Alvarez](https://github.com/dialvarezs)\n- [Nikolaos Vergoulidis](https://github.com/IceGreb)\n\nLong read processing was inspired by [caspargross/HybridAssembly](https://github.com/caspargross/HybridAssembly) written by Caspar Gross [@caspargross](https://github.com/caspargross)\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Alexander Peltzer](https://github.com/apeltzer)\n- [Phil Ewels](https://github.com/ewels)\n- [Gisela Gabernet](https://github.com/ggabernet)\n- [Harshil Patel](https://github.com/drpatelh)\n- [Johannes Alneberg](https://github.com/alneberg)\n- [Maxime Garcia](https://github.com/MaxUlysse)\n- [Michael L Heuer](https://github.com/heuermh)\n- [Alex H\u00fcbner](https://github.com/alexhbnr)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#mag` channel](https://nfcore.slack.com/channels/mag) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/mag for your analysis, please cite the preprint as follows:\n\n> **nf-core/mag: a best-practice pipeline for metagenome hybrid assembly and binning**\n>\n> Sabrina Krakau, Daniel Straub, Hadrien Gourl\u00e9, Gisela Gabernet, Sven Nahnsen.\n>\n> NAR Genom Bioinform. 2022 Feb 2;4(1):lqac007. doi: [10.1093/nargab/lqac007](https://doi.org/10.1093/nargab/lqac007).\n\nAdditionally you can cite the pipeline directly with the following doi: [10.5281/zenodo.3589527](https://doi.org/10.5281/zenodo.3589527)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "995",
        "keep": true,
        "latest_version": 30,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/995?version=30",
        "name": "nf-core/mag",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "assembly",
            "metagenomics",
            "binning",
            "long-read-sequencing",
            "metagenomes",
            "nanopore",
            "nanopore-sequencing"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-23",
        "versions": 30
    },
    {
        "create_time": "2025-05-23",
        "creators": [
            "Maxime Borry"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-coproid_logo_dark.png\">\n    <img alt=\"nf-core/coproid\" src=\"docs/images/nf-core-coproid_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/coproid/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/coproid/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/coproid/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/coproid/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/coproid/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/coproid)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23coproid-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/coproid)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/coproid** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/coproid \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/coproid/usage) and the [parameter documentation](https://nf-co.re/coproid/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/coproid/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/coproid/output).\n\n## Credits\n\nnf-core/coproid was originally written by Maxime Borry & Meriam Van Os.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#coproid` channel](https://nfcore.slack.com/channels/coproid) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/coproid for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in tags",
        "id": "974",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/974?version=4",
        "name": "nf-core/coproid",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "adna",
            "ancient-dna",
            "coprolite",
            "microbiome"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-23",
        "versions": 4
    },
    {
        "create_time": "2025-05-20",
        "creators": [
            "workflow4metabolomics"
        ],
        "description": "This workflow is composed with the XCMS tool R package (Smith, C.A. 2006) able to extract and the metaMS R package (Wehrens, R 2014) for the field of untargeted metabolomics. \n\nhttps://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/gcms/tutorial.html",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in name",
        "id": "680",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/680?version=2",
        "name": "gcms-metams/main",
        "number_of_steps": 7,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "msnbase_readmsdata",
            "abims_xcms_xcmsSet",
            "xcms_plot_chromatogram",
            "metams_runGC",
            "checkFormat",
            "Multivariate",
            "xcms_merge"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-05-14",
        "creators": [],
        "description": "This workflows contains a pipeline in Scipion that performs the following steps:\r\n\r\n1.1) Import small molecules: introduces a set of small molecular structures in the pipeline as prospective ligands\r\n\r\n1.2) Import atomic structure: introduces a protein atomic structure in the pipeline as receptor.\r\n\r\n2.1) Ligand preparation: uses RDKit to prepare the small molecules optimizing their 3D structure.\r\n\r\n2.2) Receptor preparation: uses bioPython to prepare the receptor structure, removing waters, adding hydrogens and removing unnecessary chains if asked. Also, uses PDBFixer to optimize the structure if selected.\r\n\r\n3.1) Ligand filters: uses RDKit to perform ADME and PAINS filters on the prepared ligands to remove undesired molecules\r\n\r\n3.2) Protein pocket search: uses 3 different software (P2Rank, AutoSite and FPocket) for predicting the receptor pockets.\r\n\r\n4.2) Consensus pockets: common pockets are computed by clustering their contact residues in order to obtain the most promising pocket predicted by all 3 programs.\r\n\r\n5) Receptor-ligands docking: uses 3 different software (AutoDock-GPU, AutoDock-Vina and LeDock) to dock the prepared ligands onto the receptor pockets. \r\n\r\n6) Docked poses rescoring: uses ODDT Vina scoring to rescore the poses coming from all 3 different software in order to have a comparable score of the poses.\r\n\r\n7.1) Consensus docking:  common ligand poses are computed clustering by RMSD the different molecules in order to obtain the most promising predicted poses.\r\n\r\n7.2) Ranx scoring: the scores of the different programs are combined using Ranx in order to obtain a final score for each of the molecules.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1365",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1365?version=1",
        "name": "Consensus Virtual Drug Screening Workflow",
        "number_of_steps": 0,
        "projects": [
            "Scipion CNB"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "cheminformatics",
            "consensus",
            "drug design",
            "python",
            "virtual drug screening",
            "workflows",
            "drug discovery",
            "scipion"
        ],
        "tools": [
            "Scipion",
            "Scipion-chem",
            "AutoDock",
            "AutoDock Vina",
            "P2Rank",
            "AutoSite",
            "Fpocket",
            "RDKit"
        ],
        "type": "Scipion",
        "update_time": "2025-05-14",
        "versions": 1
    },
    {
        "create_time": "2025-05-10",
        "creators": [
            "Yongyi Luo",
            "Zhen Zhang",
            "Jiandong Shi",
            "Jingyu Hao",
            "Sheng Lian",
            "Taobo Hu",
            "Toyotaka Ishibashi",
            "Depeng Wang",
            "Shu Wang",
            "Weichuan Yu",
            "Xiaodan Fan"
        ],
        "description": "# BVSim: A Benchmarking Variation Simulator Mimicking Human Variation Spectrum\r\n\r\n[![Profile views](https://komarev.com/ghpvc/?username=YongyiLuo98&repo=BVSim&label=Profile%20views&color=0e75b6&style=flat)](https://github.com/YongyiLuo98/BVSim)\r\n## Table of Contents\r\n\r\n- [Getting Started](#getting-started)\r\n- [Installation](#installation)\r\n- [General Functions and Parameters](#parameters)\r\n  - [Shared Parameters](#shared-parameters)\r\n    - [Output Naming Conventions](#output)\r\n    - [Write the Relative Positions of Simulated Variations](#write)\r\n    - [User-defined Block Regions with No Variations](#block)\r\n  - [Uniform Mode](#uniform-mode)\r\n  - [Complex SV Mode](#complex-sv-mode)\r\n    - [Parameters for CSV Mode](#parameters-for-csv-mode)\r\n  - [Uniform Parallel Mode](#uniform-parallel-mode)\r\n    - [Parameters for Uniform parallel Mode](#parameters-for-uniform-parallel-mode)\r\n  - [Wave Mode](#wave-mode)\r\n    - [User-defined Sample(s) and Input BED File Requirements](#requirements-for-the-bed-file)\r\n    - [Generate a BED File for a Single Sample](#generating-a-bed-file-for-a-single-sample-in-wave-mode)\r\n    - [Job Submission for Single Sample (BED Format)](#job-submission-for-wave-mode-single-sample)\r\n    - [Generating BED Files for Multiple Samples](#generating-bed-files-for-multiple-samples-in-wave-mode)\r\n    - [Job Submission for Multiple Samples (BED Format)](#job-submission-for-wave-mode-multiple-samples)\r\n    - [Important Note on File Placement](#important-note-on-file-placement)\r\n    - [Parameters for Wave Mode](#parameters-for-wave-mode)\r\n  - [Wave Region Mode](#wave-region-mode)\r\n    - [Extract User-defined Regions (e.g. TR region) and Generate the BED File](#step-1-extract-tr-regions)\r\n    - [Job Submission for Single Sample (BED Format)](#job-submission-for-wave-region-mode-single-sample)\r\n    - [Parameters for Wave Region Mode](#parameters-for-wave-region-mode)\r\n  - [Human Genome](#human-genome)\r\n- [Uninstallation for Updates](#uninstallation)\r\n- [Workflow of BVSim](#workflow)\r\n- [Definitions of SVs Simulated by BVSim](#definitions)\r\n\r\n## <a name=\"getting-started\"></a>Getting Started\r\n\r\nTo get started with BVSim, follow these steps to install and run the simulator:\r\n\r\n```sh\r\n# Create an envrionment called BVSim and install the dependencies\r\nconda create -n BVSim python=3.11 numpy pandas biopython scipy seaborn psutil\r\nconda activate BVSim\r\n# Run the following to install pysam or use the latest guide\r\nconda config --add channels defaults\r\nconda config --add channels conda-forge\r\nconda config --add channels bioconda\r\nconda install pysam\r\n# Installzation\r\n## Clone the repository in your home path\r\ncd your_home_path\r\ngit clone https://github.com/YongyiLuo98/BVSim.git\r\n## Navigate to the ~/BVSim/main directory and install the package\r\npip install your_home_path/BVSim/main/.\r\n\r\n# Verify the installation in your home path\r\ncd your_home_path\r\npython -m BVSim --help\r\npython -m BVSim -h\r\n\r\n## Run a toy example with a specified reference in the cloned folder\r\nconda activate BVSim\r\npython -m BVSim -ref 'your_home_path/BVSim/empirical/sub_hg19_chr1.fasta' -seed 0 -rep 0 -write -snp 2000\r\n## If you prefer using the default reference, simply execute\r\ncd your_home_path\r\npython -m BVSim\r\n\r\n\r\n# Generate variations with specific parameters\r\ncd your_home_path\r\npython -m BVSim -seed 1 -rep 1 -snp 2000\r\n\r\n# To write out the relative positions, use the following command\r\npython your_home_path/BVSim/main/write_SV.py your_home_path/BVSim/save/ BV_1_con0_chr1_SVtable.csv BV_1_con0_chr1_tem_ins_dic.npy\r\n\r\n# Create a block intervals BED file\r\ncd your_home_path\r\necho -e \"0\\t1000\\n3000\\t4000\" > block_intervals.bed\r\n\r\n# Run the simulator with block regions\r\ncd your_home_path\r\npython -m BVSim -seed 1 -rep 1 -write -snp 2000 -block_region_bed_url block_intervals.bed\r\n```\r\n\r\n## <a name=\"Installation\"></a>Installation\r\n### Create an envrionment called BVSim and install the dependencies\r\nTo start with, you need to install the dependent packages in an environment, for example called BVSim.\r\n```bash\r\n# Create an envrionment called BVSim and install the dependencies\r\nconda create -n BVSim python=3.11 numpy pandas biopython scipy seaborn psutil\r\nconda activate BVSim\r\n# Run the following to install pysam or use the latest guide\r\nconda config --add channels defaults\r\nconda config --add channels conda-forge\r\nconda config --add channels bioconda\r\nconda install pysam\r\n```\r\n### Clone the Repository\r\nNext, you need to clone the BVSim repository to your local machine. Execute the following command in your home directory:\r\n```bash\r\ncd your_home_path\r\ngit clone https://github.com/YongyiLuo98/BVSim.git\r\n```\r\n### Navigate to the Main Directory and Install the Package\r\nNext, navigate to the .../BVSim/main/ directory to install the package:\r\n```bash\r\npip install your_home_path/BVSim/main/.\r\n```\r\n### Verify the Installation\r\nAfter installation, you can verify it from your home directory. Execute the following commands:\r\n```bash\r\ncd\r\npython -m BVSim --help\r\npython -m BVSim -h\r\n```\r\nNote: You can only call BVSim in the cloned repository directory, while the installation must take place in the BVSim/main/ directory.\r\n#### Toy Example (Uniform mode):\r\n```bash\r\nconda activate BVSim\r\npython -m BVSim -ref 'your_home_path/BVSim/empirical/sub_hg19_chr1.fasta' -seed 0 -rep 0 -write -snp 2000\r\n```\r\nor you can use the default reference to test the installation by type the following in your home path. If you do not give a saving path, the outputs will go to \"your_home_path\\BVSim\\save\\\".\r\n\r\n```bash\r\ncd your_home_path\r\npython -m BVSim \r\n```\r\n## <a name=\"parameters\"></a>Functions and Parameters\r\n\r\nFive modes: uniform, uniform parallel, csv, wave, wave_region\r\n\r\n### <a name=\"shared-parameters\"></a>Shared Parameters\r\nThe BVSim package provides several functions (modes) and parameters for simulating genetic variations. Here is a table that introduces all the functions and different parameters:\r\n\r\n| Parameter | Type | Description | Default |\r\n| --- | --- | --- | --- |\r\n| `-ref` | str | Input reference file | '.../BVSim/empirical/sub_hg19_chr1.fasta' |\r\n| `-save` | str | Saving path | .../BVSim/save/ |\r\n| `-seed` | int | Seed for random number generator | 999 |\r\n| `-times` | int | Number of times | 10 |\r\n| `-rep` | int | Replication ID | 5 |\r\n| `-sv_trans` | int | Number of trans SV | 5 |\r\n| `-sv_inver` | int | Number of inversion SV | 5 |\r\n| `-sv_dup` | int | Number of tandem duplication | 5 |\r\n| `-sv_del` | int | Number of SV deletion | 5 |\r\n| `-sv_ins` | int | Number of SV insertion | 5 |\r\n| `-snp` | float | SNV number or probability | 5 |\r\n| `-snv_del` | float | SNV deletion number or probability | 5 |\r\n| `-snv_ins` | float | SNV insertion number or probability | 5 |\r\n| `-notblockN` | bool | Do not Block N positions | False |\r\n| `-write` | bool | Write full results | False |\r\n| `-delmin` | int | Minimum deletion length | 50 |\r\n| `-delmax` | int | Maximum deletion length | 60 |\r\n| `-insmin` | int | Minimum insertion length | 50 |\r\n| `-insmax` | int | Maximum insertion length | 450 |\r\n| `-dupmin` | int | Minimum duplication length | 50 |\r\n| `-dupmax` | int | Maximum duplication length | 450 |\r\n| `-invmin` | int | Minimum inversion length | 50 |\r\n| `-invmax` | int | Maximum inversion length | 450 |\r\n| `-dupmin` | int | Minimum duplication length | 50 |\r\n| `-dupmax` | int | Maximum duplication length | 450 |\r\n| `-transmin` | int | Minimum translocation length | 50 |\r\n| `-transmax` | int | Maximum translocation length | 450 |\r\n| `-block_region_bed_url` | str | local path of the block region BED file | None |\r\n\r\n#### <a name=\"output\"></a>Output Naming Conventions\r\nWhen you run the simulation tool, the output files are named based on the sequence name you input or the parameter `rep` you set (repetition number). Below is a summary of the output files you can expect:\r\n\r\n1. **FASTA File**:  \r\n   The output FASTA file will be named as follows:\r\n```\r\nBV_<rep>_seq_<seqname>.fasta\r\n```\r\nThis file contains the simulated sequence.\r\n\r\n2. **VCF File**:  \r\nThe VCF file will be named:\r\n```\r\nBV_<rep>_seq_<seqname>.vcf\r\n```\r\nThis file stores the simulated variations.\r\n\r\n3. **SV Table**:  \r\nThe SV table will have different naming conventions depending on whether you choose to include relative positions:\r\n- If you include relative positions (by using the `-write` flag):\r\n  ```\r\n  BV_<rep>_seq_<seqname>_SVtable_full.csv\r\n  ```\r\n- If you do not include relative positions:\r\n  ```\r\n  BV_<rep>_seq_<seqname>_SVtable.csv\r\n  ```\r\n\r\n4. **Numpy File**:  \r\nThe numpy file that records all inserted segments we need to update the relative positions will be named:\r\n```\r\nBV_<rep>_seq_<seqname>_tem_ins_dic.npy\r\n```\r\n#### <a name=\"write\"></a>Write the Relative Positions of Simulated Variations\r\nIf you choose not to generate the relative positions during the initial simulation run (i.e., you do not include the `-write` flag), the columns for relative positions in the SV table will be empty. However, you can still update these relative positions later using the saved intermediate files.\r\n##### Steps to Write Relative Positions After Simulation\r\n1. **Run the Initial Simulation**:  \r\nFor example, you can execute:\r\n```bash\r\ncd your_home_path\r\npython -m BVSim -seed 1 -rep 1 -snp 2000\r\n```\r\nIn this case you generated default number of elementary SVs and micro indels, as well as 20000 SNPs saved in the default directory with `BV_1_seq_chr1_SVtable.csv`, `BV_1_seq_chr1_tem_ins_dic.npy`.\r\n\r\n2. **Update Relative Positions**:\r\nYou can then run the following command to generate a table with the relative positions:\r\n```bash\r\npython your_home_path/BVSim/main/write_SV.py your_home_path/BVSim/save/ BV_1_seq_chr1_SVtable.csv BV_1_seq_chr1_tem_ins_dic.npy\r\n```\r\nThis command will create a file called called `full_BV_1_seq_chr1_SVtable.csv` in the same directory, which will contain the relative positions for all variations with respect to the consensus sequence.\r\nBy following this naming convention and steps, you can easily manage and update your output files as needed.\r\n#### <a name=\"block\"></a>User-defined Block Regions with No Variations\r\nThe input of the '-block_region_bed_url' should be two columns of positions(start;end) without headers seperated by '\\t'. To create a bed file, you can refer to the following example. In this case, positions from 0 to 999, from 3000 to 3999 cannot have any variation, so called blocked.\r\n\r\n#### Toy Example:\r\n```bash\r\ncd your_home_path\r\necho -e \"0\\t1000\\n3000\\t4000\" > block_intervals.bed\r\n# uniform.py\r\ncd your_home_path\r\npython -m BVSim -seed 1 -rep 1 -write -snp 2000 -block_region_bed_url block_intervals.bed\r\n```\r\n\r\n### <a name=\"uniform-mode\"></a>Uniform Mode\r\nIf you do not call any of the following parameters (-csv, -cores, -len_bins, -wave), the simulation will be generated one by one uniformly.\r\n\r\n#### Toy Example (Uniform mode):\r\n```bash\r\nconda activate BVSim\r\npython -m BVSim -ref 'hg19_chr1.fasta' -seed 0 -rep 0 -write -snp 2000\r\n```\r\n### <a name=\"complex-sv-mode\"></a>Complex SV Mode\r\nAdd -csv to your command, 18 types of Complex Structure Variations can be generated.\r\n\r\n* ID1: Tandem Inverted Duplication (TanInvDup)\r\n* ID2: Dispersed Inverted Duplication (DisInvDup)\r\n* ID3: Dispersed Duplication (DisDup)\r\n* ID4: Inversion with 5\u2019 or 3\u2019 Flanking Deletion (DEL+INV/INV+DEL)\r\n* ID5: 5\u2019 Deletion and Dispersed Inverted Duplication (DEL+DisInvDup)\r\n* ID6: 5\u2019 Deletion and Dispersed Duplication (DEL+DisDup)\r\n* ID7: Tandem Duplication and 3\u2019 Deletion (TanDup+DEL)\r\n* ID8: Tandem Inverted Duplication and 3\u2019 Deletion (TanInvDup+DEL)\r\n* ID9: Tandem Duplication, Deletion and Inversion (TanDup+DEL+INV)\r\n* ID10: Tandem Inverted Duplication, Deletion and Inversion (TanInvDup+DEL+INV)\r\n* ID11: Paired-Deletion Inversion (DEL+INV+DEL)\r\n* ID12: Inversion with 5\u2019 Flanking Duplication (DUP+INV)\r\n* ID13: Inversion with 3\u2019 Flanking Duplication (INV+DUP)\r\n* ID14: Paired-Duplication Inversion (DUP+INV+DUP)\r\n* ID15: Inversion with 5\u2019 Flanking Duplication and 3\u2019 Flanking Deletion (DUP+INV+DEL)\r\n* ID16: Inversion with 5\u2019 Flanking Deletion and 3\u2019 Flanking Duplication (DEL+INV+DUP)\r\n* ID17: Inverted Duplication with Flanking Triplication (DupTripDup-INV)\r\n* ID18: Insertion with Deletion (INSdel)\r\n#### Toy Example (CSV mode):\r\n```bash\r\ncd your_home_path\r\npython -m BVSim -ref 'your_home_path/BVSim/empirical/sub_hg19_chr1.fasta' -save your_saving_url -seed 1 -rep 1 -csv -write -snp 2000\r\n```\r\n#### <a name=\"parameters-for-csv-mode\"></a>Parameters for CSV Mode\r\nThe lengths of the CSVs follow different Gaussian distributions with modifiable means (-mu) and standard deviations (-sigma).\r\n| Parameter | Type | Description | Default |\r\n| --- | --- | --- | --- |\r\n| `-csv_num` | int | Number for each type of CSV, superior to -csv_total_num | 0 |\r\n| `-csv_total_num` | int | Total number for CSV, assign number of each type by empirical weights | 0 |\r\n| `-num_ID1_csv to -num_ID18_csv` | int | Number of respective CSV types | 5 |\r\n| `-mu_ID1 to -mu_ID18` | int | Mean of Gaussian distribution of CSV length | 1000 |\r\n| `-sigma_ID1 to -sigma_ID18` | int | Standard deviation of Gaussian distribution of CSV length | 100 |\r\n\r\n### <a name=\"uniform-parallel-mode\"></a>Uniform Parallel Mode\r\nAdd -cores, -len_bins to your command, and write a .job file (task01.job) as follows (-c 5 means 5 cores, should be the same as -cores 5), parallel simulation will be allowed.\r\n\r\n#### Toy Example (Uniform-parallel mode): task01.job\r\n```bash\r\n#!/bin/bash\r\n#SBATCH -J uniform_parallel\r\n#SBATCH -N 1 -c 5\r\n#SBATCH --output=output.txt\r\n#SBATCH --error=err.txt\r\n\r\nsource /opt/share/etc/miniconda3-py39.sh\r\nconda activate BVSim\r\ncd your_home_path\r\npython -m BVSim -ref your_home_path/hg19/hg19_chr21.fasta -save your_home_path/test_data/BVSim/task03/ -cores 5 -len_bins 500000 -rep 3 -snp 200 -snv_del 200 -snv_ins 200 -write\r\nconda deactivate\r\n```\r\nSubmit the job file by:\r\n```bash\r\nsbatch task01.job\r\n```\r\n#### <a name=\"parameters-for-uniform-parallel-mode\"></a>Parameters for Uniform parallel Mode\r\n\r\n| Parameter | Type | Description | Default |\r\n| --- | --- | --- | --- |\r\n| `-cores` | int | Number of kernels for parallel processing | 1 |\r\n| `-len_bins` | int | Length of bins for parallel processing | 50000 |\r\n\r\n### <a name=\"wave-mode\"></a>Wave Mode\r\n\r\nIn Wave mode, users can provide a `.bed` file generated from an empirical `.vcf` file (for example, from HG002) or multiple BED files derived from samples of a selected population (such as the 15 Cell samples). This functionality allows you to generate non-uniform insertions and deletions with various options.\r\n\r\n#### <a name=\"requirements-for-the-bed-file\"></a>User-defined Sample(s) and Input BED File Requirements\r\n\r\nThe BED file must adhere to the following requirements:\r\n\r\n- **First Column**: Location (genomic position)\r\n- **Second Column**: DEL/INS label (indicating if the variation is a deletion or insertion)\r\n- **Third Column**: Length (absolute value of the variation)\r\n\r\nEach column should be separated by a tab character (`\\t`) and must not include headers. Additionally, each BED file should represent variations on the same sequence.\r\n\r\n#### <a name=\"generating-a-bed-file-for-a-single-sample-in-wave-mode\"></a>Generate a BED File for a Single Sample\r\n\r\nTo generate a single input BED file from the HG002 `.vcf` file of chromosome 21, you can use the following commands in your terminal:\r\n\r\n```bash\r\n# Download the VCF file and its index\r\nwget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NIST_SV_v0.6/HG002_SVs_Tier1_v0.6.vcf.gz \r\nwget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NIST_SV_v0.6/HG002_SVs_Tier1_v0.6.vcf.gz.tbi\r\n\r\n# Activate the bcftools environment\r\nconda activate bcftools\r\n\r\n# Generate the BED file using bcftools and awk\r\nbcftools view -H -r 21 -i 'SVTYPE=\"INS\" || SVTYPE=\"DEL\"' /home/adduser/data/test_data/TGS/hg002/HG002_SVs_Tier1_v0.6.vcf.gz | \\\r\nawk -v OFS='\\t' '{\r\n    split($8, a, \";\");\r\n    for (i in a) {\r\n        if (a[i] ~ /^SVTYPE/) {\r\n            split(a[i], b, \"=\");\r\n            svtype = b[2];\r\n        }\r\n        else if (a[i] ~ /^SVLEN/) {\r\n            split(a[i], c, \"=\");\r\n            svlen = c[2];\r\n            if (svlen < 0) svlen = -svlen;  # Extract the absolute value of SV length\r\n        }\r\n    }\r\n    print $2, svtype, svlen;  # Print the location, SV type, and absolute SV length\r\n}' > /home/adduser/data/test_data/TGS/hg002/chr21_SV_Tier1.bed\r\n```\r\n##### <a name=\"job-submission-for-wave-mode-single-sample\"></a>Job Submission for Single Sample (BED Format)\r\n\r\nTo utilize this single BED file, users should call '-indel_input_bed' in the command. Below is the example of a SLURM job script that you can use to run the Wave mode simulation with single empirical data:\r\n\r\n```bash\r\n#!/bin/bash\r\n#SBATCH -J full_chr21_parallel\r\n#SBATCH -N 1 -c 5\r\n#SBATCH --output=output_chr21_wave.txt\r\n#SBATCH --error=err_chr21_wave.txt\r\n\r\nsource /opt/share/etc/miniconda3-py39.sh\r\nconda activate BVSim\r\ncd your_home_path\r\npython -m BVSim -ref your_home_path/hg19/hg19_chr21.fasta -save your_home_path/test_data/BVSim -seed 0 -rep 2 -cores 5 -len_bins 500000 -wave -indel_input_bed your_home_path/hg002/chr21_SV_Tier1_2.bed -mode empirical -snp 2000 -snv_del 1000 -snv_ins 100 -write\r\nconda deactivate\r\n```\r\nSubmit the job file by:\r\n```bash\r\nsbatch task02_single.job\r\n```\r\n#### <a name=\"generating-bed-files-for-multiple-samples-in-wave-mode\"></a>Generating BED Files for Multiple Samples\r\n\r\nIn this section, we will outline the steps to generate `.bed` files for multiple cell samples from the original Excel spreadsheet, using the 15 Cell samples as an example.\r\n\r\n##### Step 1: Download the Original Excel File\r\n\r\nFirst, download the Excel file containing the cell samples data:\r\n\r\n```python\r\nimport os\r\n\r\n# Download the Excel file\r\nos.system('wget https://ars.els-cdn.com/content/image/1-s2.0-S0092867418316337-mmc1.xlsx')\r\n\r\n```\r\n##### Step 2: Load and View the Data\r\nNext, load the Excel file into a Pandas DataFrame and view the first few rows:\r\n```python\r\nimport pandas as pd\r\n\r\n# Read the Excel file into a DataFrame\r\nfile_path = '1-s2.0-S0092867418316337-mmc1.xlsx'\r\ndf = pd.read_excel(file_path, sheet_name=0)  # Choose the correct sheet based on the file\r\n\r\n# Display the first 5 rows of the DataFrame\r\nprint(df.head(5))\r\n```\r\n##### Step 3: Filter the Data\r\nExtract the required columns and rename the first column:\r\n```python\r\n# Extract the necessary columns\r\ncolumns_to_keep = ['#CHROM', 'POS', 'END', 'ID', 'SVTYPE', 'SVLEN', 'MERGE_SAMPLES']\r\ncell_df = df[columns_to_keep]\r\n\r\n# List of all sample strings\r\nsamples = ['CHM1', 'CHM13', 'HG00514', 'HG00733', 'NA19240', 'HG02818', 'NA19434', 'HG01352', 'HG02059', 'NA12878', 'HG04217', 'HG02106', 'HG00268', 'AK1', 'HX1']\r\n# selected population: the African population\r\nAFR_samples = ['NA19240', 'HG02818', 'NA19434']\r\n\r\n# Specify the columns to save in the BED file\r\ncolumns_to_save = ['POS', 'SVTYPE', 'SVLEN']\r\n\r\n# Extract rows where CHROM equals 'chr21'\r\nchr21_df = cell_df[cell_df['CHROM'] == 'chr21']\r\n\r\n# Display the first 10 rows for verification\r\nprint(chr21_df.head(10))\r\n\r\n# Generate BED files for each sample in the AFR_samples list\r\nfor sample in AFR_samples:\r\n    # Create a new DataFrame containing only rows where 'MERGE_SAMPLES' contains the current sample\r\n    sample_df = chr21_df[chr21_df['MERGE_SAMPLES'].str.contains(sample)]\r\n\r\n    # Specify the path for the new BED file\r\n    bed_file_path = f'.../BVSim/empirical/{sample}_chr21.bed'\r\n\r\n    # Save the specified columns to a BED file\r\n    sample_df[columns_to_save].to_csv(bed_file_path, sep='\\t', header=False, index=False)\r\n\r\n```\r\n#### <a name=\"job-submission-for-wave-mode-multiple-samples\"></a>Job Submission for Multiple Samples (BED Format)\r\n\r\nWe provide an example of a Job submission script using SLURM for running the Wave mode with BVSim. This script utilizes the generated multiple sample BED files. Below is the example of a SLURM job script that you can use to run the Wave mode simulation with multiple samples:\r\n\r\n```bash\r\n#!/bin/bash\r\n#SBATCH -J wave\r\n#SBATCH -N 1 -c 5\r\n#SBATCH --output=/home/project18/code/BVSim_code/wave2_out.txt\r\n#SBATCH --error=/home/project18/code/BVSim_code/wave2_err.txt\r\n\r\nsource /opt/share/etc/miniconda3-py39.sh\r\nconda activate BVSim\r\ncd /home/project18/\r\n\r\npython -m BVSim -ref your_home_path/hg38/chr21.fasta \\\r\n-save your_home_path/BVSim/task01/ -seed 0 -rep 1 -cores 5 \\\r\n-len_bins 500000 -wave -mode empirical -snp 2000 -snv_del 1000 -snv_ins 100 \\\r\n-write -file_list NA19240_chr21 HG02818_chr21 NA19434_chr21\r\n\r\nconda deactivate\r\n```\r\n#### <a name=\"important-note-on-file-placement\"></a>Important Note on File Placement\r\nEnsure that both the single sample and multiple sample BED files are placed in the .../BVSim/empirical/ directory. This organization simplifies the command structure, allowing you to specify only the base names of the files (without extensions) directly in the -file_list option, as demonstrated in the script above.\r\n\r\n#### <a name=\"parameters-for-wave-mode\"></a>Parameters for Wave Mode\r\n\r\n| Parameter | Type | Description | Default |\r\n| --- | --- | --- | --- |\r\n| `-cores` | int | Number of kernels for parallel processing | 1 |\r\n| `-len_bins` | int | Length of bins for parallel processing | 50000 |\r\n| `-wave` | bool | Run Wave.py script | False |\r\n| `-mode` | str | Mode for calculating probabilities | 'probability' |\r\n| `-sum` | bool | Total indel SV equals sum of the input bed | False |\r\n| `-indel_input_bed` | str | Input single BED file | None |\r\n| `-file_list` | str | Input list of multiple BED files | None |\r\n\r\n##### Mode and Sum Parameters\r\n\r\nThe `-mode` parameter determines how the simulation calculates probabilities for insertions and deletions. It accepts two values:\r\n\r\n- **'probability'**: In this mode, probabilities for insertions and deletions are derived from the empirical data provided in the input BED files. The total number of variations can be defined by the `-sum` parameter. If `-sum` is set to `True`, the total number of insertions or deletions will be the maximum of the calculated empirical total or the specified values in `-sv_ins` or `-sv_del`. This allows for flexibility in controlling the total number of SVs in the simulation.\r\n\r\n- **'empirical'**: When set to this mode, the simulation directly uses the empirical values from the input data without any probability calculations. The total number of variations will be the sum of the provided empirical data.\r\n\r\nThe `-sum` parameter, when enabled, alters the total number of insertions and deletions based on the specified empirical data. If disabled, the simulation uses the fixed total values defined in `-sv_ins` and `-sv_del`, regardless of the empirical input.\r\n\r\n\r\n### <a name=\"wave-region-mode\"></a>Wave Region Mode\r\n\r\nIn Wave region mode, you can specify different INDEL probabilities using a BED file defined by `region_bed_url`. For example, if you want to increase the insertion and deletion probabilities in the tandem repeat (TR) regions of hg19, you can follow these steps.\r\n\r\n#### <a name=\"step-1-extract-tr-regions\"></a>Extract User-defined Regions (e.g. TR region) and Generate the BED File\r\n\r\nFirst, extract the TR regions' positions from UCSC and create a BED file with two columns (start; end) separated by a tab character (`\\t`).\r\n\r\nYou can generate the BED file using the following commands:\r\n\r\n```bash\r\n# Download the TR regions data\r\nwget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh37/resources/hg19.simpleRepeat.bed.gz \r\nwget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh37/resources/hg19.simpleRepeat.bed.gz.tbi\r\n\r\n# Extract the relevant columns and create the BED file\r\nzcat hg19.simpleRepeat.bed.gz | awk 'BEGIN{OFS=\"\\t\"} {print $1, $2, $3}' > your_home_path/hg002/windows_TR.bed\r\n\r\n# Merge overlapping intervals and remove duplicates\r\nbedtools sort -i your_home_path/hg002/windows_TR.bed | bedtools merge -i stdin | awk 'BEGIN{OFS=\"\\t\"} {$4=\"TR\"; print}' | uniq > your_home_path/hg002/windows_TR_unique.bed\r\n\r\n# Filter for chromosome 21\r\nawk '$1 == \"chr21\"' your_home_path/hg002/windows_TR_unique.bed > your_home_path/hg002/windows_TR_unique_chr21.bed\r\n\r\n# Create a final BED file with start and end positions\r\nawk '{print $2 \"\\t\" $3}' your_home_path/hg002/windows_TR_unique_chr21.bed > your_home_path/hg002/chr21_TR_unique.bed\r\n```\r\n#### <a name=\"job-submission-for-wave-region-mode-single-sample\"></a>Job Submission for Single Sample (BED Format)\r\nIn this example, we set the seed to `0` and use a replication ID of `4`. The job is configured to utilize `5` cores for parallel processing, with a bin size of `500,000`. We will generate `10,000` SNPs, along with `100` micro deletions and `100` micro insertions. The probabilities for these insertions and deletions are specified in the input BED file (`-indel_input_bed`) using the empirical mode (`-mode`). Additionally, we have set the probabilities for insertions (`-p_ins_region`) and deletions (`-p_del_region`) to approximately `0.6` for the total located in the TR region defined by `-region_bed_url`.\r\n\r\n```bash\r\n#!/bin/bash\r\n#SBATCH -J full_chr21_parallel\r\n#SBATCH -N 1 -c 5\r\n#SBATCH --output=output_chr21_wave_region.txt\r\n#SBATCH --error=err_chr21_wave_region.txt\r\n\r\nsource /opt/share/etc/miniconda3-py39.sh\r\nconda activate BVSim\r\ncd your_home_path\r\npython -m BVSim -ref your_home_path/hg19/hg19_chr21.fasta -save your_home_path/test_data/BVSim -seed 0 -rep 4 -cores 5 -len_bins 500000 -wave_region -indel_input_bed your_home_path/hg002/chr21_SV_Tier1.bed -mode empirical -snp 10000 -snv_del 100 -snv_ins 100 -write -p_del_region 0.6 -p_ins_region 0.6 -region_bed_url your_home_path/hg002/chr21_TR_unique.bed\r\nconda deactivate\r\n```\r\n\r\nSubmit the job file using the following command:\r\n```bash\r\nsbatch task03.job\r\n```\r\n#### <a name=\"parameters-for-wave-region-mode\"></a>Parameters for Wave Region Mode\r\nThe table below summarizes the parameters available for Wave region mode:\r\n| Parameter | Type | Description | Default |\r\n| --- | --- | --- | --- |\r\n| `-cores` | int | Number of kernels for parallel processing | 1 |\r\n| `-len_bins` | int | Length of bins for parallel processing | 50000 |\r\n| `-wave` | bool | Run Wave.py script | False |\r\n| `-mode` | str | Mode for calculating probabilities | 'probability' |\r\n| `-sum` | bool | Total number of insertions and deletions equals sum of the input bed | False |\r\n| `-indel_input_bed` | str | Input single BED file | None |\r\n| `-file_list` | str | Input list of multiple BED files | None |\r\n| `-wave_region` | bool | Run Wave_TR.py script | False |\r\n| `-p_del_region` | float | Probability of SV DEL in the user-defined region for deletion | 0.5 |\r\n| `-p_ins_region` | float | Probability of SV INS in the user-defined region for insertion | 0.5 |\r\n| `-region_bed_url` | str | Path of the BED file for the user-defined region | 'your_home_path/hg002/chr21_TR_unique.bed' |\r\n\r\n### <a name=\"human-genome\"></a>Human Genome\r\nFor the human genome, we derive the length distributions of SVs from HG002 and the 15 representative samples. For SNPs, we embed a learned substitution transition matrix from the dbSNP database. With a user-specified bin size, BVSim learns the distribution of SV positions per interval. It can model the SVs per interval as a multinomial distribution parameterized by the observed frequencies in HG002 (GRCh37/hg19 as reference) or sample the SV numbers per interval from a Gaussian distribution with the mean and standard deviation computed across the 15 samples (GRCh38/hg38 as reference). Calling \u2018-hg19\u2019 or \u2018-hg38\u2019 and specifying the chromosome name can activate the above procedures automatically for the human genome.\r\n\r\nIn the following example, we use 5 cores and 500,000 as length of the intervals. The reference is chromosome 21 of hg19, so we call \"-hg19 chr21\" in the command line to utilize the default procedure. In addition, we generated 1,000 SNPs, 99 duplications, 7 inversions, 280 deletions, and 202 insertions. The ratio of deletions/insertions in the tandem repeat regions with respect to the total number is 0.810/0.828. We also set the minimum and maximum lengths of some SVs.\r\n#### Toy example (-hg19)\r\n```bash\r\n#!/bin/bash\r\n#SBATCH -J 0_hg19_chr21\r\n#SBATCH -N 1 -c 5\r\n#SBATCH --output=output.txt\r\n#SBATCH --error=err.txt\r\n\r\nsource /opt/share/etc/miniconda3-py39.sh\r\nconda activate BVSim\r\ncd your_home_path\r\npython -m BVSim -ref your_home_path/hg19/hg19_chr21.fasta -save your_home_path/test_data/BVSim/ -seed 0 -rep 0 -cores 5 -len_bins 500000 -hg19 chr21 -mode probability -snp 1000 -sv_trans 0 -dup 99 -sv_inver 7 -sv_del 280 -sv_ins 202 -snv_del 0 -snv_ins 0 -p_del_region 0.810 -p_ins_region 0.828 -region_bed_url /home/project18/data/test_data/TGS/hg002/chr21_TR_unique.bed -delmin 50 -delmax 2964912 -insmin 50 -insmax 187524\r\nconda deactivate\r\n```\r\n\r\n\r\n## <a name=\"uninstallation\"></a>Uninstallation for Updates\r\nTo update to the latest version of BVSim, you can uninstall and delete the cloned files. Then, try to clone from the new repository and install again.\r\n```bash\r\ncd your_home_path\r\npip uninstall BVSim\r\n```\r\n## <a name=\"workflow\"></a>Workflow of BVSim\r\nThe following figure illustrates the workflow of BVSim, encapsulated within a dashed box, and demonstrates how the output files interact with read simulators, the alignment tool Minimap2, Samtools, and evaluation tools.\r\n![Workflow of BVSim](flow_chart_pipline.png)\r\n## <a name=\"definitions\"></a>Definitions of SVs Simulated by BVSim\r\n![Definitions of SVs](Fig1a_BVSim.png)\r\n",
        "doi": "10.48546/workflowhub.workflow.1361.1",
        "edam_operation": [
            "Statistical modelling"
        ],
        "edam_topic": [
            "Genetic variation"
        ],
        "filtered_on": "binn* in description",
        "id": "1361",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1361?version=1",
        "name": "BVSim: A Benchmarking Variation Simulator Mimicking Human Variation Spectrum",
        "number_of_steps": 0,
        "projects": [
            "Structural Variation Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genomics",
            "python"
        ],
        "tools": [],
        "type": "Unrecognized workflow type",
        "update_time": "2025-05-10",
        "versions": 1
    },
    {
        "create_time": "2025-05-10",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-methylong_logo_dark.png\">\n    <img alt=\"nf-core/methylong\" src=\"docs/images/nf-core-methylong_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/methylong/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/methylong/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/methylong/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/methylong/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/methylong/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/methylong)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23methylong-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/methylong)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/methylong** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/methylong \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/methylong/usage) and the [parameter documentation](https://nf-co.re/methylong/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/methylong/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/methylong/output).\n\n## Credits\n\nnf-core/methylong was originally written by Jin Yan Khoo.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#methylong` channel](https://nfcore.slack.com/channels/methylong) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/methylong for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1360",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1360?version=1",
        "name": "nf-core/methylong",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-10",
        "versions": 1
    },
    {
        "create_time": "2025-05-10",
        "creators": [
            "Christopher Mohr",
            "Alexander Peltzer"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-epitopeprediction_logo_dark.png\">\n    <img alt=\"nf-core/epitopeprediction\" src=\"docs/images/nf-core-epitopeprediction_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/epitopeprediction/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/epitopeprediction/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/epitopeprediction/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/epitopeprediction/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/epitopeprediction/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/epitopeprediction)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23epitopeprediction-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/epitopeprediction)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/epitopeprediction** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/epitopeprediction \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/epitopeprediction/usage) and the [parameter documentation](https://nf-co.re/epitopeprediction/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/epitopeprediction/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/epitopeprediction/output).\n\n## Credits\n\nnf-core/epitopeprediction was originally written by Christopher Mohr, Jonas Scheid.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#epitopeprediction` channel](https://nfcore.slack.com/channels/epitopeprediction) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/epitopeprediction for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in tags",
        "id": "984",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/984?version=9",
        "name": "nf-core/epitopeprediction",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "epitope",
            "epitope-prediction",
            "mhc-binding-prediction"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-10",
        "versions": 9
    },
    {
        "create_time": "2025-05-10",
        "creators": [
            "VGP",
            " Galaxy"
        ],
        "description": "Create Meryl Database used for the estimation of assembly parameters and quality control with Merqury. Part of the VGP pipeline.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in name",
        "id": "631",
        "keep": true,
        "latest_version": 6,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/631?version=6",
        "name": "kmer-profiling-hifi-trio-VGP2/main",
        "number_of_steps": 11,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "meryl_count_kmers",
            "genomescope",
            "meryl_groups_kmers",
            "meryl_histogram_kmers",
            "pick_value",
            "meryl"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 6
    },
    {
        "create_time": "2025-05-09",
        "creators": [],
        "description": "# CausalCoxMGM\r\nImplementation of CausalCoxMGM algorithm and scripts for analysis of simulated and real-world biomedical datasets.\r\n\r\n## Installation\r\n\r\nTo install CoxMGM and CausalCoxMGM, run the following command in the terminal:\r\n\r\n```\r\nR CMD INSTALL rCausalMGM\r\n```\r\n\r\nor alternatively:\r\n\r\n```\r\nR CMD INSTALL rCausalMGM/rCausalMGM_1.0.tar.gz\r\n```\r\n\r\n## Demonstration of CausalCoxMGM with the WHAS500 dataset\r\n\r\n\r\nFirst, we begin by loading the necessray R packages for this analysis.\r\n\r\n\r\n``` r\r\nlibrary(rCausalMGM)\r\nlibrary(survival)\r\nlibrary(dplyr)\r\nlibrary(survminer)\r\nlibrary(ggplot2)\r\n```\r\n\r\nNext, we load the WHAS500 dataset and format the data appropriately for analysis by CausalCoxMGM. First, we create our censored variables, Survival and LengthOfStay, using the `Surv` function from the `survival` R package. Then, we exclude features that are not clinical measurements or patient outcomes, and convert discrete variables into factors.\r\n\r\n\r\n``` r\r\ndata <- read.csv('cvd/whas500.csv', row.names=1)\r\n\r\ndata$LengthOfStay <- Surv(data$los, 1-data$dstat)\r\n\r\ndata$Survival <- Surv(data$lenfol, data$fstat)\r\n\r\ndata <- data %>% select(-c(\"admitdate\", \"disdate\", \"fdate\", \"dstat\", \"lenfol\", \"fstat\", \"year\", \"los\"))\r\n\r\ndata <- data %>% mutate_at(c(\"cvd\", \"afb\", \"sho\", \"chf\", \"av3\", \"gender\", \"miord\", \"mitype\"), factor)\r\n\r\nhead(data)\r\n```\r\n\r\n```\r\n##   age gender hr sysbp diasbp      bmi cvd afb sho chf av3 miord mitype\r\n## 1  83      0 89   152     78 25.54051   1   1   0   0   0     1      0\r\n## 2  49      0 84   120     60 24.02398   1   0   0   0   0     0      1\r\n## 3  70      1 83   147     88 22.14290   0   0   0   0   0     0      1\r\n## 4  70      0 65   123     76 26.63187   1   0   0   1   0     0      1\r\n## 5  70      0 63   135     85 24.41255   1   0   0   0   0     0      1\r\n## 6  70      0 76    83     54 23.24236   1   0   0   0   1     0      0\r\n##   LengthOfStay Survival\r\n## 1            5    2178+\r\n## 2            5    2172+\r\n## 3            5    2190+\r\n## 4           10      297\r\n## 5            6    2131+\r\n## 6           1+        1\r\n```\r\n\r\nNext, we perform stratified 5-fold cross-validation to select model hyperparameters. Cross-validation folds are stratified to have approximately the same number of all-cause mortality and hospital discharge events. We measure model performance based on the total deviance of the censored outcomes. Rather than performing a grid search over the hyperparameters, we perform a random search.\r\n\r\n\r\n``` r\r\nset.seed(43)\r\n\r\nidx00 <- which(data$Survival[,2]==0 & data$LengthOfStay[,2]==0)\r\nidx10 <- which(data$Survival[,2]==1 & data$LengthOfStay[,2]==0)\r\nidx01 <- which(data$Survival[,2]==0 & data$LengthOfStay[,2]==1)\r\nidx11 <- which(data$Survival[,2]==1 & data$LengthOfStay[,2]==1)\r\n\r\nfoldid <- rep(0, 500)\r\nfoldid[idx00] <- sample(((1:length(idx00))-1) %% 5 + 1)\r\nfoldid[idx10] <- sample(((1:length(idx10))-1) %% 5 + 1)\r\nfoldid[idx01] <- sample(((1:length(idx01))-1) %% 5 + 1)\r\nfoldid[idx11] <- sample(((1:length(idx11))-1) %% 5 + 1)\r\n\r\ntable(foldid, data$Survival[,2])\r\n```\r\n\r\n```\r\n##       \r\n## foldid  0  1\r\n##      1 57 44\r\n##      2 57 43\r\n##      3 57 43\r\n##      4 57 43\r\n##      5 57 42\r\n```\r\n\r\n``` r\r\ntable(foldid, data$LengthOfStay[,2])\r\n```\r\n\r\n```\r\n##       \r\n## foldid  0  1\r\n##      1  8 93\r\n##      2  8 92\r\n##      3  8 92\r\n##      4  8 92\r\n##      5  7 92\r\n```\r\n\r\n``` r\r\nlambdas <- runif(100, 0.05, 0.5)\r\n\r\nalphas <- runif(100, 0.01, 0.25)\r\n\r\nloglik <- matrix(0, 100, 5)\r\nsize <- matrix(0, 100, 5)\r\nfor (k in 1:5) {\r\n    ig.path <- coxmgmPath(data[foldid!=k,], lambdas=lambdas, rank=F)\r\n    idx <- 0\r\n    for (ig in ig.path$graphs) {\r\n        idx <- idx + 1\r\n        g <- fciStable(data[foldid!=k,], initialGraph=ig,\r\n                       alpha=alphas[idx], orientRule=\"maxp\", rank=F)\r\n        mb <- g$markov.blankets$Survival\r\n        size[idx,k] <- length(mb)\r\n        if (length(mb)==1) {\r\n            mb <- c(1)\r\n        }\r\n        f <- as.formula(paste(\"Survival ~\", paste(mb, collapse=\" + \")))\r\n        res <- coxph(f, data[foldid!=k,])\r\n        test.risk <- predict(res, newdata=data[foldid==k,])\r\n        res.test <- coxph(Survival ~ offset(test.risk), data[foldid==k,])\r\n        loglik[idx,k] <- -as.numeric(logLik(res.test))\r\n\r\n        mb <- g$markov.blankets$LengthOfStay\r\n        size[idx,k] <- size[idx,k] + length(mb)\r\n        if (length(mb)==1) {\r\n            mb <- c(1)\r\n        }\r\n        f <- as.formula(paste(\"LengthOfStay ~\", paste(mb, collapse=\" + \")))\r\n        res <- coxph(f, data[foldid!=k,])\r\n        test.risk <- predict(res, newdata=data[foldid==k,])\r\n        res.test <- coxph(LengthOfStay ~ offset(test.risk), data[foldid==k,])\r\n        loglik[idx,k] <- loglik[idx,k] + -as.numeric(logLik(res.test))\r\n    }\r\n}\r\n\r\nsizeMean <- rowMeans(size)\r\nloglikMean <- rowMeans(loglik)\r\nloglikSd <- apply(loglik, 1, sd)\r\n\r\nplot(sizeMean, loglikMean, pch=19, col='red')\r\n```\r\n\r\n![plot of chunk modelselectcv](figure/modelselectcv-1.png)\r\n\r\n``` r\r\nminIdx <- which.min(loglikMean)\r\n```\r\n\r\nNow that we have selected the best set of hyperparameters, we learn the final causal graphical model of all-cause mortality and hospital discharge after hospitilazation with acute myocardial infarction. This is done in two stages: first, we learn the undirected CoxMGM that serves as an initial estimate of the adjacencies in the causal graph. Second, we use FCI-Max to prune adjacencies and orient edges.\r\n\r\n\r\n``` r\r\nig.path <- coxmgmPath(data, lambda=lambdas)\r\n\r\ng <- fciStable(data, initialGraph=ig.path$graphs[[minIdx]],\r\n               alpha=alphas[minIdx], verbose=T, orientRule=\"maxp\")\r\n```\r\n\r\n```\r\n## Starting FCI-Stable algorithm...\r\n##   Starting FAS Stable...\r\n##     Searching at depth 0...\r\n##     Searching at depth 1...\r\n##     Searching at depth 2...\r\n##     Searching at depth 3...\r\n##     Searching at depth 4...\r\n##   FAS Stable Elapsed Time =  0.029 s\r\n##   RFCI adjacency pruning...\r\n##   Starting Posssible DSep search\r\n##     Starting Conservative Orientations...\r\n##       Filling Triple Map...\r\n##     Orienting colliders...\r\n##     Checking Possible-Dsep sets...\r\n##   Starting Final Orientations...\r\n##     Filling Triple Map...\r\n##   Orienting colliders...\r\n##   Orienting implied edges...\r\n##   FCI-Stable Elapsed Time =  0.25 s\r\n```\r\n\r\n``` r\r\ng\r\n```\r\n\r\n```\r\n## Algorithm:  CoxMGM-FCI-Max \r\n## Nodes:  15 \r\n## Edges:  17 \r\n##   Unoriented:  4 \r\n##   Partially Oriented:  6 \r\n##   Directed:  6 \r\n##   Bidirected:  1 \r\n## lambda = {0.3173977, 0.3173977, 0.3173977, 0.3173977, 0.3173977}\r\n## alpha =  0.02566852\r\n```\r\n\r\nFinally, we can generate a simple plot of the causal graphical model in R.\r\n\r\n\r\n``` r\r\nplot(g, nodeAttr=list(fontsize=36))\r\n```\r\n\r\n![plot of chunk finalmodelplot](figure/finalmodelplot-1.png)\r\n\r\n## License\r\n\r\nThe following data files are under the CC0 public domain:\r\n\r\n```\r\nwhas500.csv\r\nmetabric.rna.full.csv\r\nmetabric.rna.erp.full.csv\r\nmetabric.rna.ern.full.csv\r\nmeta_cohort_common_genes.rds\r\n```\r\n\r\nWhile all other files composing CausalCoxMGM are under the GPL-3.0 license.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Computational biology",
            "Machine learning",
            "Statistics and probability"
        ],
        "filtered_on": "metap* in description",
        "id": "1359",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1359?version=1",
        "name": "CausalCoxMGM",
        "number_of_steps": 0,
        "projects": [
            "CausalCoxMGM Team"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "R markdown",
        "update_time": "2025-05-09",
        "versions": 1
    },
    {
        "create_time": "2025-05-09",
        "creators": [
            "Ignacio Tripodi",
            "Margaret Gruca"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-nascent_logo_dark.png\">\n    <img alt=\"nf-core/nascent\" src=\"docs/images/nf-core-nascent_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/nascent/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/nascent/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/nascent/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/nascent/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/nascent/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/nascent)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23nascent-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/nascent)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/nascent** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/nascent \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/nascent/usage) and the [parameter documentation](https://nf-co.re/nascent/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/nascent/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/nascent/output).\n\n## Credits\n\nnf-core/nascent was originally written by Edmund Miller, Ignacio Tripodi, Margaret Gruca.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#nascent` channel](https://nfcore.slack.com/channels/nascent) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/nascent for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1004",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1004?version=6",
        "name": "nf-core/nascent",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gro-seq",
            "nascent",
            "pro-seq",
            "rna",
            "transcription",
            "tss"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-09",
        "versions": 6
    },
    {
        "create_time": "2025-06-22",
        "creators": [
            "B\u00e9r\u00e9nice Batut",
            "Paul Zierep",
            "Mina Hojat Ansari",
            "Patrick B\u00fchler",
            "Santino Faack"
        ],
        "description": "This workflow constructs Metagenome-Assembled Genomes (MAGs) using SPAdes or MEGAHIT as assemblers, followed by binning with four different tools and refinement using Binette. The resulting MAGs are dereplicated across the entire input sample set, then annotated and evaluated for quality.\nYou can provide pooled reads (for co-assembly/binning), individual read sets, or a combination of both. The input samples must consist of the original reads, which are used for abundance estimation. In all cases, reads should be trimmed, adapters removed, and cleaned of host or other contaminants before processing.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "mags in name",
        "id": "1352",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1352?version=3",
        "name": "mags-building/main",
        "number_of_steps": 36,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "__UNZIP_COLLECTION__",
            "binette",
            "megahit",
            "samtools_sort",
            "metabat2",
            "__BUILD_LIST__",
            "concoct_cut_up_fasta",
            "pick_value",
            "tp_awk_tool",
            "metabat2_jgi_summarize_bam_contig_depths",
            "__FLATTEN__",
            "bakta",
            "checkm_lineage_wf",
            "gtdbtk_classify_wf",
            "metaspades",
            "Fasta_to_Contig2Bin",
            "semibin",
            "quast",
            "multiqc",
            "concoct",
            "concoct_merge_cut_up_clustering",
            "concoct_extract_fasta_bins",
            "collection_column_join",
            "bowtie2",
            "checkm2",
            "coverm_genome",
            "concoct_coverage_table",
            "maxbin2",
            "drep_dereplicate",
            "map_param_value"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 3
    },
    {
        "create_time": "2025-04-28",
        "creators": [
            "Zargham Ahmad",
            "Helge Hecht",
            "Wudmir Rojas"
        ],
        "description": "High-Performance Computing (HPC) environments are integral to quantum chemistry and computationally intense research, yet their complexity poses challenges for non-HPC experts. Navigating these environments proves challenging for researchers lacking extensive computational knowledge, hindering efficient use of domain specific research software. The prediction of mass spectra for in silico annotation is therefore inaccessible for many wet lab scientists. Our main goal is to facilitate non-experts in HPC navigate this complexity and make semi-empirical Quantum Chemistry (QC)-based predictions available without needing advanced computational skills. To address this challenge, a comprehensive approach is proposed. We chose specific file formats for storing molecular structures, ensuring compatibility across diverse tools and platforms. The xTB quantum chemistry package for molecular geometry optimization is leveraged for its capability to balance between accuracy and computational cost, making it well-suited for non-HPC focused applications. Integrating QC-based Mass Spectrometry (QCxMS) into Galaxy enables the prediction of mass spectra and offers insights into molecular composition and properties. Our workflow demonstrates the utility of computing spectra using QCxMS along with complementary tools. We also present details of runtime performance metrics for four distinct molecules. This work highlights how non-HPC users can execute these predictions with ease, without requiring advanced computational skills. Additionally, a Docker image is created to encapsulate necessary tools, accompanied by user-friendly wrappers, simplifying the entire process for non-expert users. Within this context, potential improvements are considered, focusing on improving the Conda package for better performance by incorporating Fortran and Intel compiler optimizations. These considerations play a crucial role in refining the proposed methodology, enhancing user experience, and expanding the reach of semi-empirical predictions in quantum chemistry for mass spectra predictions.\r\n",
        "doi": "10.48546/workflowhub.workflow.897.3",
        "edam_operation": [
            "Modelling and simulation"
        ],
        "edam_topic": [
            "Computational chemistry"
        ],
        "filtered_on": "metap* in tags",
        "id": "897",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/897?version=3",
        "name": "End-to-end EI+ mass spectra prediction workflow using QCxMS",
        "number_of_steps": 8,
        "projects": [
            "RECETOX SpecDatRI",
            "ELIXIR Metabolomics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "exposomics",
            "gc-ms",
            "metabolomics",
            "qcxms"
        ],
        "tools": [
            "openbabel_compound_convert",
            "Remove failed runs\n__FILTER_FAILED_DATASETS__",
            "xtb_molecular_optimization",
            "qcxms_getres",
            "qcxms_production_run",
            "ctb_im_conformers",
            "qcxms_neutral_run"
        ],
        "type": "Galaxy",
        "update_time": "2025-04-28",
        "versions": 3
    },
    {
        "create_time": "2025-04-26",
        "creators": [
            "Chelsea Sawyer",
            "Edmund Miller",
            "Matthias De Smet"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-demultiplex_logo_dark.png\">\n    <img alt=\"nf-core/demultiplex\" src=\"docs/images/nf-core-demultiplex_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/demultiplex/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/demultiplex/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/demultiplex/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/demultiplex/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/demultiplex/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7153103-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7153103)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/demultiplex)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23demultiplex-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/demultiplex)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/demultiplex** is a bioinformatics pipeline used to demultiplex the raw data produced by next generation sequencing machines. The following platforms are supported:\n\n1. Illumina (via `bcl2fastq` or `bclconvert`)\n2. Element Biosciences (via `bases2fastq`)\n3. Singular Genomics (via [`sgdemux`](https://github.com/Singular-Genomics/singular-demux))\n4. FASTQ files with user supplied read structures (via [`fqtk`](https://github.com/fulcrumgenomics/fqtk))\n5. 10x Genomics (via [`mkfastq`](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/mkfastq))\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources.The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/demultiplex/results).\n\n## Pipeline summary\n\n1. [samshee](#samshee) - Validates illumina v2 samplesheets.\n2. Demultiplexing\n\n- [bcl-convert](#bcl-convert) - converting bcl files to fastq, and demultiplexing (CONDITIONAL)\n- [bases2fastq](#bases2fastq) - converting bases files to fastq, and demultiplexing (CONDITIONAL)\n- [bcl2fastq](#bcl2fastq) - converting bcl files to fastq, and demultiplexing (CONDITIONAL)\n- [sgdemux](#sgdemux) - demultiplexing bgzipped fastq files produced by Singular Genomics (CONDITIONAL)\n- [fqtk](#fqtk) - a toolkit for working with FASTQ files, written in Rust (CONDITIONAL)\n- [mkfastq](#mkfastq) - converting bcl files to fastq, and demultiplexing for single-cell sequencing data (CONDITIONAL)\n\n3. [checkqc](#checkqc) - (optional) Check quality criteria after demultiplexing (bcl2fastq only)\n4. [fastp](#fastp) - Adapter and quality trimming\n5. [Falco](#falco) - Raw read QC\n6. [md5sum](#md5sum) - Creates an MD5 (128-bit) checksum of every fastq.\n7. [MultiQC](#multiqc) - aggregate report, describing results of the whole pipeline\n\n![subway map](docs/demultiplex.png)\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\n```console\nnextflow run nf-core/demultiplex --input samplesheet.csv --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\n```\n\n```bash\nnextflow run nf-core/demultiplex \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/demultiplex/usage) and the [parameter documentation](https://nf-co.re/demultiplex/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/demultiplex/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/demultiplex/output).\n\n## Credits\n\nThe nf-core/demultiplex pipeline was written by Chelsea Sawyer from The Bioinformatics & Biostatistics Group for use at The Francis Crick Institute, London.\n\nThe pipeline was re-written in Nextflow DSL2 and is primarily maintained by Matthias De Smet([@matthdsm](https://github.com/matthdsm)) from [Center For Medical Genetics Ghent, Ghent University](https://github.com/CenterForMedicalGeneticsGhent) and Edmund Miller([@edmundmiller](https://github.com/edmundmiller)) from [Element Biosciences](https://www.elementbiosciences.com/)\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [`@ChristopherBarrington`](https://github.com/ChristopherBarrington)\n- [`@drpatelh`](https://github.com/drpatelh)\n- [`@danielecook`](https://github.com/danielecook)\n- [`@escudem`](https://github.com/escudem)\n- [`@crickbabs`](https://github.com/crickbabs)\n- [`@nh13`](https://github.com/nh13)\n- [`@sam-white04`](https://github.com/sam-white04)\n- [`@maxulysse`](https://github.com/maxulysse)\n- [`@atrigila`](https://github.com/atrigila)\n- [`@nschcolnicov`](https://github.com/nschcolnicov)\n- [`@aratz`](https://github.com/aratz)\n- [`@grst`](https://github.com/grst)\n- [`@apeltzer`](https://github.com/apeltzer)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#demultiplex` channel](https://nfcore.slack.com/channels/demultiplex) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/demultiplex for your analysis, please cite it using the following doi: [10.5281/zenodo.7153103](https://doi.org/10.5281/zenodo.7153103)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "978",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/978?version=15",
        "name": "nf-core/demultiplex",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bases2fastq",
            "bcl2fastq",
            "demultiplexing",
            "elementbiosciences",
            "illumina"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-04-26",
        "versions": 15
    },
    {
        "create_time": "2025-04-25",
        "creators": [
            "Dan Parsons"
        ],
        "description": "# Barcode Gene Extractor & Evaluator (BGEE) Snakemake workflow #\r\nSnakemake workflow for recovering high-quality barcode sequences from genome skim data, built around MitoGeneExtractor and adapted for genome skims of museum specimens.\r\n\r\n# Contents # \r\n - [Requirements](#requirements)\r\n - [Workflow](#workflow)\r\n - [Running](#running)\r\n - [Cluster configuration](#cluster-configuration)\r\n - [Results structure](#results-structure)\r\n - [Integrated and supplementary scripts](#Integrated-and-supplementary-scripts)\r\n - [Contributing](#Contributing)\r\n\r\n# Requirements #\r\n- [MitoGeneExtractor](https://github.com/cmayer/MitoGeneExtractor) installed. See [installation](https://github.com/cmayer/MitoGeneExtractor?tab=readme-ov-file#installation) instructions.\r\n- Paired-end reads in .fastq.gz or .fastq format.\r\n- samples_file.csv (Either manuualy or as [below](https://github.com/SchistoDan/MGE_snakemake_workflow?tab=readme-ov-file#2-generate-samplescsv)).\r\n- sequence_references_file.csv (Either manually, or using [Gene Fetch](https://github.com/bge-barcoding/gene_fetch?tab=readme-ov-file)).\r\n- Activated conda env (see mge_env.yaml)\r\n\r\n# Workflow #\r\n1. Preprocessing mode:\r\n  - 'concat':\r\n    - fastp_pe_concat - Adapter, trimming, quality trimming, poly-g trimming, and deduplication of paired-end reads (fastp).\r\n    - fastq_concat - Concatenates trimmed R1 and R2 files.\r\n    - Aggregate_concat_logs - Combines individual concatenation logs into a single log file.\r\n    - quality_trim - Performs additional quality trimming on concatenated reads (Trim Galore).\r\n    - Aggregate_trim_galore_logs - Combines individual Trim Galore logs into a single log file.\r\n  - 'merge:\r\n    - fastp_pe_merge - Adapter, trimming, quality trimming, Poly-G trimming, deduplication, and merging of paired-end reads (fastp).\r\n    - clean_headers_merge - Cleans sequence headers, as required by MitoGeneExtractor.\r\n    - Aggregate_clean_headers_logs - Combines individual header cleaning logs into a single log file.\r\n![image](https://github.com/user-attachments/assets/139b8c7c-b0dc-465c-8c95-e3a58ea1ab96)\r\n2. MitoGeneExtractor (MGE) -  Extracts gene of interest from processed reads by aligning them to protein references using Exonerate.\r\n3. rename_and_combine_con - Renames consensus sequence headers and concatenates them into a single FASTA file (uses supplementary [rename_headers.py](https://github.com/SchistoDan/BGEE/blob/main/workflow/scripts/rename_headers.py).\r\n4. create_alignment_log - Creates a list of MGE alignment files for downstream processing.\r\n5. [fasta_cleaner](https://github.com/bge-barcoding/fasta-cleaner) - Filters alignment files to remove low-quality, contaminant, or outlier sequences before consensus sequence generation.\r\n6. [fasta_compare](https://github.com/SchistoDan/BGEE/blob/main/workflow/scripts/fasta_compare.py) - Evaluates barcode consensus sequence quality based on various metrics (length, ambiguous base content, etc.), and selects the best sequences according to specific (BOLD BIN) quality criteria.\r\n7. extract_stats_to_csv - Compiles statistics from several fastp trimming, MGE, and fasta_cleaner output files into a CSV report (uses supplementary [mge_stats.py](https://github.com/SchistoDan/BGEE/blob/main/workflow/scripts/mge_stats.py)).\r\n8. cleanup_files - Removes temporary files and superfluous logs.\r\n\r\n# Running: #\r\n## Set up conda environment and clone this github repository ##\r\n- [Install miniconda](https://www.anaconda.com/docs/getting-started/miniconda/install#quickstart-install-instructions).\r\n```bash\r\ngit clone https://github.com/bge-barcoding/MGE_snakemake_workflow.git [path/to/where/you/want/to/install/]\r\ncd MGE_snakemake_workflow/installation/dir/\r\nconda env create -f /workflow/scripts/mge_env.yaml\r\ngit status\r\n```\r\n\r\n## Generate samples.csv ###\r\n- Can be created manually, or via [sample-processing](https://github.com/bge-barcoding/sample-processing) workflow.\r\n- Must contain ID, forward (read paths), reverse (read paths), and taxid columns (see below for example). Column 1 can be named 'ID', 'process_id', 'Process ID', 'process id', 'Process id', 'PROCESS ID', 'sample', 'SAMPLE', or 'Sample'.\r\n- Due to regex matching and statistics aggregation, the sample ID will be considered as the string before the first underscore. It is therefore recommended that sample names do not use '_' characters. E.g. BSNHM002-24 instead of BSNHM002_24, or P3-1-A10-2-G1 instead of P3_1_A10_2_G1.\r\n- taxid's can be found manually by searching the expected species/genus/family of each sample in the [NCBI taxonomy database](https://www.ncbi.nlm.nih.gov/taxonomy).\r\n  \r\n**samples.csv example**\r\n| ID | forward | reverse | taxid |\r\n| --- | --- | --- | --- |\r\n| BSNHM002-24  | abs/path/to/R1.fq.gz | abs/path/to/R2.fq.gz | 177658 |\r\n| BSNHM038-24 | abs/path/to/R1.fq.gz | abs/path/to/R2.fq.gz | 177627 |\r\n| BSNHM046-24 | abs/path/to/R1.fq.gz | abs/path/to/R2.fq.gz | 3084599 |\r\n\r\n## Gathering sample-specific pseudo-references using Gene Fetch ##\r\n- gene_fetch.py retrieves the protein pseudo-references for each sample using the samples taxonomic identifier (taxid).\r\n- The tool can fetch both protein and nucleotide sequences from NCBI databases for a given gene. See [gene_fetch](https://github.com/SchistoDan/gene_fetch/tree/main) repository for more information.\r\n\r\n## Customising snakemake configuration file ##\r\n- Pre-processing modes:\r\n  - 'merge' = adapter- and poly g-trimming, deduplication and PE read merging (fastp) -> 'cleaning' of sequence headers -> MGE\r\n  - 'concat' = gunzip and 'cleaning' of sequence headers -> adapter- and poly g-trimming, and deduplication (fastp) -> concatenation of PE reads -> read trimming (Trim Galore (cutadapt)) -> MGE\r\n\r\n- Update config.yaml with neccessary paths and variables.\r\n  - See [MitoGeneExtractor README.md](https://github.com/bge-barcoding/MitoGeneExtractor-BGE/blob/main/README.md) for explanation of Exonernate run paramters.\r\n  - See [fasta_cleaner.py repository](https://github.com/bge-barcoding/fasta-cleaner) for information on filtering variables and thresholds (default below suitable in most cases).\r\n  - See [Gene Fetch repository](https://github.com/bge-barcoding/gene_fetch) for guidance on creating [sequence_reference_file.csv](https://github.com/bge-barcoding/gene_fetch?tab=readme-ov-file#normal-mode).\r\n```\r\n# config.yaml\r\n## Run parameters\r\n# MGE run name identifier\r\nrun_name: \"mge_concat_r1_13_15_s50_100\"\r\n\r\n# Path to MGE installation (MitoGeneExtractor-vX.X.X file)\r\nmge_path: \"/absolute/path/to/MitoGeneExtractor/MitoGeneExtractor-v1.9.5\"\r\n\r\n# Path to samples_file.csv\r\nsamples_file: \"/absolute/path/to/samples_file.csv\"\r\n\r\n# Path to sequence_reference_file.csv\r\nsequence_reference_file: \"/absolute/path/to/sequence_reference_file.csv\"\r\n\r\n# Path to output directory. Will make final dir if it does not exist already\r\noutput_dir: \"/absolute/path/to/output/directory\"\r\n\r\n## MGE parameters\r\n# Exonerate relative score threshold parameter (reasonable: 0.7-2)\r\nr:\r\n  - 1\r\n  - 1.3\r\n  - 1.5\r\n\r\n# Exonerate minimum score threshold parameter (e.g. 50-100)\r\ns:\r\n  - 50\r\n  - 100\r\n  \r\n## Read pre-processing method\r\n# Options: \"merge\" or \"concat\" (fastp-merge (i.e. 'merge') or standard PE fastq concatenation (i.e. 'concat')). \r\npreprocessing_mode: \"concat\"\r\n\r\n## Consensus sequence post-processing (using fasta_cleaner.py)\r\nfasta_cleaner:\r\n  consensus_threshold: 0.5\r\n  human_threshold: 0.95\r\n  at_difference: 0.1\r\n  at_mode: \"absolute\"\r\n  outlier_percentile: 90.0\r\n  disable_human: false\r\n  disable_at: false\r\n  disable_outliers: false\r\n  reference_dir: null \r\n```\r\n\r\n# Cluster configuration #\r\n- [snakemake.sh](https://github.com/bge-barcoding/MitoGeneExtractor-BGE/blob/main/snakemake/snakemake.sh) cluster submission script:\r\n  - `--cluster`: Defines how jobs are submitted to SLURM.\r\n    - `--parsable`: Tells sbatch to only return the job ID.\r\n    - `--signal=USR2@90`: Sends a signal 90 seconds before job time limit (for clean termination).\r\n    - `--cluster-config`: Lists path to cluster configuration file (see below for explanation) and enables use of rule-specific resource requirements.\r\n    - `--mem={resources.mem_mb}MB`: Dynamically sets memory allocation by using the mem parameter from each snakemake rule.\r\n    - `--cpus-per-task={threads}`: Uses the threads parameter from each snakemake rule.\r\n    - `--output=slurm-%j-%x.out` & `--error=slurm-%j-%x.err`: Sets naming convention for .out and .err files of individual jobs. '%j' = Job ID. '%x' = Job name.\r\n  - `--snakefile`: Path to Snakefile.\r\n  - `--configfile`: Path to snakemake workflow configuration file.\r\n  - `--latency-wait`: Required when working on a distributed filesystem (e.g. NFS/GPFS). Set at 60 seconds by default. May be necessary to increase if experiencing latency/'missing' file issues.\r\n  - `--rerun-incomplete`: If the snakemake workflow fails or is stopped for any reason, adding this option to the run command will enable the workflow to carry on from where it stopped.\r\n\r\n- [cluster_config.yaml](https://github.com/SchistoDan/BGEE/blob/main/config/cluster_config.yaml)\r\n  - Enables job-specific resource allocation based on job requirements and system capability. \r\n  - Default: Sets the default/minimum parameters to fallback on if not listed for a specific rule\r\n\r\n- The aforementioned files will need tweaking to run on your cluster set up.\r\n\r\n## Resource allocation ##\r\n- SBATCH scheduler job parameters:\r\n  - 'cpus-per-task' and 'mem' only apply to the 'master' job that coordinates the workflow and submits individual jobs to the job scheduler. Specified resources are only allocated for this 'master' job. Therefore, only 5-15G of memory and 2-4 CPUs are likely needed. It is recommended to set a relatively 'long' partition (e.g. several days-week) for this 'master' job, as it will be active for the entire run.\r\n- Rule-specific resources in Snakefile:\r\n  - Each rule can specify threads and memory resources (in Mb). These are the base values Snakemake uses initially.\r\n- [Cluster config](https://github.com/SchistoDan/BGEE/blob/main/config/cluster_config.yaml) values:\r\n  - The 'cpus-per-task' and 'mem' values override or supplement rule-specific values in the Snakefile. If a rule doesn't specify resources, it will fallback to the listed defaults.\r\n- Global limits:\r\n  - '--cores': Limits total cores used across all concurrent jobs in the workflow.\r\n  - '--jobs': Maximum number of simultaneous cluster jobs that will be run. E.g., '--jobs 25' = Up to 25 separate SLURM jobs can run simultaneously. 100 parallel is the maximum allowe\r\n\r\n- **A workflow running 570 samples (genome skims) in 'concat' preprocessing mode and using default MGE parameters (r:1, s:100) ran end-to-end in approximately 11.5 hours (using listed resources allocated in cluster_config.yaml)**\r\n  \r\n# Results structure #\r\n```\r\nresults/\r\n\u251c\u2500\u2500 alignment/                      # MGE sequence alignments\r\n\u2502   \u2514\u2500\u2500 sample1_alignment.fasta\r\n\u2502   \u2514\u2500\u2500 sample2_alignment.fasta\r\n\u2502   \u2514\u2500\u2500 ...\r\n\u251c\u2500\u2500 consensus/                      # MGE consensus sequences   \r\n\u2502   \u2514\u2500\u2500 sample1_consensus.fasta\r\n\u2502   \u2514\u2500\u2500 sample2_consensus.fasta  \r\n\u2502   \u2514\u2500\u2500 ...\r\n\u2502   \u2514\u2500\u2500 <run_name>.fasta            # Concatenated consensus multi-fasta\r\n\u251c\u2500\u2500 err/                            # MGE error/alignment logs\r\n\u2502   \u2514\u2500\u2500 sample1.err\r\n\u2502   \u2514\u2500\u2500 sample2.err\r\n\u2502   \u2514\u2500\u2500 ...\r\n\u251c\u2500\u2500 out/                            # MGE raw outputs/run logs\r\n\u2502   \u2514\u2500\u2500 sample1.out\r\n\u2502   \u2514\u2500\u2500 sample2.out\r\n\u2502   \u2514\u2500\u2500 ...\r\n\u251c\u2500\u2500 logs/\r\n\u2502   \u2514\u2500\u2500 mge/\r\n\u2502   \u2514\u2500\u2500 (gunzip.log)                # Aggregated logs from gunzip_and_clean_headers rule (if run in 'concat' mode)\r\n\u2502   \u2514\u2500\u2500 (concat_reads.log)          # Aggregated logs from fastq_concat rule (if run in 'concat' mode)\r\n\u2502   \u2514\u2500\u2500 (trim_galore.log)           # Aggregated logs from quality_trim rule (if run in 'concat' mode)\r\n\u2502   \u2514\u2500\u2500 (clean_headers.log)         # Aggregated logs from clean_headers_merge rule (if run in 'merge' mode)\r\n\u2502   \u2514\u2500\u2500 alignment_files.log         # List of alignment files in 'alignment/'\r\n\u2502   \u2514\u2500\u2500 concat_consensus.log        # Log from concatenate_fasta rule \r\n\u2502   \u2514\u2500\u2500 rename_complete.txt         # Confirmation of consensus sequence filename and header renaming complete \r\n\u2502   \u2514\u2500\u2500 rename_fasta.log            # Log from concatenate_fasta rule \r\n\u2502   \u2514\u2500\u2500 fasta_cleaner.log           # Log from fasta_cleaner rule/fasta_cleaner.py\r\n\u2502   \u2514\u2500\u2500 mge_stats.log               # Log from extract_stats_to_csv rule\r\n\u2502   \u2514\u2500\u2500 cleaning_complete.txt       # Confirmation cleanup_files (final) rule has run\r\n\u251c\u2500\u2500 trimmed_data/\r\n\u2502   \u2514\u2500\u2500 reports/                    # Per-sample fastp HTML and JSON reports (and trim_galore reports if run in 'concat' mode)\r\n\u2502   \u2514\u2500\u2500 sample1_fastp.out\r\n\u2502   \u2514\u2500\u2500 sample1_fastp.err\r\n\u2502   \u2514\u2500\u2500 (sample1_merged_clean.fq)   # If run in 'merge' mode\r\n\u2502   \u2514\u2500\u2500 (sample1_R1_trimmed.fastq)  # If run in 'concat' mode\r\n\u2502   \u2514\u2500\u2500 (sample1_R2_trimmed.fastq)  # If run in 'concat' mode\r\n\u2502   \u2514\u2500\u2500 sample1_R1_trimmed.fq.gz\r\n\u2502   \u2514\u2500\u2500 sample1_R2_trimmed.fq.gz\r\n\u2502   \u2514\u2500\u2500 ...\r\n\u251c\u2500\u2500 fasta_cleaner/\r\n\u2502   \u2514\u2500\u2500 consensus_seqs/             # 'Cleaned' consensus sequences per sample\r\n\u2502   \u2514\u2500\u2500 filter_annotated_seqs/      # Aligned and annotated (kept/removed) sequences per sample\r\n\u2502   \u2514\u2500\u2500 filter_pass_seqs/           # Aligned sequences passing 'cleaning' parameters per sample\r\n\u2502   \u2514\u2500\u2500 logs/                       # Fasta_cleaner.py logs containing running parameters and raw stats per sample\r\n\u2502   \u2514\u2500\u2500 metrics/                    # Scores for each kept and removed read in the input MGE alignment \r\n\u2502   \u2514\u2500\u2500 all_consensus_seqeunces.fasta\r\n\u2502   \u2514\u2500\u2500 combined_statistics.csv     # Summary stats file of parsed stats from logs\r\n\u2514\u2500\u2500 <run_name>.csv                  # Summary stats file produced by extract_stats_to_csv rule (see below for example)\r\n```\r\n\r\n# Integrated and supplementary scripts #\r\nSee scripts/\r\n- [**1_gene_fetch.py**](https://github.com/bge-barcoding/gene_fetch) = Supplementary script that fetches protein references for each sample using taxids from samples.csv to query NCBI DBs (via BioEntrez API). Fetches closest reference available to input taxid. See [1_gene_fetch.py](https://github.com/bge-barcoding/gene_fetch) github repository for more information.\r\n- [**rename_headers.py**](https://github.com/SchistoDan/MitoGeneExtractor/blob/main/snakemake/scripts/rename_headers.py) = Script to rename headers of consensus sequence FASTA files and filenames.\r\n- [**fasta_cleaner_mge.py**](https://github.com/SchistoDan/MitoGeneExtractor/blob/main/snakemake/scripts/fasta_cleaner_mge.py) = This script (incorproated into 'fasta_cleaner' rule) 'cleans' MGE alignment files using AT% thresholds, base consensus similarity, human COI similarity, and (if supplied) reference sequence similarity. Outputs 'cleaned' consensus sequences for each sample. Modified from [fasta_cleaner.py](https://github.com/bge-barcoding/fasta-cleaner), see original github repository for more information.\r\n- [**mge_stats.py**](https://github.com/SchistoDan/MitoGeneExtractor/blob/main/snakemake/scripts/mge_stats.py) = This script (incorporated into 'rule extract_stats_to_csv') uses alignment fasta files and MGE.out files to generate summary statistics for each sample.\r\n- [csv_combiner_mge.py](https://github.com/SchistoDan/BGEE/blob/main/workflow/scripts/csv_combiner_mge.py) = This script combines both summary stats files from 'concat' and 'merge' runs of the workflow.\r\n- [**fasta_compare.py**](https://github.com/SchistoDan/MitoGeneExtractor/blob/main/snakemake/scripts/fasta_compare.py) = Supplementary script that can be run after the core MGE pipeline is finished. It will compare barcodes produced using different parameter combinations (from one run or multiple runs) for each sample, ranks each barcode 1-5 based on [BOLD BIN criteria](https://v3.boldsystems.org/index.php/resources/handbook?chapter=2_databases.html&section=bins), and select the 'best' (BOLD BIN compliant) barcode.\r\n\r\n\r\n\r\n# Contributing #\r\n- Please feel free to submit issues, fork the repository, and create pull requests for any improvements.\r\n- This snakemake pipeline was produced by Dan Parsons @ NHMUK for BioDiversity Genomics Europe.\r\n- Since this snakemake pipeline uses [MitogeneExtractor](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.14075) at its core, please cite:\r\n  Brasseur, M.V., Astrin, J.J., Geiger, M.F., Mayer, C., 2023. MitoGeneExtractor: Efficient extraction of mitochondrial genes from next-generation sequencing libraries. Methods in Ecology and Evolution.\r\n\r\n\r\n\r\n  ## To do ##\r\n- Split Snakefile into .smk files\r\n- Update test data and associated files.\r\n- Integrate 1_gene_fetch.py into snakefile.\r\n- Make Workflow Hub compatible.\r\n- Generate RO-crates.\r\n  \r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1346",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1346?version=1",
        "name": "Barcode Gene Extractor & Evaluator (BGEE) Snakemake workflow",
        "number_of_steps": 0,
        "projects": [
            "iBOL Europe Museum Skimming",
            "Biodiversity Genomics Europe (general)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bge",
            "biodiversity",
            "dna barcoding",
            "natural history collections"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2025-05-28",
        "versions": 1
    },
    {
        "create_time": "2025-04-18",
        "creators": [
            "Ryo Nozu",
            "Sora Yonezawa"
        ],
        "description": "[![License](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE)\r\n![GitHub last commit (branch)](https://img.shields.io/github/last-commit/RyoNozu/CWL4IncorporateTSSintoGXF/main)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![Lab Website](https://img.shields.io/badge/Lab%20Website-bonohulab-informational?style=flat-square)](https://bonohu.hiroshima-u.ac.jp/)\r\n\r\n&nbsp;\r\n\r\n# CWL4IncorporateTSSintoGXF\r\n\r\nThis workflow determines TSS based on the analysis of CAGE-seq data and incorporates TSS information and 5'UTR information calculated based on TSS information into the gene annotation file (gff/gtf). The R package, [TSSr](https://github.com/Linlab-slu/TSSr), is used to determine TSS.  \r\n\r\n## Requirements\r\n\r\n- [cwltool](https://github.com/common-workflow-language/cwltool)  \r\n\r\n    Install using pip  \r\n    ```\r\n    pip  install cwltool  \r\n    ```\r\n\r\n    Install using conda  \r\n    ```\r\n    conda create -n cwltool  \r\n    conda activate cwltool  \r\n    conda install -c conda-forge cwltool \r\n    ``` \r\n\r\n- [docker](https://www.docker.com/)  \r\n\r\n    \u2020 and Docker Desktop must be running  \r\n\r\n## Simple usage  \r\n\r\n- Clone this repository  \r\n\r\n    ```\r\n    git clone https://github.com/RyoNozu/CWL4IncorporateTSSintoGXF.git\r\n    cd CWL4IncorporateTSSintoGXF\r\n    ```\r\n\r\n- Run workflow  \r\n\r\n    ```\r\n    # for paired-end reads case\r\n    cwltool --debug --cachedir ./cwl_cache/ --outdir ./test/ ./workflow/cageseq_gtf_update_pe.cwl ./config/Workflow_config/cageseq_gtf_update_pe.yml\r\n    ```\r\n    - Prep your case yml file referring to the [template](https://github.com/RyoNozu/CWL4IncorporateTSSintoGXF/blob/main/config/workflow_template.yml)  \r\n        \u2022 Refer to the [Link](https://view.commonwl.org/workflows/github.com/RyoNozu/CWL4IncorporateTSSintoGXF/blob/main/workflow/cageseq_gtf_update_pe.cwl) for details on each parameter that needs to be specified  \r\n    - A single-ended version (cageseq_gtf_update_se.cwl) is in prep as of 20240417  \r\n\r\n## Input files  \r\n\r\n- CAGE-seq Read (fastq, paried/single-end)  \r\n- reference genome (fasta)  \r\n- gene annotation file (gff/gtf)  \r\n- (BSgenome_data_package_seed_file (.txt))  \r\n        > refere to forgeBSgenomeDataPkg function in [BSgenomeForge](https://bioconductor.org/packages/release/bioc/html/BSgenomeForge.html) package  \r\n\r\n## Output files  \r\n\r\n- updated gxf file (.gff/gtf)  \r\n\r\n## FYI: Running time\r\n\r\n***\r\n",
        "doi": null,
        "edam_operation": [
            "Genome annotation"
        ],
        "edam_topic": [
            "Functional genomics",
            "Transcriptomics"
        ],
        "filtered_on": "annot* in tags",
        "id": "1343",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1343?version=1",
        "name": "CWL4IncorporateTSSintoGXF (paired-end file)",
        "number_of_steps": 0,
        "projects": [
            "bonohulab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "gtf",
            "transcription start sites",
            "cage",
            "cage-seq",
            "cageseq-data",
            "genome-annotation"
        ],
        "tools": [
            "seqkit",
            "fastp",
            "STAR",
            "TSSr",
            "BioContainers"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-04-22",
        "versions": 1
    },
    {
        "create_time": "2025-04-18",
        "creators": [
            "Ben Price"
        ],
        "description": "# NGS Equimolar Pooling Calculator\r\n\r\nA web-based tool for calculating equimolar pooling volumes from complex sub-pools for Next-Generation Sequencing (NGS).\r\n\r\n## About\r\n\r\nThis calculator helps researchers determine the correct volumes to take from each pool when combining multiple complex sub-pools of NGS libraries for sequencing. It ensures that each individual sample contributes an equal number of molecules to the final sequencing run, resulting in more balanced coverage.\r\n\r\n## Features\r\n\r\n- Calculate equimolar pooling volumes for any number of input pools\r\n- Support for both calculated (estimated / target) and measured concentrations\r\n- Option to maximize the usage of original pools\r\n- Adjustable final volume\r\n- Real-time results with detailed metrics\r\n- Verification of equimolarity across samples\r\n\r\n## Live Calculator\r\n\r\nThe calculator is available at: https://bwprice.github.io/ngs-equimolar-pooling-calculator/\r\n\r\n## Usage\r\n\r\n1. Enter information for each input pool:\r\n   - Molarity that each sample was normalized to in the sub-pool - i.e. the target molarity when pooling (nM)\r\n   - Total pool volume (\u03bcl)\r\n   - Number of samples in the pool\r\n   - (Optional) Measured concentration of each sub-pool from lab quantification\r\n\r\n2. Choose whether to:\r\n   - Use measured concentrations (if available from Qubit, Bioanalyzer, qPCR, etc.)\r\n   - Maximize usage of original pools\r\n   - Set a specific final pool volume\r\n\r\n3. The calculator will automatically determine:\r\n   - How much volume to take from each sub-pool\r\n   - The percentage of each sub-pool that will be used\r\n   - The final nM contribution per sample\r\n   - Total samples in the final pool\r\n\r\n## Installation for Local Development\r\n\r\n1. Clone this repository:\r\n   ```\r\n   git clone https://github.com/bwprice/ngs-equimolar-pooling-calculator.git\r\n   cd ngs-equimolar-pooling-calculator\r\n   ```\r\n\r\n2. Install dependencies:\r\n   ```\r\n   npm install\r\n   ```\r\n\r\n3. Start the development server:\r\n   ```\r\n   npm start\r\n   ```\r\n\r\n4. Open [http://localhost:3000](http://localhost:3000) to view it in your browser.\r\n\r\n## Deployment\r\n\r\nTo deploy to GitHub Pages:\r\n\r\n```\r\nnpm run deploy\r\n```\r\n\r\n## Contributing\r\n\r\nContributions are welcome! Please feel free to submit a Pull Request.\r\n\r\n## License\r\n\r\nThis project is licensed under the MIT License - see the LICENSE file for details.\r\n\r\n## Citation\r\n\r\nIf you use this tool in your research, please cite:\r\n\r\n```\r\nBen Price, NGS Equimolar Pooling Calculator, GitHub Repository, 2025, \r\nAvailable at: https://github.com/bwprice/ngs-equimolar-pooling-calculator\r\n```\r\n",
        "doi": "10.48546/workflowhub.workflow.1344.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1344",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1344?version=1",
        "name": "NGS Equimolar Pooling Calculator",
        "number_of_steps": 0,
        "projects": [
            "iBOL Europe Museum Skimming"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Unrecognized workflow type",
        "update_time": "2025-04-18",
        "versions": 1
    },
    {
        "create_time": "2025-04-16",
        "creators": [
            "Christopher Mohr",
            "Alexander Peltzer",
            "Sven Fillinger"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-hlatyping_logo_dark.png\">\n    <img alt=\"nf-core/hlatyping\" src=\"docs/images/nf-core-hlatyping_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/hlatyping/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/hlatyping/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/hlatyping/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/hlatyping/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/hlatyping/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/hlatyping)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23hlatyping-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/hlatyping)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/hlatyping** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/hlatyping \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/hlatyping/usage) and the [parameter documentation](https://nf-co.re/hlatyping/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/hlatyping/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/hlatyping/output).\n\n## Credits\n\nnf-core/hlatyping was originally written by Christopher Mohr, Alexander Peltzer, Sven Fillinger.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#hlatyping` channel](https://nfcore.slack.com/channels/hlatyping) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/hlatyping for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "991",
        "keep": true,
        "latest_version": 11,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/991?version=11",
        "name": "nf-core/hlatyping",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dna",
            "hla",
            "hla-typing",
            "immunology",
            "optitype",
            "personalized-medicine",
            "rna"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-04-16",
        "versions": 11
    },
    {
        "create_time": "2025-04-20",
        "creators": [
            "Jonas Kasmanas"
        ],
        "description": "## gSpreadComp: Streamlining Microbial Community Analysis for Resistance, Virulence, and Plasmid-Mediated Spread\r\n\r\n\r\n<p align=\"center\" width=\"100%\">\r\n\t<img width=\"30%\" src=\"/gspreadcomp_logo_noback.png\">\r\n</p>\r\n\r\n### Overview\r\n\r\ngSpreadComp is a UNIX-based, modular bioinformatics toolkit designed to streamline comparative genomics for analyzing microbial communities. It integrates genome annotation, gene spread calculation, plasmid-mediated horizontal gene transfer (HGT) detection and resistance-virulence ranking within the analysed microbial community to help researchers identify potential resistance-virulence hotspots in complex microbial datasets.\r\n\r\n> [!TIP]\r\n> After installation, the user may want to check a detailed tutorial with example input and output data [here](usage_tutorial.md)\r\n\r\n### Objectives and Features\r\n- **Six Integrated Modules**: Offers modules for taxonomy assignment, genome quality estimation, ARG annotation, plasmid/chromosome classification, virulence factor annotation, and in-depth downstream analysis, including target-based gene spread analysis and prokaryotic resistance-virulence ranking.\r\n- **Weighted Average Prevalence (WAP)**: Employs WAP for calculating the spread of target genes at different taxonomical levels or target groups, enabling refined analyses and interpretations of microbial communities.\r\n- **Reference Pathogen Identification**: Compares genomes to the NCBI pathogens database to create a resistance-virulence ranking within the community.\r\n- **HTML Reporting**: Culminates in a structured HTML report after the complete downstream analysis, providing users with an overview of the results.\r\n\r\n### Modular Approach and Flexibility\r\n`gSpreadComp`\u2019s modular nature enables researchers to use the tool's main analysis and report generation steps independently or to integrate only specific pieces of `gSpreadComp` into their pipelines, providing flexibility and accommodating the varying software management needs of investigators.\r\n\r\n#### Using other annotation tools with gSpreadComp\r\n> [!TIP]\r\n> Users can incorporate results from other annotation tools within gSpreadComp's workflow, provided the input is formatted according to gSpreadComp's specifications. This allows for the integration of preferred or specialized tools for specific steps (e.g., alternative ARG or plasmid detection methods) while still benefiting from gSpreadComp's downstream analysis capabilities.\r\n> \r\n> For the quality data it should look like: [Quality DataFrame Format](test_data/checkm_df_format_gSpread.csv)\r\n> \r\n> For the taxonomy data it should look like: [Taxonomy DataFrame Format](test_data/gtdb_df_format_gSpread.csv)\r\n> \r\n> For the gene annotation (e.g. ARGs) data it should look like: [Gene annotation DataFrame Format](test_data/deeparg_df_format_gSpread.csv)\r\n> \r\n> For the plasmid identification data it should look like: [Plasmid identification DataFrame Format](test_data/plasflow_combined_format_gSpread.csv)\r\n>\r\n> Metadata information data should look like: [Metadata Sample](test_data/02_metadata_gspread_sample.csv)\r\n\r\nBy the end of a successful run, you should have a report that looks like this: [Download Example Report](https://raw.githubusercontent.com/mdsufz/gSpreadComp/refs/heads/main/test_data/gSpread_example_result_report.html)\r\n\r\n### Comprehensive Workflow\r\n\r\n![ScreenShot](/test_data/01_Kasmanas_gSpread_Fig_1.png)\r\n\r\ngSpreadComp consists of the following modules:\r\n\r\n1. **Taxonomy Assignment**: Uses [GTDBtk v2](https://academic.oup.com/bioinformatics/article/38/23/5315/6758240) for taxonomic classification.\r\n2. **Genome Quality Estimation**: Employs [CheckM](https://genome.cshlp.org/content/25/7/1043) for assessing genome completeness and contamination.\r\n3. **ARG Annotation**: Utilizes [DeepARG](https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-018-0401-z) for antimicrobial resistance gene prediction.\r\n4. **Plasmid Classification**: Implements [Plasflow](https://academic.oup.com/nar/article/46/6/e35/4807335) for plasmid sequence identification.\r\n5. **Virulence Factor Annotation**: Annotates virulence factors using the [Victors](https://academic.oup.com/nar/article/47/D1/D693/5144967?login=false) and/or [VFDB](http://www.mgc.ac.cn/VFs/main.htm) databases.\r\n6. **Downstream Analysis**: Performs gene spread analysis, resistance-virulence ranking, and potential plasmid-mediated HGT detection.\r\n\r\n\r\n# Requirements\r\n\r\nBefore installing and running `gSpreadComp`, ensure that your system meets the following requirements:\r\n\r\n## 1. Operating System\r\n- Linux x64 system\r\n\r\n## 2. Package Managers\r\n- [Miniconda](https://docs.conda.io/en/latest/miniconda.html): Required for creating environments and managing packages.\r\n- [Mamba](https://mamba.readthedocs.io/en/latest/user_guide/mamba.html): A faster package manager used within the `gSpreadComp` installation.\r\n\r\n## 3. Storage\r\n- Approximately 15 GB for software installation.\r\n- Around 92 GB for the entire database requirements.\r\n\r\n# Installation\r\n\r\n## Database Management\r\n`gSpreadComp` includes an easy-to-use script for automatic download and configuration of the required databases, with scheduled updates every January and July.\r\n\r\n## Compatibility and Requirements\r\nDesigned to support Linux x64 systems, requiring approximately 15 GB for software installation and around 92 GB for the entire database requirements.\r\n\r\n## 1 - Install miniconda\r\n\r\nTo bypass conflicting dependencies, the gSpreadComp approach uses miniconda to create automatically orchestrated environments. [Mamba](https://mamba.readthedocs.io/en/latest/user_guide/mamba.html) is a much faster package manager than conda and is used within the gSpreadComp installation. Consequently, miniconda and mamba are required to be previously installed in your system. Below is a possible way of installing miniconda and mamba. Please, be aware that mamba works best when installed in your base environment.\r\n\r\n```console\r\n# See documentation: https://docs.conda.io/en/latest/miniconda.html\r\n\r\n$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\r\n$ chmod +x Miniconda3-latest-Linux-x86_64.sh\r\n$ ./Miniconda3-latest-Linux-x86_64.sh\r\n$ export PATH=~/miniconda3/bin:$PATH\r\n\r\n# Install mamba. See documentation: https://mamba.readthedocs.io/en/latest/installation.html\r\n$ conda install mamba -n base -c conda-forge\r\n```\r\n\r\n## 2 - Install gSpreadComp\r\n\r\nOnce you have miniconda and mamba installed and on your PATH, you can proceed to install gSpreadComp.\r\nThe installation script was designed to install and set up all necessary tools and packages.\r\n\r\n```console\r\n# Clone repository\r\n$ git clone https://github.com/mdsufz/gSpreadComp.git\r\n\r\n# Go to the gSpreadComp cloned repository folder\r\n$ cd gSpreadComp\r\n\r\n# Make sure you have conda ready and that you are in your base environment.\r\n$ conda activate base\r\n$ echo $CONDA_PREFIX\r\n\r\n# You should see something like the following:\r\n/path/to/miniconda3\r\n\r\n# Run the installation script as follows\r\n$ bash -i installation/install.sh\r\n\r\n# Follow the instructions on the screen:\r\n# Enter \"y\" if you want to install all modules; otherwise, enter \"n\".\r\n# If you entered \"n\", enter \"y\" for each of the modules you would like to install individually.\r\n\r\n\tThe MuDoGeR's installation will begin..\r\n\r\n\r\n\t      (  )   (   )  )\t\t\t\r\n\t       ) (   )  (  (\t\t\t\r\n\t       ( )  (    ) )\t\t\t\r\n\t       _____________\t\t\t\r\n\t      <_____________> ___\t\t\r\n\t      |             |/ _ \\\t\t\r\n\t      |               | | |\t\t\r\n\t      |               |_| |\t\t\r\n\t   ___|             |\\___/\t\t\r\n\t  /    \\___________/    \\\t\t\r\n\t  \\_____________________/\t\t\r\n\r\n\tThis might take a while. Time to grab a coffee...\r\n```\r\n\r\n## 3 - Install necessary databases\r\n\r\n**Make sure to run the database setup after gSpreadComp is installed.**\r\n\r\nSome bioinformatics tools used within gSpreadComp require specific databases to work. We developed a database download and set up tool to make our lives easier. You can choose to install only the databases you intend to use. You can use the flag `--dbs` to choose and set up the selected databases (all [default], install all databases).\r\n\r\nUse this script if you want gSpreadComp to take care of everything.\r\n\r\n```console\r\n# Make sure gSpreadComp_env is activated. It should have been created when you ran 'bash -i installation/install.sh'\r\n$ conda activate gspreadcomp_env\r\n\r\n# Go to gSpreadComp cloned directory\r\n$ cd gSpreadComp\r\n\r\n# Run the database setup script\r\n$ bash -i installation/database-setup.sh --dbs all -o /path/to/save/databases\r\n\r\n# You can also check out the database-setup help information\r\n$ bash -i installation/database-setup.sh --help\r\n\r\n        gSpreadComp database script v=1.0\r\n        Usage: bash -i database-setup.sh --dbs [module] -o output_folder_for_dbs\r\n\t\t    USE THE SAME DATABASE LOCATION OUTPUT FOLDER FOR ALL DATABASES USED WITH gSpreadComp\r\n          --dbs all\t\t\t\tdownload and install the required and optional databases [default]\"\r\n          --dbs required              \t\tdownload and install the required databases (Victors and VFDB) for gSpreadComp\r\n          --dbs optional              \t\tdownload and install all the optional (ARGs, GTDB-tk, CheckM) databases for gSpreadComp\r\n          --dbs args\t\t\t\tdownload and install the required and the ARGs databases.\r\n          -o path/folder/to/save/dbs\t\toutput folder where you want to save the downloaded databases\r\n          --help | -h\t\t\t\tshow this help message\r\n          --version | -v\t\t\tshow database install script version\r\n\r\n\r\n```\r\n\r\n## Usage\r\n\r\n### Activating the Conda Environment\r\nBefore using `gSpreadComp`, activate the appropriate conda environment using the following command:\r\n```sh\r\nconda activate gSpreadComp_env\r\n```\r\n\r\n### Command-Line Usage\r\n`gSpreadComp` provides several modules, each performing a specific task within the pipeline. The quick command-line usage is as follows:\r\n```sh\r\ngspreadcomp --help\r\n```\r\n\r\n### Modules and Their Descriptions\r\n`gSpreadComp` comprises several modules, each serving a specific purpose in the genome analysis workflow:\r\n\r\n#### 1. Taxonomy Assignment\r\n```sh\r\ngspreadcomp taxonomy [options] --genome_dir genome_folder -o output_dir\r\n```\r\n- Assigns taxonomy to genomes using [GTDBtk v2](https://academic.oup.com/bioinformatics/article/38/23/5315/6758240).\r\n- Options:\r\n  - `--genome_dir STR`: folder with the bins to be classified (in fasta format)\r\n  - `--extension STR`: fasta file extension (e.g. fa or fasta) [default: fa]\r\n  - `-o STR`: output directory\r\n  - `-t INT`: number of threads\r\n\r\n#### 2. Genome Quality Estimation\r\n```sh\r\ngspreadcomp quality [options] --genome_dir genome_folder -o output_dir\r\n```\r\n- Estimates genome completeness and contamination using [CheckM](https://genome.cshlp.org/content/25/7/1043).\r\n- Options:\r\n  - `--genome_dir STR`: folder with the genomes to estimate quality (in fasta format)\r\n  - `--extension STR`: fasta file extension (e.g. fa or fasta) [default: fa]\r\n  - `-o STR`: output directory\r\n  - `-t INT`: number of threads [default: 1]\r\n  - `-h --help`: print this message\r\n\r\n#### 3. ARG Prediction\r\n```sh\r\ngspreadcomp args [options] --genome_dir genome_folder -o output_dir\r\n```\r\n- Predicts the Antimicrobial Resistance Genes (ARGs) in a genome using [DeepARG](https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-018-0401-z).\r\n- Options:\r\n  - `--genome_dir STR`: folder with the genomes to be classified (in fasta format)\r\n  - `--extension STR`: fasta file extension (e.g. fa or fasta) [default: fa]\r\n  - `--min_prob NUM`: Minimum probability cutoff for DeepARG [Default: 0.8]\r\n  - `--arg_alignment_identity NUM`: Identity cutoff for sequence alignment for DeepARG [Default: 35]\r\n  - `--arg_alignment_evalue NUM`: Evalue cutoff for DeepARG [Default: 1e-10]\r\n  - `--arg_alignment_overlap NUM`: Alignment read overlap for DeepARG [Default: 0.8]\r\n  - `--arg_num_alignments_per_entry NUM`: Diamond, minimum number of alignments per entry [Default: 1000]\r\n  - `-o STR`: output directory\r\n  - `-h --help`: print this message\r\n\r\n#### 4. Plasmid Prediction\r\n```sh\r\ngspreadcomp plasmid [options] --genome_dir genome_folder -o output_dir\r\n```\r\n- Predicts if a sequence within a fasta file is a chromosome, plasmid, or undetermined using [Plasflow](https://academic.oup.com/nar/article/46/6/e35/4807335).\r\n- Options:\r\n  - `--genome_dir STR`: folder with the genomes to be classified (in fasta format)\r\n  - `--extension STR`: fasta file extension (e.g. fa or fasta) [default: fa]\r\n  - `--threshold NUM`: threshold for probability filtering [default: 0.7]\r\n  - `-o STR`: output directory\r\n  - `-h --help`: print this message\r\n\r\n#### 5. Virulence Factor annotation\r\n```sh\r\ngspreadcomp pathogens [options] --genome_dir genome_folder -o output_dir\r\n```\r\n- Aligns provided genomes to Virulence Factors databases and formats the output.\r\n- Options:\r\n  - `--genome_dir STR`: folder with the genomes to be aligned against Virulence factors (in fasta format)\r\n  - `--extension STR`: fasta file extension (e.g. fa or fasta) [default: fa]\r\n  - `--evalue NUM`: evalue, expect value, threshold as defined by NCBI-BLAST [default: 1e-50]\r\n  - `-t INT`: number of threads\r\n  - `-o STR`: output directory\r\n  - `-h --help`: print this message\r\n\r\n#### 6. Main Analysis\r\n```sh\r\ngspreadcomp gspread [options] -o output_dir\r\n```\r\n- Runs the main `gSpreadComp` to compare spread and plasmid-mediated HGT.\r\n- Options:\r\n  - `--checkm STR`: Path to the formatted Quality estimation dataframe\r\n  - `--gene STR`: Path to the formatted target Gene dataframe to calculate the spread\r\n  - `--gtdbtk STR`: Path to the formatted Taxonomy assignment dataframe\r\n  - `--meta STR`: Path to the formatted Sample's Metadata dataframe\r\n  - `--vf STR`: Path to the formatted Virulence Factors assignment dataframe\r\n  - `--plasmid STR`: Path to the formatted Plasmid prediction dataframe\r\n  - `--nmag INT`: Minimum number of Genomes per Library accepted [default=0]\r\n  - `--spread_taxa STR`: Taxonomic level to check gene spread [default=Phylum]\r\n  - `--target_gene_col STR`: Name of the column from the gene dataset with the Gene_ids to analyse [default=Gene_id]\r\n  - `-t INT`: number of threads\r\n  - `-o STR`: output directory\r\n  - `-h --help`: print this message\r\n\r\n\r\n## Important Considerations\r\n\r\n- gSpreadComp is designed for hypothesis generation and is not a standalone risk assessment tool.\r\n- Results should be interpreted cautiously and used to guide further experimental validation.\r\n- The tool provides relative rankings within analyzed communities, not absolute risk assessments.\r\n\r\n## Citation\r\n\r\nIf you use gSpreadComp in your research, please cite:\r\n\r\n[Citation information will be added upon publication]\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.1340.3",
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Comparative genomics",
            "Data curation and archival",
            "Microbiology"
        ],
        "filtered_on": "microbiome in tags",
        "id": "1340",
        "keep": true,
        "latest_version": 3,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1340?version=3",
        "name": "gSpreadComp",
        "number_of_steps": 0,
        "projects": [
            "Kasmanas"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "antimicrobial resistance",
            "microbiome"
        ],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2025-04-20",
        "versions": 3
    },
    {
        "create_time": "2025-04-15",
        "creators": [
            "Ra\u00fcl Sirvent",
            "Rosa M Badia"
        ],
        "description": "COMPSs Matrix Multiplication resourceUsage profiling example.\r\n\r\nMN5 MSIZE=20 BSIZE=768 7 Nodes (6 workers) (--num_nodes=7 --worker_in_master_cpus=0). \r\n\r\n* Total number of tasks: 20^3 = 8000\r\n* Maximum code parallelism: 20^2 = 400\r\n* Total cores: 112*6 = 672 \r\n* Maximum utilisation: 400 / 112 = 3,57 Nodes\r\n\r\nOverall stats from \"pycompss inspect\":\r\n\r\n```\r\n    \u2502   \u2514\u2500\u2500 overall\r\n    \u2502       \u251c\u2500\u2500 matmul_tasks\r\n    \u2502       \u2502   \u2514\u2500\u2500 multiply\r\n    \u2502       \u2502       \u251c\u2500\u2500 maxTime = 91,111 ms\r\n    \u2502       \u2502       \u251c\u2500\u2500 executions = 8,000 \r\n    \u2502       \u2502       \u251c\u2500\u2500 avgTime = 84,839 ms\r\n    \u2502       \u2502       \u2514\u2500\u2500 minTime = 79,278 ms\r\n    \u2502       \u2514\u2500\u2500 executionTime = 1,929,944 ms\r\n```\r\n\r\nThis example shows misuse of resources (from 6 workers, only 3 and a half can be exploited due to the application's inherent parallelism), which can be seen in the profiling folder plots.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in name",
        "id": "1339",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1339?version=1",
        "name": "COMPSs Matrix Multiplication resourceUsage profiling example MN5 MSIZE=20 BSIZE=768 7 Nodes total",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2025-04-15",
        "versions": 1
    },
    {
        "create_time": "2025-04-14",
        "creators": [
            "Ra\u00fcl Sirvent",
            "Nicol\u00f2 Giacomini"
        ],
        "description": "Application that perform the multiplication between matrices.\r\nIn this experiment, a new profiling visualization is available, showing the resource usage such as CPU, memory, data read and written to disk, and data sent and received over the network.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1338",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1338?version=1",
        "name": "Matrix Multiplication \u2013 resource usage visualization",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2025-04-14",
        "versions": 1
    },
    {
        "create_time": "2025-06-22",
        "creators": [
            "Cameron Watson"
        ],
        "description": "Complete multiplex tissue image (MTI) analysis pipeline for tissue microarray (TMA) data imaged using cyclic immunofluorescence: Performs illumination correction, stitching and registration, and tissue microarray segmentation. Tissue-segmented images undergo nuclear segmentation, cell/nuclei feature quantification (mean marker intensities, cell coordinates, and morphological features), and cell phenotyping. Produces outputs that are compatible with downstream single-cell/spatial analysis and interactive image viewers including: Pyramidal OME-TIFF images, nuclear segmentation masks (TIFF), quantified feature tables (CSV, h5ad) with cell type annotations, and an interactive Vitessce dashboard that combines image viewing with linked single-cell data visualizations. ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1337",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1337?version=2",
        "name": "tissue-microarray-analysis/main",
        "number_of_steps": 10,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "basic_illumination",
            "unet_coreograph",
            "scimap_mcmicro_to_anndata",
            "ip_convertimage",
            "vitessce_spatial",
            "scimap_phenotyping",
            "mesmer",
            "rename_tiff_channels",
            "ashlar",
            "quantification"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-04-13",
        "creators": [
            "Nils Homer"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-fastquorum_logo_dark.png\">\n    <img alt=\"nf-core/fastquorum\" src=\"docs/images/nf-core-fastquorum_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/fastquorum/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/fastquorum/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/fastquorum/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/fastquorum/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/fastquorum/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.11267672-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.11267672)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/nf-core/fastquorum)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23fastquorum-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/fastquorum)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/fastquorum** is a bioinformatics pipeline that implements the pipeline implements the [fgbio Best Practices FASTQ to Consensus Pipeline][fgbio-best-practices-link] to produce consensus reads using unique molecular indexes/barcodes (UMIs).\n`nf-core/fastquorum` can produce consensus reads from single or multi UMI reads, and even [Duplex Sequencing][duplex-seq-link] reads.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/fastquorum/results).\n\n| Tools                                                                                                              | Description                                                                                                                   |\n| ------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------- |\n| <p align=\"center\"><img title=\"Fastquorum Workflow (Tools)\" src=\"docs/images/fastquorum_subway.png\" width=100%></p> | <p align=\"center\"><img title=\"Fastquorum Workflow (Description)\" src=\"docs/images/fastquorum_subway.desc.png\" width=100%></p> |\n\n1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n2. Fastq to BAM, extracting UMIs ([`fgbio FastqToBam`](http://fulcrumgenomics.github.io/fgbio/tools/latest/FastqToBam.html))\n3. Align ([`bwa mem`](https://github.com/lh3/bwa)), reformat ([`fgbio ZipperBam`](http://fulcrumgenomics.github.io/fgbio/tools/latest/ZipperBam.html)), and template-coordinate sort ([`samtools sort`](http://www.htslib.org/doc/samtools.html))\n4. Group reads by UMI ([`fgbio GroupReadsByUmi`](http://fulcrumgenomics.github.io/fgbio/tools/latest/GroupReadsByUmi.html))\n5. Call consensus reads\n   1. For [Duplex-Sequencing][duplex-seq-link] data\n      1. Call duplex consensus reads ([`fgbio CallDuplexConsensusReads`](http://fulcrumgenomics.github.io/fgbio/tools/latest/CallDuplexConsensusReads.html))\n      2. Collect duplex sequencing specific metrics ([`fgbio CollectDuplexSeqMetrics`](http://fulcrumgenomics.github.io/fgbio/tools/latest/CollectDuplexSeqMetrics.html))\n   2. For non-Duplex-Sequencing data:\n      1. Call molecular consensus reads ([`fgbio CallMolecularConsensusReads`](http://fulcrumgenomics.github.io/fgbio/tools/latest/CallMolecularConsensusReads.html))\n6. Align ([`bwa mem`](https://github.com/lh3/bwa))\n7. Filter consensus reads ([`fgbio FilterConsensusReads`](http://fulcrumgenomics.github.io/fgbio/tools/latest/FilterConsensusReads.html))\n8. Present QC ([`MultiQC`](http://multiqc.info/))\n\n## Verified Vendors, Kits, and Assays\n\n> [!WARNING]\n> The following Vendors, Kits, and Assays are provided for informational purposes only.\n> _No warranty for the accuracy or completeness of the information or parameters is implied._\n\n| Verified | Assay                                                     | Company                     | Strand | Randomness | UMI Location     | Read Structure  | URL                                                                                                                                                                                 |\n| -------- | --------------------------------------------------------- | --------------------------- | ------ | ---------- | ---------------- | --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| No       | SureSelect XT HS                                          | Agilent Technologies        | Single | Random     |                  |                 | [link](https://www.agilent.com/en/product/next-generation-sequencing/ngs-library-prep-target-enrichment-reagents/dna-seq-reagents/sureselectxt-hs-reagent-kits-4252208)             |\n| No       | SureSelect XT HS2 (MBC)                                   | Agilent Technologies        | Dual   | Random     |                  |                 | [link](https://www.agilent.com/en/product/next-generation-sequencing/ngs-library-prep-target-enrichment-reagents/dna-seq-reagents/sureselect-xt-hs2-dna-reagent-kit-4252207)        |\n| No       | TruSight Oncology (TSO)                                   | Illumina                    | Dual   | Nonrandom  |                  |                 | [link](https://www.illumina.com/products/by-type/clinical-research-products/trusight-oncology-umi.html)                                                                             |\n| No       | xGen dual index UMI Adapters                              | Integrated DNA Technologies | Single | Random     | index1 (i7)      |                 | [link](https://www.idtdna.com/pages/products/next-generation-sequencing/workflow/xgen-ngs-library-preparation/ngs-adapters-indexing-primers/adapters-indexing-primers-for-illumina) |\n| No       | xGen Prism (xGen cfDNA & FFPE DNA Library Prep MC v2 Kit) | Integrated DNA Technologies | Dual   | Nonrandom  |                  |                 | [link](https://www.idtdna.com/pages/products/next-generation-sequencing/workflow/xgen-ngs-library-preparation/dna-library-preparation/cfdna-ffpe-prep-kit)                          |\n| No       | NEBNext                                                   | New England Biosciences     | Single | Random     | index1 (i7)      |                 | [link](https://www.neb.com/en-us/products/e7874nebnext-multiplex-oligos-for-illumina-unique-dual-index-umi-adaptors-dna-set-2)                                                      |\n| No       | AML MRD                                                   | TwinStrand Biosciences      | Dual   | Random     |                  |                 | [link](https://twinstrandbio.com/aml-assay/)                                                                                                                                        |\n| No       | Mutagenesis                                               | TwinStrand Biosciences      | Dual   | Random     |                  |                 | [link](https://twinstrandbio.com/mutagenesis-assay/)                                                                                                                                |\n| No       | UMI Adapter System                                        | Twist Biosciences           | Dual   | Random     | Inline (R1 & R2) | `5M2S+T 5M2S+T` | [link](https://www.twistbioscience.com/products/ngs/library-preparation/twist-umi-adapter-system)                                                                                   |\n\nColumn Definitions:\n\n- Assay: the name of the assay or kit\n- Company: the name of the company or vendor providing the assay or kit\n- Strand: Dual if both strands of a double-stranded source molecule are sequences (e.g. Duplex Sequencing), Single otherwise\n- Randomness: if the unique molecular identifiers (UMIs) are fully random (degenerate) or are synthesized from a fixed set\n- UMI Location: the location of UMIs within the reads.\n- Read Structure: the [`read_structure`][read-structure-link] describes how the bases in a sequencing run should be allocated into logical reads, including the unique molecular index(es)\n- URL: link(s) to vendor documentation or further information\n\nTo become \"Verified\" by `nf-core/fastquorum`, please open an issue and provide the maintainers with an example dataset that can be shared publicly.\nThe dataset or a subset will be added to [nf-core/test-datasets](https://github.com/nf-core/test-datasets/tree/fastquorum).\nPlease reach out to maintainers if additional support is needed to prepare or select such data.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow.Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2,read_structure\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,5M2S+T 5M2S+T\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\nThe `sample` column provides a unique identifier for the given sample, while the `read_structure` describes how the bases in a sequencing run should be allocated into logical reads, including the unique molecular index(es).\n(Please see the [fgbio documentation](https://github.com/fulcrumgenomics/fgbio/wiki/Read-Structures) for detailed information on read structure syntax and formatting.)\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/fastquorum \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --genome GRCh38 \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nTwo modes of running this pipeline are supported:\n\n1. Research and Development (R&D): use `--mode rd` or `params.mode=rd`. This mode is desirable to be able to branch off from the pipeline and test e.g. multiple consensus calling or filtering parameters\n2. High Throughput (HT): use `--mode ht` or `params.mode=ht`. This mode is intended for high throughput production environments where performance and throughput take precedence over flexibility\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/fastquorum/usage) and the [parameter documentation](https://nf-co.re/fastquorum/parameters).\n\nSee also:\n\n1. The [fgbio Best Practice FASTQ -> Consensus Pipeline][fgbio-best-practices-link]\n2. [Read structures](https://github.com/fulcrumgenomics/fgbio/wiki/Read-Structures) as required in the input sample sheet.\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/fastquorum/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/fastquorum/output).\n\n## Credits\n\nnf-core/fastquorum was originally written and is primarily maintained by Nils Homer.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Nils Homer](https://github.com/nh13)\n\n## Acknowledgements\n\nWe thank [Fulcrum Genomics](https://www.fulcrumgenomics.com/) for their extensive assistance in the development of this pipeline.\n\n<p align=\"left\">\n<a href=\"https://fulcrumgenomics.com\">\n<img width=\"500\" height=\"100\" src=\"docs/images/Fulcrum.svg\" alt=\"Fulcrum Genomics\"/>\n</a>\n</p>\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#fastquorum` channel](https://nfcore.slack.com/channels/fastquorum) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/fastquorum for your analysis, please cite [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.11267672.svg)](https://doi.org/10.5281/zenodo.11267672) for this pipeline and [![DOI](https://zenodo.org/badge/53011104.svg)](https://zenodo.org/doi/10.5281/zenodo.10456900) for [`fgbio`](https://github.com/fulcrumgenomics/fgbio).\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\n[fgbio-best-practices-link]: https://github.com/fulcrumgenomics/fgbio/blob/main/docs/best-practice-consensus-pipeline.md\n[duplex-seq-link]: https://en.wikipedia.org/wiki/Duplex_sequencing\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "985",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/985?version=4",
        "name": "nf-core/fastquorum",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "consensus",
            "umi",
            "umis",
            "unique-molecular-identifier"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-04-13",
        "versions": 4
    },
    {
        "create_time": "2025-04-03",
        "creators": [
            "Phuong Doan"
        ],
        "description": "# AnnoAudit - Annotation Auditor\r\n\r\nAnnoAudit is a robust Nextflow pipeline designed to evaluate the quality of genomic annotations through a multifaceted approach.\r\n\r\n## Overview of the workflow\r\n\r\nThe workflow assess the annotation quality based on different criteria:\r\n- Protein evidence support\r\n- RNASeq evidence support\r\n- Statistics of the predictions (i.e., gene length, exon number, etc.)\r\n- Ortholog analysis (BUSCO, OMArk)\r\n\r\n### Input data\r\n\r\n- Reference genome `genome.[.fna, .fa, .fasta]`\r\n- Annotation output `annotation.gff`\r\n- RNAseq data listed in a metadata csv file. Input type can be mixed between long and short reads, with the option of single-end read. The input file should follow the format below:\r\n\r\n```\r\nsample_id,R1_path,R2_path,read_type\r\nSAM1,/path/to/R1,,long             # For long reads\r\nSAM2,/path/to/R1,/path/to/R2,short # For PE reads\r\nSAM3,/path/to/R1,,short            # For SE reads\r\n```\r\n\r\n- Protein reference data in `fasta` format for evaluation, if not given, then the `Uniprot-SwissProt` will be downloaded and used.\r\n\r\n### Pipeline steps\r\n\r\n![Pipeline](./assets/images/annoaudit-workflow.svg)\r\n\r\nThe main pipeline is divided into five different subworkflows.\r\n- `General statistics`: Calculate the statistics obtained from the GFF file.\r\n- `RNASeq analysis`: Map the RNASeq data to the genome (or with provided mapping bam file) to generate exon, intron, transcript coverage.\r\n- `Ortholog analysis`: Compare the predicted proteome to known database using BUSCO and OMArk (OMA database).\r\n- `Protein analysis`: Blast the predicted proteome to a known database (could be of relative species) to obtain best reciprocal hits (BRH), then generate statistics based on the BRH results.\r\n\r\n### Output data\r\n\r\n- Output text file contain the statistic calculated from the input `GFF` file: \r\n  - General statistics\r\n  - BUSCO\r\n  - OMArk\r\n  - PSAURON\r\n  - Best reciprocal hits\r\n  - RNASeq analysis\r\n\r\n## Prerequisites\r\n\r\nThe following programs are required to run the workflow and the listed version were tested. \r\n\r\n`nextflow v23.04.0 or higher`\r\n\r\n`singularity`\r\n\r\n`docker` (have not been tested but in theory should work fine)\r\n\r\n## Installation\r\n\r\nSimply get the code from github or workflowhub and directly use it for the analysis with `nextflow`.\r\n\r\n```\r\ngit clone https://github.com/ERGA-consortium/pipelines\r\n```\r\n\r\n## Running AnnoAudit\r\n\r\n### Before running the pipeline (IMPORTANT)\r\n\r\nOne thing with Nextflow is that it is running off a Java Virtual Machine (JVM), and it will try to use all available memory for Nextflow even though it is unnecessary (for workflow management and job control). This will cause much trouble if you run a job on an HPC cluster. Thus, to minimize the effect of it, we need to limit the maximum memory the JVM can use.\r\n\r\n```\r\nexport NFX_OPTS=\"-Xms=512m -Xmx=3g\"\r\n```\r\n\r\n`-Xms` is the lower limit, which is set as 512 MB.\r\n`-Xmx` is the upper limit, which in this case is set as 3 GB.\r\nPlease modify this according to your situation.\r\n\r\n### How to run the code\r\n\r\n```\r\nnextflow run main.nf --genome genome.fasta \\\r\n      --gff annotation.gff3 \\\r\n      --rnaseq metadata.csv [--genome_bam path/to/the/mapped/bam]\\\r\n      --outdir OUTDIR_NAME \\\r\n      --taxon_id 9606 [Optional] \\\r\n      --ncbi_query_email xxxx \\\r\n      --rm -resume\r\n```\r\n\r\n### Other parameters for running the analysis\r\n\r\n```\r\nInput parameter:\r\n--genome                  Draft genome fasta file contain the assembled contigs/scaffolds.\r\n--gff                     Annotation file that need to be evaluated.\r\n--genome_bam              BAM file contain the mapped information from the RNASeq to the genome FASTA.\r\n--rnaseq                  A metadata CSV file following the pattern: sample_id,R1_path,R2_path,read_type. Required if `genome_bam` is not provided.\r\n--taxon_id                Taxon ID for identifying BUSCO lineage and download protein data from NCBI if needed.\r\n--ncbi_query_email        Email for querying protein from NCBI database.\r\n\r\nOptional input:\r\n--protein                  Fasta file containing translated protein sequences from the GFF for running evaluation. If not specified, the workflow will automatically extract it from the\r\n `genome` and `gff`.\r\n--ref_protein              Fasta file containing the reference protein sequences to be used for evaluation. Ideally this should come from the same species and/or closely related specie\r\ns. If not provided, the workflow will download the proteome from NCBI or using Uniprot SwissProt database.\r\n--lineage                  Lineage information providing for BUSCO, if not provided, the workflow will automatically search for the closest lineage. Example: eudicots_odb10.\r\n--genetic_code             Genetic code for translation of protein.\r\n--stranding                Strandness of the RNASeq reads used for extraction of junction position using `regtools`.\r\n\r\nDatabase input:\r\n--odb_version              odb version to choose to run BUSCO, option: odb12, odb10. [default: odb12]\r\n--busco_database           Pathway to the BUSCO databse store locally. [default: null]\r\n--oma_database             Pathway to the OMA database, if not specified, the workflow will download it automatically. [default: null]\r\n--ref_protein              Pathway to the reference proteome for comparison. [default: null]\r\n--ncbi_query_count         Number of protein to extract from the NCBI database. [default: 100000]\r\n--ncbi_query_batch         Number of protein to query for each batch. [default: 1000]\r\n\r\nOutput option:\r\n--pdf                      Output PDF name. [default: AnnoAudit_Report.pdf]\r\n--outdir                   Output directory. [default: /env/export/bigtmp2/pdoan/evaluate_pipeline]\r\n--tracedir                 Pipeline information. [default: /env/export/bigtmp2/pdoan/evaluate_pipeline/pipeline_info]\r\n--publish_dir_mode         Option for nextflow to move data to the output directory. [default: copy]\r\n--tmpdir                   Database directory. [default: /env/export/bigtmp2/pdoan/evaluate_pipeline/tmpdir]\r\n\r\nConditioning options:\r\n--rnaseq_single             If specify, will run `featureCounts` in single read mode, this is necessary if the mapped RNASeq is single-ended. [default: false]\r\n--run_blast                 If specify, will use `blast` for running best reciprocal hits instead of DIAMOND. [default: false]\r\n--query_ncbi_prot           If specify, will download the reference proteome from NCBI, other wise, will use the provided proteom or Uniprot SwissProt. [default: true]\r\n--cds_only                  If specify, only extracting information from the GFF file using the CDS line. [default: \"False\"]\r\n\r\n--help                   Print help message.\r\n\r\nExecution/Engine profiles:\r\nThe pipeline supports profiles to run via different Executers and Engines e.g.: -profile local,conda\r\n\r\nExecuter (choose one):\r\n  local\r\n  slurm\r\n\r\nEngines (choose one):\r\n  docker\r\n  singularity\r\n  apptainer\r\n\r\nPer default: -profile slurm,singularity is executed.\r\n```\r\n\r\n## Example output\r\n\r\nBelow is the sample output of this workflow. The example PDF output is located in `assest` folder.\r\n\r\n```\r\n|General Statistics                 | Value           |\r\n-------------------------------------------------------\r\n|num_genes                          | 36391           |\r\n|num_genes_without_introns          | 12968 (35.64%)  |\r\n|mean_gene_length                   | 2359.57         |\r\n|median_gene_length                 | 1562.0          |\r\n|num_exons                          | 149725          |\r\n|mean_exons_per_gene                | 4.11            |\r\n|median_exons_per_gene              | 2.0             |\r\n|num_exon_3n                        | 76783 (51.28%)  |\r\n|num_exon_3n1                       | 36932 (24.67%)  |\r\n|num_exon_3n2                       | 36010 (24.05%)  |\r\n|mean_cds_length                    | 1091.4          |\r\n|median_cds_length                  | 873.0           |\r\n|total_cds_length                   | 39717145        |\r\n|percentage_cds_coverage            | 10.64%          |\r\n|num_introns                        | 113334          |\r\n|mean_intron_length                 | 407.2           |\r\n|median_intron_length               | 149.0           |\r\n|short_intron_<120_3n0_without_stop | 4324 (3.82)%    |\r\n|long_intron_>120_3n0_without_stop  | 1185 (1.05)%    |\r\n|short_intron_<120_3n1_without_stop | 4205 (3.71)%    |\r\n|long_intron_>120_3n1_without_stop  | 1291 (1.14)%    |\r\n|short_intron_<120_3n2_without_stop | 4319 (3.81)%    |\r\n|long_intron_>120_3n2_without_stop  | 1249 (1.10)%    |\r\n|short_intron_<120_3n0_with_stop    | 12073 (10.65)%  |\r\n|long_intron_>120_3n0_with_stop     | 20332 (17.94)%  |\r\n|short_intron_<120_3n1_with_stop    | 11652 (10.28)%  |\r\n|long_intron_>120_3n1_with_stop     | 20486 (18.08)%  |\r\n|short_intron_<120_3n2_with_stop    | 11733 (10.35)%  |\r\n|long_intron_>120_3n2_with_stop     | 20485 (18.07)%  |\r\n\r\n|BUSCO                              | Value           |\r\n-------------------------------------------------------\r\n|lineage_dataset                    | poales_odb10    |\r\n|complete                           | 97.6%           |\r\n|single_copy                        | 95.8%           |\r\n|multi_copy                         | 1.8%            |\r\n|fragmented                         | 0.2%            |\r\n|missing                            | 2.2%            |\r\n|num_markers                        | 4896            |\r\n|domain                             | eukaryota       |\r\n\r\n\r\n|OMARK                              | Value           |\r\n-------------------------------------------------------\r\n|OMA_clade                          | Oryza           |\r\n|num_conserved_hogs                 | 15087           |\r\n|single                             | 13316 (88.26%)  |\r\n|duplicated                         | 1353 (8.97%)    |\r\n|duplicated_unexpected              | 1101 (7.30%)    |\r\n|duplicated_expected                | 252 (1.67%)     |\r\n|missing                            | 418 (2.77%)     |\r\n|num_proteins_in_proteome           | 36387           |\r\n|total_consistent                   | 30365 (83.45%)  |\r\n|consistent_partial_hits            | 1803 (4.96%)    |\r\n|consistent_fragmented              | 1625 (4.47%)    |\r\n|total_inconsistent                 | 2283 (6.27%)    |\r\n|inconsistent_partial_hits          | 517 (1.42%)     |\r\n|inconsistent_fragmented            | 1444 (3.97%)    |\r\n|total_contaminants                 | 0 (0.00%)       |\r\n|contaminants_partial_hits          | 0 (0.00%)       |\r\n|contaminants_fragmented            | 0 (0.00%)       |\r\n|total_unknown                      | 3739 (10.28%)   |\r\n\r\n|PSAURON                            | Value           |\r\n-------------------------------------------------------\r\n|psauron_score                      | 83.8            |\r\n|true_count                         | 30494           |\r\n|false_count                        | 5893            |\r\n|median_score                       | 0.98278         |\r\n|max_score                          | 1.0             |\r\n|min_score                          | 0.00022         |\r\n\r\n\r\n|Best Reciprocal Hits               | Value           |\r\n-------------------------------------------------------\r\n|num_best_reciprocal_hits           | 29185           |\r\n|num_splitting_genes_08             | 932 (3.19%)     |\r\n|num_splitting_genes_05             | 0 (0.0%)        |\r\n|num_fusion_genes_12                | 437 (1.5%)      |\r\n|num_fusion_genes_15                | 482 (1.65%)     |\r\n|KL_divergence_normalized           | 0.0105          |\r\n|JS_divergence_normalized           | 0.0023          |\r\n|Wasserstein_distance               | 2.480915        |\r\n\r\n\r\n|RNASeq                             | Value           |\r\n-------------------------------------------------------\r\n|mapping_rate                       | 96.27%          |\r\n|primary_mapping_rate               | 95.83%          |\r\n|properly_paired                    | 92.47%          |\r\n|num_gene_unsupported               | 9445 (25.95%)   |\r\n|num_exon_unsupported               | 20232 (13.51%)  |\r\n|num_intron_supported               | 107202          |\r\n|num_intron_supported_canonical     | 107131 (99.93%) |\r\n|num_intron_supported_non_canonical | 71 (0.07%)      |\r\n```\r\n\r\n## Performance of the workflow on assessing annotation\r\n\r\nTo be added\r\n\r\n## Future work\r\n\r\n- Adding other plots for easier evaluation\r\n- Perform comparative performance with different genomes",
        "doi": "10.48546/workflowhub.workflow.1330.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1330",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1330?version=1",
        "name": "AnnoAudit - Annotation Auditor",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics Laboratory for Genomics and Biodiversity (LBGB)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "biodiversity",
            "bioinformatics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-04-03",
        "versions": 1
    },
    {
        "create_time": "2025-03-28",
        "creators": [],
        "description": "Lysozyme in water full COMPSs application. Added new WRROC profile: [Provenance Run Crate](https://www.researchobject.org/workflow-run-crate/profiles/provenance_run_crate/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1326",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1326?version=1",
        "name": "Lysozyme in water full with new provenance run features",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2025-03-28",
        "versions": 1
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Engy Nasr",
            "B\u00e9r\u00e9nice Batut",
            "Paul Zierep"
        ],
        "description": "Microbiome - Variant calling and Consensus Building",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in description",
        "id": "1063",
        "keep": true,
        "latest_version": 5,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1063?version=5",
        "name": "allele-based-pathogen-identification/main",
        "number_of_steps": 23,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "snpSift_filter",
            "regexColumn1",
            "Count1",
            "minimap2",
            "Cut1",
            "CONVERTER_gz_to_uncompressed",
            "tp_head_tool",
            "bcftools_norm",
            "samtools_depth",
            "samtools_coverage",
            "Remove beginning1",
            "table_compute",
            "collapse_dataset",
            "Paste1",
            "clair3",
            "tp_cut_tool",
            "bcftools_consensus",
            "snpSift_extractFields"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 5
    },
    {
        "create_time": "2025-10-08",
        "creators": [
            "Delphine Lariviere"
        ],
        "description": "# Contiging Solo w/HiC:\n\nGenerate phased assembly based on PacBio Hifi Reads using HiC data from the same individual for phasing.\n\n## Inputs\n\n1. Hifi long reads [fastq]\n2. HiC forward reads (if multiple input files, concatenated in same order as reverse reads) [fastq]\n3. HiC reverse reads (if multiple input files, concatenated in same order as forward reads) [fastq]\n4. K-mer database [meryldb]\n5. Genome profile summary generated by Genomescope [txt]\n6. Name of first assembly\n7. Name of second assembly\n\n## Outputs\n\n1. Haplotype 1 assembly ([fasta] and [gfa])\n2. Haplotype 2 assembly ([fasta] and [gfa])\n3. QC: BUSCO report for both assemblies\n4. QC: Merqury report for both assemblies\n5. QC: Assembly statistics for both assemblies\n6. QC: Nx plot for both assemblies\n7. QC: Size plot for both assemblies",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "641",
        "keep": true,
        "latest_version": 31,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/641?version=31",
        "name": "Assembly-Hifi-HiC-phasing-VGP4/main",
        "number_of_steps": 32,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "reviewed",
            "vgp"
        ],
        "tools": [
            "",
            "cutadapt",
            "tp_replace_in_line",
            "tp_awk_tool",
            "Add_a_column1",
            "tp_grep_tool",
            "Convert characters1",
            "Cut1",
            "gfastats",
            "tp_sed_tool",
            "bandage_image",
            "merqury",
            "Paste1",
            "param_value_from_file",
            "hifiasm",
            "multiqc",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-08",
        "versions": 31
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Peter J Bailey",
            "Bailey PJ",
            "Alexander Peltzer",
            "Botvinnik O",
            "Olga Botvinnik",
            "Marques de Almeida F",
            "Peltzer A",
            "Sturm G"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-scrnaseq_logo_dark.png\">\n    <img alt=\"nf-core/scrnaseq\" src=\"docs/images/nf-core-scrnaseq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/scrnaseq/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/scrnaseq/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/scrnaseq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/scrnaseq/actions/workflows/linting.yml)\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/scrnaseq/results)\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.3568187-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.3568187)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/scrnaseq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23scrnaseq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/scrnaseq)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/scrnaseq** is a bioinformatics best-practice analysis pipeline for processing 10x Genomics single-cell RNA-seq data.\n\nThis is a community effort in building a pipeline capable to support:\n\n- SimpleAF(Alevin-Fry) + AlevinQC\n- STARSolo\n- Kallisto + BUStools\n- Cellranger\n\n> [!IMPORTANT]\n> Cellranger is a commercial tool from 10X Genomics Inc. and falls under the EULA from 10X Genomics Inc. The container provided for the CellRanger functionality in this pipeline has been built by the nf-core community and is therefore _not supported by 10X genomics_ directly. We are in discussions with 10X on how to improve the user experience and licence situation for both us as a community as well as 10X and end users and will update this statement here accordingly.\n\n## Documentation\n\nThe nf-core/scrnaseq pipeline comes with documentation about the pipeline [usage](https://nf-co.re/scrnaseq/usage), [parameters](https://nf-co.re/scrnaseq/parameters) and [output](https://nf-co.re/scrnaseq/output).\n\n![scrnaseq workflow](docs/images/scrnaseq_pipeline_V3.0-metro_clean.png)\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2,expected_cells\npbmc8k,pbmc8k_S1_L007_R1_001.fastq.gz,pbmc8k_S1_L007_R2_001.fastq.gz,10000\npbmc8k,pbmc8k_S1_L008_R1_001.fastq.gz,pbmc8k_S1_L008_R2_001.fastq.gz,10000\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/scrnaseq \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --fasta GRCm38.p6.genome.chr19.fa \\\n   --gtf gencode.vM19.annotation.chr19.gtf \\\n   --protocol 10XV2 \\\n   --aligner <simpleaf/kallisto/star/cellranger> \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/scrnaseq/usage) and the [parameter documentation](https://nf-co.re/scrnaseq/parameters).\n\n## Decision Tree for users\n\nThe nf-core/scrnaseq pipeline features several paths to analyze your single cell data. Future additions will also be done soon, e.g. the addition of multi-ome analysis types. To aid users in analyzing their data, we have added a decision tree to help people decide on what type of analysis they want to run and how to choose appropriate parameters for that.\n\n```mermaid\ngraph TD\n    A[sc RNA] -->|alevin-fry| B(h5ad/seurat/mtx matrices)\n    A[sc RNA] -->|CellRanger| B(h5ad/seurat/mtx matrices)\n    A[sc RNA] -->|kbpython| B(h5ad/seurat/mtx matrices)\n    A[sc RNA] -->|STARsolo| B(h5ad/seurat/mtx matrices)\n```\n\nOptions for the respective alignment method can be found [here](https://github.com/nf-core/scrnaseq/blob/dev/docs/usage.md#aligning-options) to choose between methods.\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/scrnaseq/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/scrnaseq/output).\n\n## Credits\n\nnf-core/scrnaseq was originally written by Bailey PJ, Botvinnik O, Marques de Almeida F, Gabernet G, Peltzer A, Sturm G.\n\nWe thank the following people and teams for their extensive assistance in the development of this pipeline:\n\n- @heylf\n- @KevinMenden\n- @FloWuenne\n- @rob-p\n- [GHGA](https://www.ghga.de/)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#scrnaseq` channel](https://nfcore.slack.com/channels/scrnaseq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/scrnaseq for your analysis, please cite it using the following doi: [10.5281/zenodo.3568187](https://doi.org/10.5281/zenodo.3568187)\n\nThe basic benchmarks that were used as motivation for incorporating the available modular workflows can be found in [this publication](https://www.biorxiv.org/content/10.1101/673285v2).\n\nWe offer all three paths for the processing of scRNAseq data so it remains up to the user to decide which pipeline workflow is chosen for a particular analysis question.\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1021",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1021?version=17",
        "name": "nf-core/scrnaseq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "10x-genomics",
            "10xgenomics",
            "cellranger",
            "alevin",
            "bustools",
            "kallisto",
            "rna-seq",
            "single-cell",
            "star-solo"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-26",
        "versions": 17
    },
    {
        "create_time": "2025-03-26",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-pacvar_logo_dark.png\">\n    <img alt=\"nf-core/pacvar\" src=\"docs/images/nf-core-pacvar_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/pacvar/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/pacvar/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/pacvar/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/pacvar/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/pacvar/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/pacvar)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23pacvar-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/pacvar)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\nnf-core/pacvar is a bioinformatics pipeline that processes long-read PacBio data. Specifically, the pipeline provides two workflows: one for processing whole-genome sequencing data, and another for processing reads from the PureTarget expansion panel offered by PacBio. This second workflow characterizes tandem repeats. Because the pipeline is designed for PacBio reads, it uses PacBio\u2019s officially released tools.\n\n![nf-core/pacvar metro map](docs/images/pacvar_white_background.png)\n\nWorkflow Overview\n\n1. Demultiplex reads ([`lima`](https://lima.how))\n2. Align reads ([`pbmm2`](https://github.com/PacificBiosciences/pbmm2))\n3. Sort and index alignments ([`SAMtools`](https://sourceforge.net/projects/samtools/files/samtools/))\n\nWGS Workflow Overview\n\n1. Choice of SNP calling routes:\n   a. ([`deepvariant`](https://github.com/google/deepvariant))\n   b. ([`HaplotypeCaller`](https://gatk.broadinstitute.org/hc/en-us/articles/360037225632-HaplotypeCaller))\n2. Call SVs ([`pbsv`](https://github.com/PacificBiosciences/pbsv))\n3. Index VCF files ([`bcftools`](https://samtools.github.io/bcftools/bcftools.html))\n4. Phase SNPs, SVs and BAM files ([`hiphase`](https://github.com/PacificBiosciences/HiPhase))\n\nTandem Repeat Workflow Overview\n\n1. Genotype tandem repeats - produce spanning bams and vcf ([`TRGT`](https://github.com/PacificBiosciences/trgt))\n2. Index and Sort tandem tepeat spanning bam ([`SAMtools`](https://sourceforge.net/projects/samtools/files/samtools/))\n3. Plot repeat motif plots ([`TRGT`](https://github.com/PacificBiosciences/trgt))\n4. Sort spanning VCF ([`bcftools`](https://samtools.github.io/bcftools/bcftools.html))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,bam,pbi\nCONTROL,AEG588A1_S1_L002_R1_001.bam,AEG588A1_S1_L002_R1_001.pbi\n```\n\nNote that the `.pbi` file is not required. If you choose not to include it, your input file might look like this:\n\n```csv\nsample,bam,pbi\nCONTROL,AEG588A1_S1_L002_R1_001.bam\n```\n\nEach row represents an unaligned bam file and their associated index (optional).\n\nNow, you can run the pipeline. Below is an example\n\n```bash\nnextflow run nf-core/pacvar \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --workflow <wgs/repeat> \\\n   --barcodes barcodes.bed \\\n   --intervals intervals.bed \\\n   --genome <GENOME NAME (e.g. GATK.GRCh38)> \\\n   --outdir <OUTDIR>\n```\n\noptional paramaters include: `--skip_demultiplexing`, `--skip_snp`, `--skip_sv`, `--skip_phase`.\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/pacvar/usage) and the [parameter documentation](https://nf-co.re/pacvar/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/pacvar/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/pacvar/output).\n\n## Credits\n\nnf-core/pacvar was originally written by Tanya Sarkin Jain.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#pacvar` channel](https://nfcore.slack.com/channels/pacvar) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/pacvar for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1293",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1293?version=3",
        "name": "nf-core/pacvar",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "wgs",
            "long-read",
            "pacbio",
            "puretarget",
            "variant-calling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-26",
        "versions": 3
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "@kbestak None",
            "@kbestak None",
            "@FloWuenne None",
            "@FloWuenne None"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-molkart_logo_dark.png\">\n    <img alt=\"nf-core/molkart\" src=\"docs/images/nf-core-molkart_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/molkart/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/molkart/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/molkart/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/molkart/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/molkart/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.10650748-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.10650748)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/molkart)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23molkart-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/molkart)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/molkart** is a pipeline for processing Molecular Cartography data from Resolve Bioscience (combinatorial FISH). It takes as input a table of FISH spot positions (x,y,z,gene), a corresponding DAPI image (`TIFF` format) and optionally an additional staining image in the `TIFF` format. nf-core/molkart performs end-to-end processing of the data including image processing, QC filtering of spots, cell segmentation, spot-to-cell assignment and reports quality metrics such as the spot assignment rate, average spots per cell and segmentation mask size ranges.\n\n<p align=\"center\">\n    <img title=\"Molkart Workflow\" src=\"docs/images/molkart_workflow.png\" width=100%>\n</p>\n\nImage preprocessing\n\n- Fill the grid pattern in provided images ([`Mindagap`](https://github.com/ViriatoII/MindaGap))\n- Optionally apply contrast-limited adaptive histogram equalization\n- If a second (membrane) image is present, combine images into a multichannel stack (if required for segmentation)\n\nCell segmentation\n\n- Apply cell segmentation based on provided images, available options are: - [`Cellpose`](https://www.cellpose.org/) - [`Mesmer`](https://deepcell.readthedocs.io/en/master/API/deepcell.applications.html#mesmer) - [`ilastik`](https://www.ilastik.org/) - [`Stardist`](https://github.com/stardist/stardist)\n- Filter cells based on cell size to remove artifacts\n\nSpot processing\n\n- Find duplicated spots near grid lines ([`Mindagap`](https://github.com/ViriatoII/MindaGap))\n- Assign spots to segmented cells\n\nQuality control\n\n- Create quality-control metrics specific to this pipeline\n- provide them to ([`MultiQC`](http://multiqc.info/)) to create a report\n\n## Usage\n\n:::note\nIf you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how\nto set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline)\nwith `-profile test` before running the workflow on actual data.\n:::\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,nuclear_image,spot_locations,membrane_image\nsample0,sample0_DAPI.tiff,sample0_spots.txt,sample0_WGA.tiff\n```\n\nEach row represents an FOV (field-of-view). Columns represent the sample ID (all must be unique), the path to the respective nuclear image, the spot table, and optionally the path to the respective membrane image (or any additional image to improve segmentation).\n\nNow, you can run the pipeline using all default values with:\n\n```bash\nnextflow run nf-core/molkart \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/molkart/usage) and the [parameter documentation](https://nf-co.re/molkart/parameters).\n\n## Pipeline output\n\nThe pipeline outputs a matched cell-by-transcript table based on deduplicated spots and segmented cells, as well as preprocessing and segmentation intermediaries.\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/molkart/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/molkart/output).\n\n## Credits\n\nnf-core/molkart was originally written by @kbestak, @FloWuenne.\n\nWe thank [Maxime U Garcia](https://github.com/maxulysse) for his assistance and support in the development of this pipeline.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#molkart` channel](https://nfcore.slack.com/channels/molkart) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/molkart for your analysis, please cite it using the following doi: [10.5281/zenodo.10650749](https://doi.org/10.5281/zenodo.10650749)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1001",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1001?version=2",
        "name": "nf-core/molkart",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "segmentation",
            "transcriptomics",
            "fish",
            "image-processing",
            "imaging",
            "molecularcartography",
            "single-cell",
            "spatial"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-26",
        "versions": 2
    },
    {
        "create_time": "2025-03-19",
        "creators": [
            "zhantian xu"
        ],
        "description": "# PISAD - Phsaed Intraspecies Sample Anomalies Detection tool\r\n\r\n## Summary\r\n\r\nWe developed PISAD, a tool designed to detect anomalies in cohort samples without requiring reference information. It is primarily divided into two stages. Stage 1: We select low-error data from the cohort and conduct reference-free SNP calling to construct a variant sketch. Stage 2: By comparing the k-mer counts of other cohort data to the variant sketch, we infer the relationships between the sample and other samples to detect the sample swap.\r\n\r\n## Dependencies\r\n\r\nrecommend use conda to install\r\n\r\n- GCC (Tested on 8.5.0)\r\n- gperftools(2.10)\r\n- hdf5(1.14.3)\r\n- boost(1.85.0)\r\n\r\n## Installation\r\n\r\ncloning the PISAD repository to your machine and enter its directory.\r\n\r\n```bash\r\n git clone https://github.com/ZhantianXu/PISAD.git\r\n cd pisad/\r\n```\r\n\r\nCompiling should be as easy as:\r\n\r\n```bash\r\n./configure && make\r\n```\r\n\r\nTo install in a specified directory:\r\n\r\n```bash\r\n./configure --prefix=/PATH && make install\r\n```\r\n\r\n## Usage\r\n\r\n##### Stage1: SNP callng :\r\n\r\nFirst, we select a low-error-rate sequencing dataset as the target sample for rapid SNP calling. It supports multi-threaded processing.\r\n\r\nExample:\r\n\r\n```bash\r\n./run.sh -i /data/hg002.fastq.gz -m 0\r\n```\r\n\r\n```bash\r\n    Required parameters:\r\n      -i:        Input files ( *.fastq or *.fastq.gz files)\r\n      -m:        Heterozygosity parameter (0 for <1.2%, 1 otherwise)\r\n    Optional parameters:\r\n      -k:        kmer-size (default: 21)\r\n      -t:        thread (default: 8)\"\r\n      -o:        Output prefix (defaults: first input file's prefix)\r\n      -d1:       Directory for dsk files (default: current directory)\r\n      -d2:       Directory for output plot (default: current directory)\r\n      -d3:       Directory for SNP output (default: current directory)\r\n      -h:        Show this help message\r\n    Advanced optional parameters:\r\n      -est:      est_kmercov (default: Estimated by algorithm)\r\n      -cutoff:   cutoff threshold (defaults: 0.95)\r\n      -het:      Initial heterozygosity (defaults: 0/0.12)\r\n      -rho:      Initial rho value (defaults: 0.2)\r\n      -setleft:  Left boundary of the heterozygous region (defaults: Estimated by algorithm)\r\n      -setright: Right boundary of the heterozygous region (defaults: Estimated by algorithm)\r\n```\r\n\r\n##### Stage1: construct variant sketch:\r\n\r\nNext, we convert the called SNPs into a variant sketch.\r\n\r\n```bash\r\n./create -i /snp/hg002_21_2_4_pairex.snp\r\n```\r\n\r\n```bash\r\n    Required parameters:\r\n      -i:        Input files ( .snp file)\r\n    Optional parameters:\r\n      -k:        kmer-size (default: 21)\r\n      -l:        Filtering threshold (default: 21)\r\n      -o:        Output prefix (defaults: current directory)\r\n```\r\n\r\n##### Stage2: count the k-mers:\r\n\r\nwe compare the k-mer counts of other cohort samples to the variant sketch to infer relationships between them. Files may be gzipped and multiple threads can be used.\r\n\r\n```bash\r\n./pisadCount -s /fa/hg002.fa /data/hg003.fastq.gz\r\n```\r\n\r\n```bash\r\n    Usage: ./pisadCount -s [FASTA] [OPTION]... [FILES...]\r\n    Required options:\r\n        -s:         variant sketch (one or more)\r\n    Optional options:\r\n        -t:      Number of threads to run (default: 1)\r\n        -m:      k-mer coverage threshold for early termination (default: inf)\r\n        -i:      extra debug information\r\n        -k:      k-mer size used (default: 21)\r\n        -o:      Evaluation file path (defaults: current directory)\r\n        -h:      Display this dialog\r\n\r\n```\r\n\r\nHere, the -s option allows inputting multiple FA files for variant sketching, separated by commas, such as `-s /fa/hg002.fa,/fa/hg001.fa`.\r\nIf your input file has a high coverage, you can also add the `-m` parameter to control the reading process and save time, such as `-m 10`.\r\n\r\n##### Stage2:Evaluate the samples:\r\n\r\nInput the statistics of samples to calculate their relationship and detect sample swaps.\r\n\r\n```bash\r\n./pisadEval /homeb/xuzt/coverage/eval/hg002_hg003.txt > summary.tsv\r\n```\r\n\r\n```bash\r\n    Usage: ./pisadEval [OPTION]... [FILES...]\r\n    Optional options:\r\n        -t:      Number of threads to run(default: 1)\r\n        -h:      Display this dialog\r\n\r\n```\r\n",
        "doi": "10.48546/workflowhub.workflow.1322.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "1322",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1322?version=1",
        "name": "PISAD - Phsaed Intraspecies Sample Anomalies Detection tool",
        "number_of_steps": 0,
        "projects": [
            "CSUbioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Argo Workflow",
        "update_time": "2025-03-19",
        "versions": 1
    },
    {
        "create_time": "2025-03-12",
        "creators": [
            "Damon-Lee Pointon"
        ],
        "description": "# sanger-tol/curationpretext\r\n\r\n[![GitHub Actions CI Status](https://github.com/sanger-tol/curationpretext/workflows/nf-core%20CI/badge.svg)](https://github.com/sanger-tol/curationpretext/actions?query=workflow%3A%22nf-core+CI%22)\r\n[![GitHub Actions Linting Status](https://github.com/sanger-tol/curationpretext/workflows/nf-core%20linting/badge.svg)](https://github.com/sanger-tol/curationpretext/actions?query=workflow%3A%22nf-core+linting%22)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.12773958-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.12773958)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/sanger-tol/curationpretext)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/curationpretext** is a bioinformatics pipeline typically used in conjunction with [TreeVal](https://github.com/sanger-tol/treeval) to generate pretext maps (and optionally telomeric, gap, coverage, and repeat density plots which can be ingested into pretext) for the manual curation of high quality genomes.\r\n\r\nThis is intended as a supplementary pipeline for the [treeval](https://github.com/sanger-tol/treeval) project. This pipeline can be simply used to generate pretext maps, information on how to run this pipeline can be found in the [usage documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/usage).\r\n\r\n![Workflow Diagram](./assets/CurationPretext.png)\r\n\r\n1. Generate Maps - Generates pretext maps as well as a static image.\r\n\r\n2. Accessory files - Generates the repeat density, gap, telomere, and coverage tracks.\r\n\r\n## Usage\r\n\r\n> **Note**\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how\r\n> to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline)\r\n> with `-profile test` before running the workflow on actual data.\r\n\r\nCurrently, the pipeline uses the following flags:\r\n\r\n- `--input`\r\n\r\n  - The absolute path to the assembled genome in, e.g., `/path/to/assembly.fa`\r\n\r\n- `--reads`\r\n\r\n  - The directory of the fasta files generated from longread reads, e.g., `/path/to/fasta/`\r\n\r\n- `--read_type`\r\n\r\n  - The type of longread data you are utilising, e.g., ont, illumina, hifi.\r\n\r\n- `--aligner`\r\n\r\n  - The aligner yopu wish to use for the coverage generation, defaults to bwamem2 but minimap2 is also supported.\r\n\r\n- `--cram`\r\n\r\n  - The directory of the cram _and_ cram.crai files, e.g., `/path/to/cram/`\r\n\r\n- `--map_order`\r\n\r\n  - hic map scaffold order, input either `length` or `unsorted`\r\n\r\n- `--teloseq`\r\n\r\n  - A telomeric sequence, e.g., `TTAGGG`\r\n\r\n- `-entry`\r\n  - ALL_FILES is the default and generates all accessory files as well as pretext maps\r\n  - MAPS_ONLY generates only the pretext maps and static images\r\n\r\nNow, you can run the pipeline using:\r\n\r\n#### For ALL_FILES run\r\n\r\n```bash\r\nnextflow run sanger-tol/curationpretext \\\r\n  --input { input.fasta } \\\r\n  --cram { path/to/cram/ } \\\r\n  --reads { path/to/longread/fasta/ } \\\r\n  --read_type { default is \"hifi\" }\r\n  --sample { default is \"pretext_rerun\" } \\\r\n  --teloseq { default is \"TTAGGG\" } \\\r\n  --map_order { default is \"unsorted\" } \\\r\n  --outdir { OUTDIR } \\\r\n  -profile <docker/singularity/{institute}>\r\n\r\n```\r\n\r\n#### For MAPS_ONLY run\r\n\r\n```bash\r\nnextflow run sanger-tol/curationpretext \\\r\n  --input { input.fasta } \\\r\n  --cram { path/to/cram/ } \\\r\n  --reads { path/to/longread/fasta/ } \\\r\n  --read_type { default is \"hifi\" }\r\n  --sample { default is \"pretext_rerun\" } \\\r\n  --teloseq { default is \"TTAGGG\" } \\\r\n  --map_order { default is \"unsorted\" } \\\r\n  --outdir { OUTDIR } \\\r\n  -profile <docker/singularity/{institute}> \\\r\n  -entry MAPS_ONLY \\\r\n```\r\n\r\n> **Warning:**\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\r\n> provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n\r\nFor more details, please refer to the [usage documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/usage) and the [parameter documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/parameters).\r\n\r\n## Pipeline output\r\n\r\nTo see the the results of a test run with a full size dataset refer to the [results](https://pipelines.tol.sanger.ac.uk/curationpretext/results) tab on the sanger-tol/curationpretext website pipeline page.\r\nFor more details about the output files and reports, please refer to the\r\n[output documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/output).\r\n\r\n## Credits\r\n\r\nsanger-tol/curationpretext was originally written by Damon-Lee B Pointon (@DLBPointon).\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n- @muffato - For reviews.\r\n\r\n- @yumisims - TreeVal and Software.\r\n\r\n- @weaglesBio - TreeVal and Software.\r\n\r\n- @josieparis - Help with better docs and testing.\r\n\r\n- @mahesh-panchal - Large support with 1.2.0 in making the pipeline more robust with other HPC environments.\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#curationpretext` channel](https://nfcore.slack.com/channels/curationpretext) (you can join with [this invite](https://nf-co.re/join/slack)).\r\n\r\n## Citations\r\n\r\nIf you use sanger-tol/curationpretext for your analysis, please cite it using the following doi: [10.5281/zenodo.12773958](https://doi.org/10.5281/zenodo.12773958)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1321",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1321?version=1",
        "name": "sanger-tol/curationpretext",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Assembly",
            "Tree of Life Genome Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics"
        ],
        "tools": [
            "Nextflow",
            "PretextView"
        ],
        "type": "Nextflow",
        "update_time": "2025-03-12",
        "versions": 1
    },
    {
        "create_time": "2025-03-12",
        "creators": [
            "Damon-Lee Pointon"
        ],
        "description": "# ![sanger-tol/curationpretext](docs/images/curationpretext-light.png#gh-light-mode-only) ![sanger-tol/curationpretext](docs/images/curationpretext-dark.png#gh-dark-mode-only)\r\n\r\n[![GitHub Actions CI Status](https://github.com/sanger-tol/curationpretext/workflows/nf-core%20CI/badge.svg)](https://github.com/sanger-tol/curationpretext/actions?query=workflow%3A%22nf-core+CI%22)\r\n[![GitHub Actions Linting Status](https://github.com/sanger-tol/curationpretext/workflows/nf-core%20linting/badge.svg)](https://github.com/sanger-tol/curationpretext/actions?query=workflow%3A%22nf-core+linting%22)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.12773958-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.12773958)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/sanger-tol/curationpretext)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/curationpretext** is a bioinformatics pipeline typically used in conjunction with [TreeVal](https://github.com/sanger-tol/treeval) to generate pretext maps (and optionally telomeric, gap, coverage, and repeat density plots which can be ingested into pretext) for the manual curation of high quality genomes.\r\n\r\nThis is intended as a supplementary pipeline for the [treeval](https://github.com/sanger-tol/treeval) project. This pipeline can be simply used to generate pretext maps, information on how to run this pipeline can be found in the [usage documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/usage).\r\n\r\n![Workflow Diagram](./assets/CurationPretext.png)\r\n\r\n1. Generate Maps - Generates pretext maps as well as a static image.\r\n\r\n2. Accessory files - Generates the repeat density, gap, telomere, and coverage tracks.\r\n\r\n## Usage\r\n\r\n> **Note**\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how\r\n> to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline)\r\n> with `-profile test` before running the workflow on actual data.\r\n\r\nCurrently, the pipeline uses the following flags:\r\n\r\n- `--input`\r\n\r\n  - The absolute path to the assembled genome in, e.g., `/path/to/assembly.fa`\r\n\r\n- `--reads`\r\n\r\n  - The directory of the fasta files generated from longread reads, e.g., `/path/to/fasta/`\r\n\r\n- `--read_type`\r\n\r\n  - The type of longread data you are utilising, e.g., ont, illumina, hifi.\r\n\r\n- `--aligner`\r\n\r\n  - The aligner yopu wish to use for the coverage generation, defaults to bwamem2 but minimap2 is also supported.\r\n\r\n- `--cram`\r\n\r\n  - The directory of the cram _and_ cram.crai files, e.g., `/path/to/cram/`\r\n\r\n- `--map_order`\r\n\r\n  - hic map scaffold order, input either `length` or `unsorted`\r\n\r\n- `--teloseq`\r\n\r\n  - A telomeric sequence, e.g., `TTAGGG`\r\n\r\n- `-entry`\r\n  - ALL_FILES is the default and generates all accessory files as well as pretext maps\r\n  - MAPS_ONLY generates only the pretext maps and static images\r\n\r\nNow, you can run the pipeline using:\r\n\r\n#### For ALL_FILES run\r\n\r\n```bash\r\nnextflow run sanger-tol/curationpretext \\\r\n  --input { input.fasta } \\\r\n  --cram { path/to/cram/ } \\\r\n  --reads { path/to/longread/fasta/ } \\\r\n  --read_type { default is \"hifi\" }\r\n  --sample { default is \"pretext_rerun\" } \\\r\n  --teloseq { default is \"TTAGGG\" } \\\r\n  --map_order { default is \"unsorted\" } \\\r\n  --outdir { OUTDIR } \\\r\n  -profile <docker/singularity/{institute}>\r\n\r\n```\r\n\r\n#### For MAPS_ONLY run\r\n\r\n```bash\r\nnextflow run sanger-tol/curationpretext \\\r\n  --input { input.fasta } \\\r\n  --cram { path/to/cram/ } \\\r\n  --reads { path/to/longread/fasta/ } \\\r\n  --read_type { default is \"hifi\" }\r\n  --sample { default is \"pretext_rerun\" } \\\r\n  --teloseq { default is \"TTAGGG\" } \\\r\n  --map_order { default is \"unsorted\" } \\\r\n  --outdir { OUTDIR } \\\r\n  -profile <docker/singularity/{institute}> \\\r\n  -entry MAPS_ONLY \\\r\n```\r\n\r\n> **Warning:**\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\r\n> provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n\r\nFor more details, please refer to the [usage documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/usage) and the [parameter documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/parameters).\r\n\r\n## Pipeline output\r\n\r\nTo see the the results of a test run with a full size dataset refer to the [results](https://pipelines.tol.sanger.ac.uk/curationpretext/results) tab on the sanger-tol/curationpretext website pipeline page.\r\nFor more details about the output files and reports, please refer to the\r\n[output documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/output).\r\n\r\n## Credits\r\n\r\nsanger-tol/curationpretext was originally written by Damon-Lee B Pointon (@DLBPointon).\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n- @muffato - For reviews.\r\n\r\n- @yumisims - TreeVal and Software.\r\n\r\n- @weaglesBio - TreeVal and Software.\r\n\r\n- @josieparis - Help with better docs and testing.\r\n\r\n- @mahesh-panchal - Large support with 1.2.0 in making the pipeline more robust with other HPC environments.\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#curationpretext` channel](https://nfcore.slack.com/channels/curationpretext) (you can join with [this invite](https://nf-co.re/join/slack)).\r\n\r\n## Citations\r\n\r\nIf you use sanger-tol/curationpretext for your analysis, please cite it using the following doi: [10.5281/zenodo.12773958](https://doi.org/10.5281/zenodo.12773958)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [
            "Genome visualisation"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Mapping",
            "Workflows"
        ],
        "filtered_on": "profil* in description",
        "id": "1320",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1320?version=1",
        "name": "sanger-tol/curationpretext",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Assembly",
            "Tree of Life Genome Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-12",
        "versions": 1
    },
    {
        "create_time": "2025-02-25",
        "creators": [
            "Katerina Hanakova",
            "David Potesil"
        ],
        "description": "KNIME workflow describing the analysis of mass spectrometry dataset related to the publication \"Anti-Cancer potential of a new Derivative of Caffeic Acid Phenethyl Ester targeting the centrosome\". Workflow was built using the [KNIME software container environment](https://github.com/OmicsWorkflows/KNIME_docker_vnc/tree/version_4.1.3a), version 4.1.3a, which can be created using \"docker pull cfprot/knime:4.1.3a\" command in Docker. Please consult Github pages for more information on how to use the container.\r\n\r\nBriefly, the KNIME workflow contains the contaminants removal, log2 intensities transformation, data filtering, normalization and statistical evaluation using the limma test.\r\n\r\nThe input data for the KNIME workflow (the report.tsv from DIA-NN) as well as raw LC-MS data can be found on PRIDE repository under the identifier PXD061079. Processed data and figures from the data quality control are located in the \\_\\_outputs\\_\\_ folder, sobfolder tables and figures, respectively.",
        "doi": "10.48546/workflowhub.workflow.1309.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1309",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1309?version=1",
        "name": "Anti-Cancer potential of a new Derivative of Caffeic Acid Phenethyl Ester targeting the centrosome - DIA-NN data KNIME processing workflow",
        "number_of_steps": 0,
        "projects": [
            "Proteomics CEITEC"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "KNIME",
        "update_time": "2025-03-06",
        "versions": 1
    },
    {
        "create_time": "2025-03-03",
        "creators": [],
        "description": "![Lin_X_NFDI4BIOIMAGE](/imgs/lin_x_nfdi4bioimage.png)\r\n# FAIR Statistics Aggregator for DOIs\r\n\r\n## Table of Contents\r\n1. [Introduction](#introduction)\r\n2. [Features](#features)\r\n3. [Requirements](#requirements)\r\n4. [Installation](#installation)\r\n5. [Usage](#usage)\r\n6. [Output](#output)\r\n7. [Limitations](#limitations)\r\n8. [License](#license)\r\n\r\n## Introduction\r\nThis repository hosts a prototype tool designed to analyze and aggregate FAIR (Findable, Accessible, Interoperable, and Reusable) statistics for a list of Digital Object Identifiers (DOIs). The tool currently utilizes the F-UJI FAIR checker to evaluate the FAIRness of the metadata associated with each DOI. Future versions aim to incorporate additional FAIR checkers to provide a more comprehensive analysis.\r\n\r\nThe tool processes a list of DOIs, which can be sourced from a website or fetched using a metasearch API like Crossref or DataCite. It calculates FAIR statistics for each DOI, aggregates these statistics by publication year, and identifies common metadata errors that impact FAIRness. The results are presented in an aggregated FAIR-statistic per publication year diagram and a summary of the most frequent metadata issues.\r\n\r\nThis tool also serves as a justification for metadata providers (e.g., Springer, Nature) to ensure their metadata is hosted in a machine-readable format, as this is crucial for optimal FAIRness evaluation.\r\n\r\n**Warning**: The F-UJI FAIR checker must be initialized beforehand using a Docker container. Instructions for setting up the F-UJI checker can be found [here](https://github.com/FAIR-IMPACT/fuji). Please note that F-UJI and other FAIR checkers are in a very early beta status.\r\n\r\n## Features\r\n- **DOI List Processing**: Accepts a list of DOIs from a file or fetched via APIs like Crossref or DataCite.\r\n- **FAIR Evaluation**: Uses the F-UJI FAIR checker to evaluate the FAIRness of each DOI's metadata.\r\n- **Aggregation**: Aggregates FAIR statistics by publication year.\r\n- **Error Summary**: Identifies and summarizes the most common metadata errors affecting FAIRness.\r\n- **Visualization**: Generates an aggregated FAIR-statistic per publication year diagram.\r\n\r\n## Requirements\r\n- Python 3.x\r\n- Docker (for running the F-UJI FAIR checker)\r\n- Required Python packages (listed in `requirements.txt`)\r\n\r\n## Installation\r\n1. Clone the repository:\r\n   ```bash\r\n   git clone https://github.com/saibotmagd/fair_stats_aggregator.git\r\n   cd fair_stats_aggregator\r\n   ```\r\n2. Install the required Python packages:\r\n   ```bash\r\n   pip install -r requirements.txt\r\n   ```\r\n3. Set up the F-UJI FAIR checker (https://github.com/FAIR-IMPACT/fuji) using Docker:\r\n   ```bash\r\n   docker pull fairimpact/fuji\r\n   docker run -d -p 1071:1071 fairimpact/fuji\r\n   ```\r\n\r\n## Usage\r\n1. Prepare a list of DOIs in a text file (one DOI per line) or use an API to fetch DOIs.\r\n2. Run the tool:\r\n   ```bash\r\n   python fair_stats_agg.py --doi-file path/to/doi_list.txt\r\n   ```\r\n   There's an \"example_DOI_list.txt\" including the publications of the Leibniz Institute for Neurobiology Magdeburg.\r\n3. The tool will output the aggregated FAIR statistics and a summary of metadata errors. \r\n\r\n## Output\r\n- **Aggregated FAIR-statistic per Publication Year Diagram**: A visual representation of FAIR statistics aggregated by publication year.\r\n- **Metadata Error Summary**: A list of the most common metadata errors affecting FAIRness.\r\n- **Justification for Metadata Providers**: A summary highlighting the importance of machine-readable metadata for optimal FAIRness evaluation.\r\n\r\n## Limitations\r\n- **Beta Status**: The F-UJI FAIR checker and other FAIR checkers are in a very early beta status. Results may vary and should be interpreted with caution.\r\n- **Dependency on Docker**: The F-UJI FAIR checker requires Docker to be initialized beforehand.\r\n\r\n## License\r\n\r\n# CC BY-NC License\r\n\r\nThis project is licensed under the **Creative Commons Attribution-NonCommercial 4.0 International License** (CC BY-NC 4.0).\r\n\r\n## You are free to:\r\n\r\n- **Share** \u2014 Copy and redistribute the material in any medium or format.\r\n- **Adapt** \u2014 Remix, transform, and build upon the material.\r\n\r\nThe licensor cannot revoke these freedoms as long as you follow the license terms.\r\n\r\n## Under the following terms:\r\n\r\n- **Attribution** \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\r\n- **NonCommercial** \u2014 You may not use the material for commercial purposes.\r\n\r\n## Notices:\r\n\r\n- You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\r\n- No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.\r\n\r\nFor more details, please refer to the full license text: [CC BY-NC 4.0 License](https://creativecommons.org/licenses/by-nc/4.0/).\r\n\r\n[Back to Top](#fair-statistics-aggregator-for-dois)\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1316",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1316?version=1",
        "name": "FAIR Statistics Aggregator for DOIs",
        "number_of_steps": 0,
        "projects": [
            "BioImage Informatics and Analysis Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2025-03-03",
        "versions": 1
    },
    {
        "create_time": "2025-03-03",
        "creators": [],
        "description": "![Lin_X_NFDI4BIOIMAGE](/docs/imgs/lin_x_nfdi4bioimage.png)\r\n# RDM_system_connector\r\n# **WARNING** \r\nThis is a proof of concept, it has not been decided whether it will be developed into a fully functional tool. \r\nFeedback is therefore essential, especially as it is unclear whether this type of tool is useful at all, and if so, which parts, as the concept consists of many different parts.\r\n(source code readme: \r\n- [installation guide and short description](/docs/src_README.md)\r\n- [sphinx code documentation](/docs/_build/html/index.html)\r\n\r\n---\r\n\r\n# Table of Contents:\r\n- [RDM\\_system\\_connector](#rdm_system_connector)\r\n- [**WARNING**](#warning)\r\n- [Table of Contents:](#table-of-contents)\r\n- [RDM system connector](#rdm-system-connector)\r\n\t\t\t- [overview graph](#overview-graph)\r\n\t- [Internal project study registration](#internal-project-study-registration)\r\n\t- [ELN (e.g. RSpace, elabFTW) \\& inventory](#eln-eg-rspace-elabftw--inventory)\r\n\t- [Omero Image \\& metadata hub](#omero-image--metadata-hub)\r\n\t- [long-term archive storage](#long-term-archive-storage)\r\n\t- [matching](#matching)\r\n\t- [(fuzzy similarity matching, direct matching, manual\\_linking)](#fuzzy-similarity-matching-direct-matching-manual_linking)\r\n- [long-term vision](#long-term-vision)\r\n\t- [of a connected RDM \\_structure](#of-a-connected-rdm-_structure)\r\n# RDM system connector\r\n\r\n- The purpose of this tool will be to connect different platforms that have been or will be used as part of research data management. \r\n- Every part of the system is replaceable as the connection is the central point of the tool. \r\n- the benefits in day-to-day research result from the cooperation of different stakeholders who work together on a project and do not necessarily have access to the same systems or do not use them in their work process despite having access\r\n- making essential information usable in all connected systems makes it possible to have it available more quickly and clearly\r\n- in the best case scenario, stakeholders receive information that they were previously unable to obtain\r\n[see a real practical example](practical%20example%20lin.md)\r\n\r\n#### overview graph\r\n\r\n```mermaid\r\ngraph TD\r\n    A[project registration] --> B[ELN e.g. RSpace]\r\n    B --> C[Omero hub]\r\n    C --> D[Long-term archive storage]\r\n    A -- matching e.g. fuzzy_similarity_matching --> D\r\n```\r\n\r\n## Internal project study registration\r\n- the main point of this part is that every scientific project has a study registration somewhere\r\n- the registration can be a proposal (e.g. a pdf/text file to apply for a funding programme or a thesis)\r\n- we use a separate platform (egroupware) where people can register their study and book time slots for specific instruments (e.g. MR, EEG, microscopes, computer servers)\r\n## ELN (e.g. RSpace, elabFTW) & inventory\r\n- a platform where protocols of preparation procedures or plans for procedures can be written\r\n- there should be basic protocols and subject-specific ones (e.g. keeping track of daily events)\r\n- be used to plan and structure the interaction between people working on different parts of a project (e.g. principal investigators set the protocol and delegate work; technical assistants prepare the tissue; doctoral candidates take the images). \r\n\r\n## Omero Image & metadata hub\r\n- use inplace import to link the images from [long-term_archive_storage](#long-term archive storage) to Omero\r\n- use key-value pairs to display the metadata\r\n- create tags from [(semi-)automatic tag creation](/docs/(semi-)%20automatic%20tag%20creation.md) including tag descriptions from [(semi-)_automatic_description&_ontology_linking_creation](/docs/(semi-)%20automatic%20description%20&%20ontology%20linking%20creation.md)\r\n## long-term archive storage\r\n- crawl a mounted drive to find images, metadata files, projects, studies and add them to [ELN_(e.g._RSpace,_elabFTW)_+_inventory](#ELN%20(e.g.%20RSpace,%20elabFTW)%20&%20inventory) and [Omero_Image_+_metadata_hub](#Omero%20Image%20&%20metadata%20hub)\r\n- use file names, folder names, metadata for [(semi-)_automatic_tag_creation](/docs/(semi-)%20automatic%20tag%20creation.md) and [(semi-)_automatic_description_&_ontology_linking_creation](/docs/(semi-)%20automatic%20description%20&%20ontology%20linking%20creation.md)\r\n## matching \r\n## (fuzzy similarity matching, direct matching, manual_linking)\r\n- **fuzzy** = Calculate the overlap of project names (from [internal_project_study_registration](#internal%20project%20study%20registration) and folder names (from [long-term_archive_storage](#long-term%20archive%20storage)); \r\n\t- where a percentage of overlap of consecutive letters is specified; if the shortest name (either projectname or foldername) is completely contained in the other (~one is the substring of the other), by convention the overlap is set to 100% \r\n- TODO: **direct matching** = define a file (TODO: metadata entry) Define a file or a metadata entry from a file as the project name, which must be identical to that of the study application, character for character; i.e. a 100% match is assumed\r\n\t- e.g. our project leaders have to sign an application letter which is included in [internal_project_study_registration](#internal%20project%20study%20registration) and in [long-term_archive_storage](#long-term%20archive%20storage) for every new project or study. \r\n\t- as both files are identical, the project duration, project manager and project name can be read from them\r\n- TODO: **manual linking** = the linking table in the database could be filled manually to force a specific project/study to be linked to another;  \r\n\t- however, a browser interface is planned to display the automatically generated matches, validate them by eye and create your own\r\n\r\n\r\n# long-term vision\r\n## of a connected RDM _structure\r\n![vision](/docs/imgs/longterm_vision_rdm.png)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1315",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1315?version=1",
        "name": "RDM_system_connector",
        "number_of_steps": 0,
        "projects": [
            "BioImage Informatics and Analysis Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics tool",
            "labbook"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2025-03-03",
        "versions": 1
    },
    {
        "create_time": "2025-02-25",
        "creators": [
            "Playbook Partnership NIH CFDE"
        ],
        "description": "The workflow starts with selecting KLF4 as the search term. Gene sets with set labels containing KLF4 were queried from Enrichr[1]. Identified matching terms from the ENCODE TF ChIP-seq 2015[2] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for ENCODE_TF_ChIP-seq_2015. Identified matching terms from the ChEA 2022[4] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for ChEA_2022. Identified matching terms from the ARCHS4 TF Co-Expression[5] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for ARCHS4_TFs_Coexp. Multiple GMTs were combined into one GMT. A consensus gene set was created by only retaining genes that appear in at least two sets. The gene set was submitted to Enrichr[1]. The gene set was enriched against the GTEx Tissues V8 2023[6] library to identify statistically significant GTEx Tissue Signatures. \r\n1. Xie, Z. et al. Gene Set Knowledge Discovery with Enrichr. Current Protocols vol. 1 (2021). doi:10.1002/cpz1.90\r\n2. An integrated encyclopedia of DNA elements in the human genome. Nature vol. 489 57\u201374 (2012). doi:10.1038/nature11247\r\n4. Keenan, A. B. et al. ChEA3: transcription factor enrichment analysis by orthogonal omics integration. Nucleic Acids Research vol. 47 W212\u2013W224 (2019). doi:10.1093/nar/gkz446\r\n5. Lachmann, A. et al. Massive mining of publicly available RNA-seq data from human and mouse. Nature Communications vol. 9 (2018). doi:10.1038/s41467-018-03751-6\r\n6. Lonsdale, J. et al. The Genotype-Tissue Expression (GTEx) project. Nature Genetics vol. 45 580\u2013585 (2013). doi:10.1038/ng.2653",
        "doi": "10.48546/workflowhub.workflow.1239.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in name",
        "id": "1239",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1239?version=2",
        "name": "Use Case 4: Identify the Tissue Activity for a TF based on its Targets",
        "number_of_steps": 12,
        "projects": [
            "NIH CFDE Playbook Workflow Partnership"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Find Gene Terms in Enrichr Libraries",
            "Extract Terms from the ARCHS4 TF Co-Expression Library",
            "Join several GMTs into one",
            "Extract Terms from the ChEA 2022 Library",
            "Extract Terms from the ENCODE TF ChIP-seq 2015 Library",
            "Start with a Gene",
            "Load Enrichr set as GMT",
            "Find genes which appear in more than one set",
            "Extract Significant Terms from the GTEx Tissues V8 2023 Library",
            "Perform Enrichment Analysis"
        ],
        "type": "Unrecognized workflow type",
        "update_time": "2025-02-25",
        "versions": 2
    },
    {
        "create_time": "2025-02-25",
        "creators": [
            "Playbook Partnership NIH CFDE"
        ],
        "description": "The workflow starts with selecting Autophagy as the search term. Gene sets with set labels containing Autophagy were queried from Enrichr[1]. Identified matching terms from the MGI Mammalian Phenotype Level 4 2019[2] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for MGI_Mammalian_Phenotype_Level_4_2019. All the identified gene sets were combined using the union set operation. Reversers and mimickers from over 1 million signatures were identified using SigCom LINCS[4]. Resolved drugs from the LINCS L1000 Chemical Perturbagens library. Identified matching terms from the KEGG 2021 Human[6] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for KEGG_2021_Human. All the identified gene sets were combined using the union set operation. Reversers and mimickers from over 1 million signatures were identified using SigCom LINCS[4]. Identified matching terms from the GO Biological Process 2021[7] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for GO_Biological_Process_2021. All the identified gene sets were combined using the union set operation. Reversers and mimickers from over 1 million signatures were identified using SigCom LINCS[4]. Resolved drugs from the LINCS L1000 Chemical Perturbagens library. Resolved drugs from the LINCS L1000 Chemical Perturbagens library. The mean across multiple Scored Drugs is computed. The drugs were filtered by FDA Approved Drugs with the help of PubChem APIs[8]. \r\n1. Xie, Z. et al. Gene Set Knowledge Discovery with Enrichr. Current Protocols vol. 1 (2021). doi:10.1002/cpz1.90\r\n2. Blake, J. A. et al. Mouse Genome Database (MGD): Knowledgebase for mouse\u2013human comparative biology. Nucleic Acids Research vol. 49 D981\u2013D987 (2020). doi:10.1093/nar/gkaa1083\r\n4. Evangelista, J. E. et al. SigCom LINCS: data and metadata search engine for a million gene expression signatures. Nucleic Acids Research vol. 50 W697\u2013W709 (2022). doi:10.1093/nar/gkac328\r\n6. Kanehisa, M., Furumichi, M., Sato, Y., Kawashima, M. & Ishiguro-Watanabe, M. KEGG for taxonomy-based analysis of pathways and genomes. Nucleic Acids Research vol. 51 D587\u2013D592 (2022). doi:10.1093/nar/gkac963\r\n7. Ashburner, M. et al. Gene Ontology: tool for the unification of biology. Nature Genetics vol. 25 25\u201329 (2000). doi:10.1038/75556\r\n8. Kim, S. et al. PubChem 2023 update. Nucleic Acids Research vol. 51 D1373\u2013D1380 (2022). doi:10.1093/nar/gkac956",
        "doi": "10.48546/workflowhub.workflow.1240.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1240",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1240?version=2",
        "name": "Use Case 5: Small Molecules to Induce a Biological Process",
        "number_of_steps": 19,
        "projects": [
            "NIH CFDE Playbook Workflow Partnership"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Take the mean value across multiple scores",
            "Filter the drugs with PubChem APIs",
            "Extract Terms from the MGI Mammalian Phenotype Level 4 2019 Library",
            "Extract Terms from the GO Biological Process 2021 Library",
            "Extract Terms from the KEGG 2021 Human Library",
            "Extract signatures from the results",
            "Find the union set of all genes in the GMT",
            "Query LINCS L1000 Signatures",
            "Load Enrichr set as GMT",
            "Start with a Pathway or Biological Process",
            "Find Pathway or Biological Process Terms in Enrichr Libraries"
        ],
        "type": "Unrecognized workflow type",
        "update_time": "2025-02-25",
        "versions": 2
    },
    {
        "create_time": "2025-02-25",
        "creators": [
            "Playbook Partnership NIH CFDE"
        ],
        "description": "The workflow starts with selecting chr10:g.3823823G>A as the search term. The closest gene to the variant was found using MyVariant.info[1]. RNA-seq-like LINCS L1000 Signatures[3] which mimick or reverse the the expression of KLF6 were visualized. Median expression of KLF6 was obtained from the GTEx Portal[8] using the portal's API. To visualize the scored tissues, a vertical bar plot was created Fig.. \r\n1. Lelong, S. et al. BioThings SDK: a toolkit for building high-performance data APIs in biomedical research. Bioinformatics vol. 38 2077\u20132079 (2022). doi:10.1093/bioinformatics/btac017\r\n3. Evangelista, J. E. et al. SigCom LINCS: data and metadata search engine for a million gene expression signatures. Nucleic Acids Research vol. 50 W697\u2013W709 (2022). doi:10.1093/nar/gkac328\r\n8. Lonsdale, J. et al. The Genotype-Tissue Expression (GTEx) project. Nature Genetics vol. 45 580\u2013585 (2013). doi:10.1038/ng.2653",
        "doi": "10.48546/workflowhub.workflow.1241.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1241",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1241?version=2",
        "name": "Use Case 6: CFDE Knowledge about a Variant",
        "number_of_steps": 5,
        "projects": [
            "NIH CFDE Playbook Workflow Partnership"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Use GTEx API to obtain median tissue expression for the given gene",
            "Start with a Variant",
            "Construct a vertical bar plot with Scored Tissues",
            "Identify RNA-seq-like LINCS L1000 Signatures which reverse the expression of the gene.",
            "Identify the closest gene to this variant"
        ],
        "type": "Unrecognized workflow type",
        "update_time": "2025-02-25",
        "versions": 2
    },
    {
        "create_time": "2025-02-25",
        "creators": [
            "Daniel Lundin"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-phyloplace_logo_dark.png\">\n    <img alt=\"nf-core/phyloplace\" src=\"docs/images/nf-core-phyloplace_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/phyloplace/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/phyloplace/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/phyloplace/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/phyloplace/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/phyloplace/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/phyloplace)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23phyloplace-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/phyloplace)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/phyloplace** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/phyloplace \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/phyloplace/usage) and the [parameter documentation](https://nf-co.re/phyloplace/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/phyloplace/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/phyloplace/output).\n\n## Credits\n\nnf-core/phyloplace was originally written by Daniel Lundin.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#phyloplace` channel](https://nfcore.slack.com/channels/phyloplace) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/phyloplace for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1009",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1009?version=2",
        "name": "nf-core/phyloplace",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "evolution",
            "evolutionary-tree",
            "phylogenetic-placement",
            "phylogenetics",
            "sequence-classification",
            "taxonomy-assignment"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-02-25",
        "versions": 2
    },
    {
        "create_time": "2025-02-05",
        "creators": [
            "Diego De Panis"
        ],
        "description": "The workflow requires the user to provide:\r\n* ENSEMBL link address of the annotation GFF3 file\r\n* ENSEMBL link address of the assembly FASTA file\r\n* NCBI taxonomy ID\r\n* BUSCO lineage\r\n* OMArk database\r\n\r\nThw workflow will produce statistics of the annotation based on AGAT, BUSCO and OMArk.",
        "doi": null,
        "edam_operation": [
            "Genome annotation"
        ],
        "edam_topic": [
            "Genomics"
        ],
        "filtered_on": "annot* in tags",
        "id": "1096",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1096?version=2",
        "name": "ERGA-BGE Genome Report ANNOT analyses",
        "number_of_steps": 13,
        "projects": [
            "ERGA Annotation"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "biodiversity",
            "erga",
            "genomics",
            "qc"
        ],
        "tools": [
            "tp_text_file_with_recurring_lines",
            "agat",
            "collapse_dataset",
            "pick_value",
            "omark",
            "lftp",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-02-24",
        "versions": 2
    },
    {
        "create_time": "2025-02-12",
        "creators": [
            "Albert Puiggros"
        ],
        "description": "This workflow demonstrates the integration of FAIR principles into the workflow management ecosystem through provenance integration in Autosubmit, a workflow manager developed at the Barcelona Supercomputing Center (BSC), and SUNSET (SUbseasoNal to decadal climate forecast post-processing and aSSEssmenT suite), an R-based verification workflow also developed at BSC.\r\n\r\nAutosubmit supports the generation of data provenance information based on RO-Crate, facilitating the creation of machine-actionable digital objects that encapsulate detailed metadata about its executions. However, the provenance metadata provided by Autosubmit focuses on the workflow process and does not encapsulate the details of the data transformation processes. This is where SUNSET plays a complementary role. SUNSET\u2019s approach to provenance information is based on the METACLIP (METAdata for CLImate Products) ontologies. METACLIP offers a semantic approach to describing climate products and their provenance. This framework enables SUNSET to provide specific, high-resolution provenance metadata for its operations, improving transparency and compliance with FAIR principles. The generated files provide detailed information about each transformation the data has undergone, as well as additional details about the data's state, location, structure, and associated source code, all represented in a tree-like structure.\r\n\r\nThe workflow uses a SUNSET configuration file, referred to as a \"recipe,\" to generate a set of JSON files containing the provenance information of the workflow execution based on the METACLIP ontologies. For this, we compute some skill metrics and scorecard plots with SUNSET, using Autosubmit to dispatch jobs in parallel. In the recipe, we request three start dates for January, February, and March (0101, 0201, 0301). SUNSET will split the recipe into three atomic recipes, and Autosubmit will run three jobs, processing the verification for each recipe in parallel. When all the scorecards are generated, the \"transfer_provenance\" job will be triggered, transferring the SUNSET-generated provenance files to the Autosubmit experiment folder. Finally, an RO-Crate object will be created, encapsulating the entire process description.\r\n\r\nCurrently, this workflow can only be executed within the BSC infrastructure. Here is the complete use case: [Use Case Documentation](https://earth.bsc.es/gitlab/es/sunset/-/blob/Dev-Provenance/use_cases/ex1_4_provenance_autosubmit/ex1_4-handson.md)\r\n\r\nThe METACLIP-based JSON files can be interactively visualized using the [METACLIP Interpreter.](http://metaclip.org)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1304",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1304?version=1",
        "name": "Provenance generation when running SUNSET with Autosubmit",
        "number_of_steps": 0,
        "projects": [
            "BSC-CES"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Autosubmit",
        "update_time": "2025-02-12",
        "versions": 1
    },
    {
        "create_time": "2025-02-10",
        "creators": [
            "Simon Heumos",
            "Michael L Heuer"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-pangenome_logo_dark.png\">\n    <img alt=\"nf-core/pangenome\" src=\"docs/images/nf-core-pangenome_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/pangenome/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/pangenome/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/pangenome/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/pangenome/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/pangenome/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.8202636-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.8202636)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/pangenome)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23pangenome-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/pangenome)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/pangenome** is a bioinformatics best-practice analysis pipeline for pangenome graph construction. The pipeline renders a collection of sequences into a pangenome graph. Its goal is to build a graph that is locally directed and acyclic while preserving large-scale variation. Maintaining local linearity is important for interpretation, visualization, mapping, comparative genomics, and reuse of pangenome graphs.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/pangenome/results).\n\n<p align=\"center\">\n    <img title=\"Pangenome Workflow\" src=\"docs/images/pangenome_workflow.png\" width=100%>\n</p>\n\n## Pipeline summary\n\n- All versus all alignment (`WFMASH`)\n- Graph induction (`SEQWISH`)\n- Graph normalization (`SMOOTHXG`)\n- Remove redundancy (`GFAFFIX`)\n- Graph statistics and qualitative visualizations (`ODGI`)\n- Combine diagnostic information into a report (`MULTIQC`)\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/pangenome -r dev --input <BGZIPPED_FASTA> --n_haplotypes <NUM_HAPS_IN_FASTA> --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/pangenome/usage) and the [parameter documentation](https://nf-co.re/pangenome/parameters).\n\n## Advantages over [`PGGB`](https://github.com/pangenome/pggb)\n\nThis Nextflow pipeline version's major advantage is that it can distribute the usually computationally heavy all versus all alignment step across a whole cluster. It is capable of splitting the initial approximate alignments into problems of equal size. The base-level alignments are then distributed across several processes. Assuming you have a cluster with 10 nodes and you are the only one using it, we would recommend to set `--wfmash_chunks 10`.\nIf you have a cluster with 20 nodes, but you have to share it with others, maybe setting it to `--wfmash_chunks 10` could be a good fit, because then you don't have to wait too long for your jobs to finish.\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/pangenome/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/pangenome/output).\n\n## Credits\n\nnf-core/pangenome was originally adapted from [PGGB](https://github.com/pangenome/pggb) by [Simon Heumos](https://github.com/subwaystation), [Michael Heuer](https://github.com/heuermh).\n\n> [Simon Heumos](https://github.com/subwaystation) is currently the sole developer.\n\nMany thanks to all who have helped out and contributed along the way, including (but not limited to)\\*:\n\n| Name                                                       | Affiliation                                                                                                                                                                                                                                                                                                                                                                       |\n| ---------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [Philipp Ehmele](https://github.com/imipenem)              | [Institute of Computational Biology, Helmholtz Zentrum M\u00fcnchen, Munich, Germany](https://www.helmholtz-muenchen.de/icb/index.html)                                                                                                                                                                                                                                                |\n| [Gisela Gabernet](https://github.com/ggabernet)            | [Quantitative Biology Center (QBiC) T\u00fcbingen, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/research/research-infrastructure/quantitative-biology-center-qbic/) <br> [Department of Pathology, Yale School of Medicine, New Haven, USA](https://medicine.yale.edu/pathology/)                                                                                      |\n| [Erik Garrison](https://github.com/ekg)                    | [University of Tennessee Health Science Center, Memphis, Tennessee, TN, USA](https://uthsc.edu/)                                                                                                                                                                                                                                                                                  |\n| [Andrea Guarracino](https://github.com/AndreaGuarracino)   | [University of Tennessee Health Science Center, Memphis, Tennessee, TN, USA](https://uthsc.edu/)                                                                                                                                                                                                                                                                                  |\n| [Friederike Hanssen](https://github.com/FriederikeHanssen) | [Seqera](https://seqera/io)                                                                                                                                                                                                                                                                                                                                                       |\n| [Peter Heringer](https://github.com/heringerp)             | [Quantitative Biology Center (QBiC) T\u00fcbingen, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/research/research-infrastructure/quantitative-biology-center-qbic/) <br> [Biomedical Data Science, Department of Computer Science, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/department/) |\n| [Michael Heuer](https://github.com/heuermh)                | [Mammoth Biosciences, Inc., San Francisco, CA, USA](https://mammoth.bio)                                                                                                                                                                                                                                                                                                          |\n| [Lukas Heumos](https://github.com/zethson)                 | [Institute of Computational Biology, Helmholtz Zentrum M\u00fcnchen, Munich, Germany](https://www.helmholtz-muenchen.de/icb/index.html) <br> [Institute of Lung Biology and Disease and Comprehensive Pneumology Center, Helmholtz Zentrum M\u00fcnchen, Munich, Germany](https://www.helmholtz-muenchen.de/ilbd/the-institute/cpc/index.html)                                              |\n| [Simon Heumos](https://github.com/subwaystation)           | [Quantitative Biology Center (QBiC) T\u00fcbingen, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/research/research-infrastructure/quantitative-biology-center-qbic/) <br> [Biomedical Data Science, Department of Computer Science, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/department/) |\n| [Susanne Jodoin](https://github.com/SusiJo)                | [Quantitative Biology Center (QBiC) T\u00fcbingen, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/research/research-infrastructure/quantitative-biology-center-qbic/)                                                                                                                                                                                                    |\n| [J\u00falia Mir Pedrol](https://github.com/mirpedrol)           | [Quantitative Biology Center (QBiC) T\u00fcbingen, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/research/research-infrastructure/quantitative-biology-center-qbic/)                                                                                                                                                                                                    |\n\n> \\* Listed in alphabetical order\n\n## Acknowledgments\n\n- [QBiC](https://www.qbic.uni-tuebingen.de)\n- [deNBI](https://www.denbi.de/)\n- [Human Pangenome Reference Consortium](https://humanpangenome.org)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#pangenome` channel](https://nfcore.slack.com/channels/pangenome) (you can join with [this invite](https://nf-co.re/join/slack)), or contact me [Simon Heumos](mailto:simon.heumos@qbic.uni-tuebingen.de?subject=[GitHub]%20nf-core/pangenome).\n\n## Citations\n\nIf you use nf-core/pangenome for your analysis, please cite it using the following doi: [10.5281/zenodo.8202636](https://doi.org/10.5281/zenodo.8202636)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\n## Changelog\n\n[CHANGELOG](CHANGELOG.md)\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1007",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1007?version=5",
        "name": "nf-core/pangenome",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "pangenome"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-14",
        "versions": 5
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi"
        ],
        "description": "This workflow creates taxonomic summary tables for a specified taxonomic rank out of MAPseq's OTU tables output collection.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "OTU in description",
        "id": "1296",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1296?version=2",
        "name": "taxonomic-rank-abundance-summary-table/main",
        "number_of_steps": 8,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "tp_awk_tool",
            "filter_tabular",
            "collection_column_join",
            "Grouping1",
            "map_param_value"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-02-04",
        "creators": [
            "Jonathan Manning"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-riboseq_logo_dark.png\">\n    <img alt=\"nf-core/riboseq\" src=\"docs/images/nf-core-riboseq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/riboseq/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/riboseq/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/riboseq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/riboseq/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/riboseq/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/riboseq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23riboseq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/riboseq)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/riboseq** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/riboseq \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/riboseq/usage) and the [parameter documentation](https://nf-co.re/riboseq/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/riboseq/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/riboseq/output).\n\n## Credits\n\nnf-core/riboseq was originally written by Jonathan Manning.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#riboseq` channel](https://nfcore.slack.com/channels/riboseq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/riboseq for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1016",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1016?version=3",
        "name": "nf-core/riboseq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-14",
        "versions": 3
    },
    {
        "create_time": "2025-02-04",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-fastqrepair_logo_dark.png\">\n    <img alt=\"nf-core/fastqrepair\" src=\"docs/images/nf-core-fastqrepair_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/fastqrepair/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/fastqrepair/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/fastqrepair/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/fastqrepair/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/fastqrepair/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/fastqrepair)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23fastqrepair-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/fastqrepair)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/fastqrepair** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/fastqrepair \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/fastqrepair/usage) and the [parameter documentation](https://nf-co.re/fastqrepair/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/fastqrepair/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/fastqrepair/output).\n\n## Credits\n\nnf-core/fastqrepair was originally written by Tommaso Mazza.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#fastqrepair` channel](https://nfcore.slack.com/channels/fastqrepair) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/fastqrepair for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1276",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1276?version=1",
        "name": "nf-core/fastqrepair",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "fastq",
            "corruption",
            "data-cleaning",
            "data-recovery",
            "fastq-corrupted",
            "fastq-format",
            "reads-interleaving",
            "recovery-tool",
            "unpaired-reads",
            "well-formed"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-14",
        "versions": 1
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi",
            "Mara Besemer"
        ],
        "description": "The MAPseq to Ampvis workflow processes MAPseq OTU tables and associated metadata for analysis in Ampvis2. This workflow involves reformatting MAPseq output datasets to produce structured output files suitable for Ampvis2.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1275",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1275?version=2",
        "name": "mapseq-to-ampvis2/main",
        "number_of_steps": 9,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "tp_awk_tool",
            "query_tabular",
            "ampvis2_load",
            "collection_column_join",
            "collapse_dataset"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi",
            "Paul Zierep"
        ],
        "description": "MGnify's amplicon pipeline v5.0. Including the Quality control for single-end and paired-end reads, rRNA-prediction, and ITS sub-WFs.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in name",
        "id": "1274",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1274?version=2",
        "name": "mgnify-amplicon-pipeline-v5-complete/main",
        "number_of_steps": 20,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "",
            "tp_awk_tool",
            "CONVERTER_gz_to_uncompressed",
            "CONVERTER_uncompressed_to_gz",
            "__MERGE_COLLECTION__",
            "fastq_dl"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi",
            "Paul Zierep"
        ],
        "description": "Classification and visualization of ITS regions.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in name",
        "id": "1273",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1273?version=2",
        "name": "mgnify-amplicon-pipeline-v5-its/main",
        "number_of_steps": 30,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "__FILTER_EMPTY_DATASETS__",
            "",
            "tp_awk_tool",
            "biom_convert",
            "bedtools_maskfastabed",
            "collection_element_identifiers",
            "taxonomy_krona_chart",
            "mapseq",
            "__FILTER_FROM_FILE__"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi",
            "Paul Zierep"
        ],
        "description": "Quality control subworkflow for paired-end reads. ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in name",
        "id": "1272",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1272?version=2",
        "name": "mgnify-amplicon-pipeline-v5-quality-control-paired-end/main",
        "number_of_steps": 17,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "__UNZIP_COLLECTION__",
            "fastp",
            "fastq_filter",
            "fastqc",
            "cshl_fasta_formatter",
            "fastq_to_fasta_python",
            "prinseq",
            "tp_find_and_replace",
            "mgnify_seqprep",
            "trimmomatic",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi",
            "Paul Zierep"
        ],
        "description": "Quality control subworkflow for single-end reads.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in name",
        "id": "1271",
        "keep": true,
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1271?version=3",
        "name": "mgnify-amplicon-pipeline-v5-quality-control-single-end/main",
        "number_of_steps": 14,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "fastq_filter",
            "fastqc",
            "cshl_fasta_formatter",
            "fastq_to_fasta_python",
            "prinseq",
            "tp_find_and_replace",
            "trimmomatic",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 3
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi",
            "Paul Zierep"
        ],
        "description": "Classification and visualization of SSU, LSU sequences.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in name",
        "id": "1270",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1270?version=2",
        "name": "mgnify-amplicon-pipeline-v5-rrna-prediction/main",
        "number_of_steps": 47,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "__FILTER_EMPTY_DATASETS__",
            "",
            "bedtools_getfastabed",
            "tp_awk_tool",
            "query_tabular",
            "biom_convert",
            "infernal_cmsearch",
            "cshl_fasta_formatter",
            "gops_concat_1",
            "collection_element_identifiers",
            "taxonomy_krona_chart",
            "cmsearch_deoverlap",
            "mapseq",
            "__FILTER_FROM_FILE__"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi"
        ],
        "description": "This workflow creates taxonomic summary tables out of the amplicon pipeline results. ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in name",
        "id": "1269",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1269?version=2",
        "name": "mgnify-amplicon-taxonomic-summary-tables/main",
        "number_of_steps": 10,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "tp_awk_tool",
            "filter_tabular",
            "query_tabular",
            "collection_column_join",
            "Grouping1"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-01-31",
        "creators": [
            "Riccardo Massei"
        ],
        "description": "This KNIME workflow is designed to facilitate the loading of image data from OMERO.\r\nIt includes key preprocessing steps for VAST data, such as metadata creation and the linking of Key-Value Pairs. \r\n\r\n\r\n* **Fetching Images**: The first step involves fetching images from a locally accessible folder.\r\n* **User Authentication:** Users are prompted to input their OMERO username and password through a Java snippet. This information is then converted into variables that can be used by the Python script node.\r\n* **Image Import:** The Python script node utilizes ezomero to execute the image import process.\r\n* **Import Metadata:** Purple and Green branch allows to upload metada as Key-Value Pairs and Tables\r\n* **Import ROIs:** ROI created with Fish Inspector will be automatically uploaded into OMERO. (ROIs need to be reported in a json file)\r\n* **Temporary Folder Deletion:** After the import process is complete, the temporary folder is deleted.\r\n\r\nA dataset for testing can be found at: https://zenodo.org/records/14790777\r\n\r\n**Important Security Note:** It is crucial to be aware that storing credentials as variables can pose security risks, particularly if accessed by administrators. Therefore, it is essential to handle user credentials securely and in accordance with best practices.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioimaging",
            "Imaging",
            "Workflows"
        ],
        "filtered_on": "metap* in name",
        "id": "1265",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1265?version=1",
        "name": "OMERO: VAST Data Preparation, Metadata Creation, and ROI Upload with FishInspector Annotations",
        "number_of_steps": 0,
        "projects": [
            "UFZ - Image Data Management and Processing Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [
            "knime",
            "omero",
            "rdm",
            "workflows",
            "bioimaging",
            "imaging"
        ],
        "tools": [
            "Konstanz Information Miner (KNIME)"
        ],
        "type": "KNIME",
        "update_time": "2025-03-03",
        "versions": 1
    },
    {
        "create_time": "2025-01-28",
        "creators": [
            "Riccardo Massei"
        ],
        "description": "Workflow to perform nuclei cell counting on High Content Screening (HCS) Data and upload result into OMERO\r\n\r\nIn this workflow, cell images are first uploaded to both Galaxy and OMERO using the  \u201cOMERO Image Import\u201d tool. \r\nConcurrently, image processing is performed. After thresholding and binarization, key features of nuclei, such as area, label number, and perimeter, are computed from the processed images and saved as a CSV file. \r\nThe result file is then attached to each image stored in OMERO using the \u201cOMERO Metadata Import\u201d tool. \r\nThe \u201cLabel Extraction\u201d tool generates ROI coordinates from the binary image, which are subsequently uploaded into OMERO using the \u201cOMERO ROI Import\u201d tool. \r\n\r\nA dataset for testing can be found at: https://zenodo.org/records/14205500\r\n\r\n**Important Security Note:** It is crucial to be aware that storing credentials as variables can pose security risks, particularly if accessed by administrators. Therefore, it is essential to handle user credentials securely and in accordance with best practices.",
        "doi": "10.48546/workflowhub.workflow.1259.1",
        "edam_operation": [],
        "edam_topic": [
            "Bioimaging",
            "Imaging"
        ],
        "filtered_on": "annot* in name",
        "id": "1259",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1259?version=1",
        "name": "OMERO Nuclei Cell Counting, Image Import, Annotation and Region of Interests (ROIs) Workflow",
        "number_of_steps": 20,
        "projects": [
            "UFZ - Image Data Management and Processing Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [
            "fair",
            "fair workflows",
            "omero",
            "rdm",
            "workflows",
            "bioimaging",
            "imaging"
        ],
        "tools": [
            "tp_awk_tool",
            "ip_threshold",
            "ip_convertimage",
            "Convert characters1",
            "omero_roi_import",
            "tp_replace_in_column",
            "omero_import",
            "collection_element_identifiers",
            "ip_filter_standard",
            "tp_find_and_replace",
            "imagej2_analyze_particles_binary",
            "ip_histogram_equalization",
            "ip_2d_feature_extraction",
            "omero_filter",
            "param_value_from_file",
            "ip_binary_to_labelimage",
            "split_file_to_collection",
            "omero_metadata_import"
        ],
        "type": "Galaxy",
        "update_time": "2025-03-03",
        "versions": 1
    },
    {
        "create_time": "2025-01-28",
        "creators": [
            "Riccardo Massei",
            "Matthias Bernt"
        ],
        "description": "General workflow to upload data into OMERO using Galaxy\r\n\r\nA dataset for testing can be found at: https://zenodo.org/records/14205500\r\n\r\n**Important Security Note:** It is crucial to be aware that storing credentials as variables can pose security risks, particularly if accessed by administrators. Therefore, it is essential to handle user credentials securely and in accordance with best practices.\r\n",
        "doi": "10.48546/workflowhub.workflow.1258.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "1258",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1258?version=1",
        "name": "OMERO Image Import, Annotation and Region of Interests (ROIs) Workflow",
        "number_of_steps": 7,
        "projects": [
            "UFZ - Image Data Management and Processing Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [
            "fair",
            "fair workflows",
            "omero",
            "rdm",
            "workflows",
            "bioimaging",
            "imaging"
        ],
        "tools": [
            "tp_replace_in_line",
            "Convert characters1",
            "omero_roi_import",
            "omero_import",
            "param_value_from_file",
            "split_file_to_collection",
            "omero_metadata_import"
        ],
        "type": "Galaxy",
        "update_time": "2025-03-03",
        "versions": 1
    },
    {
        "create_time": "2025-01-28",
        "creators": [
            "Riccardo Massei"
        ],
        "description": "**General workflow to upload data into OMERO using KNIME**\r\n\r\nThe workflow consists of two main branches: the Green Branch, which imports a folder containing images, and the Purple Branch, which enables the annotation of metadata as key-value pairs.\r\n\r\n* **Fetching Images:** The first step involves fetching images from a locally accessible folder.\r\n* **User Authentication:** Users are prompted to input their OMERO username and password through a Java snippet. This information is then converted into variables that can be used by the Python script node.\r\n* **Image Import:** The Python script node utilizes ezomero to execute the image import process.\r\n* **Temporary Folder Deletion:** After the import process is complete, the temporary folder is deleted.\r\n\r\nA dataset for testing can be found at: https://zenodo.org/records/14205500\r\n\r\n**Important Security Note:** It is crucial to be aware that storing credentials as variables can pose security risks, particularly if accessed by administrators. Therefore, it is essential to handle user credentials securely and in accordance with best practices.\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioimaging",
            "Imaging"
        ],
        "filtered_on": "annot* in name",
        "id": "1257",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1257?version=1",
        "name": "OMERO Image Import and Annotation Workflow",
        "number_of_steps": 0,
        "projects": [
            "UFZ - Image Data Management and Processing Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [
            "omero",
            "rdm",
            "workflows",
            "bioimaging",
            "imaging"
        ],
        "tools": [
            "Konstanz Information Miner (KNIME)"
        ],
        "type": "KNIME",
        "update_time": "2025-03-03",
        "versions": 1
    },
    {
        "create_time": "2025-01-28",
        "creators": [
            "Peltzer None",
            "Alexander & Mohr",
            "Christopher None"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-nanostring_logo_dark.png\">\n    <img alt=\"nf-core/nanostring\" src=\"docs/images/nf-core-nanostring_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/nanostring/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/nanostring/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/nanostring/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/nanostring/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/nanostring/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/nanostring)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23nanostring-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/nanostring)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/nanostring** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/nanostring \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/nanostring/usage) and the [parameter documentation](https://nf-co.re/nanostring/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/nanostring/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/nanostring/output).\n\n## Credits\n\nnf-core/nanostring was originally written by Peltzer, Alexander & Mohr, Christopher.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#nanostring` channel](https://nfcore.slack.com/channels/nanostring) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/nanostring for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1003",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1003?version=7",
        "name": "nf-core/nanostring",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "nanostring",
            "nanostringnorm"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-14",
        "versions": 7
    },
    {
        "create_time": "2025-01-21",
        "creators": [
            "Tatiana Gurbich",
            "Martin Beracochea"
        ],
        "description": "[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n\r\n# mettannotator\r\n\r\n<img align=\"right\" width=\"162\" height=\"149\" src=\"media/mettannotator-logo.png\">\r\n\r\n- [ Introduction ](#intro)\r\n- [ Workflow and tools](#wf)\r\n- [ Installation and dependencies ](#install)\r\n  - [Reference databases](#reference-databases)\r\n- [ Usage ](#usage)\r\n- [ Test ](#test)\r\n- [ Outputs ](#out)\r\n- [Preparing annotations for ENA or GenBank submission](#submission)\r\n- [ Mobilome annotation ](#mobilome)\r\n- [ Credits ](#credit)\r\n- [ Contributions and Support ](#contribute)\r\n- [ Citation ](#cite)\r\n\r\n<a name=\"intro\"></a>\r\n\r\n## Introduction\r\n\r\n**mettannotator** is a bioinformatics pipeline that generates an exhaustive annotation of prokaryotic genomes using existing tools. The output is a GFF file that integrates the results of all pipeline components. Results of each individual tool are also provided.\r\n\r\n<a name=\"wf\"></a>\r\n\r\n## Workflow and tools\r\n\r\n<img src=\"media/mettannotator-schema.png\">\r\n<br />\r\n<br />\r\n\r\nThe workflow uses the following tools and databases:\r\n\r\n| Tool/Database                                                                                    | Version                                       | Purpose                                                                                                                |\r\n| ------------------------------------------------------------------------------------------------ | --------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |\r\n| [Prokka](https://github.com/tseemann/prokka)                                                     | 1.14.6                                        | CDS calling and functional annotation (default)                                                                        |\r\n| [Bakta](https://github.com/oschwengers/bakta)                                                    | 1.9.3                                         | CDS calling and functional annotation (if --bakta flag is used)                                                        |\r\n| [Bakta db](https://zenodo.org/record/10522951/)                                                  | 2024-01-19 with AMRFinderPlus DB 2024-01-31.1 | Bakta DB (when Bakta is used as the gene caller)                                                                       |\r\n| [Pseudofinder](https://github.com/filip-husnik/pseudofinder)                                     | v1.1.0                                        | Identification of possible pseudogenes                                                                                 |\r\n| [Swiss-Prot](https://www.uniprot.org/help/downloads)                                             | 2024_06                                       | Database for Pseudofinder                                                                                              |\r\n| [InterProScan](https://www.ebi.ac.uk/interpro/about/interproscan/)                               | 5.62-94.0                                     | Protein annotation (InterPro, Pfam)                                                                                    |\r\n| [eggNOG-mapper](https://github.com/eggnogdb/eggnog-mapper)                                       | 2.1.11                                        | Protein annotation (eggNOG, KEGG, COG, GO-terms)                                                                       |\r\n| [eggNOG DB](http://eggnog6.embl.de/download/)                                                    | 5.0.2                                         | Database for eggNOG-mapper                                                                                             |\r\n| [UniFIRE](https://gitlab.ebi.ac.uk/uniprot-public/unifire)                                       | 2023.4                                        | Protein annotation                                                                                                     |\r\n| [AMRFinderPlus](https://github.com/ncbi/amr)                                                     | 3.12.8                                        | Antimicrobial resistance gene annotation; virulence factors, biocide, heat, acid, and metal resistance gene annotation |\r\n| [AMRFinderPlus DB](https://ftp.ncbi.nlm.nih.gov/pathogen/Antimicrobial_resistance/)              | 3.12 2024-01-31.1                             | Database for AMRFinderPlus                                                                                             |\r\n| [DefenseFinder](https://github.com/mdmparis/defense-finder)                                      | 1.2.0                                         | Annotation of anti-phage systems                                                                                       |\r\n| [DefenseFinder models](https://github.com/mdmparis/defense-finder-models)                        | 1.2.3                                         | Database for DefenseFinder                                                                                             |\r\n| [GECCO](https://github.com/zellerlab/GECCO)                                                      | 0.9.8                                         | Biosynthetic gene cluster annotation                                                                                   |\r\n| [antiSMASH](https://antismash.secondarymetabolites.org/#!/download)                              | 7.1.0                                         | Biosynthetic gene cluster annotation                                                                                   |\r\n| [SanntiS](https://github.com/Finn-Lab/SanntiS)                                                   | 0.9.3.4                                       | Biosynthetic gene cluster annotation                                                                                   |\r\n| [run_dbCAN](https://github.com/linnabrown/run_dbcan)                                             | 4.1.2                                         | PUL prediction                                                                                                         |\r\n| [dbCAN DB](https://bcb.unl.edu/dbCAN2/download/Databases/)                                       | V12                                           | Database for run_dbCAN                                                                                                 |\r\n| [CRISPRCasFinder](https://github.com/dcouvin/CRISPRCasFinder)                                    | 4.3.2                                         | Annotation of CRISPR arrays                                                                                            |\r\n| [cmscan](http://eddylab.org/infernal/)                                                           | 1.1.5                                         | ncRNA predictions                                                                                                      |\r\n| [Rfam](https://rfam.org/)                                                                        | 14.9                                          | Identification of SSU/LSU rRNA and other ncRNAs                                                                        |\r\n| [tRNAscan-SE](https://github.com/UCSC-LoweLab/tRNAscan-SE)                                       | 2.0.9                                         | tRNA predictions                                                                                                       |\r\n| [pyCirclize](https://github.com/moshi4/pyCirclize)                                               | 1.4.0                                         | Visualise the merged GFF file                                                                                          |\r\n| [VIRify](https://github.com/EBI-Metagenomics/emg-viral-pipeline)                                 | 2.0.0                                         | Viral sequence annotation (runs separately)                                                                            |\r\n| [Mobilome annotation pipeline](https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline) | 2.0                                           | Mobilome annotation (runs separately)                                                                                  |\r\n\r\n<a name=\"install\"></a>\r\n\r\n## Installation and dependencies\r\n\r\nThis workflow is built using [Nextflow](https://www.nextflow.io/). It uses containers (Docker or Singularity) making installation simple and results highly reproducible.\r\n\r\n- Install [Nextflow version >=21.10](https://www.nextflow.io/docs/latest/getstarted.html#installation)\r\n- Install [Singularity](https://github.com/apptainer/singularity/blob/master/INSTALL.md)\r\n- Install [Docker](https://docs.docker.com/engine/install/)\r\n\r\nAlthough it's possible to run the pipeline on a personal computer, due to the compute requirements, we encourage users to run it on HPC clusters. Any HPC scheduler supported by [Nextflow](https://www.nextflow.io/) is compatible; however, our team primarily uses [Slurm](https://slurm.schedmd.com/) and [IBM LSF](https://www.ibm.com/docs/en/spectrum-lsf) for the EBI HPC cluster, so those are the profiles we ship with the pipeline.\r\n\r\n<a name=\"reference-databases\"></a>\r\n\r\n### Reference databases\r\n\r\nThe pipeline needs reference databases in order to work, they take roughly 180G.\r\n\r\n| Path                | Size |\r\n| ------------------- | ---- |\r\n| amrfinder           | 217M |\r\n| antismash           | 9.4G |\r\n| bakta               | 71G  |\r\n| dbcan               | 7.5G |\r\n| defense_finder      | 242M |\r\n| eggnog              | 48G  |\r\n| interproscan        | 45G  |\r\n| interpro_entry_list | 2.6M |\r\n| rfam_models         | 637M |\r\n| pseudofinder        | 273M |\r\n| total               | 182G |\r\n\r\n`mettannotator` has an automated mechanism to download the databases using the `--dbs <db_path>` flag. When this flag is provided, the pipeline inspects the folder to verify if the required databases are already present. If any of the databases are missing, the pipeline will automatically download them.\r\n\r\nUsers can also provide individual paths to each reference database and its version if needed. For detailed instructions, please refer to the Reference databases section in the `--help` of the pipeline.\r\n\r\nIt's important to note that users are not allowed to mix the `--dbs` flag with individual database paths and versions; they are mutually exclusive. We recommend users to run the pipeline with the `--dbs` flag for the first time in an appropriate path and to avoid downloading the individual databases separately.\r\n\r\n<a name=\"usage\"></a>\r\n\r\n## Usage\r\n\r\n### Input file\r\n\r\nFirst, prepare an input file in the CSV format that looks as follows:\r\n\r\n`assemblies_sheet.csv`:\r\n\r\n```csv\r\nprefix,assembly,taxid\r\nBU_ATCC8492VPI0062,/path/to/BU_ATCC8492VPI0062_NT5002.fa,820\r\nEC_ASM584v2,/path/to/GCF_000005845.2.fna,562\r\n...\r\n```\r\n\r\nHere,\r\n`prefix` is the prefix and the locus tag that will be assigned to output files and proteins during the annotation process;\r\nmaximum length is 24 characters;\r\n\r\n`assembly` is the path to where the assembly file in FASTA format is located;\r\n\r\n`taxid` is the NCBI TaxId (if the species-level TaxId is not known, a TaxId for a higher taxonomic level can be used). If the taxonomy is known, look up the TaxID [here](https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi).\r\n\r\n#### Finding TaxIds\r\n\r\nIf NCBI taxonomies of input genomes are not known, a tool such as [CAT/BAT](https://github.com/MGXlab/CAT_pack) can be used.\r\nFollow the [instructions](https://github.com/MGXlab/CAT_pack?tab=readme-ov-file#installation) for getting the tool and downloading the NCBI nr database for it.\r\n\r\nIf using CAT/BAT, here is the suggested process for making the `mettannotator` input file:\r\n\r\n```bash\r\n# Run BAT on each input genome, saving all results to the same folder\r\nCAT bins -b ${genome_name}.fna -d ${path_to_CAT_database} -t ${path_to_CAT_tax_folder} -o BAT_results/${genome_name}\r\n\r\n# Optional: to check what taxa were assigned, you can add names to them\r\nCAT add_names -i BAT_results/${genome_name}.bin2classification.txt -o BAT_results/${genome_name}.name.txt -t ${path_to_CAT_tax_folder}\r\n```\r\n\r\nTo generate an input file for `mettannotator`, use [generate_input_file.py](preprocessing/generate_input_file.py):\r\n\r\n```\r\npython3 preprocessing/generate_input_file.py -h\r\nusage: generate_input_file.py [-h] -i INFILE -d INPUT_DIR -b BAT_DIR -o OUTFILE [--no-prefix]\r\n\r\nThe script takes a list of genomes and the taxonomy results generated by BAT and makes a\r\nmettannotator input csv file. The user has the option to either use the genome file name\r\n(minus the extension) as the prefix for mettannotator or leave the prefix off and fill it\r\nout themselves after the script generates an input file with just the FASTA location and\r\nthe taxid. It is expected that for all genomes, BAT results are stored in the same folder\r\nand are named as {fasta_base_name}.bin2classification.txt. The script will use the lowest-\r\nlevel taxid without an asterisk as the taxid for the genome.\r\n\r\noptional arguments:\r\n  -h, --help    show this help message and exit\r\n  -i INFILE     A file containing a list of genome files to include (file name only, with file\r\n                extension, unzipped, one file per line).\r\n  -d INPUT_DIR  Full path to the directory where the input FASTA files are located.\r\n  -b BAT_DIR    Folder with BAT results. Results for all genomes should be in the same folder\r\n                and should be named {fasta_base_name}.bin2classification.txt\r\n  -o OUTFILE    Path to the file where the output will be saved to.\r\n  --no-prefix   Skip prefix generation and leave the first column of the output file empty for\r\n                the user to fill out. Default: False\r\n```\r\n\r\nFor example:\r\n\r\n```bash\r\npython3 generate_input_file.py -i list_of_genome_fasta_files.txt -d /path/to/the/fasta/files/folder/ -b BAT_results/ -o mettannotator_input.csv\r\n```\r\n\r\nIt is always best to check the outputs to ensure the results are as expected. Correct any wrongly detected taxa before starting `mettannotator`.\r\n\r\nNote, that by default the script uses FASTA file names as prefixes and truncates them to 24 characters if they exceed the limit.\r\n\r\n### Running mettannotator\r\n\r\nRunning `mettannotator` with the `--help` option will pull the repository and display the help message:\r\n\r\n> [!NOTE]\r\n> We use the `-latest` flag with the `nextflow run` command, which ensures that the latest available version of the pipeline is pulled.\r\n> If you encounter any issues with the `nextflow run` command, please refer to the [Nextflow documentation](https://www.nextflow.io/docs/latest/reference/cli.html#run).\r\n\r\n```angular2html\r\n$ nextflow run -latest ebi-metagenomics/mettannotator/main.nf --help\r\nN E X T F L O W  ~  version 23.04.3\r\nLaunching `mettannotator/main.nf` [disturbed_davinci] DSL2 - revision: f2a0e51af6\r\n\r\n\r\n------------------------------------------------------\r\n  ebi-metagenomics/mettannotator <version>\r\n------------------------------------------------------\r\nTypical pipeline command:\r\n\r\n  nextflow run ebi-metagenomics/mettannotator --input assemblies_sheet.csv -profile docker\r\n\r\nInput/output options\r\n  --input                            [string]  Path to comma-separated file containing information about the assemblies with the prefix to be used.\r\n  --outdir                           [string]  The output directory where the results will be saved. You have to use absolute paths to storage on Cloud\r\n                                               infrastructure.\r\n  --fast                             [boolean] Run the pipeline in fast mode. In this mode, InterProScan, UniFIRE, and SanntiS won't be executed, saving\r\n                                               resources and speeding up the pipeline.\r\n  --email                            [string]  Email address for completion summary.\r\n  --multiqc_title                    [string]  MultiQC report title. Printed as page header, used for filename if not otherwise specified.\r\n\r\nReference databases\r\n  --dbs                              [string]  Folder for the tools' reference databases used by the pipeline for downloading. It's important to note that\r\n                                               mixing the --dbs flag with individual database paths and versions is not allowed; they are mutually\r\n                                               exclusive.\r\n  --interproscan_db                  [string]  The InterProScan reference database, ftp://ftp.ebi.ac.uk/pub/software/unix/iprscan/\r\n  --interproscan_db_version          [string]  The InterProScan reference database version. [default: 5.62-94.0]\r\n  --interpro_entry_list              [string]  TSV file listing basic InterPro entry information - the accessions, types and names,\r\n                                               ftp://ftp.ebi.ac.uk/pub/databases/interpro/releases/94.0/entry.list\r\n  --interpro_entry_list_version      [string]  InterPro entry list version [default: 94]\r\n  --eggnog_db                        [string]  The EggNOG reference database folder,\r\n                                               https://github.com/eggnogdb/eggnog-mapper/wiki/eggNOG-mapper-v2.1.5-to-v2.1.12#requirements\r\n  --eggnog_db_version                [string]  The EggNOG reference database version. [default: 5.0.2]\r\n  --rfam_ncrna_models                [string]  Rfam ncRNA models, ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/ncrna/\r\n  --rfam_ncrna_models_rfam_version   [string]  Rfam release version where the models come from. [default: 14.9]\r\n  --amrfinder_plus_db                [string]  AMRFinderPlus reference database,\r\n                                               https://ftp.ncbi.nlm.nih.gov/pathogen/Antimicrobial_resistance/AMRFinderPlus/database/. Go to the following\r\n                                               documentation for the db setup https://github.com/ncbi/amr/wiki/Upgrading#database-updates.\r\n  --amrfinder_plus_db_version        [string]  The AMRFinderPlus reference database version. [default: 2023-02-23.1]\r\n  --defense_finder_db                [string]  Defense Finder reference models, https://github.com/mdmparis/defense-finder#updating-defensefinder. The\r\n                                               Microbiome Informatics team provides a pre-indexed version of the models for version 1.2.3 on this ftp location:\r\n                                               ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/pipelines/tool-dbs/defense-finder/defense-finder-models_1.2.3.tar.gz.\r\n  --defense_finder_db_version        [string]  The Defense Finder models version. [default: 1.2.3]\r\n  --antismash_db                     [string]  antiSMASH reference database, go to this documentation to do the database setup\r\n                                               https://docs.antismash.secondarymetabolites.org/install/#installing-the-latest-antismash-release.\r\n  --antismash_db_version             [string]  The antiSMASH reference database version. [default: 7.1.0]\r\n  --dbcan_db                         [string]  dbCAN indexed reference database, please go to the documentation for the setup\r\n                                               https://dbcan.readthedocs.io/en/latest/. The Microbiome Informatics team provides a pre-indexed version of the\r\n                                               database for version 4.0 on this ftp location:\r\n                                               ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/pipelines/tool-dbs/dbcan/dbcan_4.0.tar.gz\r\n  --dbcan_db_version                 [string]  The dbCAN reference database version. [default: 4.1.3_V12]\r\n  --pseudofinder_db                  [string]  Pseudofinder reference database. Mettannotator uses SwissProt as the database for Pseudofinder.\r\n  --pseudofinder_db_version          [string]  SwissProt version. [default: 2024_06]\r\n\r\nGeneric options\r\n  --multiqc_methods_description      [string]  Custom MultiQC yaml file containing HTML including a methods description.\r\n\r\nOther parameters\r\n  --bakta                            [boolean] Use Bakta instead of Prokka for CDS annotation. Prokka will still be used for archaeal genomes.\r\n\r\n !! Hiding 17 params, use --validationShowHiddenParams to show them !!\r\n------------------------------------------------------\r\nIf you use ebi-metagenomics/mettannotator for your analysis please cite:\r\n\r\n* The nf-core framework\r\n  https://doi.org/10.1038/s41587-020-0439-x\r\n\r\n* Software dependencies\r\n  https://github.com/ebi-metagenomics/mettannotator/blob/master/CITATIONS.md\r\n------------------------------------------------------\r\n\r\n```\r\n\r\nNow, you can run the pipeline using:\r\n\r\n```bash\r\nnextflow run ebi-metagenomics/mettannotator \\\r\n   -profile <docker/singularity/...> \\\r\n   --input assemblies_sheet.csv \\\r\n   --outdir <OUTDIR> \\\r\n   --dbs <PATH/TO/WHERE/DBS/WILL/BE/SAVED>\r\n```\r\n\r\n> [!WARNING]\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\r\n> provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\r\n\r\n#### Running the pipeline from the source code\r\n\r\nIf the Nextflow integration with Git does not work, users can download the tarball from the releases page. After extracting the tarball, the pipeline can be run directly by executing the following command:\r\n\r\n```bash\r\n$ nextflow run path-to-source-code/main.nf --help\r\n```\r\n\r\n#### Local execution\r\n\r\nThe pipeline can be run on a desktop or laptop, with the caveat that it will take a few hours to complete depending on the resources. There is a local profile in the Nextflow config that limits the total resources the pipeline can use to 8 cores and 12 GB of RAM. In order to run it (Docker or Singularity are still required):\r\n\r\n```bash\r\nnextflow run -latest ebi-metagenomics/mettannotator \\\r\n   -profile local,<docker or singulairty> \\\r\n   --input assemblies_sheet.csv \\\r\n   --outdir <OUTDIR> \\\r\n   --dbs <PATH/TO/WHERE/DBS/WILL/BE/SAVED>\r\n```\r\n\r\n### Gene caller choice\r\n\r\nBy default, `mettannotator` uses Prokka to identify protein-coding genes. Users can choose to use Bakta instead by\r\nrunning `mettannotator` with the `--bakta` flag. `mettannotator` runs Bakta without ncRNA and CRISPR\r\nannotation as these are produced by separate tools in the pipeline. Archaeal genomes will continue to be annotated using\r\nProkka as Bakta is only intended for annotation of bacterial genomes.\r\n\r\n### Fast mode\r\n\r\nTo reduce the compute time and the amount of resources used, the pipeline can be executed with the `--fast` flag. When\r\nrun in the fast mode, `mettannotator` will skip InterProScan, UniFIRE and SanntiS. This could be a suitable option\r\nfor a first-pass of annotation or if computational resources are limited, however, we recommend running the full version\r\nof the pipeline whenever possible.\r\n\r\nWhen generating an input file for a fast mode run, it is sufficient to indicate the taxid of the superkingdom (`2` for\r\nbacteria and `2157` for Archaea) in the \"taxid\" column rather than the taxid of the lowest known taxon.\r\n\r\n<a name=\"test\"></a>\r\n\r\n## Test\r\n\r\nTo run the pipeline using a test dataset, execute the following command:\r\n\r\n```bash\r\nwget https://raw.githubusercontent.com/EBI-Metagenomics/mettannotator/master/tests/test.csv\r\n\r\nnextflow run -latest ebi-metagenomics/mettannotator \\\r\n   -profile <docker/singularity/...> \\\r\n   --input test.csv \\\r\n   --outdir <OUTDIR> \\\r\n   --dbs <PATH/TO/WHERE/DBS/WILL/BE/SAVED>\r\n```\r\n\r\n<a name=\"out\"></a>\r\n\r\n## Outputs\r\n\r\nThe output folder structure will look as follows:\r\n\r\n```\r\n\u2514\u2500<PREFIX>\r\n   \u251c\u2500antimicrobial_resistance\r\n   \u2502  \u2514\u2500amrfinder_plus\r\n   \u251c\u2500antiphage_defense\r\n   \u2502  \u2514\u2500defense_finder\r\n   \u251c\u2500biosynthetic_gene_clusters\r\n   \u2502  \u251c\u2500antismash\r\n   \u2502  \u251c\u2500gecco\r\n   \u2502  \u2514\u2500sanntis\r\n   \u251c\u2500functional_annotation\r\n   \u2502  \u251c\u2500dbcan\r\n   \u2502  \u251c\u2500eggnog_mapper\r\n   \u2502  \u251c\u2500interproscan\r\n   \u2502  \u251c\u2500merged_gff\r\n   \u2502  \u251c\u2500prokka\r\n   \u2502  \u251c\u2500pseudofinder\r\n   \u2502  \u2514\u2500unifire\r\n   \u251c\u2500mobilome\r\n   \u2502  \u2514\u2500crisprcas_finder\r\n   \u251c\u2500quast\r\n   \u2502  \u2514\u2500<PREFIX>\r\n   \u2502      \u251c\u2500basic_stats\r\n   \u2502      \u2514\u2500icarus_viewers\r\n   \u251c\u2500rnas\r\n   \u2502  \u251c\u2500ncrna\r\n   \u2502  \u2514\u2500trna\r\n   \u251c\u2500multiqc\r\n   \u2502  \u251c\u2500multiqc_data\r\n   \u2502  \u2514\u2500multiqc_plots\r\n   \u2502      \u251c\u2500pdf\r\n   \u2502      \u251c\u2500png\r\n   \u2502      \u2514\u2500svg\r\n   \u251c\u2500pipeline_info\r\n   \u2502  \u251c\u2500software_versions.yml\r\n   \u2502  \u251c\u2500execution_report_<timestamp>.txt\r\n   \u2502  \u251c\u2500execution_report_<timestamp>.html\r\n   \u2502  \u251c\u2500execution_timeline_<timestamp>.txt\r\n   \u2502  \u251c\u2500execution_timeline_<timestamp>.html\r\n   \u2502  \u251c\u2500execution_trace_<timestamp>.txt\r\n   \u2502  \u251c\u2500execution_trace_<timestamp>.html\r\n   \u2502  \u2514\u2500pipeline_dag_<timestamp>.html\r\n\r\n```\r\n\r\n### Merged GFF\r\n\r\nThe two main output files for each genome are located in `<OUTDIR>/<PREFIX>/functional_annotation/merged_gff/`:\r\n\r\n- `<PREFIX>_annotations.gff`: annotations produced by all tools merged into a single file\r\n\r\n- `<PREFIX>_annotations_with_descriptions.gff`: a version of the GFF file above that includes descriptions of all InterPro terms to make the annotations human-readable. Not generated if `--fast` flag was used.\r\n\r\nBoth files include the genome sequence in the FASTA format at the bottom of the file.\r\n\r\nAdditionally, for genomes with no more than 50 annotated contigs, a Circos plot of the `<PREFIX>_annotations.gff` file is generated and included in the same folder. An example of such plot is shown below:\r\n\r\n<img src=\"media/circos-plot-example.png\">\r\n\r\n#### Data sources\r\n\r\nBelow is an explanation of how each field in column 3 and 9 of the final GFF file is populated. In most cases, information is taken as is from the reporting tool's output.\r\n\r\n| Feature (column 3)    | Attribute Name (column 9)                                               | Reporting Tool  | Description                                                                                                                                                                                                 |\r\n| --------------------- | ----------------------------------------------------------------------- | --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\r\n| ncRNA                 | all\\*                                                                   | cmscan + Rfam   | ncRNA annotation (excluding tRNA)                                                                                                                                                                           |\r\n| tRNA                  | all\\*                                                                   | tRNAscan-SE     | tRNA annotation                                                                                                                                                                                             |\r\n| LeftFLANK, RightFLANK | all\\*                                                                   | CRISPRCasFinder | CRISPR array flanking sequence                                                                                                                                                                              |\r\n| CRISPRdr              | all\\*                                                                   | CRISPRCasFinder | Direct repeat region of a CRISPR array                                                                                                                                                                      |\r\n| CRISPRspacer          | all\\*                                                                   | CRISPRCasFinder | CRISPR spacer                                                                                                                                                                                               |\r\n| CDS                   | `ID`, `eC_number`, `Name`, `Dbxref`, `gene`, `inference`, `locus_tag`   | Prokka/Bakta    | Protein annotation                                                                                                                                                                                          |\r\n| CDS                   | `product`                                                               | mettannotator   | Product assigned as described in [ Determining the product ](#product)                                                                                                                                      |\r\n| CDS                   | `product_source`                                                        | mettannotator   | Tool that reported the product chosen by mettannotator                                                                                                                                                      |\r\n| CDS                   | `eggNOG`                                                                | eggNOG-mapper   | Seed ortholog from eggNOG                                                                                                                                                                                   |\r\n| CDS                   | `cog`                                                                   | eggNOG-mapper   | COG category                                                                                                                                                                                                |\r\n| CDS                   | `kegg`                                                                  | eggNOG-mapper   | KEGG orthology term                                                                                                                                                                                         |\r\n| CDS                   | `Ontology_term`                                                         | eggNOG-mapper   | GO associations                                                                                                                                                                                             |\r\n| CDS                   | `pfam`                                                                  | InterProScan    | Pfam accessions                                                                                                                                                                                             |\r\n| CDS                   | `interpro`                                                              | InterProScan    | InterPro accessions. In `<PREFIX>_annotations_with_descriptions.gff` each accession is followed by its description and entry type: Domain [D], Family [F], Homologous Superfamily [H], Repeat [R], Site [S] |\r\n| CDS                   | `nearest_MiBIG`                                                         | SanntiS         | MiBIG accession of the nearest BGC to the cluster in the MIBIG space                                                                                                                                        |\r\n| CDS                   | `nearest_MiBIG_class`                                                   | SanntiS         | BGC class of nearest_MiBIG                                                                                                                                                                                  |\r\n| CDS                   | `gecco_bgc_type`                                                        | GECCO           | BGC type                                                                                                                                                                                                    |\r\n| CDS                   | `antismash_bgc_function`                                                | antiSMASH       | BGC function                                                                                                                                                                                                |\r\n| CDS                   | `amrfinderplus_gene_symbol`                                             | AMRFinderPlus   | Gene symbol according to AMRFinderPlus                                                                                                                                                                      |\r\n| CDS                   | `amrfinderplus_sequence_name`                                           | AMRFinderPlus   | Product description                                                                                                                                                                                         |\r\n| CDS                   | `amrfinderplus_scope`                                                   | AMRFinderPlus   | AMRFinderPlus database (core or plus)                                                                                                                                                                       |\r\n| CDS                   | `element_type`, `element_subtype`                                       | AMRFinderPlus   | Functional category                                                                                                                                                                                         |\r\n| CDS                   | `drug_class`, `drug_subclass`                                           | AMRFinderPlus   | Class and subclass of drugs that this gene is known to contribute to resistance of                                                                                                                          |\r\n| CDS                   | `dbcan_prot_type`                                                       | run_dbCAN       | Predicted protein function: transporter (TC), transcription factor (TF), signal transduction protein (STP), CAZyme                                                                                          |\r\n| CDS                   | `dbcan_prot_family`                                                     | run_dbCAN       | Predicted protein family                                                                                                                                                                                    |\r\n| CDS                   | `substrate_dbcan-pul`                                                   | run_dbCAN       | Substrate predicted by dbCAN-PUL search                                                                                                                                                                     |\r\n| CDS                   | `substrate_dbcan-sub`                                                   | run_dbCAN       | Substrate predicted by dbCAN-subfam                                                                                                                                                                         |\r\n| CDS                   | `defense_finder_type`, `defense_finder_subtype`                         | DefenseFinder   | Type and subtype of the anti-phage system found                                                                                                                                                             |\r\n| CDS                   | `uf_prot_rec_fullname`, `uf_prot_rec_shortname`, `uf_prot_rec_ecnumber` | UniFIRE         | Protein recommended full name, short name and EC number according to UniFIRE                                                                                                                                |\r\n| CDS                   | `uf_prot_alt_fullname`, `uf_prot_alt_shortname`, `uf_prot_alt_ecnumber` | UniFIRE         | Protein alternative full name, short name and EC number according to UniFIRE                                                                                                                                |\r\n| CDS                   | `uf_chebi`                                                              | UniFIRE         | ChEBI identifiers                                                                                                                                                                                           |\r\n| CDS                   | `uf_ontology_term`                                                      | UniFIRE         | GO associations                                                                                                                                                                                             |\r\n| CDS                   | `uf_keyword`                                                            | UniFIRE         | UniFIRE keywords                                                                                                                                                                                            |\r\n| CDS                   | `uf_gene_name`, `uf_gene_name_synonym`                                  | UniFIRE         | Gene name and gene name synonym according to UniFIRE                                                                                                                                                        |\r\n| CDS                   | `uf_pirsr_cofactor`                                                     | UniFIRE         | Cofactor names from PIRSR                                                                                                                                                                                   |\r\n\r\n\\*all attributes in column 9 are populated by the tool\r\n<br>\r\n<br>\r\n\r\n<a name=\"product\"></a>\r\n\r\n#### Determining the product\r\n\r\nThe following logic is used by `mettannotator` to fill out the `product` field in the 9th column of the GFF:\r\n\r\n<img src=\"media/mettannotator-product.png\">\r\n\r\nIf the pipeline is executed with the `--fast` flag, only the output of eggNOG-mapper is used to determine the product of proteins that were labeled as hypothetical by the gene caller.\r\n\r\n#### Detection of pseudogenes and spurious ORFs\r\n\r\n`mettannotator` uses several approaches to detect pseudogenes and spurious ORFs:\r\n\r\n- If Bakta is used as the initial annotation tool, `mettannotator` will inherit the pseudogene labels assigned by Bakta.\r\n- `mettannotator` runs Pseudofinder and labels genes that Pseudofinder predicts to be pseudogenes by adding `\"pseudo=true\"` to the 9th column of the final merged GFF file. If there is a disagreement between Pseudofinder and Bakta and one of the tools calls a gene a pseudogene, it will be labeled as a pseudogene.\r\n- AntiFam, which is a part of InterPro, is used to identify potential spurious ORFs. If an ORF has an AntiFam hit, `mettannotator` will remove it from the final merged GFF file. These ORFs will still appear in the raw outputs of Bakta/Prokka and may appear in other tool outputs.\r\n\r\n`mettannotator` produces a report file which is located in the `merged_gff` folder and includes a list of CDS with AntiFam hits and pseudogenes. For each pseudogene, the report shows which tool predicted it.\r\n\r\n### Contents of the tool output folders\r\n\r\nThe output folders of each individual tool contain select output files of the third-party tools used by `mettannotator`. For file descriptions, please refer to the tool documentation. For some tools that don't output a GFF, `mettannotator` converts the output into a GFF.\r\n\r\nNote: if the pipeline completed without errors but some of the tool-specific output folders are empty, those particular tools did not generate any annotations to output.\r\n\r\n<a name=\"submission\"></a>\r\n\r\n## Preparing annotations for ENA or GenBank submission\r\n\r\n`mettannotator` produces a final annotation file in GFF3 format. To submit the annotations to data archives, it is first necessary to convert the GFF3 file into the required format, using third-party tools available. `mettannotator` outputs a specially formatted GFF3 file, named `<prefix>_submission.gff` to be used with converters.\r\n\r\n### ENA\r\n\r\nENA accepts annotations in the EMBL flat-file format.\r\nPlease use [EMBLmyGFF3](https://github.com/NBISweden/EMBLmyGFF3) to perform the conversion; the repository includes detailed instructions. The two files required for conversion are:\r\n\r\n- the genome FASTA file\r\n- `<mettannotator_results_folder>/<prefix>/functional_annotation/merged_gff/<prefix>_submission.gff`\r\n\r\nPlease note that it is necessary to register the project and locus tags in ENA prior to conversion. Follow links in the [EMBLmyGFF3](https://github.com/NBISweden/EMBLmyGFF3) repository for more details.\r\n\r\n### GenBank\r\n\r\nTo convert annotations for GenBank submission, please use [table2asn](https://www.ncbi.nlm.nih.gov/genbank/table2asn/).\r\nThree files are required:\r\n\r\n- the genome FASTA file\r\n- `<mettannotator_results_folder>/<prefix>/functional_annotation/merged_gff/<prefix>_submission.gff`\r\n- Submission template file (can be generated [here](https://submit.ncbi.nlm.nih.gov/genbank/template/submission/))\r\n\r\nMore instructions on running `table2asn` are available via [GenBank](https://www.ncbi.nlm.nih.gov/genbank/genomes_gff/).\r\n\r\n<a name=\"mobilome\"></a>\r\n\r\n## Mobilome annotation\r\n\r\nThe mobilome annotation workflow is not currently integrated into `mettannotator`. However, the outputs produced by `mettannotator` can be used to run [VIRify](https://github.com/EBI-Metagenomics/emg-viral-pipeline) and the [mobilome annotation pipeline](https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline) and the outputs of these tools can be integrated back into the GFF file produced by `mettannotator`.\r\n\r\nAfter installing both tools, follow these steps to add the mobilome annotation:\r\n\r\n1. Run the [viral annotation pipeline](https://github.com/EBI-Metagenomics/emg-viral-pipeline):\r\n\r\n```bash\r\nnextflow run \\\r\n    emg-viral-pipeline/virify.nf \\\r\n    -profile <profile> \\\r\n    --fasta <genome_fasta.fna> \\\r\n    --output <prefix>\r\n```\r\n\r\n2. Run the [mobilome annotation pipeline](https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline):\r\n\r\n```bash\r\nnextflow run mobilome-annotation-pipeline/main.nf \\\r\n    --assembly <genome_fasta.fna> \\\r\n    --user_genes true \\\r\n    --prot_gff <mettannotator_results_folder/<prefix>/functional_annotation/merged_gff/<prefix>_annotations.gff \\\r\n    --virify true # only if the next two VIRify files exist, otherwise skip this line \\\r\n    --vir_gff Virify_output_folder/08-final/gff/<prefix>_virify.gff # only if file exists, otherwise skip this line \\\r\n    --vir_checkv Virify_output_folder/07-checkv/\\*quality_summary.tsv # only if the GFF file above exists, otherwise skip this line \\\r\n    --outdir <mobilome_output_folder> \\\r\n    --skip_crispr true \\\r\n    --skip_amr true \\\r\n    -profile <profile>\"\r\n```\r\n\r\n3. Integrate the output into the `mettannotator` GFF\r\n\r\n```bash\r\n# Add mobilome to the merged GFF produced by mettannotator\r\npython3 postprocessing/add_mobilome_to_gff.py \\\r\n    -m <mobilome_output_folder>/gff_output_files/mobilome_nogenes.gff \\\r\n    -i <mettannotator_results_folder>/<prefix>/functional_annotation/merged_gff/<prefix>_annotations.gff \\\r\n    -o <prefix>_annotations_with_mobilome.gff\r\n\r\n# Add mobilome to the GFF with descriptions produced by mettannotator\r\npython3 postprocessing/add_mobilome_to_gff.py \\\r\n    -m <mobilome_output_folder>/gff_output_files/mobilome_nogenes.gff \\\r\n    -i <mettannotator_results_folder>/<prefix>/functional_annotation/merged_gff/<prefix>_annotations_with_descriptions.gff \\\r\n    -o <prefix>_annotations_with_descriptions_with_mobilome.gff\r\n```\r\n\r\n4. Optional: regenerate the Circos plot with the mobilome track added\r\n\r\n```bash\r\npip install pycirclize\r\npip install matplotlib\r\n\r\npython3 bin/circos_plot.py \\\r\n    -i <prefix>_annotations_with_mobilome.gff \\\r\n    -o plot.png \\\r\n    -p <prefix> \\\r\n    --mobilome\r\n```\r\n\r\n<a name=\"credit\"></a>\r\n\r\n## Credits\r\n\r\nebi-metagenomics/mettannotator was originally written by the Microbiome Informatics Team at [EMBL-EBI](https://www.ebi.ac.uk/about/teams/microbiome-informatics/)\r\n\r\n<a name=\"contribute\"></a>\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\n<a name=\"cite\"></a>\r\n\r\n## Citations\r\n\r\nIf you use the software, please cite:\r\n\r\nGurbich TA, Beracochea M, De Silva NH, Finn RD. mettannotator: a comprehensive and scalable Nextflow annotation pipeline for prokaryotic assemblies. bioRxiv 2024.07.11.603040; doi: https://doi.org/10.1101/2024.07.11.603040\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nThis pipeline uses code developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Genomics",
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "1069",
        "keep": true,
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1069?version=3",
        "name": "mettannotator",
        "number_of_steps": 0,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "metagenomics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-01-21",
        "versions": 3
    },
    {
        "create_time": "2025-01-18",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-rangeland_logo_dark.png\">\n    <img alt=\"nf-core/rangeland\" src=\"docs/images/nf-core-rangeland_logo_light.png\">\n  </picture>\n</h1>[![GitHub Actions CI Status](https://github.com/nf-core/rangeland/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/rangeland/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/rangeland/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/rangeland/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/rangeland/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/rangeland)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23rangeland-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/rangeland)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/rangeland** is a geographical best-practice analysis pipeline for remotely sensed imagery.\nThe pipeline processes satellite imagery alongside auxiliary data in multiple steps to arrive at a set of trend files related to land-cover changes. The main pipeline steps are:\n\n1. Read satellite imagery, digital elevation model (dem), endmember definition, water vapor database (wvdb), datacube definition and area of interest definition (aoi)\n2. Generate allow list and analysis mask to determine which pixels from the satellite data can be used\n3. Preprocess data to obtain atmospherically corrected images alongside quality assurance information (aka. level 2 analysis read data)\n4. Merge spatially and temporally overlapping preprocessed data\n5. Classify pixels by applying linear spectral unmixing\n6. Time series analyses to obtain trends in vegetation dynamics to derive level 3 data\n7. Create mosaic and pyramid visualizations of the results\n8. Version reporting with MultiQC ([`MultiQC`](http://multiqc.info/))\n\n<p align=\"center\">\n    <img title=\"nf-core/rangeland diagram\" src=\"docs/images/rangeland_diagram.png\" width=95%>\n</p>\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow.Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nTo run, satellite imagery, water vapor data, a digital elevation model, endmember definitions, a datacube specification, and a area-of-interest specification are required as input data.\nPlease refer to the [usage documentation](https://nf-co.re/rangeland/usage) for details on the input structure.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/rangeland \\\n   -profile <docker/singularity/.../institute> \\\n   --input <SATELLITE IMAGES> \\\n   --dem <DIGITAL ELEVATION MODEL> \\\n   --wvdb <WATER VAPOR DATA> \\\n   --data_cube <DATA CUBE> \\\n   --aoi <AREA OF INTEREST> \\\n   --endmember <ENDMEMBER SPECIFICATION> \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/rangeland/usage) and the [parameter documentation](https://nf-co.re/rangeland/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/rangeland/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/rangeland/output).\n\n## Credits\n\nThe rangeland workflow was originally written by:\n\n- [Fabian Lehmann](https://github.com/Lehmann-Fabian)\n- [David Frantz](https://github.com/davidfrantz)\n\nThe original workflow can be found on [github](https://github.com/CRC-FONDA/FORCE2NXF-Rangeland).\n\nTransformation to nf-core/rangeland was conducted by [Felix Kummer](https://github.com/Felix-Kummer).\nnf-core alignment started on the [nf-core branch of the original repository](https://github.com/CRC-FONDA/FORCE2NXF-Rangeland/tree/nf-core).\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Fabian Lehmann](https://github.com/Lehmann-Fabian)\n- [Katarzyna Ewa Lewinska](https://github.com/kelewinska).\n\n## Acknowledgements\n\nThis pipeline was developed and aligned with nf-core as part of the [Foundations of Workflows for Large-Scale Scientific Data Analysis (FONDA)](https://fonda.hu-berlin.de/) initiative.\n\n[![FONDA](docs/images/fonda_logo2_cropped.png)](https://fonda.hu-berlin.de/)\n\nFONDA can be cited as follows:\n\n> **The Collaborative Research Center FONDA.**\n>\n> Ulf Leser, Marcus Hilbrich, Claudia Draxl, Peter Eisert, Lars Grunske, Patrick Hostert, Dagmar Kainm\u00fcller, Odej Kao, Birte Kehr, Timo Kehrer, Christoph Koch, Volker Markl, Henning Meyerhenke, Tilmann Rabl, Alexander Reinefeld, Knut Reinert, Kerstin Ritter, Bj\u00f6rn Scheuermann, Florian Schintke, Nicole Schweikardt, Matthias Weidlich.\n>\n> _Datenbank Spektrum_ 2021 doi: [10.1007/s13222-021-00397-5](https://doi.org/10.1007/s13222-021-00397-5)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#rangeland` channel](https://nfcore.slack.com/channels/rangeland) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/rangeland for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\nThis pipeline is based one the publication listed below.\nThe publication can be cited as follows:\n\n> **FORCE on Nextflow: Scalable Analysis of Earth Observation Data on Commodity Clusters**\n>\n> [Lehmann, F., Frantz, D., Becker, S., Leser, U., Hostert, P. (2021). FORCE on Nextflow: Scalable Analysis of Earth Observation Data on Commodity Clusters. In CIKM Workshops.](https://www.informatik.hu-berlin.de/de/forschung/gebiete/wbi/research/publications/2021/force_nextflow.pdf/@@download/file/force_nextflow.pdf)\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "1250",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1250?version=1",
        "name": "nf-core/rangeland",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "earth-observation",
            "landsat",
            "satellite-imagery",
            "spectral-unmixing",
            "trend-analysis",
            "trends"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-14",
        "versions": 1
    },
    {
        "create_time": "2025-02-25",
        "creators": [
            "Playbook Partnership NIH CFDE"
        ],
        "description": "<https://playbook-workflow-builder.cloud/report/f43bfb7a-557a-76d4-250c-7a44c81d70d7>\r\n\r\nA file containing GEO Aging Signatures was first uploaded. The file containing GEO Aging Signatures was loaded as a gene signature. A file containing GTEx Aging Signatures was first uploaded. The file containing GTEx Aging Signatures was loaded as a gene signature. Significant genes were extracted from the GEO Aging Signatures. Significant genes were extracted from the GTEx Aging Signatures. Reversers and mimickers from over 1 million signatures were identified using SigCom LINCS[1]. Resolved drugs from the LINCS L1000 Chemical Perturbagens library. Reversers and mimickers from over 1 million signatures were identified using SigCom LINCS[1]. Resolved drugs from the LINCS L1000 Chemical Perturbagens library. The mean across multiple Scored Drugs is computed. The drugs were filtered by FDA Approved Drugs with the help of PubChem APIs[3]. \r\n1. Evangelista, J. E. et al. SigCom LINCS: data and metadata search engine for a million gene expression signatures. Nucleic Acids Research vol. 50 W697\u2013W709 (2022). doi:10.1093/nar/gkac328\r\n3. Kim, S. et al. PubChem 2023 update. Nucleic Acids Research vol. 51 D1373\u2013D1380 (2022). doi:10.1093/nar/gkac956",
        "doi": "10.48546/workflowhub.workflow.1248.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1248",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1248?version=2",
        "name": "Use Case 3: Compounds to Reverse Disease Signatures",
        "number_of_steps": 12,
        "projects": [
            "NIH CFDE Playbook Workflow Partnership"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Take the mean value across multiple scores",
            "Filter the drugs with PubChem APIs",
            "Extract signatures from the results",
            "Query LINCS L1000 Signatures",
            "Treat signature as a weighted set of genes",
            "Upload a Data File",
            "Load a gene signature into a standard format"
        ],
        "type": "Playbook Workflow Builder Workflow",
        "update_time": "2025-02-25",
        "versions": 2
    },
    {
        "create_time": "2025-02-25",
        "creators": [
            "Playbook Partnership NIH CFDE"
        ],
        "description": "A file was first uploaded. The file was parsed as a gene count matrix. Significantly over-expressed genes when compared to tissue expression in GTEx[1] were identified. RNA-seq-like LINCS L1000 Signatures[3] which mimick or reverse the the expression of IMP3 were visualized. Drugs which down-regulate the expression of IMP3 were identified from the RNA-seq-like LINCS L1000 Chemical Perturbagens[3]. Genes which down-regulate the expression of IMP3 were identified from the RNA-seq-like LINCS L1000 CRISPR Knockouts[3]. Genes were filtered by IDG Understudied Proteins[8]. The gene was searched with the MetGENE tool providing pathways, reactions, metabolites, and studies from the Metabolomics Workbench[9]. IMP3 was then searched in the Metabolomics Workbench[11] to identify associated metabolites. IMP3 was then searched in the Metabolomics Workbench[11] to identify relevant reactions. A list of regulatory elements in the vicinity of the gene were retrieved from the CFDE Linked Data Hub[14]. The GlyGen database[18] was searched to identify a relevant set of protein products that originate from IMP3. \r\n1. Lonsdale, J. et al. The Genotype-Tissue Expression (GTEx) project. Nature Genetics vol. 45 580\u2013585 (2013). doi:10.1038/ng.2653\r\n3. Evangelista, J. E. et al. SigCom LINCS: data and metadata search engine for a million gene expression signatures. Nucleic Acids Research vol. 50 W697\u2013W709 (2022). doi:10.1093/nar/gkac328\r\n8. IDG Protein List, https://druggablegenome.net/IDGProteinList\r\n9. MetGENE, https://sc-cfdewebdev.sdsc.edu/MetGENE/metGene.php\r\n11. The Metabolomics Workbench, https://www.metabolomicsworkbench.org/\r\n14. CFDE Linked Data Hub, https://ldh.genome.network/cfde/ldh/\r\n18. York, W. S. et al. GlyGen: Computational and Informatics Resources for Glycoscience. Glycobiology vol. 30 72\u201373 (2019). doi:10.1093/glycob/cwz080",
        "doi": "10.48546/workflowhub.workflow.1246.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1246",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1246?version=2",
        "name": "Use Case 13: Novel Cell Surface Targets for Individual Cancer Patients Analyzed with Common Fund Datasets",
        "number_of_steps": 13,
        "projects": [
            "NIH CFDE Playbook Workflow Partnership"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Ensure a file contains a gene count matrix, load it into a standard format",
            "Select one Gene",
            "Identify RNA-seq-like LINCS L1000 Signatures which reverse the expression of the gene.",
            "Extract Metabolomics reactions for the gene from MetGENE",
            "Identify RNA-seq-like LINCS L1000 Chemical Perturbagen Signatures which reverse the expression of the gene.",
            "Identify gene-centric information from Metabolomics.",
            "Find protein product records in GlyGen for the gene",
            "Identify RNA-seq-like LINCS L1000 CRISPR KO Signatures which reverse the expression of the gene.",
            "Upload a Data File",
            "Identify significantly overexpressed genes when compared to normal tissue in GTEx",
            "Regulatory elements in 10kbps region upstream or downstream of gene body.",
            "Extract Metabolomics metabolites for the gene from MetGENE",
            "Based on IDG proteins list"
        ],
        "type": "Playbook Workflow Builder Workflow",
        "update_time": "2025-02-25",
        "versions": 2
    },
    {
        "create_time": "2025-02-25",
        "creators": [
            "Playbook Partnership NIH CFDE"
        ],
        "description": "The workflow starts with selecting RPE as the search term. For the given gene ID (SYMBOL), StringDB PPI was extracted using their API[1]. For the Given StringDB PPI, the list of nodes (Gene Set) is generated. For the Given StringDB PPI, the list of nodes (GeneSet) is generated. Reversers and mimickers from over 1 million signatures were identified using SigCom LINCS[2]. The gene set was submitted to Enrichr[4]. The gene set was then searched in the Metabolomics Workbench[5] to identify relevant reactions. The gene set was then searched in the Metabolomics Workbench [Metabolomics Workbench, [7] to identify associated metabolites. The gene set was then searched in the Metabolomics Workbench[5] to identify relevant studies related to the genes. \r\n1. Szklarczyk, D. et al. The STRING database in 2023: protein\u2013protein association networks and functional enrichment analyses for any sequenced genome of interest. Nucleic Acids Research vol. 51 D638\u2013D646 (2022). doi:10.1093/nar/gkac1000\r\n2. Evangelista, J. E. et al. SigCom LINCS: data and metadata search engine for a million gene expression signatures. Nucleic Acids Research vol. 50 W697\u2013W709 (2022). doi:10.1093/nar/gkac328\r\n4. Xie, Z. et al. Gene Set Knowledge Discovery with Enrichr. Current Protocols vol. 1 (2021). doi:10.1002/cpz1.90\r\n5. The Metabolomics Workbench, https://www.metabolomicsworkbench.org/\r\n7. https://www.metabolomicsworkbench.org/",
        "doi": "10.48546/workflowhub.workflow.1245.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in name",
        "id": "1245",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1245?version=2",
        "name": "Use Case 11: Related Proteins/Metabolites across DCCs",
        "number_of_steps": 9,
        "projects": [
            "NIH CFDE Playbook Workflow Partnership"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Compute the MetGENE metabolites for a Gene Set",
            "Given StringDB PPI, reformat for plotting",
            "Start with a Gene",
            "Query LINCS L1000 Signatures",
            "Compute the MetGENE studies function for a gene set",
            "Perform Enrichment Analysis",
            "Given a gene or gene set (SYMBOL), extract PPI using StringDB APIs",
            "Compute the MetGENE Reactions for a Gene Set",
            "Given StringDB PPI, generate the list of nodes (GeneSet)"
        ],
        "type": "Playbook Workflow Builder Workflow",
        "update_time": "2025-02-25",
        "versions": 2
    },
    {
        "create_time": "2025-02-25",
        "creators": [
            "Playbook Partnership NIH CFDE"
        ],
        "description": "The workflow starts with a gene set created from Example gene set. CTD is applied which diffuses through all nodes in STRING[1] to identify nodes that are \"guilty by association\" and highly connected to the initial gene set of interest[2][3]. A list of Highly Connected Genes was obtained from the CTD output. A list of Guilty By Association Genes was obtained from the CTD output. \r\n1. Szklarczyk, D. et al. STRING v10: protein\u2013protein interaction networks, integrated over the tree of life. Nucleic Acids Research vol. 43 D447\u2013D452 (2014). doi:10.1093/nar/gku1003\r\n2. Thistlethwaite, L. R. et al. Correction: CTD: An information-theoretic algorithm to interpret sets of metabolomic and transcriptomic perturbations in the context of graphical models. PLOS Computational Biology vol. 17 e1009551 (2021). doi:10.1371/journal.pcbi.1009551\r\n3. Petrosyan, V. et al. Identifying biomarkers of differential chemotherapy response in TNBC patient-derived xenografts with a CTD/WGCNA approach. iScience vol. 26 105799 (2023). doi:10.1016/j.isci.2022.105799",
        "doi": "10.48546/workflowhub.workflow.1244.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1244",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1244?version=2",
        "name": "Use Case 10: Guilt by Association",
        "number_of_steps": 4,
        "projects": [
            "NIH CFDE Playbook Workflow Partnership"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Extract nodes that are \"guilty by association\" and connect your initial genes of interest within the graph.",
            "Start with a set of Genes",
            "Extract nodes that are determined to be highly connected by CTD in your initial node set and display them in a table.",
            "Use CTD to \"Connect the Dots\" and identify highly connected set of proteins in the STRING protein interaction graph. *Please note 10-150 genes of interest are required to run CTD"
        ],
        "type": "Playbook Workflow Builder Workflow",
        "update_time": "2025-02-25",
        "versions": 2
    },
    {
        "create_time": "2025-02-25",
        "creators": [
            "Playbook Partnership NIH CFDE"
        ],
        "description": "The workflow starts with selecting KLF6 as the search term. RNA-seq-like LINCS L1000 Signatures[1] which mimick or reverse the the expression of KLF6 were visualized. Median expression of KLF6 was obtained from the GTEx Portal[6] using the portal's API. To visualize the scored tissues, a vertical bar plot was created Fig.. \r\n1. Evangelista, J. E. et al. SigCom LINCS: data and metadata search engine for a million gene expression signatures. Nucleic Acids Research vol. 50 W697\u2013W709 (2022). doi:10.1093/nar/gkac328\r\n6. Lonsdale, J. et al. The Genotype-Tissue Expression (GTEx) project. Nature Genetics vol. 45 580\u2013585 (2013). doi:10.1038/ng.2653",
        "doi": "10.48546/workflowhub.workflow.1242.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1242",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1242?version=2",
        "name": "Use Case 6: CFDE Knowledge about a Gene",
        "number_of_steps": 4,
        "projects": [
            "NIH CFDE Playbook Workflow Partnership"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Use GTEx API to obtain median tissue expression for the given gene",
            "Construct a vertical bar plot with Scored Tissues",
            "Identify RNA-seq-like LINCS L1000 Signatures which reverse the expression of the gene.",
            "Start with a Gene"
        ],
        "type": "Playbook Workflow Builder Workflow",
        "update_time": "2025-02-25",
        "versions": 2
    },
    {
        "create_time": "2025-02-25",
        "creators": [
            "Playbook Partnership NIH CFDE"
        ],
        "description": "The workflow starts with selecting atrial fibrillation as the search term. The workflow starts with selecting Ibrutinib as the search term. Gene sets with set labels containing atrial fibrillation were queried from Enrichr[1]. Identified matching terms from the MGI Mammalian Phenotype Level 4 2021[2] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for MGI_Mammalian_Phenotype_Level_4_2021. A consensus gene set was created by only retaining genes that appear in at least two sets. Identified matching terms from the GWAS Catalog 2019[4] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for GWAS_Catalog_2019. A consensus gene set was created by only retaining genes that appear in at least two sets. The gene sets collected were combined into one gene set library. Gene sets with set labels containing Ibrutinib were queried from Enrichr[1]. Identified matching terms from the LINCS L1000 Chem Pert Consensus Sigs[5] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for LINCS_L1000_Chem_Pert_Consensus_Sigs. Multiple GMTs were combined into one GMT. The collection of gene sets was then visualized with a Supervenn diagram Fig.. \r\n1. Xie, Z. et al. Gene Set Knowledge Discovery with Enrichr. Current Protocols vol. 1 (2021). doi:10.1002/cpz1.90\r\n2. Blake, J. A. et al. Mouse Genome Database (MGD): Knowledgebase for mouse\u2013human comparative biology. Nucleic Acids Research vol. 49 D981\u2013D987 (2020). doi:10.1093/nar/gkaa1083\r\n4. Sollis, E. et al. The NHGRI-EBI GWAS Catalog: knowledgebase and deposition resource. Nucleic Acids Research vol. 51 D977\u2013D985 (2022). doi:10.1093/nar/gkac1010\r\n5. Evangelista, J. E. et al. SigCom LINCS: data and metadata search engine for a million gene expression signatures. Nucleic Acids Research vol. 50 W697\u2013W709 (2022). doi:10.1093/nar/gkac328",
        "doi": "10.48546/workflowhub.workflow.1238.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1238",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1238?version=2",
        "name": "Use Case 2: Explain MOAs of Side Effects for Approved Drugs",
        "number_of_steps": 15,
        "projects": [
            "NIH CFDE Playbook Workflow Partnership"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Find Drug Terms in Enrichr Libraries",
            "Start with a Disease",
            "Interactively analyse overlap between sets",
            "Join several GMTs into one",
            "Extract Terms from the MGI Mammalian Phenotype Level 4 2021 Library",
            "Group multiple independently generated gene sets into a single GMT",
            "Start with a Drug",
            "Load Enrichr set as GMT",
            "Find Disease Terms in Enrichr Libraries",
            "Find genes which appear in more than one set",
            "Extract Terms from the LINCS L1000 Chem Pert Consensus Sigs Library",
            "Extract Terms from the GWAS Catalog 2019 Library"
        ],
        "type": "Playbook Workflow Builder Workflow",
        "update_time": "2025-02-25",
        "versions": 2
    },
    {
        "create_time": "2025-02-25",
        "creators": [
            "Playbook Partnership NIH CFDE"
        ],
        "description": "The workflow starts with selecting Inflammation as the search term. The workflow starts with selecting Penicillin as the search term. The workflow starts with selecting Cortisol as the search term. Gene sets with set labels containing Inflammation were queried from Enrichr[1]. Identified matching terms from the GWAS Catalog 2019[2] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for GWAS_Catalog_2019. All the identified gene sets were combined using the union set operation. Identified matching terms from the MGI Mammalian Phenotype Level 4 2019[4] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for MGI_Mammalian_Phenotype_Level_4_2019. All the identified gene sets were combined using the union set operation. Identified matching terms from the Human Phenotype Ontology[5] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for Human_Phenotype_Ontology. All the identified gene sets were combined using the union set operation. Gene sets with set labels containing Penicillin were queried from Enrichr[1]. Identified matching terms from the LINCS L1000 Chem Pert Consensus Sigs[6] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for LINCS_L1000_Chem_Pert_Consensus_Sigs. Gene sets with set labels containing Cortisol were queried from Enrichr[1]. Identified matching terms from the LINCS L1000 Chem Pert Consensus Sigs[6] library were assembled into a collection of gene sets. A GMT was extracted from the Enrichr results for LINCS_L1000_Chem_Pert_Consensus_Sigs. The gene sets collected were combined into one gene set library. Multiple GMTs were combined into one GMT. The collection of gene sets was then visualized with a Supervenn diagram Fig.. \r\n1. Xie, Z. et al. Gene Set Knowledge Discovery with Enrichr. Current Protocols vol. 1 (2021). doi:10.1002/cpz1.90\r\n2. Sollis, E. et al. The NHGRI-EBI GWAS Catalog: knowledgebase and deposition resource. Nucleic Acids Research vol. 51 D977\u2013D985 (2022). doi:10.1093/nar/gkac1010\r\n4. Blake, J. A. et al. Mouse Genome Database (MGD): Knowledgebase for mouse\u2013human comparative biology. Nucleic Acids Research vol. 49 D981\u2013D987 (2020). doi:10.1093/nar/gkaa1083\r\n5. K\u00f6hler, S. et al. The Human Phenotype Ontology in 2021. Nucleic Acids Research vol. 49 D1207\u2013D1217 (2020). doi:10.1093/nar/gkaa1043\r\n6. Evangelista, J. E. et al. SigCom LINCS: data and metadata search engine for a million gene expression signatures. Nucleic Acids Research vol. 50 W697\u2013W709 (2022). doi:10.1093/nar/gkac328",
        "doi": "10.48546/workflowhub.workflow.1237.3",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1237",
        "keep": true,
        "latest_version": 3,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1237?version=3",
        "name": "Use Case 1: Explain Drug-Drug Interactions",
        "number_of_steps": 22,
        "projects": [
            "NIH CFDE Playbook Workflow Partnership"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Find Phenotype Terms in Enrichr Libraries",
            "Find Drug Terms in Enrichr Libraries",
            "Start with a Phenotype",
            "Extract Terms from the MGI Mammalian Phenotype Level 4 2019 Library",
            "Interactively analyse overlap between sets",
            "Join several GMTs into one",
            "Extract Terms from the Human Phenotype Ontology Library",
            "Group multiple independently generated gene sets into a single GMT",
            "Find the union set of all genes in the GMT",
            "Start with a Drug",
            "Load Enrichr set as GMT",
            "Extract Terms from the LINCS L1000 Chem Pert Consensus Sigs Library",
            "Extract Terms from the GWAS Catalog 2019 Library"
        ],
        "type": "Playbook Workflow Builder Workflow",
        "update_time": "2025-02-25",
        "versions": 3
    },
    {
        "create_time": "2025-01-02",
        "creators": [
            "Tobias Bachmann"
        ],
        "description": "## Installation\r\n\r\nOther than cloning this repository, you need to have bash installed (which is most likely the case if you use Linux, *BSD or even MacOS).\r\nFor the Python code, the arguably easiest and cleanest way is to set up a Python virtual environment and install the dependencies there:\r\n\r\n``` shell\r\n$ python3 -m venv ./hcp-suite-venv # Setup the virtual environment\r\n$ source ./hcp-suite-venv/bin/activate # Activate the virtual environment\r\n$ pip install pandas pingouin networkx nilearn nibabel ray # Install dependencies within the virtual environment\r\n$ pip install ipython jupyterlab # Install interactive Python shells to run hcp-suite code in\r\n```\r\n\r\n## Usage: CPM tutorial\r\n\r\nThe following tutorial uses the gambling task as an example, the variable to be predicted is BMI. Differing commands for resting-state fMRI are provided for the corresponding steps.\r\n\r\n### Overview\r\n\r\nPython code in this repository is written to be used in an interactive Python shell. Either Jupyter Lab or Jupyer Notebook is recommended as plots and images are conveniently displayed in-line but any Python shell (e.g. iPython) should work.\r\n\r\nGenerally speaking, the procedure is as follows:\r\n\r\n1. Downloading data of subjects to be included in the analysis\r\n2. Parcellation of CIFTI files with `task.sh prepare` (at the code's current state, the HCP's folder structure is assumed)\r\n3a. Extraction of task-based fMRI time series based on EV files provided by the HCP with `get_ev_timeseries()` from `hcpsuite.py`\r\n3b. \r\n4. Computing of correlation matrices via `compute_correlations()` from `hcpsuite.py`\r\n5. Performing of CPM, the functions are provided by `cpm_ray.py`\r\n6. Establishing of statistical significance by way of permutation testing (also `cpm_ray.py`)\r\n\r\nThe following code snippets are to be run in an interactive Python shell (e.g. the \"Console\" of Jupyter Lab):\r\n\r\n#### Downloading data\r\n\r\nWe will use Amazon Web Services to download HCP data. Set up access to HCP data via Amazon Web Services by following their [documentation](https://wiki.humanconnectome.org/docs/How%20To%20Connect%20to%20Connectome%20Data%20via%20AWS.html). You should be provided with an AWS Access Key ID and Secret Access Key we are going to put into Python variables (in quotation marks) for easy access:\r\n\r\n``` python\r\naws_access_key_id=\"Replace with your access key ID\"\r\naws_secret_access_key=\"Replace with your secret access key\"\r\n\r\n# Also, specify a location you want to download files to. We will use this variable repeatedly.\r\ndata_dir = \"./HCP_1200\"\r\n```\r\n\r\nWe now need a list of subject IDs to be included in the analysis. Save them in a simple text file with one line per subject ID. For the rest of this tutorial, we will use the gambling task as an example (the list of IDs will be called ```gambling_subjects.ids```). As preprocessing for resting-state data differs, we will provide resting-state specific instructions when needed (the list of IDs will be called ```rest_subjects.ids```).\r\n\r\n``` python\r\nimport sys\r\n\r\nsys.path.append(\"../hcp-suite/lib\") # This assumes the hcp-suite directory is in the current working directory's parent directory. You can also use absolute paths.\r\nfrom hcpsuite import *\r\nfrom cpm_ray import *\r\n\r\nsubj_ids = load_list_from_file(\"./gambling_subjects.ids\") # Change the path to gambling_subjects.ids if it resides elsewhere\r\n\r\n# Check the number of subjects\r\nlen(subj_ids)\r\n```\r\n\r\nNext, we will use the function download_hcp_files() to download all needed files from the HCP's AWS storage. This function takes the following arguments in this order: List of subject IDs, download location, task name, AWS access key ID, and AWS secret access key. It will return a list of missing subjects we capture in the variable missing_subjs:\r\n\r\n``` python\r\nmissing_subjs = download_hcp_files(subj_ids, data_dir, \"gambling\", aws_access_key_id, aws_secret_access_key)\r\n```\r\n\r\nThe previous command will print a list of missing subjects (if any) since not all data from all subjects is available on AWS, unfortunately. You can either obtain data from other sources or remove the missing subjects from our list of subject IDs:\r\n\r\n\r\n``` python\r\nfor missing_subj in missing_subjs:\r\n    subj_ids.remove(missing_subj)\r\n```\r\n\r\n#### Parcellation\r\n\r\nParcellation involves combining voxels of the \"raw\" CIFTI files into parcels as specified by the combined cortical, subcortical, and cerebellar parcellation we dubbed \"RenTianGlasser\" after the authors of the individual parcellations.\r\n\r\n##### Task-based fMRI\r\n``` python\r\nparcellate_ciftis(subj_ids=subj_ids,\r\n                  parcellation_fname=\"./hcp-suite/data/parcellations/RenTianGlasser.dlabel.nii\",\r\n                  task=\"gambling\",\r\n                  data_dir=data_dir) # If you have installed Connectome Workbench in a non-standard way,\r\n                                     # so that 'wb_command' is not in your $PATH, add\r\n                                     # 'wb_command=\"/replace/with/path/to/wb_command\"'\r\n```\r\n\r\n##### Resting-state fMRI\r\n\r\nAs resting-state fMRI data was collected in two separate sessions called REST1 and REST2, we need to parcellate twice:\r\n\r\n```\r\n# First for REST1\r\n\r\nparcellate_ciftis(subj_ids=subj_ids, \r\n                  parcellation_fname=\"./hcp-suite/data/parcellations/RenTianGlasser.dlabel.nii\", \r\n                  task=\"REST1\", \r\n                  data_dir=data_dir) # If you have installed Connectome Workbench in a non-standard way, \r\n                                     # so that 'wb_command' is not in your $PATH, add \r\n                                     # 'wb_command=\"/replace/with/path/to/wb_command\"'\r\n\r\n# Now for REST2\r\nparcellate_ciftis(subj_ids=subj_ids, \r\n                  parcellation_fname=\"./hcp-suite/data/parcellations/RenTianGlasser.dlabel.nii\", \r\n                  task=\"REST2\", \r\n                  data_dir=data_dir) # If you have installed Connectome Workbench in a non-standard way, \r\n                                     # so that 'wb_command' is not in your $PATH, add \r\n                                     # 'wb_command=\"/replace/with/path/to/wb_command\"'\t\t\t\t\t\t\t\t\t\t\t \r\n```\r\n\r\n\r\n#### Extraction of time series\r\n\r\n##### Task-based fMRI\r\n``` python\r\nev_data_dict = get_ev_timeseries(subj_ids, [\"win.txt\"], task=\"gambling\",\r\n                                 runs=('LR', 'RL'),\r\n                                 parcellation=\"RenTianGlasser\",\r\n                                 data_dir=data_dir) # If you have installed Connectome Workbench in a non-standard way,\r\n                                                    # so that 'wb_command' is not in your $PATH, add\r\n                                                    # 'wb_command=\"/replace/with/path/to/wb_command\"'\r\n\r\n# Now, we save the extraced time series as text files in a directory of our choice\r\n# (in this case: ./GAMBLING_win)\r\nsave_data_dict(ev_data_dict, path=\"./GAMBLING_win\")\r\n```\r\n\r\n##### Resting-state fMRI\r\n\r\nExtraction of time series for resting-state fMRI is less complicated via the `get_rest_timeseries` function:\r\n\r\n```\r\nts_dict = get_rest_timeseries(subj_ids, data_dir)\r\n\r\n# Save time series files in directory \"REST\"\r\nsave_data_dict(ts_dict, path=\"./REST\")\r\n```\r\n\r\n\r\n#### Computation of correlation matrices\r\n\r\nWe continue in our Python shell to compute correlation matrices:\r\n\r\n``` python\r\n# We load the saved time series from the previous step\r\ncd GAMBLING_win # save_data_dict() writes file names into file \"ts_files\" but without paths,\r\n                # thus the easiest way is to change into the directory containing the files\r\ntime_series, time_series_files = load_time_series(\"./ts_files\") # ... and read them from there\r\n\r\ncorrelation_measure, correlation_matrices = compute_correlations(time_series, kind='tangent')\r\n# Tangent in our experience provides the best results, but there are alternatives:\r\n# https://nilearn.github.io/dev/modules/generated/nilearn.connectome.ConnectivityMeasure.html\r\n\r\n# We then save the matrices into a single file in the NIfTI format for downstream processing\r\nsave_matrix(cifti_dim_to_nifti_dim(correlation_matrices), \"./GAMBLING_win-tangent.nii.gz\")\r\n```\r\n\r\n#### CPM\r\n\r\nFor actual CPM, we need to install and run [Ray](https://ray.io) (run this e.g. in your Python virtual environment as described in [Installation](#installation)):\r\n\r\n``` shell\r\n$ pip install ray\r\n$ ray start --head --num-cpus=16 # Run this on your main node.\r\n                                 # Processes take up a lot of RAM, be careful not to use too many CPUs\r\n```\r\n\r\nOptionally add more Ray nodes to form a cluster, see the [Ray documentation](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/on-premises.html) for details.\r\n\r\n##### Merging behavioral/biometric data in Python\r\n\r\nFor our analysis, we need values from both the unrestricted and [restricted HCP data](https://www.humanconnectome.org/study/hcp-young-adult/document/restricted-data-usage), which are available as separate CSV files. For easier handling, we merge them into a single CSV file:\r\n\r\n``` python\r\nimport pandas as pd\r\nunrestricted = pd.read_csv(\"/path/to/unrestricted.csv\")\r\nrestricted = pd.read_csv(\"/path/to/restricted.csv\")\r\nmerged = pd.merge(restricted, unrestricted, on=\"Subject\")\r\nmerged.to_csv(\"./merged.csv\") # Save the merged DataFrame as a CSV file in the current directory\r\n```\r\n\r\n##### Loading and preparing data in Python\r\n\r\n``` python\r\nbehav = 'BMI' # Which variable do we want to predict?\r\ncovars = ['Age_in_Yrs', 'Gender' ] # What variables do we want to correct for?\r\n\r\nbehav_data = get_behav_data(\"./path/to/merged.csv\", ids) # Loading of behavioral and biometrical data as a Pandas DataFrame from a CSV file\r\n# We need to binarize gender for our analysis\r\nbehav_data[\"Gender\"] = behav_data[\"Gender\"].replace(\"F\", 0)\r\nbehav_data[\"Gender\"] = behav_data[\"Gender\"].replace(\"M\", 1)\r\n\r\nfunctional_data = convert_matrices_to_dataframe(nifti_dim_to_cifti_dim(get_nimg_data(\"./path/to/GAMBLING_win-tangent.nii.gz\")), subj_ids) # Loading of correlation matrices as Pandas DataFrame\r\n```\r\n\r\n##### Starting the Ray handler\r\n\r\nray_handler() is a Python class through which data management and the starting and coordination of Ray Actors (i.e. the processes working in parallel) is being handled.\r\n\r\n``` python\r\nn_folds = 128 # In this example, we use 128 folds, which is a good starting point\r\nn_perm = 1000 # How many permutations are we planning to do later on?\r\n\r\nray_handler = RayHandler(\r\n    functional_data.copy(),\r\n    behav_data.copy(),\r\n    behav,\r\n    covars,\r\n    address=\"auto\", # We assume that the main Ray process runs on the same host\r\n    n_perm=n_perm,\r\n) # You can safely ignore the PerformanceWarning messages\r\n\r\nray_handler.add_kfold_indices(n_folds, clean=True) # By setting \"clean\" to True, we remove twins from the fold so they don't predict each other\r\nray_handler.upload_data() # Functional and behavioral data are uploaded into the shared storage to which Ray Actors have access\r\n```\r\n\r\n##### Starting the analysis\r\n\r\nFirst we define the jobs for the actors; a job is a Python list object consisting of the following items: \"job type\", \"fold number\", \"permutation number\". The permutation number for the actual, i.e. unpermutated, data is \"-1\".\r\n\r\n``` python\r\njob_list = [[\"fselection_and_prediction\", fold, perm] for perm in [-1] for fold in range(n_folds)] # This is the job list without permutations\r\n\r\n# If we wanted to also compute the permutations (which takes a very long time), the job list can be created as follows:\r\n#job_list = [[\"fselection_and_prediction\", fold, perm] for perm in [-1, *range(n_perm)] for fold in range(n_folds)]\r\n\r\nray_handler.start_actors(job_list) # Start computing\r\n```\r\n\r\n##### Monitoring progress and retrieving results\r\n\r\n``` python\r\nn_done, n_remaining, n_held = ray_handler.status() # Prints a status report (see screenshot)\r\n\r\nresults = ray_handler.get_results(n=100) # Retrieving a huge number of results (e.g. when performing permutation analysis)\r\n                                         # and especially from distributed Ray actors can take a long time. Specifying the\r\n                                         # number of results (e.g. n=100) to be retrieved from the results store at once\r\n                                         # allows for a display of progress\r\n\r\n# Optional: Save results into a file (NumPy format)\r\nnp.save(\"./GAMBLING_win-results.npy\", results)\r\n\r\n# This file can be used to restore results\r\nresults = np.load(\"./GAMBLING_win-results.npy\", allow_pickle=True).item()\r\n```\r\n\r\nYou might consider fetching results and saving them periodically with a simple loop:\r\n\r\n``` python\r\nsleep_time = 1200 # Sleep for 20 minutes and then rerun loop\r\nn_remaining = 1 # Set to something > 0 to get the loop started\r\nresults_path = get_safe_path(\"./GAMBLING_win-results\", \".npy\")\r\nwhile n_remaining > 0: # Run until no jobs to be fetched are remaining\r\n    n_done, n_remaining, n_held = ray_handler.status()\r\n    if n_held > 0:\r\n        results = ray_handler.get_results(n=100)\r\n        # BEWARE: the file in results_path will be overwritten without asking\r\n        # but we have used get_safe_path for risk mitigation\r\n        print(\"\\nSaving results to {}...\".format(results_path), end='', flush=True)\r\n        np.save(results_path, results)\r\n        print(\" done.\")\r\n    else:\r\n        print(\"\\nNo results to fetch and save.\")\r\n    print(\"Sleeping for {} seconds...\".format(sleep_time))\r\n    sleep(sleep_time) # Will sleep for sleep_time seconds\r\n```\r\n\r\n\r\n#### Check for completion\r\n\r\nRarely single jobs or actors die before completion. Having run your analyses, you can check your results for completion as follows and rerun analyses as needed (the function ```check_for_completion``` will advise you on how do this):\r\n\r\n``` python\r\nincomplete_jobs = check_for_completion(results)\r\n```\r\n\r\n#### Presenting results\r\n\r\nTo plot observed values against values as predicted by the GLM, use plot_predictions():\r\n\r\n``` python\r\nperm = -1 # Permutation -1 selects unpermutated results\r\ng = plot_predictions(results['prediction_results'][perm], tail=\"glm\", color=\"green\")\r\n```\r\n\r\n\r\nTo plot permutation results, use plot_permutation_results(), which will take either a list of paths to saved results files (f.i. when you want to combine multiple permutation runs) or the prediction results dictionary, which we will use in the following example:\r\n\r\n``` python\r\n# plot_permutation_results() will automatically remove incomplete permutations\r\n# and return/print basic descriptive statistics (minimum and maximum r value, total\r\n# number of permutations, and the minimum p value to be achieved with these permutations\r\n\r\nplot, min_value, max_value, count, min_p = plot_permutation_results(results['prediction_results'])\r\n```\r\n\r\nFor more presentation and plotting examples including static and interactive plots of predictive networks, see ```save.py``` in folder ```utils```.\r\n\r\n#### Overlap of predictive networks\r\n\r\ncpm_ray.py has a function ```get_overlap()``` to create overlap networks of different predictive networks. For example, if you have two different CPM results as generated by ```results = ray_handler.get_results()``` and saved in two files ```results_A.npy``` and ```results_B.npy```, you can do the following:\r\n\r\n``` python\r\n# Load results into Python\r\nresults_A = np.load(\"/path/to/results_A.npy\", allow_pickle=True).item()\r\nresults_B = np.load(\"/path/to/results_B.npy\", allow_pickle=True).item()\r\n\r\n# Load coordinates of parcels for plotting\r\ncoords = np.loadtxt(\"/path/to/hcp-suite/data/parcellations/RenTianGlasser.coords\")\r\n\r\n# Use overlap() function, which takes as its main input a list of results, in this case\r\n# [results_A, results_B]. You can specify a result as the odd one out (e.g. odd_one_out=0\r\n# for results_A), which means that result_A's tails will be switched, so that the positive\r\n# tail result A will be overlapped with the negative tail of result B and vice versa.\r\n#\r\n# overlap() returns a bunch of dictionaries with the tail (positive and negative predictive\r\n# networks)  as a primary key and usually several subkeys. These are more or less\r\n# self-explanatory.\r\n\r\noverlap, degrees_sorted, top_n_edges, plot_dict, m_cons = get_overlap([results_A, results_B], odd_one_out=None, coords=coords, plot_top_nodes=True, top_n_edges=50)\r\n\r\n```\r\n",
        "doi": "10.48546/workflowhub.workflow.1234.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1234",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1234?version=1",
        "name": "Connectome-based predictive modeling (CPM) with Python using Ray for parallel processing",
        "number_of_steps": 0,
        "projects": [
            "CPM"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cpm",
            "connectome-based predictive modeling",
            "mri",
            "fmri"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2025-01-02",
        "versions": 1
    },
    {
        "create_time": "2024-12-18",
        "creators": [
            "Marie-Emilie  Gauthier",
            "Craig Windell",
            "Magdalena Antczak",
            "Roberto Barrero"
        ],
        "description": "# ONTViSc (ONT-based Viral Screening for Biosecurity)\r\n\r\n## Introduction\r\neresearchqut/ontvisc is a Nextflow-based bioinformatics pipeline designed to help diagnostics of viruses and viroid pathogens for biosecurity. It takes fastq files generated from either amplicon or whole-genome sequencing using Oxford Nanopore Technologies as input.\r\n\r\nThe pipeline can either: 1) perform a direct search on the sequenced reads, 2) generate clusters, 3) assemble the reads to generate longer contigs or 4) directly map reads to a known reference. \r\n\r\nThe reads can optionally be filtered from a plant host before performing downstream analysis.\r\n\r\n## Pipeline overview\r\n- Data quality check (QC) and preprocessing\r\n  - Merge fastq files (Fascat, optional)\r\n  - Raw fastq file QC (Nanoplot)\r\n  - Trim adaptors (PoreChop ABI - optional)\r\n  - Filter reads based on length and/or quality (Chopper - optional)\r\n  - Reformat fastq files so read names are trimmed after the first whitespace (bbmap)\r\n  - Processed fastq file QC (if PoreChop and/or Chopper is run) (Nanoplot)\r\n- Host read filtering\r\n  - Align reads to host reference provided (Minimap2)\r\n  - Extract reads that do not align for downstream analysis (seqtk)\r\n- QC report\r\n  - Derive read counts recovered pre and post data processing and post host filtering\r\n- Read classification analysis mode\r\n- Clustering mode\r\n  - Read clustering (Rattle)\r\n  - Convert fastq to fasta format (seqtk)\r\n  - Cluster scaffolding (Cap3)\r\n  - Megablast homology search against ncbi or custom database (blast)\r\n  - Derive top candidate viral hits\r\n  - Align reads back to top reference and derive coverage statistics (mosdepth and coverM)\r\n- De novo assembly mode\r\n  - De novo assembly (Canu or Flye)\r\n  - Megablast homology search against ncbi or custom database or reference (blast)\r\n  - Derive top candidate viral hits\r\n  - Align reads back to top reference and derive coverage statistics (mosdepth and coverM)\r\n- Read classification mode\r\n  - Option 1 Nucleotide-based taxonomic classification of reads (Kraken2, Braken)\r\n  - Option 2 Protein-based taxonomic classification of reads (Kaiju, Krona)\r\n  - Option 3 Convert fastq to fasta format (seqtk) and perform direct homology search using megablast (blast)\r\n- Map to reference mode\r\n  - Align reads to reference fasta file (Minimap2) and derive bam file and alignment statistics (Samtools)\r\n\r\nCode and detailed instructions can be found [here](https://github.com/eresearchqut/ontvisc). A comprehensive, step-by-step guide on setting up and executing the ONTViSc pipeline across three high-performance computing systems hosted by Australian research and computing facilities - Lyra (Queensland University of Technology), Gadi (National Computational Infrastructure), and Setonix (Pawsey) - utilising the Australian Nextflow Seqera Service, can be found [here](https://mantczakaus.github.io/ontvisc_hpc_seqera_service_guide/).\r\n\r\n## Authors\r\nMarie-Emilie Gauthier <gauthiem@qut.edu.au>  \r\nCraig Windell <c.windell@qut.edu.au>  \r\nMagdalena Antczak <magdalena.antczak@qcif.edu.au>  \r\nRoberto Barrero <roberto.barrero@qut.edu.au>  \r\n",
        "doi": "10.48546/workflowhub.workflow.683.3",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in description",
        "id": "683",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/683?version=3",
        "name": "ONTViSc (ONT-based Viral Screening for Biosecurity)",
        "number_of_steps": 0,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "bioinformatics",
            "nextflow",
            "ont",
            "virology",
            "virus",
            "blast",
            "singularity"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-12-18",
        "versions": 3
    },
    {
        "create_time": "2024-12-17",
        "creators": [
            "Qiang Ye"
        ],
        "description": "# ONT Artificial Deletion Filter-Delter\r\nA tool to filter short artificial deletion variations by Oxford Nanopore Technologies (ONT) R9 and R10 flow cells and chemistries.\r\n## Requirements\r\nThe tool has been tested on Ubuntu 20.04 with 256GB RAM, 64 CPU cores and a NVIDIA GPU with 48GB RAM. The minimal requirements should be >= 64GB RAM and a NVIDIA GPU with >= 8GB RAM. Other operating systems like Windows or Mac were not tested.\r\n\r\nONT softwares like [Guppy](https://community.nanoporetech.com/downloads), [Tombo](https://github.com/nanoporetech/tombo), and [ont-fast5-api](https://github.com/nanoporetech/ont_fast5_api) should be pre-installed before generating Tombo-resquiggled single-read fast5 files.\r\nUsers might run following commands to preprocess R9 fast5 files in shell terminal before running our pipeline. As these steps below need GPU support and might take a long time, our pipeline doesn't contain them.\r\n```bash\r\n#===basecalling the fast5 files===\r\nont-guppy/bin/guppy_basecaller -c ont-guppy/data/dna_r9.4.1_450bps_sup.cfg -i $fast5dir/barcode${barcode} -s guppy_sup_basecalled/barcode${barcode} -r --compress_fastq -x cuda:1,2 --gpu_runners_per_device 4 --chunks_per_runner 256 --num_callers 3 --fast5_out\r\n\r\n#===preprocessing R9 fast5 files===\r\nc=$(ls *.fast5 | wc -l)\r\ndeclare -i count=$c-1\r\n#multiread fast5 to single read fast5\r\nmulti_to_single_fast5 -i guppy_sup_basecalled/barcode${barcode}/workspace -s fast5_pass_single --threads 24 \r\n#copy to new directory\r\ncd fast5_pass_single\r\nmkdir all_single_fast5s\r\nfor ((j=0;j<=$count;j=j+1))\r\ndo\r\n  echo $j\r\n  cp -r ./$j/*.fast5 all_single_fast5s\r\n  rm -rf ./$j\r\ndone\r\n#align to reference genome via tombo resquiggle\r\ntombo resquiggle guppy_sup_basecalled/barcode${barcode}/workspace/fast5_pass_single/all_single_fast5s data/Refs/$refseq --processes 24 --overwrite --num-most-common-errors 5  --failed-reads-filename tombo_resquiggle_failed_fast5.txt\r\n```\r\nThe fast5 files in the directory named **all_single_fast5s** could be employed in downstream workflow, which equals the input parameter **Tombo_dir** in shell command line or config yaml (details listed in **Configure input parameters** section). \r\n## Installation\r\nThe tool runs via Snakemake workflows. Users must install workflow dependencies including [Snakemake](https://snakemake.readthedocs.io/en/latest/tutorial/tutorial.html) (**Version >= 7.3**) and [R](https://mirrors.tuna.tsinghua.edu.cn/CRAN/) before using the pipeline. The workflow dependencies, which stored in a file named environment.yaml, are listed as below:\r\n\r\nchannels:\r\n  - conda-forge\r\n  - bioconda\r\n  - anaconda\r\n\r\ndependencies:\r\n  - snakemake-minimal >=7.3\r\n  - graphviz\r\n  - seaborn\r\n  - numpy\r\n  - pandas\r\n  - h5py\r\n  - scipy\r\n  - samtools =1.15\r\n  - r-essentials\r\n  - r-base\r\n  - bioconductor-shortread\r\n  - r-stringr\r\n  - r-dplyr\r\n  - r-vegan \r\n\r\nUsers are suggested to use [Conda](https://docs.conda.io/en/latest/) or [Mamba](https://mamba.readthedocs.io/en/latest/user_guide/mamba.html) to install these dependencies. After users download the working directory containing all the necessary files, the following shell command could install **Delter** in a **conda** environment in less than half an hour.\r\n```bash\r\ncd /path/to/Delter/working/directory\r\nconda env create --name Delter --file environment.yaml\r\n```\r\nor\r\n```bash\r\ncd /path/to/Delter/working/directory\r\nconda create --name Delter\r\nconda activate Delter\r\nconda install -c bioconda -y snakemake-minimal>=7.3\r\nconda install -c anaconda -y numpy pandas \r\nconda install -c anaconda -y h5py seaborn\r\nconda install -c conda-forge -y scipy\r\nconda install -c bioconda -y samtools=1.15\r\nconda install -c conda-forge -y r-essentials r-base\r\nconda install -c conda-forge -y r-dplyr r-vegan r-stringr\r\nconda install -c bioconda -y bioconductor-shortread\r\n```\r\n\r\n## Activate and exit the environment\r\nTo activate the environment \r\n  ```bash\r\n  conda activate Delter\r\n  ```\r\nTo exit the environment (after finishing the usage of the pipeline), just execute\r\n  ```bash\r\n  conda deactivate\r\n  ```\r\n## Run the pipeline\r\nThe whole pipeline could handle ONT R9 and R10 sequencing data. The working directory contains file named `Delter.config.yaml`, which stores key input parameters for the workflow. For handling the VCF file(s) generated by [LoFreq](https://csb5.github.io/lofreq/), the **Delter.py** script should be used. For VCF file(s) generated by [Clair3](https://github.com/HKU-BAL/Clair3), the **Delter_clair3.py** script should be used.\r\n\r\nThe Demo data could be accessed via [figshare](https://doi.org/10.6084/m9.figshare.26093869.v1). User should modify the filepaths in the Delter.config.yaml in the snakemakeexample directory.\r\n\r\n### Configure input parameters for the workflow\r\nThere are two ways to configure input parameters for this workflow.\r\n\r\n(1) Via shell command line\r\n\r\nUsers could define customized input paramaters using **--config** option in Snakemake command line.\r\n```bash\r\nUsage:\r\nsnakemake -s Delter.py --cores 8 --config Ref=refname Num=5 Vcf=path/to/VCF Refseq=path/to/refseq Outdir=path/to/outputdir Bam=path/to/sorted/bam Tombo_dir=path/to/tombo_processed/fast5 Subsample=2000 Flowcell=R9 Strategy=Direct MRPPthres=0.001 HomoQthres=23 OtherQthres=20.6\r\nRef=refname                             The value of #CHROM in vcf file, e.g., 'Ref=chr1'\r\nNum=5                                   The number of bases up- and down-stream that are centered around the variation loci, default=5\r\nVcf=path/to/VCF                         The file path to vcf file, e.g., 'Vcf=/data/res/lofreq.vcf'\r\nRefseq=path/to/refseq                   The file path to reference sequence, e.g., 'Refseq=/database/COVID-19.fa'\r\nOutdir=path/to/outputdir                The file path storing the output results and intermediate files, e.g., 'Outdir=/data/res'\r\nBam=path/to/sorted/bam                  The file path to sorted bam files, e.g., 'Bam=/data/res/sorted.bam'\r\nTombo_dir=path/to/tombo_processed/fast5 The file path to tombo-resquiggled single fats5 files, e.g., 'Tombo_dir=/data/fast5'\r\nSubsample=2000                          The number to subsample from reads covering variation loci, should be larger than 200, default=2000\r\nFlowcell=R9                             The version of flow cell, should be R9 or R10, default=R9\r\nStrategy=Direct                         The sequencing strategy, should be Amplicon or Direct, default=Direct\r\nMRPPthres=0.001                         The threshold of MRPP A, default=0.001\r\nHomoQthres=23                           The threshold of homo-dels, default=23\r\nOtherQthres=20.6                        The threshold of other-dels, default=20.6\r\n```\r\n(2) Edit config.yaml\r\n\r\nUsers could also define customized input paramaters by editing config.yaml.\r\n```yaml\r\nBam: \"/public/data1/yefq/data/fast5/20220703_WGA_twist/processed/20230426_Guppy621_comparison/Sce20_guppy_sup_aligned.softclip_trimmed.endtrim10_minimap2_align.mapped.sorted.bam\"\r\nRef: \"Zymo_Saccharomyces_cerevisiae_Seq5_ref\"\r\nNum: \"5\"\r\nTombo_dir: \"/public/data1/yefq/data/fast5/20220703_WGA_twist/processed/20230426_guppy_sup_basecalled/Sce20/workspace/fast5_pass_single/all_single_fast5s\"\r\nSubsample: \"2000\"\r\nVcf: \"/public/data1/yefq/data/fast5/20220703_WGA_twist/processed/20230426_Guppy621_comparison/Sce20_guppy_sup_aligned.test.vcf\" \r\nRefseq: \"/public/data1/yefq/data/Refs/Zymo_Saccharomyces_cerevisiae_Seq5_ref.fa\" \r\nOutdir: \"/public/data1/yefq/data/fast5/20220703_WGA_twist/processed/20230426_Guppy621_comparison/snakemake-tutorial/data/test\" \r\nFlowcell: \"R9\"\r\nStrategy: \"Direct\"\r\nMRPPthres: \"0.001\"\r\nHomoQthres: \"23\"\r\nOtherQthres: \"20.6\"\r\n```\r\nUsers should note that, **config values can be overwritten via the command line** even when it has deen defined in the config.yaml.\r\n### Start a run\r\nOnce the work directory and configuration files are set up, users can run the pipeline as easy as invoking:\r\n```bash\r\ncd /path/to/Delter/working/directory\r\nconda activate Delter\r\nsnakemake -s Delter.py --cores 8\r\n```\r\nOther Snakemake-related parameters like **--cores** and **--configfile** could be checked via \r\n```bash\r\nsnakemake -h\r\n```\r\n### Output\r\nThere are several outputs according to the indexes used. \r\n\r\n**(1) target.upstream_downstream.bases.comparison.result.txt**. When the workflow used MRPP A, the main output is **target.upstream_downstream.bases.comparison.result.txt**, which contains (1) the variation locus position, (2) group1 (plus.match or minus.match, corresponding to forward-aligned reads supporting the reference allele and reverse-aligned reads supporting the reference allele), (3) group2 (plus.del or minus.del, corresponding to forward-aligned reads supporting the non-reference allele and reverse-aligned reads supporting the non-reference allele), (4) the number of reads supporting group1 (**should always be around or higher than 400 in direct sequencing**), (5) the number of reads supporting group2 (**should always be around or higher than 400 in direct sequencing**), (6) the mean current measurements of upstream and downstream config[\"Num\"] bases centered around variation locus of group1, (7) the mean current measurements of upstream and downstream config[\"Num\"] bases centered around variation locus of group2, (8) P values between current measurements of group1 and group2, (9) MRPP P values, (10) **MRPP A statistic, users could compare this value against the pre-set threshold (WTA/Amplicon sequencing: 0.01; direct sequencing: 0.001) in our article to decide whether the variation locus is artificial**.\r\n\r\n**(2) fq.Qscore.info.txt**. For R9 or R10 data, when sequencing depth is low, Q score might be used to identify artificial deletions. The main output is **fq.Qscore.info.txt**, which contains (1) the variation locus position, (2) group1 (corresponding to forward-aligned reads supporting the non-reference allele and reverse-aligned reads supporting the non-reference allele), (3) group2 (plus.match or minus.match, corresponding to corresponding to forward-aligned reads supporting the reference allele and reverse-aligned reads supporting the reference allele), (4) the number of reads supporting group1 (**should always be \u226520**), (5) the number of reads supporting group2 (**should always be \u226520**), (6) the mean Q scores of upstream and downstream config[\"Num\"] bases centered around variation locus of group1, **users could compare this value against the pre-set threshold in our article to decide whether the variation locus is artificial**, (7) the mean Q scores of upstream and downstream config[\"Num\"] bases centered around variation locus of group2, (8) the P values between group1 and group2.\r\n\r\n**(3) variant.info.txt**. This file stores basic information of each variation output by VCF, which is used by the workflow. The 2th-10th columns are identical to VCF. Users should note that DP4 could be lower than DP, and **choosing to use MRPP A or Q score mainly depends on DP4 field**. The 12th-17th columns represent the location of deletion (homo or non-homo), the 1-based strating position, the 0-based starting position of homo or non-homo region, the 0-based ending position of homo or non-homo region, the deletion length output by Variation caller, and the length of homo or non-homo region (= 15th-14th+1). Our workflow use a strict criteria to extract reads with and without deletions. For example, if a deletion lacks 3 bases relative to the reference, then reads supporting the non-reference allele should only contain 3-base deletions. Therefore, **some deletions may be omitted due to undesirable read numbers**.     \r\n\r\n**(4) MRPP.filtered.txt** or **Qscore.filtered.txt**. Users should note that before comparing the results to pre-set thresholds, they are strongly recommended to filter the 4th and 5th columns in the **target.upstream_downstream.bases.comparison.result.txt** or/and **fq.Qscore.info.txt** according to our article, or the result may be biased due to low sequencing depth. We have provided two accessory scripts bundled in the workflow to pre-filter the results based on sequencing depth and then to compare with user-defined threshold(s). Position(s) with flags marked as \"FP\" are predicted artificial deletions. If neither of the above two files are generated, it indicates that none of potential variations could be removed due to very low sequencing depth or very low threshold(s).\r\n\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.1205.2",
        "edam_operation": [
            "Statistical calculation"
        ],
        "edam_topic": [
            "Computational biology"
        ],
        "filtered_on": "binn* in description",
        "id": "1205",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1205?version=2",
        "name": "ONT Artificial Deletion Filter-Delter",
        "number_of_steps": 0,
        "projects": [
            "NkuyfqLab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "indels",
            "r",
            "workflows"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-12-17",
        "versions": 2
    },
    {
        "create_time": "2024-12-13",
        "creators": [
            "Kieran Atkins"
        ],
        "description": "# gimp-image-annotator\r\n*gimp-image-annotator or GI\u00c0, a lightweight GIMP plug-in to alllow for computer vision-assisted image annotation using the powerful GIMP selection toolbox.*\r\n\r\n**Installation**\r\n\r\nFollow the guide here: https://en.wikibooks.org/wiki/GIMP/Installing_Plugins to find how to install GIMP plug-ins on your system, save the file `image-annotator.py` in GIMP's plug-in folder. \r\n\r\nIn GIMP v2.x, the plug-in system relies on deprecated python2. On Windows, a version of python2 is included in the installation of GIMP, so you only need to follow the plug-in installation. On Linux, we recommend using the Flatpak version of GIMP, as it comes with the correct python2 binaries inlcluded. On Linux, the plug-in may need to be made executable with the command `chmod a+x /path/to/image-annotator.py` in order to be seen by GIMP.\r\n\r\n**Using the software**\r\nOnce installed, navigate to *Toolbox* then *Image Annotator*, add the labels you want, select one, use GIMP's selection tools (e.g. The Fuzzy Select tool - a guide can be found here: https://docs.gimp.org/en/gimp-tools-selection.html) to select an area (use Quick Mask or Shift+Q to quickly see the mask you have created). **Make sure antialisaing and feathering is off, you cannot turn it off for rectangle select however it isn't used**. Once you have your desired selected area, press *Save selected mask*. Repeat until all objects are annotated.\r\n\r\n**How do I use the data?**\r\n\r\n*gimp-image-annotator* saves a binary mask of each annotation, with class of mask stored in the `_annotations.json` file. The `_annotations.json` file is structured as followed\r\n\r\n\r\n`\r\n[{\r\n\"label\": \"label\",\r\n\"id\": \"0\",\r\n\"filename\": \"image.png\"\r\n}]\r\n`\r\n\r\nThe masks can be inputted using most image processing software. For example in `opencv-python` it would be:\r\n\r\n\r\n`\r\nimport cv2;\r\nmask = cv2.imread(PATH, cv2.IMREAD_GRAYSCALE)\r\n`\r\n",
        "doi": "10.48546/workflowhub.workflow.1229.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "1229",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1229?version=1",
        "name": "GIMP Image Annotator",
        "number_of_steps": 0,
        "projects": [
            "Into the deep"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Plug-in",
        "update_time": "2024-12-13",
        "versions": 1
    },
    {
        "create_time": "2024-12-10",
        "creators": [
            "Subina Mehta"
        ],
        "description": "Workflow for clinical metaproteomics database searching",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1225",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1225?version=1",
        "name": "clinicalmp-discovery/main",
        "number_of_steps": 24,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "fasta_merge_files_and_filter_unique_sequences",
            "filter_tabular",
            "search_gui",
            "query_tabular",
            "Grep1",
            "maxquant",
            "Cut1",
            "peptide_shaker",
            "Filter1",
            "Remove beginning1",
            "Grouping1",
            "msconvert",
            "tp_cat",
            "ident_params",
            "fasta_cli",
            "fasta2tab",
            "dbbuilder"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2024-12-10",
        "creators": [
            "Valentine Murigneux",
            "Mike Thang"
        ],
        "description": "The aim of this workflow is to handle the routine part of shotgun metagenomics data processing. The workflow is using the tools Kraken2 and Bracken for taxonomy classification and the KrakenTools to evaluate diversity metrics. This workflow was tested on Galaxy Australia. \r\nA How-to guide for the workflow can be found at:  https://github.com/vmurigneu/kraken_howto_ga_workflows/blob/main/pages/taxonomy_kraken2_wf_guide.md  ",
        "doi": "10.48546/workflowhub.workflow.1199.2",
        "edam_operation": [],
        "edam_topic": [
            "Metagenomics",
            "Taxonomy"
        ],
        "filtered_on": "edam",
        "id": "1199",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1199?version=2",
        "name": "Taxonomy classification using Kraken2 and Bracken",
        "number_of_steps": 29,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gucfg2galaxy",
            "metagenomics",
            "name:collection",
            "shotgun"
        ],
        "tools": [
            "Prepare alpha diversity summary file (paste sample identifiers and Simpson results)\nPaste1",
            "__RELABEL_FROM_FILE__",
            "krakentools_kreport2krona",
            "add_line_to_file",
            "Prepare alpha diversity summary file\nPaste1",
            "Fisher results contains a header line we want to exclude \"Fisher's alpha...loading\"\nShow tail1",
            "Prepare alpha diversity summary file (paste Simpson and Fisher results)\nPaste1",
            "cat_multiple",
            "collection_column_join",
            "Extract column name and fraction_total_reads from Bracken report\nCut1",
            "krakentools_combine_kreports",
            "collection_element_identifiers",
            "taxonomy_krona_chart",
            "krakentools_beta_diversity",
            "regex1",
            "kraken2",
            "est_abundance",
            "krakentools_alpha_diversity"
        ],
        "type": "Galaxy",
        "update_time": "2024-12-10",
        "versions": 2
    },
    {
        "create_time": "2024-08-28",
        "creators": [
            "Kate\u0159ina Storchmannov\u00e1"
        ],
        "description": "# Article abstract\r\n\r\nPermeability is an important molecular property in drug discovery, as it co-determines pharmacokinetics whenever a drug crosses the phospholipid bilayer, e.g., into the cell, in the gastrointestinal tract or across the blood-brain barrier. Many methods for the determination of permeability have been developed, including cell line assays, cell-free model systems like PAMPA mimicking, e.g., gastrointestinal epithelia or the skin, as well as the Black lipid membrane (BLM) and sub-micrometer liposomes. Furthermore, many in silico approaches have been developed for permeability prediction. Meta-analysis of publicly available databases for permeability data (MolMeDB and ChEMBL) was performed to establish their usability. Firstly, experimental data can only be measured between thresholds for the lowest and highest permeation rate obtainable within physical boundaries. These thresholds vary strongly between methods. Secondly, computed data do not obey these thresholds but, on the other hand, can produce incorrect results. Thirdly, even for the same method and molecule, there is often a strong discrepancy between individual measured values. These differences are based not only on the statistics but also on the varying approaches and evaluation of the measured data. Thus, when working with in-house measured or published permeability data, we recommend to be cautious with their interpretation.\r\n\r\n# Keywords\r\nmembrane, permeability, PAMPA, BLM, liposome, CACO-2, MDCK, PerMM, COSMOperm, MolMeDB\r\n\r\n# Please cite the original article:\r\n\r\nStorchmannov\u00e1 K, Balouch M, Jura\u010dka J, \u0160t\u011bp\u00e1nek F, Berka K. Meta-analysis of permeability literature data shows possibilities and limitations of popular methods. ChemRxiv. 2024;[ doi:10.26434/chemrxiv-2024-ndc8k-v2](http://chemrxiv.org/engage/chemrxiv/article-details/66d0e15ff3f4b052908e1f79)\r\n\r\nNote that this article is currently a preprint and has not undergone peer review. This section will be updated once the article is published.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Cheminformatics",
            "Data management",
            "Data mining",
            "Pharmacology"
        ],
        "filtered_on": "metap* in name",
        "id": "1109",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1109?version=1",
        "name": "Meta-analysis of permeability literature data shows possibilities and limitations of popular methods",
        "number_of_steps": 0,
        "projects": [
            "Chemical Data Lab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cheminformatics",
            "databases"
        ],
        "tools": [
            "Konstanz Information Miner (KNIME)",
            "RDKit",
            "MolMeDB"
        ],
        "type": "KNIME",
        "update_time": "2025-01-14",
        "versions": 1
    },
    {
        "create_time": "2024-11-04",
        "creators": [
            "Diego De Panis"
        ],
        "description": "**Assembly Evaluation for ERGA-BGE Reports**\r\n\r\n_One Assembly, Illumina WGS reads + HiC reads_\r\n\r\nThe workflow requires the following:\r\n* Species Taxonomy ID number\r\n* NCBI Genome assembly accession code\r\n* BUSCO Lineage\r\n* WGS accurate reads accession code\r\n* NCBI HiC reads accession code\r\n\r\nThe workflow will get the data and process it to generate genome profiling (genomescope, smudgeplot -optional-), assembly stats (gfastats), merqury stats (QV, completeness), BUSCO, snailplot, contamination blobplot, and HiC heatmap.\r\n\r\n**Use this workflow for ONT-based assemblies where the WGS accurate reads are Illumina PE**",
        "doi": null,
        "edam_operation": [
            "Genome assembly"
        ],
        "edam_topic": [
            "Genomics"
        ],
        "filtered_on": "profil* in description",
        "id": "1103",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1103?version=3",
        "name": "ERGA-BGE Genome Report ASM analyses (one-asm WGS Illumina PE + HiC)",
        "number_of_steps": 45,
        "projects": [
            "ERGA Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [
            "biodiversity",
            "genome assembly",
            "genomics",
            "qc"
        ],
        "tools": [
            "blobtoolkit",
            "CONVERTER_fasta_to_fai",
            "cooler_cload_tabix",
            "pairtools_sort",
            "Cut1",
            "gfastats",
            "sambamba_merge",
            "bg_diamond",
            "bedtools_makewindowsbed",
            "pick_value",
            "meryl",
            "lftp",
            "busco",
            "fastp",
            "tp_text_file_with_recurring_lines",
            "hicexplorer_hicplotmatrix",
            "__FLATTEN__",
            "merqury",
            "cooler_csort_tabix",
            "bwa_mem2",
            "sambamba_flagstat",
            "hicexplorer_hicmergematrixbins",
            "datasets_download_genome",
            "pairtools_parse",
            "fasterq_dump",
            "pairtools_split",
            "__EXTRACT_DATASET__",
            "genomescope",
            "smudgeplot",
            "pairtools_dedup",
            "collapse_dataset",
            "sam_merge2",
            "rseqc_bam_stat"
        ],
        "type": "Galaxy",
        "update_time": "2024-12-05",
        "versions": 3
    },
    {
        "create_time": "2024-11-04",
        "creators": [
            "Diego De Panis"
        ],
        "description": "**Assembly Evaluation for ERGA-BGE Reports**\r\n\r\n_One Assembly, HiFi WGS reads + HiC reads_\r\n\r\nThe workflow requires the following:\r\n* Species Taxonomy ID number\r\n* NCBI Genome assembly accession code\r\n* BUSCO Lineage\r\n* WGS accurate reads accession code\r\n* NCBI HiC reads accession code\r\n\r\nThe workflow will get the data and process it to generate genome profiling (genomescope, smudgeplot -optional-), assembly stats (gfastats), merqury stats (QV, completeness), BUSCO, snailplot, contamination blobplot, and HiC heatmap.\r\n\r\n**Use this workflow for HiFi-based assemblies where the WGS accurate reads are PacBio HiFi**",
        "doi": null,
        "edam_operation": [
            "Genome assembly"
        ],
        "edam_topic": [
            "Genomics"
        ],
        "filtered_on": "profil* in description",
        "id": "1104",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1104?version=2",
        "name": "ERGA-BGE Genome Report ASM analyses (one-asm HiFi + HiC)",
        "number_of_steps": 45,
        "projects": [
            "ERGA Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genome assembly",
            "genomics",
            "qc"
        ],
        "tools": [
            "blobtoolkit",
            "CONVERTER_fasta_to_fai",
            "cooler_cload_tabix",
            "pairtools_sort",
            "Cut1",
            "gfastats",
            "sambamba_merge",
            "bedtools_makewindowsbed",
            "pick_value",
            "meryl",
            "lftp",
            "busco",
            "fastp",
            "tp_text_file_with_recurring_lines",
            "hicexplorer_hicplotmatrix",
            "__FLATTEN__",
            "merqury",
            "cooler_csort_tabix",
            "bwa_mem2",
            "rseqc_bam_stat",
            "sambamba_flagstat",
            "hicexplorer_hicmergematrixbins",
            "datasets_download_genome",
            "minimap2",
            "pairtools_parse",
            "fasterq_dump",
            "pairtools_split",
            "__EXTRACT_DATASET__",
            "cutadapt",
            "genomescope",
            "smudgeplot",
            "pairtools_dedup",
            "collapse_dataset",
            "sam_merge2",
            "bg_diamond"
        ],
        "type": "Galaxy",
        "update_time": "2024-12-05",
        "versions": 2
    },
    {
        "create_time": "2024-12-05",
        "creators": [
            "Yin Zhang",
            "Lin Tang"
        ],
        "description": "# M6Allele Pipeline & M6Allele algorithm\r\n\r\n## Introduction\r\nWe have developed an algorithm called **M6Allele** for identifying allele-specific m6A modifications. To facilitate its usage by researchers, we have also encapsulated our analysis process into a pipeline. You can learn more about the pipeline and the algorithm's usage from the following two modules:\r\n* [Pipeline](#m6allele-pipeline)\r\n* [M6Allele algorithm](#m6allele-10)\r\n\r\n## M6Allele Pipeline\r\n### PARAMETER INTRODUCTION\r\n* `-g/--gtf` : required, the file name of your own GTF file\r\n* `-fa/--fasta` : required, the file name of your own reference genome file\r\n* `-sf/--skip_fastqc` : optional, whether to skip the fastqc phase. Default false\r\n* `-se/--single_end` : required, the fastq files are single-end or paired-end sequencing\r\n* `-vg/--varscan_or_gatk` : optional, use VarScan or GATK to call snp, v: VarScan, g: GATK. Default v\r\n* `-f/--function` : required, the function of M6Allele, including `AseGeneDetection`, `AsmPeakDetection`, `SampleSpecificASM`. Please refer to [M6Allele 1.0](#m6allele-10) for specific explanation\r\n* `-s/--sample` : required, the name of the file containing the sample name to be processed\r\n* `-gzip/--is_gzip` : required, whether the fastq file is compressed\r\n* `-db/--dbSnp` : optional, the name of dbSNP vcf file\r\n* `-h/--help` : help message of the pipeline\r\n\r\n### USAGE\r\n#### Overview\r\n1. Install [docker(v24.0.7)](https://www.docker.com/get-started/)\r\n2. Download a compressed docker image file from this [link](https://renlab.oss-cn-shenzhen.aliyuncs.com/M6Allele/m6allelepipe.tar.gz) and import it using the following command:\r\n   ```shell\r\n      cd your_compressed_file_directory\r\n      gunzip m6allelepipe.tar.gz\r\n      docker load -i m6allelepipe.tar\r\n   ```\r\n    * If you're unable to down the image from above link, you can download the required files for local packaging images from [here](https://renlab.oss-cn-shenzhen.aliyuncs.com/M6Allele/docker.tar.gz) and then build the image locally\r\n        ```shell\r\n            # build command\r\n            docker build -t your_image_name .\r\n        ```\r\n3. Assuming your current working directory is `your_work_directory`, you need to create the following subdirectories or files and place the required files:\r\n   * `fastq` : **required**, containing the fastq files you need to process\r\n   * `scripts` : **required**, containing three script files: main.sh, main.py and getPeakInMeT.R, which you can download from this [link](https://renlab.oss-cn-shenzhen.aliyuncs.com/M6Allele/scripts.tar.gz)\r\n   * `reference` : **required**, containing references files required for the processing workflow\r\n        * `your_gtf_file.gtf` : we provide our default fasta file, you can download it from [this](https://renlab.oss-cn-shenzhen.aliyuncs.com/M6Allele/gtfAndfasta.tar.gz)\r\n        * `your_reference_genome_file.fa` : we provide our default gtf file, you can download it from [this](https://renlab.oss-cn-shenzhen.aliyuncs.com/M6Allele/gtfAndfasta.tar.gz)\r\n        * If you want to use GATK for SNP calling, you need to provide the dbSNP dataset. Here, we provide our default dbSNP dataset. You can download the `GCF_000001405.39.dbsnp.vcf.gz`from [here](https://renlab.oss-cn-shenzhen.aliyuncs.com/M6Allele/GCF_000001405.39.dbsnp.vcf.gz) and `GCF_000001405.39.dbsnp.vcf.gz.tbi` files from [here](https://renlab.oss-cn-shenzhen.aliyuncs.com/M6Allele/GCF_000001405.39.dbsnp.vcf.gz.tbi) and place them in the `reference` folder. If you want to use a different version of the dbSNP dataset, please follow these steps:\r\n          * Download the GCF_XXX.vcf.gz and GCF_XXX.vcf.gz.tbi files from the [dbSNP database](https://ftp.ncbi.nih.gov/snp/latest_release/VCF/), and download the [chromosome conversion files](https://ftp.ncbi.nih.gov/genomes/all/GCF/000/001/405/) corresponding to the above-mentioned files. Process the downloaded dbSNP files using the following commands: \r\n              ```shell\r\n                # convert the chromosome names in the GCF_XXX.vcf.gz file to 1, 2, ..., X, Y\r\n                bcftools annotate --rename-chrs chromosome_conversion.txt --threads 10 -Oz -o your_new_process_dbsnp_file.vcf.gz your_downloaded_dbsnp_file.vcf.gz\r\n                # generate .tbi file\r\n                bcftools index -t your_new_process_dbsnp_file.vcf.gz\r\n              ```\r\n          * Move the resulting new dbSNP database `your_new_process_dbsnp_file.vcf.gz` and `your_new_process_dbsnp_file.vcf.gz.tbi` files to the `reference` folder\r\n          * You also need to provide .fai and .dict index files for the reference genome .fa file. If your fasta file was downloaded from the link we provided, you will also download the corresponding .fai and .dict files\r\n          * However, If you have your own fasta file, you can generate the corresponding .fai and .dict files using the following commands. Then move .fa and .dict file to `reference` folder\r\n            ``` shell\r\n                # Please ensure that the .fai, .dict, and .fa files have the same prefix in their filenames\r\n                # .fai generate command\r\n                samtools faidx your_reference_genome_file.fa\r\n                # .dict generate command\r\n                gatk CreateSequenceDictionary -R your_reference_genome_file.fa -O your_reference_genome_file.dict\r\n            ```\r\n   * `your_sample.txt`: **required**, containing the sample names you want to process, which are the prefixes of the fastq files. **Each line is separated by either space or tab.** According to the M6Allele function, there are three formats:\r\n        * `AseGeneDetection` : **Each line represents the name of an RNA-seq sample**. If there are multiple duplicates, use multiple lines to represent them\r\n        * `AsmPeakDetection` : **Each line consists of two columns, representing the INPUT sample name and the corresponding IP sample name for MeRIP-seq**. If there are multiple duplicates, use multiple lines to represent them\r\n        * `SampleSpecificASM` : **Each line consists of four columns, representing the INPUT sample name and the corresponding IP sample name for MeRIP-seq of sample 1, as well as the INPUT sample name and the corresponding IP sample name for MeRIP-seq of sample 2**. If there are multiple duplicates, use multiple lines to represent them\r\n\r\n### Specific Example:\r\nHere, we have listed several specific examples of using the pipeline. If you have other requirements, you can achieve them by combining different parameters.\r\n### 1. To use VarScan for calling SNPs and detecting ASE genes\r\n\r\n**data dependency:**\\\r\n`your_work_directory`: there are the following subfolders and files\r\n* `fastq` : It contains two files:\r\n  * input1.fastq.gz\r\n  * input2.fastq.gz\r\n* `scripts` : containing three script files: main.sh, main.py and getPeakInMeT.R\r\n* `reference` : \r\n  * your_fasta_file.fa\r\n  * your_gtf_file.gtf\r\n* `sample.txt` : It contains two lines:\r\n  * First line: input1\r\n  * Second line: input2\r\n\r\n**example:**\r\n```shell\r\n  docker run -v /path/to/your_work_directory:/data renlab303/m6allelepipe -f AseGeneDetection -s sample.txt -gzip true -se true -fa your_fasta_file.fa -g your_gtf_file.gtf\r\n```\r\n\r\n### 2. To use GATK for calling SNPs and detecting ASM m6A signals\r\n**data dependency:**\\\r\n`your_work_directory`: there are the following subfolders and files\r\n* `fastq` : It contains eight files: \r\n  * input1_1.fastq.gz\r\n  * input1_2.fastq.gz\r\n  * ip1_1.fastq.gz\r\n  * ip1_2.fastq.gz \r\n  * input2_1.fastq.gz\r\n  * input2_2.fastq.gz\r\n  * ip2_1.fastq.gz\r\n  * ip2_2.fastq.gz\r\n* `scripts` : containing three script files: main.sh, main.py and getPeakInMeT.R\r\n* `reference` :\r\n  * your_gtf_file.gtf\r\n  * your_fasta_file.fa\r\n  * your_fasta_file.fa.fai\r\n  * your_fasta_file.dict\r\n  * your_dbSNP_vcf_file.vcf.gz\r\n  * your_dbSNP_vcf_file.vcf.gz.tbi\r\n* `sample.txt` : It contains two lines\r\n  * input1&emsp;ip1\r\n  * input2&emsp;ip2\r\n\r\n**example:**\r\n```shell\r\n  docker run -v /path/to/your_work_directory:/data renlab303/m6allelepipe -f AsmPeakDetection -s sample.txt -gzip true -se false -g your_gtf_file.gtf -fa your_fasta_file.fa -vg g -db your_dbSNP_vcf_file.vcf.gz\r\n```\r\n\r\n### 3. To use GATK for calling SNPs and detecting Sample-specific ASM m6A signals\r\n**data dependency:**\\\r\n`your_work_directory`: there are the following subfolders and files\r\n* `fastq` : It contains eight files: \r\n  * sample1_input1.fastq.gz\r\n  * sample1_ip1.fastq.gz\r\n  * sample2_input1.fastq.gz\r\n  * sample2_ip1.fastq.gz\r\n  * sample1_input2.fastq.gz\r\n  * sample1_ip2.fastq.gz\r\n  * sample2_input2.fastq.gz\r\n  * sample2_ip2.fastq.gz \r\n* `scripts` : containing three script files: main.sh, main.py and getPeakInMeT.R\r\n* `reference` : It contains following files:\r\n    * your_gtf_file.gtf\r\n    * your_fasta_file.fa\r\n    * your_fasta_file.fa.fai\r\n    * your_fasta_file.dict\r\n    * your_dbSNP_vcf_file.vcf.gz\r\n    * your_dbSNP_vcf_file.vcf.gz.tbi\r\n* `sample.txt` : It contains two lines\r\n    * sample1_input1&emsp;sample1_ip1&emsp;sample2_input1&emsp;sample2_ip1\r\n    * sample1_input2&emsp;sample1_ip2&emsp;sample2_input2&emsp;sample2_ip2\r\n\r\n**example:**\r\n```shell\r\n  docker run -v /path/to/your_work_directory:/data renlab303/m6allelepipe -f SampleSpecificASM -s sample.txt -gzip true -se true -g your_gtf_file.gtf -fa your_fasta_file.fa -vg g -db your_sbSNP_vcf_file.vcf.gz\r\n```\r\n\r\n### Pipeline overview\r\n\r\nThis pipeline is built using shell scripts and integrates tools as follows:\r\n\r\n* **Quality control and preprocessing of raw data**\r\n    * [fastp](https://github.com/OpenGene/fastp): quality trimming and adapter clipping\r\n    * [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/): generate quality reports\r\n* **Build STAR index**\r\n  * [STAR](https://github.com/alexdobin/STAR): build index\r\n* **Read alignment**\r\n    * [STAR](https://github.com/alexdobin/STAR): Spliced Transcripts Alignment to a Reference\r\n    * [Samtools](http://www.htslib.org/): Reads sort and remove duplicates\r\n* **SNP calling**\r\n    * [VarScan](https://varscan.sourceforge.net/): Call SNPs from MeRIP-seq INPUT sample\r\n    * [bcftools](http://www.htslib.org/doc/1.1/bcftools.html): mark the result of VarScan\r\n    * [vcftools](https://vcftools.github.io/): filter the result of bcftools\r\n    * [GATK](https://gatk.broadinstitute.org/hc/en-us/articles/360035531192-RNAseq-short-variant-discovery-SNPs-Indels): Call SNPs from MeRIP-seq INPUT sample\r\n* **Peak calling**\r\n    * [MeTPeak](https://github.com/compgenomics/MeTPeak): a novel, graphical model-based peak-calling method\r\n    * [BEDTools](https://bedtools.readthedocs.io/en/latest/): using \"mergeBed\" function\r\n* **ASE or ASM m6A detection**\r\n    * [M6Allele](#M6Allele 1.0): A toolkit for detection of allele-specific RNA N6-methyladenosine modifications \r\n\r\n## M6Allele 1.0\r\n\r\n### HARDWARE/SOFTWARE REQUIREMENTS\r\n* Java 1.8\r\n* Windows / Linux / Mac OS\r\n\r\n### INSTALLATION\r\n* clone the repo,\r\n```\r\ngit clone https://github.com/Jakob666/allele-specificM6A.git\r\n```\r\n* target JAR package\r\n```\r\ncd ./allele-specificM6A\r\n```\r\nmake sure the directory contains `M6Allele.jar`.\r\n\r\n### USAGE\r\n#### Overview\r\n#### Tools Introduction\r\nM6Allele.jar provides the following tools:\r\n\r\nTool | Function\r\n---|---\r\nAseGeneDetection|detect allele-specific expression (ASE) genes (one sample test)\r\nAsmPeakDetection|detect allele-specific modification (ASM) m6A signals  (one sample test)\r\nSampleSpecificASM|detect sample-specific ASM m6A signals  (paired sample test)\r\n\r\n#### parameters description\r\n1. **AseGeneDetection**\r\n   - `-vcf/--vcf_file` : required, VCF format file generate by RNA-seq or MeRIP-seq data SNP calling process\r\n   - `-g/--gtf` : required, GTF annotation file\r\n   - `-bam/--bam_files` : required, the alignment file of the FASTQ file, if you have multiple BAM files, please separate them with commas\r\n   - `-bai/--bai_files` : required, the index file of the bam file, If you have multiple BAI files, please separate them with commas\r\n   - `-o/--output` : optional, ASE gene test output file, default `./aseGene.txt`\r\n   - `-rc/--reads_coverage` : optional, reads coverage threshold using for filter RNA-seq or MeRIP-seq data SNVs in VCF file (aim for reducing FDR), default 10\r\n   - `-s/--sampling`: optional, MCMC sampling times, larger than 500, default 50000\r\n   - `-b/--burn` : optional, MCMC burn-in times, more than 100 and less than sampling times. Default 10000\r\n   - `-t/--thread` : optional, thread number for running test. Default 2\r\n   - `-h/--help` : help message of AseGeneDetection\r\n\r\n2. **AsmPeakDetection**\r\n   - `-bed/--peak_bed_file` : required, peak calling output result in BED format\r\n   - `-vcf/--vcf_file` : required, VCF format file generate by RNA-seq or MeRIP-seq data SNP calling process\r\n   - `-g/--gtf` : required, GTF annotation file\r\n   - `-inputBam/--input_bam_file` : required, the alignment file of the INPUT sample, if you have multiple BAM files, please separate them with commas\r\n   - `-inputBai/--input_bai_file` : required, the index file of the INPUT bam file, if you have multiple BAI files, please separate them with commas\r\n   - `-ipBam/--ip_bam_file` : required, the alignment file of the Ip sample\r\n   - `-ipBai/--ip_bai_file` : required, the index file of the Ip bam file\r\n   - `-o/--output` : optional, ASM m6A signal test output file, default `./asmPeak.txt`\r\n   - `-rc/--reads_coverage` : optional, reads coverage threshold using for filter RNA-seq or MeRIP-seq data SNVs in VCF file (aim for reducing FDR), default 10\r\n   - `-s/--sampling`: optional, MCMC sampling times, larger than 500, default 50000\r\n   - `-b/--burn` : optional, MCMC burn-in times, more than 100 and less than sampling times. Default 10000\r\n   - `-t/--thread` : optional, thread number for running test. Default 2\r\n   - `-h/--help` : help message of AsmPeakDetection\r\n   \r\n3. **SampleSpecificASM**\r\n   - `-s1Vcf/--sample1VcfFile` : required, VCF format file generate by sample1 RNA-seq or MeRIP-seq data SNP calling process\r\n   - `-s2Vcf/--sample2VcfFile` : required, VCF format file generate by sample2 RNA-seq or MeRIP-seq data SNP calling process\r\n   - `-bed/--mergePeakBedFile` : required, the merge result of peak calling for Sample 1 and Sample 2 in BED format\r\n   - `-g/--gtf` : required, GTF annotation file\r\n   - `-s1InputBam/--s1InputBamFiles` : required, the alignment file of the sample1 INPUT sample, if you have multiple BAM files, please separate them with commas\r\n   - `-s1InputBai/--s1InputBaiFiles` : required, the index file of the sample1 INPUT sample bam file, if you have multiple BAI files, please separate them with commas\r\n   - `-s2InputBam/--s2InputBamFiles` : required, the alignment file of the sample2 INPUT sample, if you have multiple BAM files, please separate them with commas\r\n   - `-s2InputBai/--s2InputBaiFiles` : required, the index file of the sample2 INPUT sample bam file, if you have multiple BAI files, please separate them with commas\r\n   - `-s1IpBam/--s1IpBamFiles` : required, the alignment file of the sample1 Ip sample, if you have multiple BAM files, please separate them with commas\r\n   - `-s1IpBai/--s1IpBaiFiles` : required, the index file of the sample1 Ip sample bam file, if you have multiple BAI files, please separate them with commas\r\n   - `-s2IpBam/--s2IpBamFiles` : required, the alignment file of the sample2 Ip sample, if you have multiple BAM files, please separate them with commas\r\n   - `-s2IpBai/--s2IpBaiFiles` : required, the index file of the sample2 Ip sample bam file, if you have multiple BAI files, please separate them with commas\r\n   - `-o/--output` : optional, Sample-specific test output directory, default `.`\r\n   - `-rc/--reads_coverage` : optional, reads coverage threshold using for filter RNA-seq or MeRIP-seq data SNVs in VCF file (aim for reducing FDR), default 10\r\n   - `-s/--sampling`: optional, MCMC sampling times, larger than 500, default 50000\r\n   - `-b/--burn` : optional, MCMC burn-in times, more than 100 and less than sampling times. Default 10000\r\n   - `-t/--thread` : optional, thread number for running test. Default 2\r\n   - `-threshold/--significantThreshold` : optional, the threshold for determining whether it is a sample-specific Asm6A modification, default 0.05\r\n   - `-h/--help` : help message of SampleSpecificASM\r\n\r\n\r\n### 1. Allele-specific expression (ASE) gene detection (one sample test)\r\n**data dependency**:\r\n1. VCF format file generate by SNV calling process of `RNA-seq data` or `MeRIP-seq INPUT data` (required, the format of the file is described below)\r\n2. GTF file (required)\r\n3. bam file (required)\r\n4. bai file (required, the index file of bam file)\r\n\r\n**examples**:\\\r\nsuppose here exists files below:\r\n1. human genome GTF file `/path/to/Homo_sapiens.GRCh38.93.chr.gtf`\r\n2. VCF format file generate by RNA data `/path/to/rna_filtered.vcf`\r\n3. bam files `/path/to/repeat1.bam (/path/to/repeat2.bam)`\r\n4. bai files `/path/to/repeat1.bam.bai (/path/to/repeat2.bam.bai)`\r\n\r\n* detect ASE gene\r\n```\r\n# command\r\njava -jar ./M6Allele.jar AseGeneDetection \r\n     -g /path/to/Homo_sapiens.GRCh38.93.chr.gtf \r\n     -vcf /path/to/rna_filtered.vcf \r\n     -bam /path/to/repeat1.bam,/path/to/repeat2.bam\r\n     -bai /path/to/repeat1.bam.bai,/path/to/repeat2.bam.bai\r\n     -o /path/to/output_file \r\n     -t 6\r\n```\r\n\r\n\r\n### 2. Allele-specific modification (ASM) m6A signal detection (one sample test)\r\n**data dependency**:\r\n1. GTF format file\r\n2. VCF format file generate by SNV calling process of `RNA-seq data` or `MeRIP-seq INPUT data` (required, the format of the file is described below)\r\n3. BED format peak calling result generate by `MeRIP-seq data` (required, the format of the file is described below)\r\n4. The bam file of `MeRIP-seq INPUT data` (required)\r\n5. The bai file of `MeRIP-seq INPUT data` (required)\r\n6. The bam file of `MeRIP-seq Ip data` (required)\r\n7. The bai file of `MeRIP-seq Ip data` (required)\r\n\r\n**examples**:\\\r\nsuppose here exists files below:\r\n1. human genome GTF file `/path/to/Homo_sapiens.GRCh38.93.chr.gtf`\r\n2. VCF format file generate by RNA data `/path/to/rna_filtered.vcf`\r\n3. BED format file generate by peak calling process `/path/to/peak.bed`\r\n4. Bam files generate by `MeRIP-seq INPUT data` `/path/to/repeat1_input.bam (/path/to/repeat2_input.bam)`\r\n5. Bai files generate by `MeRIP-seq INPUT data` `/path/to/repeat1_input.bam.bai (/path/to/repeat2_input.bam.bai)`\r\n6. Bam files generate by `MeRIP-seq Ip data` `/path/to/repeat1_ip.bam (/path/to/repeat2_ip.bam)`\r\n7. Bai files generate by `MeRIP-seq Ip data` `/path/to/repeat1_ip.bam.bai (/path/to/repeat2_ip.bam.bai)`\r\n\r\n* detect ASM m6A signal\r\n```\r\n# command\r\njava -jar ./M6Allele.jar AsmPeakDetection \r\n     -g /path/to/Homo_sapiens.GRCh38.93.chr.gtf \r\n     -inputBam /path/to/repeat1_input.bam,/path/to/repeat2_input.bam\r\n     -inputBai /path/to/repeat1_input.bam.bai,/path/to/repeat2_input.bam.bai\r\n     -ipBam /path/to/repeat1_ip.bam,/path/to/repeat2_ip.bam\r\n     -ipBai /path/to/repeat1_ip.bam.bai,/path/to/repeat2_ip.bam.bai\r\n     -bed /path/to/peak.bed \r\n     -vcf /path/to/rna_filtered.vcf \r\n     -o /path/to/output_file \r\n     -t 6\r\n```\r\n\r\n### 3. Sample-specific ASM m6A signal detection (paired sample test)\r\n**data dependency**:\r\n1. GTF format file\r\n2. paired sample VCF format files generate by SNV calling process of `RNA-seq data` or `MeRIP-seq INPUT data` (required, the format of the file is described below)\r\n3. paired sample BED format peak calling results generate by `MeRIP-seq data` (required, the format of the file is described below)\r\n   * After obtaining the m6A peak calling results for two samples separately, you need to merge the results using bedtools.\r\n4. paired sample Bam files generate by `MeRIP-seq INPUT data`(required)\r\n5. paired sample Bai files generate by `MeRIP-seq INPUT data`(required)\r\n6. paired sample Bam files generate by `MeRIP-seq Ip data`(required)\r\n7. paired sample Bai files generate by `MeRIP-seq Ip data`(required)\r\n\r\n**examples**:\\\r\nsuppose here exists files below:\r\n1. human genome GTF file `/path/to/Homo_sapiens.GRCh38.93.chr.gtf`\r\n2. paired sample VCF format files generate by RNA data `/path/to/sample1_rna_filtered.vcf` & `/path/to/sample2_rna_filtered.vcf`\r\n3. paired sample BED format files generate by peak calling process `/path/to/merge_peak.bed`\r\n4. paired sample Bam files generate by MeRIP-seq INPUT data `/path/to/sample1_repeat1_input.bam (/path/to/sample1_repeat2_input.bam)` & `/path/to/sample2_repeat1_input.bam (/path/to/sample2_repeat2_input.bam)`\r\n5. paired sample Bai files generate by MeRIP-seq INPUT data `/path/to/sample1_repeat1_input.bam.bai (/path/to/sample1_repeat2_input.bam.bai)` & `/path/to/sample2_repeat1_input.bam.bai (/path/to/sample2_repeat2_input.bam.bai)`\r\n6. paired sample Bam files generate by MeRIP-seq Ip data `/path/to/sample1_repeat1_ip.bam (/path/to/sample1_repeat2_ip.bam)` & `/path/to/sample2_repeat1_ip.bam (/path/to/sample2_repeat2_ip.bam)`\r\n7. paired sample Bai files generate by MeRIP-seq Ip data `/path/to/sample1_repeat1_ip.bam.bai (/path/to/sample1_repeat2_ip.bam.bai)` & `/path/to/sample2_repeat1_ip.bam.bai (/path/to/sample2_repeat2_ip.bam.bai)`\r\n\r\n* detect sample-specific ASM m6A signal\r\n```\r\n# command\r\njava -jar ./M6Allele.jar SampleSpecificASM \r\n     -g /path/to/Homo_sapiens.GRCh38.93.chr.gtf \r\n     -bed /path/to/merge_peak.bed\r\n     -s1Vcf /path/to/sample1_rna_filtered.vcf \r\n     -s2Vcf /path/to/sample2_rna_filtered.vcf \r\n     -s1InputBam /path/to/sample1_repeat1_input.bam,/path/to/sample1_repeat2_input.bam\r\n     -s1InputBai /path/to/sample1_repeat1_input.bam.bai,/path/to/sample1_repeat2_input.bam.bai\r\n     -s1IpBam /path/to/sample1_repeat1_ip.bam,/path/to/sample1_repeat2_ip.bam\r\n     -s1IpBai /path/to/sample1_repeat1_ip.bam.bai,/path/to/sample1_repeat2_ip.bam.bai\r\n     -s2InputBam /path/to/sample2_repeat1_input.bam,/path/to/sample2_repeat2_input.bam\r\n     -s2InputBai /path/to/sample2_repeat1_input.bam.bai,/path/to/sample2_repeat2_input.bam.bai\r\n     -s2IpBam /path/to/sample2_repeat1_ip.bam,/path/to/sample2_repeat2_ip.bam\r\n     -s2IpBai /path/to/sample2_repeat1_ip.bam.bai,/path/to/sample2_repeat2_ip.bam.bai\r\n     -o /path/to/output_dir\r\n     -t 6\r\n```\r\n\r\n### FORMAT DECLARATION\r\n### 1. VCF generate by SNP calling of RNA and MeRIP sequencing data\r\nAt least 2 columns,\r\n* \\#CHROM: chromosome number, `1,2,3,...X,Y,MT`\r\n* POS: mutation position\r\n* ID (optional): mutation ID, default `.`\r\n* REF (optional): reference nucleotide\r\n* ALT (optional): alternative nucleotide\r\n* QUAL (optional): quality score\r\n* FILTER (optional): `PASS` if SNP sites were filtered, default `.`\r\n* INFO (optional): additional information\r\n* FORMAT (optional): recording variant genotype information for the sample\r\n* Sample (optional): the corresponding data in `FORMAT` field.\r\n\r\n* example\r\n> \\#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tsample\\\r\n> 1\t3025531\t.\tT\tA\t64.28\tPASS\tAC=2;AF=1.00;AN=2;DP=2;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;QD=32.14;SOR=0.693\tGT:AD:DP:GQ:PL\t1/1:0,2:2:6:76,6,0\\\r\n> 1\t3037125\t.\tA\tC\t68.28\tPASS\tAC=2;AF=1.00;AN=2;DP=2;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;QD=34.14;SOR=0.693\tGT:AD:DP:GQ:PL\t1/1:0,2:2:6:80,6,0\\\r\n> 1\t5170624\t.\tA\tG\t434.6\tSnpCluster\tAC=1;AF=0.500;AN=2;BaseQRankSum=2.479;DP=17;ExcessHet=3.0103;FS=5.315;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=25.56;ReadPosRankSum=-1.640;SOR=0.662\tGT:AD:DP:GQ:PL\t0/1:5,12:17:99:442,0,142\\\r\n> 1\t85864585\t.\tT\tA,C\t771.02\tPASS\tAC=1,1;AF=0.500,0.500;AN=2;DP=20;ExcessHet=3.0103;FS=0.000;MLEAC=1,1;MLEAF=0.500,0.500;MQ=60.00;QD=31.86;SOR=1.022\tGT:AD:DP:GQ:PL\t1/2:0,5,14:19:99:788,579,564,209,0,167\r\n\r\n\r\n### 2. BED format file\r\nContains fields below, more details see [BED format demonstration UCSC](http://genome.ucsc.edu/FAQ/FAQformat#format1)\r\n* \\# chr: chromosome number, `1,2,3,...X,Y,MT`\r\n* chromStart: m6A signal start position on chromosome\r\n* chromEnd: m6A signal end position on chromosome\r\n* name: ENSEMBL gene ID\r\n* score: significant score(adjusted p.value), generate by peak calling tools, less than `0.05`\r\n* strand: `+` or `-`\r\n* thickStart: The starting position at which the feature is drawn thickly\r\n* thickEnd: The ending position at which the feature is drawn thickly\r\n* itemRgb: An RGB value of the form R,G,B (e.g. 255,0,0). If the track line itemRgb attribute is set to \"On\", this RBG value will determine the display color of the data contained in this BED line. \r\n* blockCount: sub-block number of the m6A signal peak, integer, `\u22651`\r\n* blockSizes: block size of each sub-block, separate by `,`\r\n* blockStarts: block start position on chromosome, separate by `,`\r\n\r\n> \\# chr\tchromStart\tchromEnd\tname\tscore\tstrand\tthickStart\tthickEnd\titemRgb\tblockCount\tblockSizes\tblockStarts\\\r\n> 1\t9747647\t9747845\tENSMUSG00000097893\t7.1e-05\t+\t9747647\t9747845\t0\t1\t198,\t0\\\r\n> 1\t16105773\t16105923\tENSMUSG00000025921\t4.9e-05\t+\t16105773\t16105923\t0\t1\t150,\t0\\\r\n> 1\t33739519\t33739819\tENSMUSG00000004768\t0.0032\t+\t33739519\t33739819\t0\t1\t300,\t0\\\r\n> 1\t34180162\t34180463\tENSMUSG00000026131\t0.00022\t+\t34180162\t34180463\t0\t1\t301,\t0\\\r\n> 1\t34306583\t34307612\tENSMUSG00000026131\t0.00038\t+\t34306583\t34307612\t0\t2\t68,283,\t0,746\r\n\r\n### OUTPUT FILE DESCRIPTION\r\n### 1. ASE gene detection output\r\nWhen the algorithm finishes running, the following files will be in your output folder:\r\n* error.log: error logs generated during program execution\r\n* logout.log: normal logs generated during program execution\r\n* snp_location.txt: the information of SNP loci used in the algorithmic computation process\r\n* aseGene.txt (if you specify an output filename, it will be the name you specified): the result of ASE gene detect\r\n   * geneId: The Ensembl ID of the gene\r\n   * geneName: The name of the gene being tested\r\n   * pValue: The p-value computed by the algorithm\r\n   * qValue: The result of the p-value after undergoing Benjamini-Hochberg (BH) correction\r\n   * snpNum: The number of SNP loci used in the algorithm for identifying the gene\r\n   * major/minorAlleleReads: The number of reads available for calculation on the major/minor allele of the gene\r\n   * majorAlleleFrequency: The frequency of the major allele\r\n   * majorAlleleBase: The major allele base at the SNP loci used for calculation\r\n\r\n### 2. ASM m6A signal detection output\r\nWhen the algorithm finishes running, the following files will be in your output folder:\r\n* error.log: error logs generated during program execution\r\n* logout.log: normal logs generated during program execution\r\n* snp_location.txt: the information of SNP loci used in the ASE algorithmic computation process\r\n* aseRes.txt: the ASE gene identified by the algorithm\r\n* peak_with_snp.txt: the information of SNP loci covered by the m6A peak\r\n* asmPeak.txt (if you specify an output filename, it will be the name you specified): the result of ASM m6A signal detect\r\n   * chr: Chromosome\r\n   * peakStart: The genomic position of the start point of the m6A peak\r\n   * peakEnd: The genomic position of the end point of the m6A peak\r\n   * geneId: The Ensembl ID of the gene to which the m6A peak belongs\r\n   * geneName: The name of the gene to which the m6A peak belongs\r\n   * pValue: The p-value computed by the algorithm\r\n   * qValue: The result of the p-value after undergoing Benjamini-Hochberg (BH) correction\r\n   * snpNum: The number of SNP loci used in the algorithm for identifying the m6A peak\r\n   * major/minorAlleleReads: The number of reads available for calculation on the major/minor allele of the peak\r\n   * majorAlleleFrequency: Major allele frequency, where the major allele is based on the genotype of the INPUT sample. **If this value is less than 0.5**, it indicates allele-specific m6A modification occurring on the minor allele. Conversely, if it's greater than 0.5, it indicates allele-specific m6A modification occurring on the major allele\r\n   * majorAlleleBase: The major allele base at the SNP loci covered by the identified m6A peak\r\n\r\n### 3. Sample-Specific ASM m6A signal detection output\r\nWhen the algorithm finishes running, the following files will be in your output folder:\r\n* error.log: error logs generated during program execution\r\n* logout.log: normal logs generated during program execution\r\n* sample1/snp_location.txt: the information of SNP loci used in the ASE algorithmic computation process for sample1\r\n* sample1/peak_with_snp.txt: the information of SNP loci covered by the m6A peak in sample1\r\n* sample2/snp_location.txt: the information of SNP loci used in the ASE algorithmic computation process for sample2\r\n* sample2/peak_with_snp.txt: the information of SNP loci covered by the m6A peak in sample2\r\n* sampleSpecificAsm6A.txt: the identification results of sample-specific ASM m6A signal\r\n    * chr: Chromosome\r\n    * peakStart: The genomic position of the start point of the m6A peak\r\n    * peakEnd: The genomic position of the end point of the m6A peak\r\n    * geneId: The Ensembl ID of the gene to which the m6A peak belongs\r\n    * geneName: The name of the gene to which the m6A peak belongs\r\n    * sample1MajorFrequency: The major allele frequency of peak m6A in sample 1, where the major allele is based on the genotype of the INPUT sample. **If this value is less than 0.5**, it indicates allele-specific m6A modification occurring on the minor allele. Conversely, if it's greater than 0.5, it indicates allele-specific m6A modification occurring on the major allele. **If the m6A peak has no snp loci in sample1 that can be used for calculation, it will be represented as -.**\r\n    * sample2MajorFrequency: The major allele frequency of peak m6A in sample 2, where the major allele is based on the genotype of the INPUT sample. **If this value is less than 0.5**, it indicates allele-specific m6A modification occurring on the minor allele. Conversely, if it's greater than 0.5, it indicates allele-specific m6A modification occurring on the major allele. **If the m6A peak has no snp loci in sample2 that can be used for calculation, it will be represented as -.**\r\n    * sample1MajorHaplotype: The major allele genotype within the m6A peak in sample1. **If the m6A peak has no snp loci in sample1 that can be used for calculation, it will be represented as -.**\r\n    * sample2MajorHaplotype: The major allele genotype within the m6A peak in sample2. **If the m6A peak has no snp loci in sample2 that can be used for calculation, it will be represented as -.**\r\n    * sample1PValue: \r\n        * If the m6A peak has no snp loci in sample1 that can be used for calculation, it will be represented as -\r\n        * If the specifically modified haplotypes of the m6A peak are the same in both sample1 and sample2, then the value represents the result recalculated using the data from both samples simultaneously\r\n        * If there is no resampling calculation performed and the peak has SNP loci available for calculation in sample1, then this value is calculated solely based on the data sampled from sample1\r\n    * sample1QValue: The Benjamini-Hochberg corrected value for the sample1PValue\r\n    * sample2PValue: \r\n        * If the m6A peak has no snp loci in sample2 that can be used for calculation, it will be represented as -\r\n        * If the specifically modified haplotypes of the m6A peak are the same in both sample1 and sample2, then the value represents the result recalculated using the data from both samples simultaneously\r\n        * If there is no resampling calculation performed and the peak has SNP loci available for calculation in sample2, then this value is calculated solely based on the data sampled from sample2\r\n    * sample2QValue: The Benjamini-Hochberg corrected value for the sample2PValue\r\n    * specificSample: In which sample does the peak have sample-specific ASM\r\n        * -: There is no sample-specific m6A modification in either of the two samples\r\n        * sample1: There is sample-specific m6A modification in sample1\r\n        * sample2: There is sample-specific m6A modification in sample2\r\n        * sample1/sample2: There is sample-specific m6A modification in both sample",
        "doi": "10.48546/workflowhub.workflow.1223.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1223",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1223?version=1",
        "name": "M6Allele",
        "number_of_steps": 0,
        "projects": [
            "RenLabBioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Docker",
        "update_time": "2024-12-05",
        "versions": 1
    },
    {
        "create_time": "2024-12-04",
        "creators": [
            "Kate Farquharson",
            "Gareth Price",
            "Simon Tang",
            "Anna Syme"
        ],
        "description": "Post-genome assembly quality control workflow using Quast, BUSCO, Meryl, Merqury and Fasta Statistics, with updates November 2024.\r\n\r\nWorkflow inputs: reads as fastqsanger.gz (not fastq.gz), and primary assembly.fasta. (To change reads format: click on the pencil icon next to the file in the Galaxy history, then \"Datatypes\", then set \"New type\" as fastqsanger.gz). Note: the reads should be those that were used for the assembly (i.e., the filtered/cleaned reads), not the raw reads. \r\n\r\nWhat it does:\r\nComputes read coverage. \r\nRuns Quast. \r\nRuns Fasta Statistics.\r\nRuns Meryl and Merqury. \r\nRuns Busco. (New default settings for BUSCO: lineage = eukaryota; for Quast: lineage = eukaryotes, genome = large.)\r\n\r\nWorkflow outputs:\r\nReports assembly stats into a table called metrics.tsv, including selected metrics from Fasta Stats, and read coverage; reports BUSCO versions and dependencies; and displays these tables in the workflow report.\r\n\r\nNote: a known bug is that sometimes the workflow report text resets to default text. \r\n\r\nTo check and restore: open the workflow in Galaxy for editing.\r\n\r\nClick on the \"Edit Report\" icon (top right, pencil icon). \r\n\r\nCopy and paste the following text into the workflow report, then exit this report page, then save the workflow.\r\n\r\n# Workflow Execution Report\r\n\r\nWorkflow name: Genome assessment post assembly\r\n\r\n## Genome assembly metrics\r\n\r\nSelected statistics from the workflow outputs. Additional metrics are available in other outputs in the history.\r\n\r\n```galaxy\r\nhistory_dataset_display(output=\"Genome assembly metrics\")\r\n```\r\n## Software\r\n\r\nBusco version and dependencies:\r\n\r\n```galaxy\r\nhistory_dataset_display(output=\"Busco and dependencies version\")\r\n```\r\n## Galaxy Australia\r\n\r\nThanks for using Galaxy! When you use Galaxy Australia to support your publication or project, please acknowledge its use with the following statement: \"This work is supported by Galaxy Australia, a service provided by the Australian Biocommons and its partners. The service receives NCRIS funding through Bioplatforms Australia and the Australian Research Data Commons (https://doi.org/10.47486/PL105), as well as The University of Melbourne and Queensland Government RICF funding.\"\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "403",
        "keep": true,
        "latest_version": 9,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/403?version=9",
        "name": "Genome-assessment-post-assembly",
        "number_of_steps": 35,
        "projects": [
            "Galaxy Australia"
        ],
        "source": "WorkflowHub",
        "tags": [
            "busco",
            "hifi",
            "merqury",
            "meryl",
            "qc",
            "quast",
            "hifiasm"
        ],
        "tools": [
            "filter_tabular",
            "Add_a_column1",
            "tp_grep_tool",
            "add_line_to_file",
            "Convert characters1",
            "Cut1",
            "fastq_to_fasta_python",
            "tp_sed_tool",
            "tp_replace_in_column",
            "fasta-stats",
            "merqury",
            "Paste1",
            "cat1",
            "quast",
            "datamash_transpose",
            "meryl",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2024-12-04",
        "versions": 7
    },
    {
        "create_time": "2024-12-03",
        "creators": [
            "Ekaterina Sakharova",
            "Tatiana Gurbich",
            "Martin Beracochea"
        ],
        "description": "# MGnify genomes catalogue pipeline\r\n\r\n[MGnify](https://www.ebi.ac.uk/metagenomics/) A pipeline to perform taxonomic and functional annotation and to generate a catalogue from a set of isolate and/or metagenome-assembled genomes (MAGs) using the workflow described in the following publication:\r\n\r\nGurbich TA, Almeida A, Beracochea M, Burdett T, Burgin J, Cochrane G, Raj S, Richardson L, Rogers AB, Sakharova E, Salazar GA and Finn RD. (2023) [MGnify Genomes: A Resource for Biome-specific Microbial Genome Catalogues.](https://www.sciencedirect.com/science/article/pii/S0022283623000724) <i>J Mol Biol</i>. doi: https://doi.org/10.1016/j.jmb.2023.168016\r\n\r\nDetailed information about existing MGnify catalogues: https://docs.mgnify.org/src/docs/genome-viewer.html\r\n\r\n### Tools used in the pipeline\r\n| Tool/Database                                                                                    | Version           | Purpose                                                                                                                |\r\n|--------------------------------------------------------------------------------------------------|-------------------|------------------------------------------------------------------------------------------------------------------------|\r\n| CheckM2                                                                                          | 1.0.1             | Determining genome quality                                                                                             |\r\n| dRep                                                                                             | 3.2.2             | Genome clustering                                                                                                      |\r\n| Mash                                                                                             | 2.3               | Sketch for the catalogue; placement of genomes into clusters (update only); strain tree                                |\r\n| GUNC                                                                                             | 1.0.3             | Quality control                                                                                                        |\r\n| GUNC DB                                                                                          | 2.0.4             | Database for GUNC                                                                                                      |\r\n| GTDB-Tk                                                                                          | 2.4.0             | Assigning taxonomy; generating alignments                                                                              |\r\n| GTDB                                                                                             | r220              | Database for GTDB-Tk                                                                                                   |\r\n| Prokka                                                                                           | 1.14.6            | Protein annotation                                                                                                     |\r\n| IQ-TREE 2                                                                                        | 2.2.0.3           | Generating a phylogenetic tree                                                                                         |\r\n| Kraken 2                                                                                         | 2.1.2             | Generating a kraken database                                                                                           |\r\n| Bracken                                                                                          | 2.6.2             | Generating a bracken database                                                                                          |\r\n| MMseqs2                                                                                          | 13.45111          | Generating a protein catalogue                                                                                         |\r\n| eggNOG-mapper                                                                                    | 2.1.11            | Protein annotation (eggNOG, KEGG, COG,  CAZy)                                                                          |\r\n| eggNOG DB                                                                                        | 5.0.2             | Database for eggNOG-mapper                                                                                             |\r\n| Diamond                                                                                          | 2.0.11            | Protein annotation (eggNOG)                                                                                            |\r\n| InterProScan                                                                                     | 5.62-94.0         | Protein annotation (InterPro, Pfam)                                                                                    |\r\n| kegg-pathways-completeness tool                                                                  | 1.0.5             | Computes KEGG pathway completeness                                                                                     |\r\n| CRISPRCasFinder                                                                                  | 4.3.2             | Annotation of CRISPR arrays                                                                                            |\r\n| AMRFinderPlus                                                                                    | 3.11.4            | Antimicrobial resistance gene annotation; virulence factors, biocide, heat, acid, and metal resistance gene annotation |\r\n| AMRFinderPlus DB                                                                                 | 3.11 2023-02-23.1 | Database for AMRFinderPlus                                                                                             |\r\n| antiSMASH                                                                                        | 7.1.0             | Biosynthetic gene cluster annotation                                                                                   |\r\n| GECCO                                                                                            | 0.9.8             | Biosynthetic gene cluster annotation                                                                                   |\r\n| SanntiS                                                                                          | 0.9.3.2           | Biosynthetic gene cluster annotation                                                                                   |\r\n| DefenseFinder                                                                                    | 1.2.0             | Annotation of anti-phage systems                                                                                       |\r\n| DefenseFinder models                                                                             | 1.2.3             | Database for DefenseFinder                                                                                             |\r\n| run_dbCAN                                                                                        | 4.1.2             | Polysaccharide utilization loci prediction                                                                             |\r\n| dbCAN DB                                                                                         | V12               | Database for run_dbCAN                                                                                                 |\r\n| Infernal                                                                                         | 1.1.4             | RNA predictions                                                                                                        |\r\n| tRNAscan-SE                                                                                      | 2.0.9             | tRNA predictions                                                                                                       |\r\n| Rfam                                                                                             | 14.9              | Identification of SSU/LSU rRNA and other ncRNAs                                                                        |\r\n| Panaroo                                                                                          | 1.3.2             | Pan-genome computation                                                                                                 |\r\n| Seqtk                                                                                            | 1.3               | Generating a gene catalogue                                                                                            |\r\n| VIRify                                                                                           | 2.0.1             | Viral sequence annotation                                                                                              |\r\n| [Mobilome annotation pipeline](https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline) | 2.0.2             | Mobilome annotation                                                                                                    |\r\n| samtools                                                                                         | 1.15              | FASTA indexing                                                                                                         |\r\n\r\n## Setup\r\n\r\n### Environment\r\n\r\nThe pipeline is implemented in [Nextflow](https://www.nextflow.io/).\r\n\r\nRequirements:\r\n- [singulairty](https://sylabs.io/docs/) or [docker](https://www.docker.com/)\r\n\r\n#### Reference databases\r\n\r\nThe pipeline needs the following reference databases and configuration files (roughtly ~150G):\r\n\r\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/gunc_db_2.0.4.dmnd.gz\r\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/eggnog_db_5.0.2.tgz\r\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/rfam_14.9/\r\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/kegg_classes.tsv\r\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/continent_countries.csv\r\n- https://data.ace.uq.edu.au/public/gtdb/data/releases/release214/214.0/auxillary_files/gtdbtk_r214_data.tar.gz\r\n- ftp://ftp.ncbi.nlm.nih.gov/pathogen/Antimicrobial_resistance/AMRFinderPlus/database/3.11/2023-02-23.1\r\n- https://zenodo.org/records/4626519/files/uniref100.KO.v1.dmnd.gz\r\n\r\n### Containers\r\n\r\nThis pipeline requires [singularity](https://sylabs.io/docs/) or [docker](https://www.docker.com/) as the container engine to run pipeline.\r\n\r\nThe containers are hosted in [biocontainers](https://biocontainers.pro/) and [quay.io/microbiome-informatics](https://quay.io/organization/microbiome-informatics) repository.\r\n\r\nIt's possible to build the containers from scratch using the following script:\r\n\r\n```bash\r\ncd containers && bash build.sh\r\n```\r\n\r\n## Running the pipeline\r\n\r\n## Data preparation\r\n\r\n1. You need to pre-download your data to directories and make sure that genomes are uncompressed. Scripts to fetch genomes from ENA ([fetch_ena.py](https://github.com/EBI-Metagenomics/genomes-pipeline/blob/master/bin/fetch_ena.py)) and NCBI ([fetch_ncbi.py](https://github.com/EBI-Metagenomics/genomes-pipeline/blob/master/bin/fetch_ncbi.py)) are provided and need to be executed separately from the pipeline. If you have downloaded genomes from both ENA and NCBI, put them into separate folders.\r\n\r\n2. When genomes are fetched from ENA using the `fetch_ena.py` script, a CSV file with contamination and completeness statistics is also created in the same directory where genomes are saved to. If you are downloading genomes using a different approach, a CSV file needs to be created manually (each line should be genome accession, % completeness, % contamination). The ENA fetching script also pre-filters genomes to satisfy the QS50 cut-off (QS = % completeness - 5 * % contamination).\r\n\r\n3. You will need the following information to run the pipeline:\r\n - catalogue name (for example, zebrafish-faecal)\r\n - catalogue version (for example, 1.0)\r\n - catalogue biome (for example, root:Host-associated:Human:Digestive system:Large intestine:Fecal)\r\n - min and max accession number to be assigned to the genomes (only MGnify specific). Max - Min = #total number of genomes (NCBI+ENA)\r\n\r\n### Execution\r\n\r\nThe pipeline is built in [Nextflow](https://www.nextflow.io), and utilized containers to run the software (we don't support conda ATM).\r\nIn order to run the pipeline it's required that the user creates a profile that suits their needs, there is an `ebi` profile in `nexflow.config` that can be used as template.\r\n\r\nAfter downloading the databases and adjusting the config file:\r\n\r\n```bash\r\nnextflow run EBI-Metagenomics/genomes-pipeline -c <custom.config> -profile <profile> \\\r\n--genome-prefix=MGYG \\\r\n--biome=\"root:Host-associated:Fish:Digestive system\" \\\r\n--ena_genomes=<path to genomes> \\\r\n--ena_genomes_checkm=<path to genomes quality data> \\\r\n--mgyg_start=0 \\\r\n--mgyg_end=10 \\\r\n--preassigned_accessions=<path to file with preassigned accessions if using>\r\n--catalogue_name=zebrafish-faecal \\\r\n--catalogue_version=\"1.0\" \\\r\n--ftp_name=\"zebrafish-faecal\" \\\r\n--ftp_version=\"v1.0\" \\\r\n--outdir=\"<path-to-results>\"\r\n```\r\n\r\n### Development\r\n\r\nInstall development tools (including pre-commit hooks to run Black code formatting).\r\n\r\n```bash\r\npip install -r requirements-dev.txt\r\npre-commit install\r\n```\r\n\r\n#### Code style\r\n\r\nUse Black, this tool is configured if you install the pre-commit tools as above.\r\n\r\nTo manually run them: black .\r\n\r\n### Testing\r\n\r\nThis repo has 2 set of tests, python unit tests for some of the most critical python scripts and [nf-test](https://github.com/askimed/nf-test) scripts for the nextflow code.\r\n\r\nTo run the python tests\r\n\r\n```bash\r\npip install -r requirements-test.txt\r\npytest\r\n```\r\n\r\nTo run the nextflow ones the databases have to downloaded manually, we are working to improve this.\r\n\r\n```bash\r\nnf-test test tests/*\r\n```\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "462",
        "keep": true,
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/462?version=3",
        "name": "MGnify genomes catalogue pipeline",
        "number_of_steps": 0,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "metagenomics",
            "nextflow"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-12-03",
        "versions": 3
    },
    {
        "create_time": "2024-12-02",
        "creators": [
            "Liang Cheng"
        ],
        "description": "# EnrichDO\r\n\r\n***EnrichDO*** is a double weighted iterative model by integrating the DO graph topology on a global scale. It was based on the latest annotations of the human genome with DO terms, and double weighted the annotated protein-coding genes. On one hand, to reinforce the saliency of direct gene-DO annotations, different initial weights were assigned to directly annotated genes and indirectly annotated genes, respectively. On the other hand, to detect locally most significant node between the parent and its children, less significant nodes were dynamically down-weighted.\r\n\r\n***EnrichDO*** exhibits higher accuracy that often yield more specific significant DO terms, which alleviate the over enriched problem. The input data are the protein-coding genes of the human genome, using the ENTREZID format of NCBI.\r\n\r\n## Installation\r\n\r\nTo install this package, start R (version \"4.4\"), BiocManager (version \"3.20\") and enter:\r\n\r\n``` r\r\nif (!require(\"BiocManager\", quietly = TRUE))\r\n    install.packages(\"BiocManager\")\r\n\r\n##Release version\r\nBiocManager::install(\"EnrichDO\")\r\n\r\n\r\n## Devel version\r\nBiocManager::install(version='devel')\r\nBiocManager::install(\"EnrichDO\")\r\n```\r\n\r\n**Note:** for other R and BiocManager versions, need to manually download the source code in the Bioconductor website for installation.\r\n\r\n## Example\r\n\r\nAfter installation, check vignettes with:\r\n\r\n``` r\r\nbrowseVigenttes(\"EnrichDO\")\r\n```\r\n\r\n**Run cases** are stored in inst/scripts/EnrichDO_exampleTest.R\r\n\r\nThe **input data case** is stored at inst/extdata/Alzheimer_curated.csv\r\n\r\n**Output example** of enrichment result is available in inst/examples/result.txt\r\n\r\nThe **thesis data** is in thesisData folder (<https://github.com/liangcheng-hrbmu/EnrichDO/tree/devel/thesisData>) and extdata_interpretation.txt explains the data source.\r\n",
        "doi": "10.48546/workflowhub.workflow.1221.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1221",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1221?version=1",
        "name": "EnrichDO: a Global Weighted Model for Disease Ontology Enrichment Analysis",
        "number_of_steps": 0,
        "projects": [
            "EnrichDO"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "R markdown",
        "update_time": "2024-12-02",
        "versions": 1
    },
    {
        "create_time": "2024-11-26",
        "creators": [
            "GalaxyP"
        ],
        "description": "This workflow will perform taxonomic and functional annotations using Unipept and statistical analysis using MSstatsTMT.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1219",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1219?version=1",
        "name": "clinicalmp-data-interpretation/main",
        "number_of_steps": 6,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "msstatstmt",
            "unipept",
            "Grep1"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2024-11-26",
        "creators": [
            "Pratik Jagtap"
        ],
        "description": "In proteomics research, verifying detected peptides is essential for ensuring data accuracy and biological relevance. This tutorial continues from the clinical metaproteomics discovery workflow, focusing on verifying identified microbial peptides using the PepQuery tool.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1218",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1218?version=1",
        "name": "clinicalmp-verification/main",
        "number_of_steps": 19,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "dbbuilder",
            "uniprotxml_downloader",
            "query_tabular",
            "Cut1",
            "Filter1",
            "Remove beginning1",
            "Grouping1",
            "collapse_dataset",
            "pepquery2",
            "tp_cat",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2024-11-23",
        "creators": [
            "Sabrina Krakau",
            "Leon Kuchenbecker and Till Englert"
        ],
        "description": "From metagenomes to peptides",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in name",
        "id": "1217",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1217?version=1",
        "name": "nf-core/metapep",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-11-23",
        "versions": 1
    },
    {
        "create_time": "2024-11-23",
        "creators": [
            "Subina Mehta"
        ],
        "description": "The workflow begins with the Database Generation process. The Galaxy-P team has developed a workflow that collects protein sequences from known disease-causing microorganisms to build a comprehensive database. This extensive database is then refined into a smaller, more relevant dataset using the Metanovo tool.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1216",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1216?version=1",
        "name": "clinicalmp-database-generation/main",
        "number_of_steps": 3,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "metanovo",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2024-11-17",
        "creators": [
            "Luke Silver"
        ],
        "description": "This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \r\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \r\n\r\nThe workflows can be run in this order: \r\n* Repeat masking\r\n* RNAseq QC and read trimming\r\n* Find transcripts\r\n* Combine transcripts\r\n* Extract transcripts\r\n* Convert formats\r\n* Fgenesh annotation\r\n\r\n\r\nInputs required: assembled-genome.fasta, hard-repeat-masked-genome.fasta, and (because this workflow maps known mRNA sequences) .cdna, .pro and .dat files. It is also required to select certain databases for Fgenesh-annotate and for Busco. \r\n\r\nThis workflow splits the input genomes into single sequences (to decrease computation time), annotates using FgenesH++, and merges the output. \r\n\r\nOutputs: genome annotation in gff3 format, genome annotation stats, fasta files of mRNAs, cDNAs and proteins, Busco report of proteins. \r\n\r\nNote: The input sequences to the tools to extract mRNA and cDNA here are the assembly.fasta sequences (unmasked) but there may be a reason to prefer the masked version, we are unsure of when that may be the case. \r\n\r\nNote: If you want to use this workflow without an input of known mRNAs, you can save a copy of the workflow and edit the \"Fgenesh annotate\" tool with \"no\" at this option, you will then not need an input of .cdna .pro and .dat files. \r\n\r\nChanges made 13 Nov 2024: Added correct input files and connected them to the split steps. Added inputs for db selections in the annotation step. Added lineage input to Busco. Added genome annotation stats derived from gff3 output. Connected in assembly.fasta sequences to \"get mRNA/cDNA\" tools. Expanded this information text and clarified the need for .cdna .pro and .dat files as input. ",
        "doi": "10.48546/workflowhub.workflow.881.5",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "881",
        "keep": true,
        "latest_version": 5,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/881?version=5",
        "name": "Fgenesh annotation -TSI",
        "number_of_steps": 10,
        "projects": [
            "Australian BioCommons",
            "Galaxy Australia"
        ],
        "source": "WorkflowHub",
        "tags": [
            "tsi-annotation"
        ],
        "tools": [
            "fgenesh_merge",
            "jcvi_gff_stats",
            "fgenesh_get_mrnas_gc",
            "fgenesh_split",
            "fgenesh_annotate",
            "fgenesh_get_proteins",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2024-11-17",
        "versions": 5
    },
    {
        "create_time": "2024-11-22",
        "creators": [
            "Diana Chiang Jurado",
            "Pavankumar Videm",
            "Pablo Moreno"
        ],
        "description": "This workflow uses the decoupler tool in Galaxy to generate pseudobulk counts from an annotated AnnData file obtained from scRNA-seq analysis. Following the pseudobulk step, differential expression genes (DEG) are calculated using the edgeR tool. The workflow also includes data sanitation steps to ensure smooth operation of edgeR and minimizing potential issues. Additionally, a Volcano plot tool is used to visualize the results after the DEG analysis.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1207",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1207?version=2",
        "name": "pseudobulk-worflow-decoupler-edger/main",
        "number_of_steps": 13,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "column_remove_by_header",
            "tp_replace_in_line",
            "tp_awk_tool",
            "edger",
            "volcanoplot",
            "tp_replace_in_column",
            "collection_element_identifiers",
            "decoupler_pseudobulk",
            "param_value_from_file",
            "split_file_to_collection"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2024-11-13",
        "creators": [
            "Kristina Gomoryova",
            "David Pot\u011b\u0161il"
        ],
        "description": "KNIME workflow describing the analysis of mass spectrometry dataset related to the publication \"Armed with PRICKLE(3)s: Stabilizing WNT/PCP complexes against RNF43-mediated ubiquitination\". Workflow was built using the KNIME software container environment, version 4.7.7a, which can be created using \"docker pull cfprot/knime:4.7.7a\" command in Docker. The input data for the KNIME workflow (the report.tsv from DIA-NN) can be found on PRIDE repository under the identifier PXD057854.",
        "doi": "10.48546/workflowhub.workflow.1202.1",
        "edam_operation": [],
        "edam_topic": [
            "Cell biology",
            "Proteomics",
            "Proteomics experiment"
        ],
        "filtered_on": "binn* in description",
        "id": "1202",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1202?version=1",
        "name": "Proximity interactomes of human PRICKLE1, PRICKLE2 and PRICKLE3.",
        "number_of_steps": 0,
        "projects": [
            "Proteomics CEITEC"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioid",
            "knime",
            "proteomics",
            "mass-spectrometry"
        ],
        "tools": [
            "Konstanz Information Miner (KNIME)"
        ],
        "type": "KNIME",
        "update_time": "2024-11-16",
        "versions": 1
    },
    {
        "create_time": "2024-11-05",
        "creators": [
            "Fabian Hausmann"
        ],
        "description": "This repository contains the analytical pipeline for the MAXOMOD project, which focuses on the multi-omic analysis of axono-synaptic degeneration in the motor neuron disease amyotrophic lateral sclerosis (ALS). The project explores sex differences and molecular subclusters in ALS and investigates the MAPK pathway as a potential therapeutic target.\r\n\r\nFor a detailed understanding of the scientific background and the findings, refer to our paper published on [Nature Communications](https://www.nature.com/articles/s41467-024-49196-y).\r\n\r\n## Table of Contents\r\n\r\n- [Getting Started](#getting-started)\r\n  - [Prerequisites](#prerequisites)\r\n  - [Data Preparation](#data-preparation)\r\n  - [Organize Data](#organize-data)\r\n- [Reproducing Results](#reproducing-results)\r\n- [Contributing](#contributing)\r\n- [Collaboration](#collaboration)\r\n\r\n\r\n## Getting Started\r\n### Prerequisites\r\n- Git\r\n- [DVC](https://dvc.org/)\r\n- [Nextflow](https://www.nextflow.io/)\r\n- Container execution engine (e.g. [Docker](https://www.docker.com/) or [Podman](https://podman.io/))\r\n\r\nClone the git repository:\r\n\r\n```\r\ngit clone https://github.com/imsb-uke/MAXOMOD_Pipeline.git ./maxomod\r\n```\r\n\r\nEnter the cloned directory:\r\n\r\n   ``` cd maxomod ```\r\n\r\n### Data Preparation\r\n\r\nHuman sequencing data: [EGAS00001007318](https://ega-archive.org/datasets/) [due to patient data, access is restricted]\r\n\r\nMouse sequencing data: [GSE234246](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE234246)\r\n\r\nProteomics data: [PXD043300](https://proteomecentral.proteomexchange.org/cgi/GetDataset?ID=PXD043300)\r\n\r\nPhosphoproteomics data: [PXD043297](https://proteomecentral.proteomexchange.org/cgi/GetDataset?ID=PXD043297)\r\n\r\n### Organize Data\r\n\r\nAll data should be organized in datasets using the following structures\r\n\r\n```\r\ndatasets/\r\n    consortium/\r\n        <model>/\r\n            01_received_data/\r\n                <omic>/\r\n                cohort/\r\n```\r\n\r\nThe pipeline expects `fastq.gz` files for the sequencing data, `txt` files for the proteomics data and `csv` files for the phosphoproteomics data.\r\n\r\nPlease, use DVC to see, which exact file names are required:\r\n\r\n```bash\r\ndvc status srna_organize_samples proteomics_organize_samples phosphoproteomics_organize_samples rnaseq_nextflow\r\n```\r\n\r\n#### Automatic download (optional)\r\n\r\nTo automatically download the RNAseq & miRNAseq data we provide a download script, which can be executed using the following commands:\r\n\r\n```bash\r\ndvc unfreeze sra_prefetch sra_fastq_dump sra_organize\r\ndvc repro\r\n```\r\n\r\n\r\n## Reproducing Results\r\n\r\nTo reproduce the analysis results, execute the following command:\r\n\r\n```bash\r\ndvc repro\r\n```\r\n\r\nThis command will run the predefined pipelines to process and analyze the data according to the methodology described in the associated publication.\r\nAll steps will be executed in a docker container automatically using the `docker_wrapper.sh` script. All docker images will be automatically downloaded and are available in the [Packages section](https://github.com/orgs/imsb-uke/packages?repo_name=MAXOMOD_Pipeline) on GitHub.\r\n\r\n## Contributing\r\nWe welcome contributions to enhance the reproducibility and scope of the analysis.\r\n\r\n## Collaboration\r\n\r\nFor questions or collaboration offers, please contact the project's principal investigators via email provided on the MAXOMOD project page: [MAXOMOD Contact Information](https://www.gesundheitsforschung-bmbf.de/de/maxomod-multi-omische-analyse-axono-synaptischer-degeneration-bei-motoneuronerkrankungen-9409.php).\r\n\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.1191.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "1191",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1191?version=1",
        "name": "Multiomic ALS signatures highlight sex differences and molecular subclusters and identify the MAPK pathway as therapeutic target",
        "number_of_steps": 0,
        "projects": [
            "MAXOMOD"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "DVC",
        "update_time": "2024-11-11",
        "versions": 1
    },
    {
        "create_time": "2024-11-14",
        "creators": [
            "Helena Rasche",
            "Dennis Doll\u00e9e",
            "Birgit Rijvers"
        ],
        "description": "This is an aggregation of the work done in [Seq4AMR](https://workflowhub.eu/projects/110) consisting of the following workflows:\r\n\r\n- [WF1: AbritAMR / AMRFinderPlus](https://workflowhub.eu/workflows/634)\r\n- [WF2: Sciensano](https://workflowhub.eu/workflows/644) (**not currently included**)\r\n- [WF3: SRST2](https://workflowhub.eu/workflows/407) \r\n- [WF4: StarAMR](https://workflowhub.eu/workflows/470)\r\n\r\n## Installation\r\n\r\n- You will need to:\r\n    - run the [RGI Database Builder](https://my.galaxy.training/?path=?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fcard%2Frgi%2Frgi_database_builder%2F1.2.0) as a Galaxy admin (if this hasn't been done already)\r\n    - [Have the en_US.UTF-8 locale installed](https://github.com/galaxyproject/tools-iuc/issues/6467) on the compute nodes executing cast/melt jobs.\r\n    - Install the requisite tools with e.g. [`shed-tools`](https://ephemeris.readthedocs.io/en/latest/commands/shed-tools.html) command from the [`ephemeris`](https://ephemeris.readthedocs.io/en/latest/) suite: `shed-tools install -g https://galaxy.example.com -a API_KEY -t tools.yaml` (tools.yaml is provided in this repository.)\r\n- Then you can import this workflow\r\n    - Navigate to `/workflows/import` of your Galaxy server\r\n    - Select \"GA4GH servers\"\r\n    - Enter `name:\"AMR-Pathfinder\"`\r\n- And run it\r\n    - You must provide a Sequencing collection (list:paired of fastq files)\r\n    - And a Genomes collection (list of fasta files) \r\n    - Both of these should use **identical** collection element identifiers\r\n\r\n## Outputs\r\n\r\nThis will produce two important tables: \"Binary Comparison\" and a \"% Identity Scored Outputs\". \r\n\r\n### Binary comparison\r\n\r\nThis file reports the discovery or absence of specific AMR genes across all tested AMR Analysis tools. You will mostly see 1s (presence) or 0s (absence) but you may occasionally see higher numbers when an AMR tool reports multiple hits for a specific gene.\r\n\r\n### % Identity Scored Outputs\r\n\r\nThis is similar to binary comparison, but using the % identity reported by each AMR tool. For cases where multiple hits were detected, we take the highest.\r\n\r\n## Known Issues\r\n\r\nThe names for identified AMR genes is highly inconsistent across AMR analysis tools. We urge the AMR community to rectify this by standardising gene names used in their tooling.",
        "doi": null,
        "edam_operation": [
            "Antimicrobial resistance prediction"
        ],
        "edam_topic": [],
        "filtered_on": "amr in tags",
        "id": "1189",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1189?version=2",
        "name": "AMR-Pathfinder",
        "number_of_steps": 36,
        "projects": [
            "Seq4AMR",
            "ErasmusMC Clinical Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amr",
            "amr-detection",
            "benchamrking"
        ],
        "tools": [
            "",
            "tp_split_on_column",
            "Cut1",
            "tp_text_file_with_recurring_lines",
            "datamash_ops",
            "cat1",
            "staramr_search",
            "addValue",
            "tp_find_and_replace",
            "hamronize_summarize",
            "cast",
            "__MERGE_COLLECTION__",
            "cat_multi_datasets",
            "hamronize_tool",
            "shovill",
            "__APPLY_RULES__",
            "abricate",
            "collapse_dataset",
            "Grep1"
        ],
        "type": "Galaxy",
        "update_time": "2024-12-24",
        "versions": 2
    },
    {
        "create_time": "2024-11-01",
        "creators": [
            "Samuel Lambert",
            "Benjamin Wingfield",
            "Laurent Gil"
        ],
        "description": "# The Polygenic Score Catalog Calculator (`pgsc_calc`)\r\n\r\n[![Documentation Status](https://readthedocs.org/projects/pgsc-calc/badge/?version=latest)](https://pgsc-calc.readthedocs.io/en/latest/?badge=latest)\r\n[![pgscatalog/pgsc_calc CI](https://github.com/PGScatalog/pgsc_calc/actions/workflows/ci.yml/badge.svg)](https://github.com/PGScatalog/pgsc_calc/actions/workflows/ci.yml)\r\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5970794.svg)](https://doi.org/10.5281/zenodo.5970794)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-\u226523.10.0-23aa62.svg?labelColor=000000)](https://www.nextflow.io/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n\r\n## Introduction\r\n\r\n`pgsc_calc` is a bioinformatics best-practice analysis pipeline for calculating\r\npolygenic [risk] scores on samples with imputed genotypes using existing scoring\r\nfiles from the [Polygenic Score (PGS) Catalog](https://www.pgscatalog.org/)\r\nand/or user-defined PGS/PRS.\r\n\r\n## Pipeline summary\r\n\r\n> [!IMPORTANT]  \r\n> * Whole genome sequencing (WGS) data [are not currently supported by the calculator](https://pgsc-calc.readthedocs.io/en/latest/explanation/match.html#are-your-target-genomes-imputed-are-they-wgs)\r\n> * It\u2019s possible to [create compatible gVCFs from WGS data](https://github.com/PGScatalog/pgsc_calc/discussions/123#discussioncomment-6469422). We plan to improve support for WGS data in the near future.\r\n\r\n<p align=\"center\">\r\n  <img width=\"80%\" src=\"https://github.com/PGScatalog/pgsc_calc/assets/11425618/f766b28c-0f75-4344-abf3-3463946e36cc\">\r\n</p>\r\n\r\nThe workflow performs the following steps:\r\n\r\n* Downloading scoring files using the PGS Catalog API in a specified genome build (GRCh37 and GRCh38).\r\n* Reading custom scoring files (and performing a liftover if genotyping data is in a different build).\r\n* Automatically combines and creates scoring files for efficient parallel computation of multiple PGS\r\n    - Matching variants in the scoring files against variants in the target dataset (in plink bfile/pfile or VCF format)\r\n* Calculates PGS for all samples (linear sum of weights and dosages)\r\n* Creates a summary report to visualize score distributions and pipeline metadata (variant matching QC)\r\n\r\nAnd optionally:\r\n\r\n- Genetic Ancestry: calculate similarity of target samples to populations in a\r\n  reference dataset ([1000 Genomes (1000G)](http://www.nature.com/nature/journal/v526/n7571/full/nature15393.html)), using principal components analysis (PCA)\r\n- PGS Normalization: Using reference population data and/or PCA projections to report\r\n  individual-level PGS predictions (e.g. percentiles, z-scores) that account for genetic ancestry\r\n\r\nSee documentation for a list of planned [features under development](https://pgsc-calc.readthedocs.io/en/latest/index.html#Features-under-development).\r\n\r\n### PGS applications and libraries\r\n\r\n`pgsc_calc` uses applications and libraries internally developed at the PGS Catalog, which can do helpful things like:\r\n\r\n* Query the PGS Catalog to bulk download scoring files in a specific genome build\r\n* Match variants from scoring files to target variants\r\n* Adjust calculated PGS in the context of genetic ancestry\r\n\r\nIf you want to write Python code to work with PGS, [check out the `pygscatalog` repository to learn more](https://github.com/PGScatalog/pygscatalog).\r\n\r\nIf you want a simpler way of working with PGS, ignore this section and continue below to learn more about `pgsc_calc`.\r\n\r\n## Quick start\r\n\r\n1. Install\r\n[`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation)\r\n(`>=23.10.0`)\r\n\r\n2. Install [`Docker`](https://docs.docker.com/engine/installation/) or\r\n[`Singularity (v3.8.3 minimum)`](https://www.sylabs.io/guides/3.0/user-guide/)\r\n(please only use [`Conda`](https://conda.io/miniconda.html) as a last resort)\r\n\r\n3. Download the pipeline and test it on a minimal dataset with a single command:\r\n\r\n    ```console\r\n    nextflow run pgscatalog/pgsc_calc -profile test,<docker/singularity/conda>\r\n    ```\r\n\r\n4. Start running your own analysis!\r\n\r\n    ```console\r\n    nextflow run pgscatalog/pgsc_calc -profile <docker/singularity/conda> --input samplesheet.csv --pgs_id PGS001229\r\n    ```\r\n\r\nSee [getting\r\nstarted](https://pgsc-calc.readthedocs.io/en/latest/getting-started.html) for more\r\ndetails.\r\n\r\n## Documentation\r\n\r\n[Full documentation is available on Read the Docs](https://pgsc-calc.readthedocs.io/)\r\n\r\n## Credits\r\n\r\npgscatalog/pgsc_calc is developed as part of the PGS Catalog project, a\r\ncollaboration between the University of Cambridge\u2019s Department of Public Health\r\nand Primary Care (Michael Inouye, Samuel Lambert) and the European\r\nBioinformatics Institute (Helen Parkinson, Laura Harris).\r\n\r\nThe pipeline seeks to provide a standardized workflow for PGS calculation and\r\nancestry inference implemented in nextflow derived from an existing set of\r\ntools/scripts developed by Inouye lab (Rodrigo Canovas, Scott Ritchie, Jingqin\r\nWu) and PGS Catalog teams (Samuel Lambert, Laurent Gil).\r\n\r\nThe adaptation of the codebase, nextflow implementation, and PGS Catalog features\r\nare written by Benjamin Wingfield, Samuel Lambert, Laurent Gil with additional input\r\nfrom Aoife McMahon (EBI). Development of new features, testing, and code review\r\nis ongoing including Inouye lab members (Rodrigo Canovas, Scott Ritchie) and others. If \r\nyou use the tool we ask you to cite our paper describing software and updated PGS Catalog resource:\r\n\r\n- >Lambert, Wingfield _et al._ (2024) Enhancing the Polygenic Score Catalog with tools for score \r\n  calculation and ancestry normalization. Nature Genetics.\r\n  doi:[10.1038/s41588-024-01937-x](https://doi.org/10.1038/s41588-024-01937-x).\r\n\r\nThis pipeline is distrubuted under an [Apache License](LICENSE) amd uses code and \r\ninfrastructure developed and maintained by the [nf-core](https://nf-co.re) community \r\n(Ewels *et al. Nature Biotech* (2020) doi:[10.1038/s41587-020-0439-x](https://doi.org/10.1038/s41587-020-0439-x)), \r\nreused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\nAdditional references of open-source tools and data used in this pipeline are described in\r\n[`CITATIONS.md`](CITATIONS.md).\r\n\r\nThis work has received funding from EMBL-EBI core funds, the Baker Institute,\r\nthe University of Cambridge, Health Data Research UK (HDRUK), and the European\r\nUnion\u2019s Horizon 2020 research and innovation programme under grant agreement No\r\n101016775 INTERVENE.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "DNA polymorphism",
            "Genetic variation",
            "Genomics",
            "Human genetics"
        ],
        "filtered_on": "metap* in description",
        "id": "556",
        "keep": true,
        "latest_version": 7,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/556?version=7",
        "name": "The Polygenic Score Catalog Calculator",
        "number_of_steps": 0,
        "projects": [
            "Polygenic Score Catalog"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gwas",
            "nextflow",
            "workflows",
            "genomic ancestry",
            "polygenic risk score",
            "polygenic score",
            "prediction"
        ],
        "tools": [
            "PLINK"
        ],
        "type": "Nextflow",
        "update_time": "2024-11-01",
        "versions": 7
    },
    {
        "create_time": "2024-10-21",
        "creators": [],
        "description": "# Metagenome-Atlas\r\n\r\n[![Anaconda-Server Badge](https://anaconda.org/bioconda/metagenome-atlas/badges/latest_release_relative_date.svg)](https://anaconda.org/bioconda/metagenome-atlas)\r\n[![Bioconda](https://img.shields.io/conda/dn/bioconda/metagenome-atlas.svg?label=Bioconda )](https://anaconda.org/bioconda/metagenome-atlas)\r\n[![Documentation Status](https://readthedocs.org/projects/metagenome-atlas/badge/?version=latest)](https://metagenome-atlas.readthedocs.io/en/latest/?badge=latest)\r\n![Mastodon Follow](https://img.shields.io/mastodon/follow/109273833677404282?domain=https%3A%2F%2Fmstdn.science&style=social)\r\n<!--[![follow on twitter](https://img.shields.io/twitter/follow/SilasKieser.svg?style=social&label=Follow)](https://twitter.com/search?f=tweets&q=%40SilasKieser%20%23metagenomeAtlas&src=typd) -->\r\n\r\n\r\nMetagenome-atlas is a easy-to-use metagenomic pipeline based on snakemake. It handles all steps from QC, Assembly, Binning, to Annotation.\r\n\r\n![scheme of workflow](resources/images/atlas_list.png?raw=true)\r\n\r\nYou can start using atlas with three commands:\r\n```\r\n    mamba install -y -c bioconda -c conda-forge metagenome-atlas={latest_version}\r\n    atlas init --db-dir databases path/to/fastq/files\r\n    atlas run all\r\n```\r\nwhere `{latest_version}` should be replaced by [![Version](https://anaconda.org/bioconda/metagenome-atlas/badges/version.svg)](https://anaconda.org/bioconda/metagenome-atlas)\r\n\r\n\r\n# Webpage\r\n\r\n[metagenome-atlas.github.io](https://metagenome-atlas.github.io/)\r\n\r\n# Documentation\r\n\r\nhttps://metagenome-atlas.readthedocs.io/\r\n\r\n[Tutorial](https://github.com/metagenome-atlas/Tutorial)\r\n\r\n# Citation\r\n\r\n> ATLAS: a Snakemake workflow for assembly, annotation, and genomic binning of metagenome sequence data.  \r\n> Kieser, S., Brown, J., Zdobnov, E. M., Trajkovski, M. & McCue, L. A.   \r\n> BMC Bioinformatics 21, 257 (2020).  \r\n> doi: [10.1186/s12859-020-03585-4](https://doi.org/10.1186/s12859-020-03585-4)\r\n\r\n\r\n# Developpment/Extensions\r\n\r\nHere are some ideas I work or want to work on when I have time. If you want to contribute or have some ideas let me know via a feature request issue.\r\n\r\n- Optimized MAG recovery (e.g. [Spacegraphcats](https://github.com/spacegraphcats/spacegraphcats))\r\n- Integration of viruses/plasmid that live for now as [extensions](https://github.com/metagenome-atlas/virome_atlas)\r\n- Add statistics and visualisations as in [atlas_analyze](https://github.com/metagenome-atlas/atlas_analyze)\r\n- Implementation of most rules as snakemake wrapper\r\n- Cloud execution\r\n- Update to new Snakemake version and use cool reports.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "1183",
        "keep": true,
        "latest_version": 1,
        "license": "BSD-3-Clause",
        "link": "https:/workflowhub.eu/workflows/1183?version=1",
        "name": "Metaenome-Atlas",
        "number_of_steps": 0,
        "projects": [
            "Snakemake-Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2024-10-21",
        "versions": 1
    },
    {
        "create_time": "2024-10-07",
        "creators": [],
        "description": "# skim2mito\r\n\r\n**skim2mito** is a snakemake pipeline for the batch assembly, annotation, and phylogenetic analysis of mitochondrial genomes from low coverage genome skims. The pipeline was designed to work with sequence data from museum collections. However, it should also work with genome skims from recently collected samples.\r\n\r\n## Contents\r\n - [Setup](#setup)\r\n - [Example data](#example-data)\r\n - [Input](#input)\r\n - [Output](#output)\r\n - [Filtering contaminants](#filtering-contaminants)\r\n - [Assembly and annotation only](#assembly-and-annotation-only)\r\n - [Running your own data](#running-your-own-data)\r\n - [Getting help](#getting-help)\r\n - [Citations](#citations)\r\n\r\n## Setup\r\n\r\nThe pipeline is written in Snakemake and uses conda to install the necessary tools.\r\n\r\nIt is *strongly recommended* to install conda using Mambaforge. See details here https://snakemake.readthedocs.io/en/stable/getting_started/installation.html\r\n\r\nOnce conda is installed, you can pull the github repo and set up the base conda environment.\r\n\r\n```\r\n# get github repo\r\ngit clone https://github.com/o-william-white/skim2mito\r\n\r\n# change dir\r\ncd skim2mito\r\n\r\n# setup conda env\r\nconda env create -n skim2mito_env -f workflow/envs/conda_env.yaml\r\nconda config --set channel_priority flexible\r\n```\r\n\r\nIf you need to install the conda environment to a specific location, use the following example, where the prefix argument can be updated to include a specific path:\r\n\r\n```\r\nconda env create -n skim2mito_env --prefix /your_path/skim2mito_env -f workflow/envs/conda_env.yaml\r\n```\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2mito\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Example data\r\n\r\nBefore you run your own data, it is recommended to run the example datasets provided. This will confirm there are no user-specific issues with the setup and it also installs all the dependencies. The example data includes simulated mitochondrial data from 25 different butterfly species. \r\n\r\nTo run the example data, use the code below. **Note that you need to change the user email to your own address**. The email is required by the Bio Entrez package to fetch reference sequences. The first time you run the pipeline, it will take some time to install each of the conda environments, so it is a good time to take a tea break :).\r\n```\r\nconda activate skim2mito_env\r\n\r\nsnakemake --cores 4 --use-conda --config user_email=user@example_email.com\r\n```\r\n\r\nOnce this has finished, you can generate a snakemake report using the following command. As above, you need to change the user email to your own address.\r\n\r\n```\r\nsnakemake --cores 4 --use-conda --config user_email=user@example_email.com --report skim2mito_report.html\r\n```\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2mito\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Input\r\n\r\nSnakemake requires a `config.yaml` and `samples.csv` to define input parameters and sequence data for each sample. \r\n\r\nFor the example data provided, the config file is located here `config/config.yaml` and it looks like this:\r\n```\r\n# path to sample sheet csv with columns for ID,forward,reverse,taxid,seed,gene\r\nsamples: config/samples.csv\r\n\r\n# user email\r\nuser_email: user@example_email.com\r\n\r\n# getorganelle reference (go_fetch, custom)\r\ngo_reference: go_fetch\r\n\r\n# forward adapter\r\nforward_adapter: AGATCGGAAGAGCACACGTCTGAACTCCAGTCA\r\n\r\n# reverse adapter\r\nreverse_adapter: AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\r\n\r\n# fastp deduplication (True/False)\r\nfastp_dedup: True\r\n\r\n# mitos refseq database (refseq39, refseq63f, refseq63m, refseq63o, refseq89f, refseq89m, refseq89o)\r\nmitos_refseq: refseq39\r\n\r\n# mito code (2 = Vertebrate, 4 = Mold, 5 = Invertebrate, 9 = Echinoderm, 13 = Ascidian, 14 = Alternative flatworm)\r\nmitos_code: 5\r\n\r\n# alignment trimming method to use (gblocks or clipkit)\r\nalignment_trim: gblocks\r\n\r\n# alignment missing data threshold for alignment (0.0 - 1.0)\r\nmissing_threshold: 0.5\r\n\r\n# name of outgroup sample (optional)\r\n# use \"NA\" if there is no obvious outgroup\r\n# if more than one outgroup use a comma separated list i.e. \"sampleA,sampleB\"\r\noutgroup: Eurema_blanda\r\n\r\n# plot dimensions (cm)\r\nplot_height: 20\r\nplot_width: 20\r\n```\r\n\r\nThe example samples.csv file is located here `config/samples.csv` and it looks like this (note that the seed and gene columns are only required if the custom getorganelle database option is specified in the config file):\r\n\r\n\r\n ID | forward | reverse | taxid | seed | gene \r\n----|---------|---------|-------|------|------\r\nAdelpha_iphiclus | .test/reads/Adelpha_iphiclus_1.fq.gz | .test/reads/Adelpha_iphiclus_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nAnartia_jatrophae_saturata | .test/reads/Anartia_jatrophae_saturata_1.fq.gz | .test/reads/Anartia_jatrophae_saturata_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nAraschnia_levana | .test/reads/Araschnia_levana_1.fq.gz | .test/reads/Araschnia_levana_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nAuzakia_danava | .test/reads/Auzakia_danava_1.fq.gz | .test/reads/Auzakia_danava_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nBaeotus_beotus | .test/reads/Baeotus_beotus_1.fq.gz | .test/reads/Baeotus_beotus_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nCatacroptera_cloanthe | .test/reads/Catacroptera_cloanthe_1.fq.gz | .test/reads/Catacroptera_cloanthe_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nChalinga_pratti | .test/reads/Chalinga_pratti_1.fq.gz | .test/reads/Chalinga_pratti_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nDiaethria_gabaza_eupepla | .test/reads/Diaethria_gabaza_eupepla_1.fq.gz | .test/reads/Diaethria_gabaza_eupepla_2.fq.gz | 127268 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nDoleschallia_melana | .test/reads/Doleschallia_melana_1.fq.gz | .test/reads/Doleschallia_melana_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nEurema_blanda | .test/reads/Eurema_blanda_1.fq.gz | .test/reads/Eurema_blanda_2.fq.gz | 42450 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nHypolimnas_usambara | .test/reads/Hypolimnas_usambara_1.fq.gz | .test/reads/Hypolimnas_usambara_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nJunonia_villida | .test/reads/Junonia_villida_1.fq.gz | .test/reads/Junonia_villida_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nKallima_paralekta | .test/reads/Kallima_paralekta_1.fq.gz | .test/reads/Kallima_paralekta_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nKallimoides_rumia | .test/reads/Kallimoides_rumia_1.fq.gz | .test/reads/Kallimoides_rumia_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nLitinga_cottini | .test/reads/Litinga_cottini_1.fq.gz | .test/reads/Litinga_cottini_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nMallika_jacksoni | .test/reads/Mallika_jacksoni_1.fq.gz | .test/reads/Mallika_jacksoni_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nModuza_procris | .test/reads/Moduza_procris_1.fq.gz | .test/reads/Moduza_procris_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nParasarpa_zayla | .test/reads/Parasarpa_zayla_1.fq.gz | .test/reads/Parasarpa_zayla_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nPhaedyma_columella | .test/reads/Phaedyma_columella_1.fq.gz | .test/reads/Phaedyma_columella_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nPrecis_pelarga | .test/reads/Precis_pelarga_1.fq.gz | .test/reads/Precis_pelarga_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nProtogoniomorpha_temora | .test/reads/Protogoniomorpha_temora_1.fq.gz | .test/reads/Protogoniomorpha_temora_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nSalamis_cacta | .test/reads/Salamis_cacta_1.fq.gz | .test/reads/Salamis_cacta_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nSmyrna_blomfildia | .test/reads/Smyrna_blomfildia_1.fq.gz | .test/reads/Smyrna_blomfildia_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nTacola_larymna | .test/reads/Tacola_larymna_1.fq.gz | .test/reads/Tacola_larymna_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nYoma_algina | .test/reads/Yoma_algina_1.fq.gz | .test/reads/Yoma_algina_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\n\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2mito\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Output\r\n\r\nAll output files are saved to the `results` direcotry. Below is a table summarising all of the output files generated by the pipeline.\r\n\r\n| Directory             | Description               |\r\n|-----------------------|---------------------------|\r\n| fastqc_raw            | Fastqc reports for raw input reads |\r\n| fastp                 | Fastp reports from quality control of raw reads |\r\n| fastqc_qc             | Fastqc reports for quality controlled reads |\r\n| go_fetch              | Optional output containing reference databasesused by GetOrganelle |\r\n| getorganelle          | GetOrganelle output with a directory for each sample |\r\n| assembled_sequence    | Assembled sequences selected from GetOrganelle output and renamed |\r\n| seqkit                | Seqkit summary of each assembly |\r\n| blastn                | Blastn output of each assembly |\r\n| minimap               | Mapping output of quality filtered reads against each assembly |\r\n| blobtools             | Blobtools assembly summary collating blastn and mapping output |\r\n| assess_assembly       | Plots of annotations, mean depth, GC content and proportion mismatches |\r\n| annotations           | Annotation outputs of mitos |\r\n| summary               | Summary per sample (seqkit stats), contig (GC content, length, coverage, taxonomy and annotations) and annotated gene counts |\r\n| annotated_genes  | Unaligned fasta files of annotated genes identified across all samples |\r\n| mafft                 | Mafft aligned fasta files of annotated genes identified across all samples |\r\n| mafft_filtered        | Mafft aligned fasta files after the removal of sequences based on a missing data threshold |\r\n| alignment_trim        | Ambiguous parts of alignment removed using either gblocks or clipkit |\r\n| iqtree                | Iqtree phylogenetic analysis of annotated genes |\r\n| plot_tree             | Plots of phylogenetic trees |\r\n| multiqc               | Multiqc summary report |\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2mito\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Filtering contaminants\r\n\r\nIf you are working with museum collections, it is possible that you may assemble and annotate sequences from contaminant/non-target species. *Contaminant sequences can be identified based on the blast search output or unusual placement in the phylogenetic trees* (see blobtools and plot_tree outputs). \r\n\r\nA supplementary python script `format_alignments.py `is provided to remove putative contaminants from alignments, and format the alignments for downstream phylogenetic analysis.\r\n\r\nFor example, let's say we wanted to remove all sequences from the sample \"Kallima_paralekta\" and atp6 gene sequences, you could run the script as shown below. The script works by identifying and removing sequences that have names with  `Kallima_paralekta` or `atp6` in the sequence names. The filtered alignments are written to a new output directory `filter_alignments_output`.\r\n\r\n```\r\npython workflow/scripts/format_alignments.py  \\\r\n   --input results/mafft_filtered/ \\\r\n   --cont Kallima_paralekta atp6 \\\r\n   --output filter_alignments_output\r\n```\r\n\r\n*Note that the output fasta files have been reformatted so each alignment file is named after the gene and each sequence is named after the sample.* This is useful if you would like to run our related pipeline **gene2phylo** for further phylogenetic analyses.\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2mito\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Assembly and annotation only\r\n\r\nIf you are only interested in the assembly of mitochondrial sequences and annotation of genes without the phylogenetic analysis, you can stop the pipeline from running the gene alignment and phylogenetic analyses using the `--omit-from` parameter.\r\n```\r\nsnakemake --cores 4 --use-conda --config user_email=user@example_email.com --omit-from mafft\r\n```\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2mito\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Running your own data\r\n\r\nThe first thing you need to do is generate your own config.yaml and samples.csv files, using the files provided as a template.\r\n\r\nGetOrganelle requires reference data in the format of seed and gene reference fasta files. By default the pipeline uses a basic python script called go_fetch.py https://github.com/o-william-white/go_fetch to download and format reference data formatted for GetOrganelle. \r\n\r\ngo_fetch.py works by searching NCBI based on the NCBI taxonomy specified by the taxid column in the samples.csv file. Note that the seed and gene columns in the samples.csv file are only required if you want to provide your own custom GetOrganelle seed and gene reference databases. \r\n\r\nYou can use the default reference data for GetOrganelle, but I would recommend using custom reference databases where possible. See here for details of how to set up your own databases https://github.com/Kinggerm/GetOrganelle/wiki/FAQ#how-to-assemble-a-target-organelle-genome-using-my-own-reference\r\n\r\n## Getting help\r\n\r\nIf you have any questions, please do get in touch in the issues or by email o.william.white@gmail.com\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2mito\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Citations\r\n\r\nIf you use the pipeline, please cite our bioarxiv preprint: https://doi.org/10.1101/2023.08.11.552985\r\n\r\nSince the pipeline is a wrapper for several other bioinformatic tools we also ask that you cite the tools used by the pipeline:\r\n - Fastqc https://github.com/s-andrews/FastQC\r\n - Fastp https://doi.org/10.1093/bioinformatics/bty560\r\n - GetOrganelle https://doi.org/10.1186/s13059-020-02154-5\r\n - Blastn https://doi.org/10.1186/1471-2105-10-421\r\n - Minimap2 https://doi.org/10.1093/bioinformatics/bty191\r\n - Blobtools https://doi.org/10.12688/f1000research.12232.1\r\n - Seqkit https://doi.org/10.1371/journal.pone.0163962\r\n - MITOS2 https://doi.org/10.1016/j.ympev.2012.08.023\r\n - Gblocks (default) https://doi.org/10.1093/oxfordjournals.molbev.a026334\r\n - Clipkit (optional) https://doi.org/10.1371/journal.pbio.3001007\r\n - Mafft https://doi.org/10.1093/molbev/mst010\r\n - Iqtree https://doi.org/10.1093/molbev/msu300\r\n - ete3 https://doi.org/10.1093/molbev/msw046\r\n - ggtree https://doi.org/10.1111/2041-210X.12628\r\n - Multiqc https://doi.org/10.1093/bioinformatics/btw354\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2mito\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n",
        "doi": null,
        "edam_operation": [
            "Phylogenetic inference",
            "Sequence annotation",
            "Sequence assembly",
            "Sequencing quality control"
        ],
        "edam_topic": [
            "Biodiversity",
            "Genomics",
            "Sequence assembly"
        ],
        "filtered_on": "annot* in tags",
        "id": "791",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/791?version=2",
        "name": "skim2mito",
        "number_of_steps": 0,
        "projects": [
            "NHM Clark group"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bge",
            "bioscaneurope",
            "biodiversity",
            "bioinformatics",
            "genome assembly",
            "genomics",
            "natural history collections",
            "sequence annotation"
        ],
        "tools": [
            "fastp",
            "FastQC",
            "GetOrganelle",
            "MITOS",
            "Biopython",
            "Snakemake",
            "BLAST",
            "Minimap2",
            "BlobToolKit",
            "seqkit",
            "gblocks",
            "MAFFT",
            "iqtree",
            "ggtree",
            "MultiQC"
        ],
        "type": "Snakemake",
        "update_time": "2024-10-07",
        "versions": 2
    },
    {
        "create_time": "2024-10-01",
        "creators": [
            "Jasper Koehorst",
            "Bart Nijsse"
        ],
        "description": "### Workflow for microbial (meta-)genome annotation\r\n\r\nInput is a (meta)genome sequence in fasta format.\r\n\r\n* bakta\r\n* KoFamScan (optional)\r\n* InterProScan (optional)\r\n* eggNOG mapper (optional)\r\n\r\n* To RDF conversion with SAPP (optional, default on) --> [SAPP conversion Workflow in WorkflowHub](https://workflowhub.eu/workflows/1174/)\r\n\r\ngit: [https://gitlab.com/m-unlock/cwl](https://gitlab.com/m-unlock/cwl)",
        "doi": null,
        "edam_operation": [
            "Annotation",
            "Gene functional annotation",
            "Genome annotation"
        ],
        "edam_topic": [
            "Microbiology"
        ],
        "filtered_on": "metage* in tags",
        "id": "1170",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1170?version=1",
        "name": "Microbial (meta-) genome annotation",
        "number_of_steps": 9,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "bioinformatics",
            "genome",
            "metagenome",
            "microbial"
        ],
        "tools": [
            "KofamScan",
            "eggNOG-mapper",
            "Move all Bakta files to a folder",
            "Bacterial genome annotation tool",
            "Compress files when compression is true",
            "Compress Bakta",
            "Gather files when compression is false",
            "InterProScan 5"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-03",
        "versions": 1
    },
    {
        "create_time": "2024-10-01",
        "creators": [
            "Bart Nijsse",
            "Jasper Koehorst"
        ],
        "description": "### Workflow for converting (genome) annotation tool output into a GBOL RDF file (TTL/HDT) using SAPP\r\n\r\nCurrent formats / tools:\r\n* \tEMBL format\r\n* \tInterProScan (JSON/TSV)\r\n* \teggNOG-mapper (TSV)\r\n* \tKoFamScan (TSV)\r\n\r\ngit: [https://gitlab.com/m-unlock/cwl](https://gitlab.com/m-unlock/cwl)\r\n\r\n**SAPP** (Semantic Annotation Platform with Provenance): <br>\r\nhttps://gitlab.com/sapp <br>\r\nhttps://academic.oup.com/bioinformatics/article/34/8/1401/4653704\r\n\r\n",
        "doi": null,
        "edam_operation": [
            "Annotation",
            "Genome annotation"
        ],
        "edam_topic": [
            "Bioinformatics"
        ],
        "filtered_on": "annot* in tags",
        "id": "1174",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1174?version=1",
        "name": "SAPP conversion Workflow",
        "number_of_steps": 7,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "bioinformatics",
            "rdf",
            "semantic web standards"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2024-10-03",
        "versions": 1
    },
    {
        "create_time": "2024-10-07",
        "creators": [
            "GalaxyP"
        ],
        "description": "Clinical Metaproteomics 4: Quantitation ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "1177",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1177?version=1",
        "name": "clinicalmp-quantitation/main",
        "number_of_steps": 7,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "extract peptides\nCut1",
            "maxquant",
            "extracting microbial Peptides\nGrep1",
            "extracting microbial Proteins\nGrep1",
            "Quantified-Proteins\nGrouping1",
            "extract proteins\nCut1",
            "Quantified-Peptides\nGrouping1"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Wolfgang Maier"
        ],
        "description": "COVID-19: variation analysis on ARTIC PE data\r\n---------------------------------------------\r\n\r\nThe workflow for Illumina-sequenced ampliconic data builds on the RNASeq workflow\r\nfor paired-end data using the same steps for mapping and variant calling, but\r\nadds extra logic for trimming amplicon primer sequences off reads with the ivar\r\npackage. In addition, this workflow uses ivar also to identify amplicons\r\naffected by primer-binding site mutations and, if possible, excludes reads\r\nderived from such \"tainted\" amplicons when calculating allele-frequencies\r\nof other variants.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "110",
        "keep": true,
        "latest_version": 11,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/110?version=11",
        "name": "sars-cov-2-pe-illumina-artic-variant-calling/COVID-19-PE-ARTIC-ILLUMINA",
        "number_of_steps": 25,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "artic",
            "by-covid",
            "covid-19",
            "covid19.galaxyproject.org"
        ],
        "tools": [
            "snpSift_filter",
            "vcfvcfintersect",
            "qualimap_bamqc",
            "lofreq_viterbi",
            "bwa_mem",
            "fastp",
            "samtools_stats",
            "ivar_trim",
            "\n __FILTER_FAILED_DATASETS__",
            "lofreq_indelqual",
            "multiqc",
            "lofreq_filter",
            "\n __FLATTEN__",
            "compose_text_param",
            "bcftools_annotate",
            "samtools_view",
            "tp_replace_in_line",
            "snpeff_sars_cov_2",
            "lofreq_call",
            "ivar_removereads"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 11
    },
    {
        "create_time": "2024-07-24",
        "creators": [
            "Kristina Gomoryova"
        ],
        "description": "KNIME workflow describing the analysis of mass spectrometry dataset related to the publication \"Proximity interactomics identifies RAI14, EPHA2 and PHACTR4 as essential components of Wnt/planar cell polarity pathway in vertebrates\". Workflow was built using the KNIME software container environment, version 4.7.7a, which can be created using \"docker pull cfprot/knime:4.7.7a\" command in Docker. The input data for the KNIME workflow (the report.tsv from DIA-NN) can be found on PRIDE repository under the identifier PXD048678.",
        "doi": "10.48546/workflowhub.workflow.1083.1",
        "edam_operation": [],
        "edam_topic": [
            "Cell biology"
        ],
        "filtered_on": "binn* in description",
        "id": "1083",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1083?version=1",
        "name": "miniTurboID of RAI14, EPHA2 and PHACTR4",
        "number_of_steps": 0,
        "projects": [
            "Proteomics CEITEC"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioid",
            "knime",
            "proteomics",
            "mass-spectrometry",
            "miniturboid",
            "wnt signaling"
        ],
        "tools": [
            "Konstanz Information Miner (KNIME)"
        ],
        "type": "KNIME",
        "update_time": "2024-09-20",
        "versions": 1
    },
    {
        "create_time": "2024-09-11",
        "creators": [
            "Jen-Hung Wang"
        ],
        "description": "# **Stratum corneum nanotexture feature detection using deep learning and spatial analysis: a non-invasive tool for skin barrier assessment**\r\n\r\n<img src=\"./source/Overview.png\" alt=\"Data Processing\" width=\"95%\" />\r\n\r\nThis repository presents an objective, quantifiable method for assessing atopic dermatitis (AD) severity. The program integrates deep learning object detection with spatial analysis algorithms to accurately calculate the density of circular nano-size objects (CNOs), termed the Effective Corneocyte Topographical Index (ECTI). The ECTI demonstrates remarkable robustness in overcoming the inherent challenges of nano-imaging, such as environmental noise and structural occlusions on the corneocyte surface, further enhancing its applicability in clinical settings.\r\n\r\n## **Dependencies**\r\n- Python 3.9+\r\n- matplotlib\r\n- numpy\r\n- opencv-python\r\n- scipy\r\n- scikit-image\r\n- ultralytics\r\n- scikit-learn\r\n- customtkinter\r\n\r\n## **Directories**\r\n- `AD_Assessment_GUI.zip` contains a cross-platform executable GUI, sample data, and a tutorial video.\r\n- `utils/Img_Preprocessing.py` demonstrates the image enhancement algorithms applied to the corneocyte nanotexture images.\r\n\r\n## **Usage**\r\n1. Execution via cross-platform executable GUI\r\n    - Download [AD_Assessment_GUI.zip](https://huggingface.co/jenhung/ECTI_Assessment_GUI)\r\n    - Run `AD_Assessment_GUI.exe`\r\n    - Analysis results will be saved within the selected path in a folder titled `CNO_Detection`\r\n\r\n2. Execution via python script\r\n    - Install packages in terminal:\r\n        ```    \r\n        pip install -r requirements.txt\r\n        ```\r\n    - Run `AD_Assessment_GUI.py`\r\n    - Analysis results will be saved within the selected path in a folder titled `CNO_Detection`\r\n\r\n## **Executable**\r\n\r\n1. Install PyInstaller in terminal:\r\n                \r\n    ```    \r\n    pip install pyinstaller\r\n    ```\r\n   \r\n2. Run command in terminal:\r\n\r\n    ```    \r\n    pyinstaller --onedir .\\AD_Assessment_GUI.py\r\n    ```\r\n   \r\n## **Performance**\r\n\r\n| Model                                                                | Test Size | #Parameter (M) | FLOPs (G) | AP<sup>50</sup> (%) | AP<sup>50-95</sup> (%) | Latency (ms) |\r\n|:---------------------------------------------------------------------|:---------:|:--------------:|:---------:|:-------------------:|:----------------------:|:------------:|\r\n| [YOLOv10-N](https://huggingface.co/jenhung/CNO_DETECTION_YOLOv8-L)   |    512    |      2.7       |    8.2    |        89.6         |          51.4          |     3.3      |\r\n| [YOLOv10-S](https://huggingface.co/jenhung/CNO_DETECTION_YOLOv8-L)   |    512    |      8.0       |   24.4    |        90.8         |          55.5          |     4.58     |\r\n| [YOLOv10-M](https://huggingface.co/jenhung/CNO_DETECTION_YOLOv8-L)   |    512    |      16.5      |   63.4    |        91.3         |          59.7          |     7.17     |\r\n| [YOLOv10-B](https://huggingface.co/jenhung/CNO_DETECTION_YOLOv8-L)   |    512    |      20.4      |   97.7    |        91.1         |          62.5          |     7.58     |\r\n| [YOLOv10-L](https://huggingface.co/jenhung/CNO_DETECTION_YOLOv8-L)   |    512    |      25.7      |   126.3   |        91.4         |          63.2          |     9.01     |\r\n| [YOLOv10-X](https://huggingface.co/jenhung/CNO_DETECTION_YOLOv8-L)   |    512    |      31.6      |   169.8   |        91.2         |          62.9          |    10.95     |\r\n| [RT-DETRv2-S](https://huggingface.co/jenhung/CNO_DETECTION_YOLOv8-L) |    512    |      20.0      |   60.0    |        87.6         |          39.6          |     5.51     |\r\n| [RT-DETRv2-M](https://huggingface.co/jenhung/CNO_DETECTION_YOLOv8-L) |    512    |      31.0      |   100.0   |        84.0         |          37.2          |     7.48     |\r\n| [RT-DETRv2-L](https://huggingface.co/jenhung/CNO_DETECTION_YOLOv8-L) |    512    |      42.0      |   136.0   |        84.3         |          33.4          |    13.50     |\r\n| [RT-DETRv2-X](https://huggingface.co/jenhung/CNO_DETECTION_YOLOv8-L) |    512    |      76.0      |   259.0   |        83.3         |          32.0          |    21.15     |\r\n\r\n## **Dataset**\r\nThe corneocyte nanotexture dataset is available for download at the following link: [Corneocyte Nanotexture Dataset](https://huggingface.co/datasets/jenhung/Corneocyte_Nanotexture_Dataset).\r\n\r\n## **Contributions**\r\n\r\n[1] Liao, H-S., Wang, J-H., Raun, E., N\u00f8rgaard, L. O., Dons, F. E., & Hwu, E. E-T. (2022). Atopic Dermatitis Severity Assessment using High-Speed Dermal Atomic Force Microscope. Abstract from AFM BioMed Conference 2022, Nagoya-Okazaki, Japan.\r\n\r\n[2] Pereda, J., Liao, H-S., Werner, C., Wang, J-H., Huang, K-Y., Raun, E., N\u00f8rgaard, L. O., Dons, F. E., & Hwu, E. E. T. (2022). Hacking Consumer Electronics for Biomedical Imaging. Abstract from 5th Global Conference on Biomedical Engineering & Annual Meeting of TSBME, Taipei, Taiwan, Province of China.\r\n\r\n[3] Liao, H. S., Akhtar, I., Werner, C., Slipets, R., Pereda, J., Wang, J. H., Raun, E., N\u00f8rgaard, L. O., Dons, F. E., & Hwu, E. E. T. (2022). Open-source controller for low-cost and high-speed atomic force microscopy imaging of skin corneocyte nanotextures. HardwareX, 12, [e00341]. https://doi.org/10.1016/j.ohx.2022.e00341\r\n\r\n----\r\n\r\n### Contact: [Jen-Hung Wang](mailto:jenhw@dtu.dk) / [Assoc. Professor En-Te Hwu](mailto:etehw@dtu.dk)\r\n",
        "doi": "10.48546/workflowhub.workflow.1161.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "1161",
        "keep": true,
        "latest_version": 1,
        "license": "CC0-1.0",
        "link": "https:/workflowhub.eu/workflows/1161?version=1",
        "name": "ECTI Atopic Dermatitis",
        "number_of_steps": 0,
        "projects": [
            "IDUN - Drug Delivery and Sensing"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "image processing"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2024-09-11",
        "versions": 1
    },
    {
        "create_time": "2024-09-02",
        "creators": [
            "Aldar Cabrelles"
        ],
        "description": "# EuCanImage FHIR ETL Implementation\r\n\r\nThis repository contains the ETL implementation for EuCanImage, encouraging semantic interoperability of the clinical data obtained in the studies by transforming it into a machine-readable format following FHIR standards. This parser uses [FHIR Resources](https://github.com/nazrulworld/fhir.resources) in order to create the dictionaries following a FHIR compliant structure.\r\n- Code Language is written in [Python 3.11](https://www.python.org/downloads/release/python-3110/).\r\n- The outputs are JSON files compliant with [FHIR 4.3](https://hl7.org/fhir/R4B/) schemas.\r\n- This script is specifically created for the Extract, Transform and Load implementation for EuCanImage, and will follow the structures obtained from the REDCap databases within the study. To create your own implementation in a different study, you may use the previously mentioned [FHIR Resources](https://github.com/nazrulworld/fhir.resources).\r\n\r\n#### Data conversion process:\r\nThis code followed the structure to go through the following steps:\r\n- Importing and transforming CSV with patient data\r\n- Defining dictionaries for ontologies and functions to populate FHIR dictionaries\r\n- Transforming dictionaries into FHIR resources\r\n- Grouping FHIR resources into a defined bundle/envelope of resources\r\n- Exporting as json file\r\n\r\n#### Input & Output\r\n- CSV file for each use case (CSV folder)\r\n- JSON file following FHIR standards (OUTPUT folder)\r\n\r\n## Installation and Guide\r\nThe first step is to clone or download the repository to your computer\r\n```bash\r\ngit clone https://github.com/EGA-archive/EuCanImage-FHIR.git\r\n```\r\n#### Requirements\r\n- Python 3.11.2\r\n- [FHIR Resources](https://github.com/nazrulworld/fhir.resources) 6.5.0\r\n- pandas 2.1.3\r\n- numpy 1.26.2\r\n\r\nIn order to use these scripts, you will need to have access to [Python 3.11](https://www.python.org/downloads/release/python-3110/) in your systems.\r\n\r\nTo install the libraries used for this study, it can easily be done with `pip install`. The latest versions of each library should not cause any incompatibility.\r\n```bash\r\npip install fhir.resources\r\npip install pandas\r\npip install numpy\r\n```\r\n### Instructions\r\nThe steps are the same on each Use Case, so we will be using Use Case 1 as an example for the steps to follow.\r\n\r\nFirst of all, you will need to provide with a [CSV file](https://github.com/EGA-archive/EuCanImage-FHIR/blob/main/UC1_Hepatocellular_Carcinoma/CSV/UseCase1_testdata.csv) that follows the structure of the eCRF of the study. Each use case will have its own eCRF. Save the CSV file in the [CSV folder](https://github.com/EGA-archive/EuCanImage-FHIR/tree/main/UC1_Hepatocellular_Carcinoma/CSV) of the specific use case you will be using.\r\n\r\nNext, in the beginning of each python file (For example, for Use Case 1 it would be [UC1-ETL.py](https://github.com/EGA-archive/EuCanImage-FHIR/blob/main/UC1_Hepatocellular_Carcinoma/UC1-ETL.py), you will need to change the variable `relative_path_csv` to change the name of the file matching the one of the input.\r\n```bash\r\nrelative_path_csv = \"/UC1_Hepatocellular_Carcinoma/CSV/UseCase1_testdata.csv\"\r\n```\r\nThen, you can run the parser in the terminal, changing `PATH-TO-FOLDER` to the specific folder the parser is in, unless the terminal is run in the folder itself.\r\n```bash\r\npython PATH-TO-FOLDER/UC1-ETL.py\r\n```\r\nOnce it is finished, you will have all of the parsed JSON files in the [OUTPUT](https://github.com/EGA-archive/EuCanImage-FHIR/tree/main/UC1_Hepatocellular_Carcinoma/OUTPUT) folder\r\n",
        "doi": "10.48546/workflowhub.workflow.1112.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "1112",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1112?version=1",
        "name": "EuCanImage FHIR ETL Implementation I: Hepatocellular Carcinoma",
        "number_of_steps": 0,
        "projects": [
            "EGA"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2024-09-09",
        "versions": 1
    },
    {
        "create_time": "2024-09-09",
        "creators": [
            "Aldar Cabrelles"
        ],
        "description": "# EuCanImage FHIR ETL Implementation\r\n\r\nThis repository contains the ETL implementation for EuCanImage, encouraging semantic interoperability of the clinical data obtained in the studies by transforming it into a machine-readable format following FHIR standards. This parser uses [FHIR Resources](https://github.com/nazrulworld/fhir.resources) in order to create the dictionaries following a FHIR compliant structure.\r\n- Code Language is written in [Python 3.11](https://www.python.org/downloads/release/python-3110/).\r\n- The outputs are JSON files compliant with [FHIR 4.3](https://hl7.org/fhir/R4B/) schemas.\r\n- This script is specifically created for the Extract, Transform and Load implementation for EuCanImage, and will follow the structures obtained from the REDCap databases within the study. To create your own implementation in a different study, you may use the previously mentioned [FHIR Resources](https://github.com/nazrulworld/fhir.resources).\r\n\r\n#### Data conversion process:\r\nThis code followed the structure to go through the following steps:\r\n- Importing and transforming CSV with patient data\r\n- Defining dictionaries for ontologies and functions to populate FHIR dictionaries\r\n- Transforming dictionaries into FHIR resources\r\n- Grouping FHIR resources into a defined bundle/envelope of resources\r\n- Exporting as json file\r\n\r\n#### Input & Output\r\n- CSV file for each use case (CSV folder)\r\n- JSON file following FHIR standards (OUTPUT folder)\r\n\r\n## Installation and Guide\r\nThe first step is to clone or download the repository to your computer\r\n```bash\r\ngit clone https://github.com/EGA-archive/EuCanImage-FHIR.git\r\n```\r\n#### Requirements\r\n- Python 3.11.2\r\n- [FHIR Resources](https://github.com/nazrulworld/fhir.resources) 6.5.0\r\n- pandas 2.1.3\r\n- numpy 1.26.2\r\n\r\nIn order to use these scripts, you will need to have access to [Python 3.11](https://www.python.org/downloads/release/python-3110/) in your systems.\r\n\r\nTo install the libraries used for this study, it can easily be done with `pip install`. The latest versions of each library should not cause any incompatibility.\r\n```bash\r\npip install fhir.resources\r\npip install pandas\r\npip install numpy\r\n```\r\n### Instructions\r\nThe steps are the same on each Use Case, so we will be using Use Case 3 as an example for the steps to follow.\r\n\r\nFirst of all, you will need to provide with a [CSV file](https://github.com/EGA-archive/EuCanImage-FHIR/blob/main/UC3_Colorectal_Liver_metastasis/CSV/UseCase3_testdata.csv) that follows the structure of the eCRF of the study. Each use case will have its own eCRF. Save the CSV file in the [CSV folder](https://github.com/EGA-archive/EuCanImage-FHIR/blob/main/UC3_Colorectal_Liver_metastasis/CSV/) of the specific use case you will be using.\r\n\r\nNext, in the beginning of each python file (For example, for Use Case 3 it would be [UC3-ETL.py](https://github.com/EGA-archive/EuCanImage-FHIR/blob/main/UC3_Colorectal_Liver_metastasis/UC3-ETL.py), you will need to change the variable `relative_path_csv` to change the name of the file matching the one of the input.\r\n```bash\r\nrelative_path_csv = \"/UC3_Colorectal_Liver_metastasis/CSV/UseCase3_testdata.csv\"\r\n```\r\nThen, you can run the parser in the terminal, changing `PATH-TO-FOLDER` to the specific folder the parser is in, unless the terminal is run in the folder itself.\r\n```bash\r\npython PATH-TO-FOLDER/UC3-ETL.py\r\n```\r\nOnce it is finished, you will have all of the parsed JSON files in the [OUTPUT](https://github.com/EGA-archive/EuCanImage-FHIR/tree/main/UC3_Colorectal_Liver_metastasis/OUTPUT) folder\r\n",
        "doi": "10.48546/workflowhub.workflow.1156.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in name",
        "id": "1156",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1156?version=1",
        "name": "EuCanImage FHIR ETL Implementation III: Colorectal Liver metastasis",
        "number_of_steps": 0,
        "projects": [
            "EGA"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2024-09-09",
        "versions": 1
    },
    {
        "create_time": "2024-09-09",
        "creators": [
            "Aldar Cabrelles"
        ],
        "description": "# EuCanImage FHIR ETL Implementation\r\n\r\nThis repository contains the ETL implementation for EuCanImage, encouraging semantic interoperability of the clinical data obtained in the studies by transforming it into a machine-readable format following FHIR standards. This parser uses [FHIR Resources](https://github.com/nazrulworld/fhir.resources) in order to create the dictionaries following a FHIR compliant structure.\r\n- Code Language is written in [Python 3.11](https://www.python.org/downloads/release/python-3110/).\r\n- The outputs are JSON files compliant with [FHIR 4.3](https://hl7.org/fhir/R4B/) schemas.\r\n- This script is specifically created for the Extract, Transform and Load implementation for EuCanImage, and will follow the structures obtained from the REDCap databases within the study. To create your own implementation in a different study, you may use the previously mentioned [FHIR Resources](https://github.com/nazrulworld/fhir.resources).\r\n\r\n#### Data conversion process:\r\nThis code followed the structure to go through the following steps:\r\n- Importing and transforming CSV with patient data\r\n- Defining dictionaries for ontologies and functions to populate FHIR dictionaries\r\n- Transforming dictionaries into FHIR resources\r\n- Grouping FHIR resources into a defined bundle/envelope of resources\r\n- Exporting as json file\r\n\r\n#### Input & Output\r\n- CSV file for each use case (CSV folder)\r\n- JSON file following FHIR standards (OUTPUT folder)\r\n\r\n## Installation and Guide\r\nThe first step is to clone or download the repository to your computer\r\n```bash\r\ngit clone https://github.com/EGA-archive/EuCanImage-FHIR.git\r\n```\r\n#### Requirements\r\n- Python 3.11.2\r\n- [FHIR Resources](https://github.com/nazrulworld/fhir.resources) 6.5.0\r\n- pandas 2.1.3\r\n- numpy 1.26.2\r\n\r\nIn order to use these scripts, you will need to have access to [Python 3.11](https://www.python.org/downloads/release/python-3110/) in your systems.\r\n\r\nTo install the libraries used for this study, it can easily be done with `pip install`. The latest versions of each library should not cause any incompatibility.\r\n```bash\r\npip install fhir.resources\r\npip install pandas\r\npip install numpy\r\n```\r\n### Instructions\r\nThe steps are the same on each Use Case, so we will be using Use Case 6 & 8 as an example for the steps to follow.\r\n\r\nFirst of all, you will need to provide with a [CSV file](https://github.com/EGA-archive/EuCanImage-FHIR/tree/main/UC6%268_Breast_Cancer_MMG/CSV/UseCase68_testdata.csv) that follows the structure of the eCRF of the study. Each use case will have its own eCRF. Save the CSV file in the [CSV folder](https://github.com/EGA-archive/EuCanImage-FHIR/tree/main/UC6%268_Breast_Cancer_MMG/CSV) of the specific use case you will be using.\r\n\r\nNext, in the beginning of each python file (For example, for Use Case 6 & 8 it would be [UC68-ETL.py](https://github.com/EGA-archive/EuCanImage-FHIR/blob/main/UC6%268_Breast_Cancer_MMG/UC68-ETL.py), you will need to change the variable `relative_path_csv` to change the name of the file matching the one of the input.\r\n```bash\r\nrelative_path_csv = \"UC6&8_Breast_Cancer_MMG/CSV/UseCase68_testdata.csv\"\r\n```\r\nThen, you can run the parser in the terminal, changing `PATH-TO-FOLDER` to the specific folder the parser is in, unless the terminal is run in the folder itself.\r\n```bash\r\npython PATH-TO-FOLDER/UC68-ETL.py\r\n```\r\nOnce it is finished, you will have all of the parsed JSON files in the [OUTPUT](https://github.com/EGA-archive/EuCanImage-FHIR/tree/main/UC6%268_Breast_Cancer_MMG/OUTPUT) folder\r\n",
        "doi": "10.48546/workflowhub.workflow.1158.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "1158",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1158?version=1",
        "name": "EuCanImage FHIR ETL Implementation VI & VIII: Breast Cancer MMG",
        "number_of_steps": 0,
        "projects": [
            "EGA"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2024-09-09",
        "versions": 1
    },
    {
        "create_time": "2024-09-09",
        "creators": [
            "Aldar Cabrelles"
        ],
        "description": "# EuCanImage FHIR ETL Implementation\r\n\r\nThis repository contains the ETL implementation for EuCanImage, encouraging semantic interoperability of the clinical data obtained in the studies by transforming it into a machine-readable format following FHIR standards. This parser uses [FHIR Resources](https://github.com/nazrulworld/fhir.resources) in order to create the dictionaries following a FHIR compliant structure.\r\n- Code Language is written in [Python 3.11](https://www.python.org/downloads/release/python-3110/).\r\n- The outputs are JSON files compliant with [FHIR 4.3](https://hl7.org/fhir/R4B/) schemas.\r\n- This script is specifically created for the Extract, Transform and Load implementation for EuCanImage, and will follow the structures obtained from the REDCap databases within the study. To create your own implementation in a different study, you may use the previously mentioned [FHIR Resources](https://github.com/nazrulworld/fhir.resources).\r\n\r\n#### Data conversion process:\r\nThis code followed the structure to go through the following steps:\r\n- Importing and transforming CSV with patient data\r\n- Defining dictionaries for ontologies and functions to populate FHIR dictionaries\r\n- Transforming dictionaries into FHIR resources\r\n- Grouping FHIR resources into a defined bundle/envelope of resources\r\n- Exporting as json file\r\n\r\n#### Input & Output\r\n- CSV file for each use case (CSV folder)\r\n- JSON file following FHIR standards (OUTPUT folder)\r\n\r\n## Installation and Guide\r\nThe first step is to clone or download the repository to your computer\r\n```bash\r\ngit clone https://github.com/EGA-archive/EuCanImage-FHIR.git\r\n```\r\n#### Requirements\r\n- Python 3.11.2\r\n- [FHIR Resources](https://github.com/nazrulworld/fhir.resources) 6.5.0\r\n- pandas 2.1.3\r\n- numpy 1.26.2\r\n\r\nIn order to use these scripts, you will need to have access to [Python 3.11](https://www.python.org/downloads/release/python-3110/) in your systems.\r\n\r\nTo install the libraries used for this study, it can easily be done with `pip install`. The latest versions of each library should not cause any incompatibility.\r\n```bash\r\npip install fhir.resources\r\npip install pandas\r\npip install numpy\r\n```\r\n### Instructions\r\nThe steps are the same on each Use Case, so we will be using Use Case 4 & 5 as an example for the steps to follow.\r\n\r\nFirst of all, you will need to provide with a [CSV file](https://github.com/EGA-archive/EuCanImage-FHIR/tree/main/UC4%265_Rectal_Cancer/CSV/UseCase45_testdata.csv) that follows the structure of the eCRF of the study. Each use case will have its own eCRF. Save the CSV file in the [CSV folder](https://github.com/EGA-archive/EuCanImage-FHIR/tree/main/UC4%265_Rectal_Cancer/CSV) of the specific use case you will be using.\r\n\r\nNext, in the beginning of each python file (For example, for Use Case 4 & 5 it would be [UC45-ETL.py](https://github.com/EGA-archive/EuCanImage-FHIR/blob/main/UC4%265_Rectal_Cancer/UC45-ETL.py), you will need to change the variable `relative_path_csv` to change the name of the file matching the one of the input.\r\n```bash\r\nrelative_path_csv = \"/UC4&5_Rectal_Cancer/CSV/UseCase45_testdata.csv\"\r\n```\r\nThen, you can run the parser in the terminal, changing `PATH-TO-FOLDER` to the specific folder the parser is in, unless the terminal is run in the folder itself.\r\n```bash\r\npython PATH-TO-FOLDER/UC45-ETL.py\r\n```\r\nOnce it is finished, you will have all of the parsed JSON files in the [OUTPUT](https://github.com/EGA-archive/EuCanImage-FHIR/tree/main/UC4%265_Rectal_Cancer/OUTPUT) folder\r\n",
        "doi": "10.48546/workflowhub.workflow.1157.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "1157",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1157?version=1",
        "name": "EuCanImage FHIR ETL Implementation IV & V: Rectal Cancer",
        "number_of_steps": 0,
        "projects": [
            "EGA"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2024-09-09",
        "versions": 1
    },
    {
        "create_time": "2024-09-09",
        "creators": [
            "Aldar Cabrelles"
        ],
        "description": "# EuCanImage FHIR ETL Implementation\r\n\r\nThis repository contains the ETL implementation for EuCanImage, encouraging semantic interoperability of the clinical data obtained in the studies by transforming it into a machine-readable format following FHIR standards. This parser uses [FHIR Resources](https://github.com/nazrulworld/fhir.resources) in order to create the dictionaries following a FHIR compliant structure.\r\n- Code Language is written in [Python 3.11](https://www.python.org/downloads/release/python-3110/).\r\n- The outputs are JSON files compliant with [FHIR 4.3](https://hl7.org/fhir/R4B/) schemas.\r\n- This script is specifically created for the Extract, Transform and Load implementation for EuCanImage, and will follow the structures obtained from the REDCap databases within the study. To create your own implementation in a different study, you may use the previously mentioned [FHIR Resources](https://github.com/nazrulworld/fhir.resources).\r\n\r\n#### Data conversion process:\r\nThis code followed the structure to go through the following steps:\r\n- Importing and transforming CSV with patient data\r\n- Defining dictionaries for ontologies and functions to populate FHIR dictionaries\r\n- Transforming dictionaries into FHIR resources\r\n- Grouping FHIR resources into a defined bundle/envelope of resources\r\n- Exporting as json file\r\n\r\n#### Input & Output\r\n- CSV file for each use case (CSV folder)\r\n- JSON file following FHIR standards (OUTPUT folder)\r\n\r\n## Installation and Guide\r\nThe first step is to clone or download the repository to your computer\r\n```bash\r\ngit clone https://github.com/EGA-archive/EuCanImage-FHIR.git\r\n```\r\n#### Requirements\r\n- Python 3.11.2\r\n- [FHIR Resources](https://github.com/nazrulworld/fhir.resources) 6.5.0\r\n- pandas 2.1.3\r\n- numpy 1.26.2\r\n\r\nIn order to use these scripts, you will need to have access to [Python 3.11](https://www.python.org/downloads/release/python-3110/) in your systems.\r\n\r\nTo install the libraries used for this study, it can easily be done with `pip install`. The latest versions of each library should not cause any incompatibility.\r\n```bash\r\npip install fhir.resources\r\npip install pandas\r\npip install numpy\r\n```\r\n### Instructions\r\nThe steps are the same on each Use Case, so we will be using Use Case 7 as an example for the steps to follow.\r\n\r\nFirst of all, you will need to provide with a [CSV file](https://github.com/EGA-archive/EuCanImage-FHIR/tree/main/UC7_Breast_Cancer_MRI/CSV/UseCase7_testdata.csv) that follows the structure of the eCRF of the study. Each use case will have its own eCRF. Save the CSV file in the [CSV folder](https://github.com/EGA-archive/EuCanImage-FHIR/tree/main/UC7_Breast_Cancer_MRI/CSV) of the specific use case you will be using.\r\n\r\nNext, in the beginning of each python file (For example, for Use Case 7 it would be [UC7-ETL.py](https://github.com/EGA-archive/EuCanImage-FHIR/blob/main/UC7_Breast_Cancer_MRI/UC7-ETL.py), you will need to change the variable `relative_path_csv` to change the name of the file matching the one of the input.\r\n```bash\r\nrelative_path_csv = \"/UC7_Breast_Cancer_MRI/CSV/UseCase7_testdata.csv\"\r\n```\r\nThen, you can run the parser in the terminal, changing `PATH-TO-FOLDER` to the specific folder the parser is in, unless the terminal is run in the folder itself.\r\n```bash\r\npython PATH-TO-FOLDER/UC7-ETL.py\r\n```\r\nOnce it is finished, you will have all of the parsed JSON files in the [OUTPUT](https://github.com/EGA-archive/EuCanImage-FHIR/tree/main/UC7_Breast_Cancer_MRI/OUTPUT) folder\r\n",
        "doi": "10.48546/workflowhub.workflow.1159.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "1159",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1159?version=1",
        "name": "EuCanImage FHIR ETL Implementation VII: Breast Cancer MRI",
        "number_of_steps": 0,
        "projects": [
            "EGA"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2024-09-09",
        "versions": 1
    },
    {
        "create_time": "2022-11-24",
        "creators": [],
        "description": "With this galaxy pipeline you can use Salmonella sp. next generation sequencing results to predict bacterial AMR phenotypes and compare the results against gold standard Salmonella sp. phenotypes obtained from food.\r\n\r\nThis pipeline is based on the work of the National Food Agency of Canada.  \r\nDoi: [10.3389/fmicb.2020.00549](https://doi.org/10.3389/fmicb.2020.00549)\r\n\r\n| tool | version | license |\r\n| -- | -- | -- |\r\n| SeqSero2 | 1.2.1 | [GNU GPL v2.0](https://github.com/denglab/SeqSero2/blob/master/LICENSE) |\r\n| BBTools | 39.01 | [MIT License](https://github.com/kbaseapps/BBTools/blob/master/LICENSE) |\r\n| SRST2 | 0.2.0 | [BSD License](https://github.com/katholt/srst2/blob/master/LICENSE.txt) |\r\n| hamronize | 1.0.3 | [GNU LGPL v3.0](https://github.com/pha4ge/hAMRonization/blob/master/LICENSE.txt) |\r\n| SPAdes | v3.15.5 | [GNU GPL v2.0](https://github.com/ablab/spades/blob/main/LICENSE) |\r\n| SKESA | 3.0.0 | [Public Domain](https://github.com/ncbi/SKESA/blob/master/LICENSE) |\r\n| pilon | 1.1.0 | [GNU GPL v2.0](https://github.com/broadinstitute/pilon/blob/master/LICENSE) |\r\n| shovill | 1.0.4 | [GPL-3.0 license](https://github.com/tseemann/shovill/blob/master/LICENSE) |\r\n| sistr | 1.1.1 | [Apache-2.0 license](https://github.com/phac-nml/sistr_cmd/blob/master/LICENSE) |\r\n| MOB-Recon | 3.0.3 | [Apache-2.0 license](https://github.com/phac-nml/mob-suite/blob/master/LICENSE) |",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "antimicrobial in tags",
        "id": "407",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/407?version=1",
        "name": "Workflow 3: AMR - SeqSero2/SISTR",
        "number_of_steps": 14,
        "projects": [
            "Seq4AMR"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "antimicrobial resistance"
        ],
        "tools": [
            "__UNZIP_COLLECTION__",
            "kma_map",
            "srst2",
            "hamronize_tool",
            "sistr_cmd",
            "bbtools_tadpole",
            "shovill",
            "hamronize_summarize",
            "mob_recon",
            "seqsero2",
            "bbtools_bbduk"
        ],
        "type": "Galaxy",
        "update_time": "2024-09-09",
        "versions": 1
    },
    {
        "create_time": "2023-05-11",
        "creators": [],
        "description": "Correlation between Phenotypic and In Silico Detection of Antimicrobial Resistance in Salmonella enterica in Canada Using Staramr. \r\n\r\nDoi: [10.3390/microorganisms10020292](https://doi.org/10.3390/microorganisms10020292)\r\n\r\n| tool | version | license |\r\n| -- | -- | -- |\r\n| staramr | 0.8.0 | [Apache-2.0 license](https://github.com/phac-nml/staramr/blob/development/LICENSE) |\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "antimicrobial in tags",
        "id": "470",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/470?version=1",
        "name": "Workflow 4: Staramr",
        "number_of_steps": 10,
        "projects": [
            "Seq4AMR"
        ],
        "source": "WorkflowHub",
        "tags": [
            "10.3390/microorganisms10020292",
            "amr",
            "amr-detection",
            "bioinformatics",
            "antimicrobial resistance"
        ],
        "tools": [
            "hamronize_tool",
            "shovill",
            "abricate",
            "tp_find_and_replace",
            "hamronize_summarize",
            "collapse_dataset",
            "staramr_search"
        ],
        "type": "Galaxy",
        "update_time": "2024-09-09",
        "versions": 1
    },
    {
        "create_time": "2023-10-31",
        "creators": [],
        "description": "| tool | version | license |\r\n| -- | -- | -- |\r\n| abritAMR | 1.0.14 | [CC-BY-4.0](https://zenodo.org/records/12514579) | ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amr in name",
        "id": "634",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/634?version=1",
        "name": "Workflow 1: AbritAMR",
        "number_of_steps": 1,
        "projects": [
            "Seq4AMR"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "abritamr"
        ],
        "type": "Galaxy",
        "update_time": "2024-09-09",
        "versions": 1
    },
    {
        "create_time": "2024-09-03",
        "creators": [
            "Anna Syme"
        ],
        "description": "**Genome assembly workflow for nanopore reads, for TSI**\r\n\r\nInput: \r\n* Nanopore reads (can be in format: fastq, fastq.gz, fastqsanger, or fastqsanger.gz) \r\n\r\nOptional settings to specify when the workflow is run:\r\n* [1] how many input files to split the original input into (to speed up the workflow). default = 0. example: set to 2000 to split a 60 GB read file into 2000 files of ~ 30 MB. \r\n* [2] filtering: min average read quality score. default = 10\r\n* [3] filtering: min read length. default = 200\r\n* [4] trimming: trim this many nucleotides from start of read. default = 50\r\n* [5] note: these are suggestions and will depend on the characteristics of your raw reads and downstream aims. If filtering and trimming settings are too stringent, there may be no reads remaining and workflow will fail. \r\n\r\nWorkflow steps:\r\n* [1] runs FastQC on raw reads\r\n* [2] splits input reads file into separate files to speed up the next step of Porechop\r\n* [3] trims nanopore adapters using Porechop\r\n* [4] trims and filters nanopore reads by quality and length using Nanofilt\r\n* [5] collapses back into a single read file, fastqsanger format\r\n* [6] runs FastqQC on trimmed/filtered reads\r\n* [7] assembles genome with Flye\r\n* [8] calculates statistics on genome assembly contigs with Fasta Statistics\r\n* [9] draws genome assembly graph with Bandage \r\n\r\nMain outputs:\r\n* [1] FastQC report on raw reads, html\r\n* [2] Adpater-chopped, trimmed, filtered reads in fastqsanger format\r\n* [3] FastQC report on filtered reads, html\r\n* [4] genome assembly contigs in fasta format (primary assembly)\r\n* [5] genome assembly statistics\r\n* [6] genome assembly graph in Bandage format \r\n\r\nNote: You may wish to plot raw reads first (e.g. using the tool NanoPlot), to get a better of idea of read lengths and quality, to decide on filtering/trimming settings.",
        "doi": "10.48546/workflowhub.workflow.1114.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "1114",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1114?version=1",
        "name": "Genome assembly workflow for nanopore reads, for TSI",
        "number_of_steps": 10,
        "projects": [
            "Australian BioCommons",
            "Galaxy Australia"
        ],
        "source": "WorkflowHub",
        "tags": [
            "tsi",
            "genome_assembly",
            "nanopore"
        ],
        "tools": [
            "bandage_info",
            "fastqc",
            "porechop",
            "bandage_image",
            "fasta-stats",
            "collapse_dataset",
            "nanofilt",
            "flye",
            "split_file_to_collection"
        ],
        "type": "Galaxy",
        "update_time": "2024-09-03",
        "versions": 1
    },
    {
        "create_time": "2024-08-23",
        "creators": [
            "\u4ed5\u5353 \u5f20"
        ],
        "description": "GraphRBF is a state-of-the-art protein-protein/nucleic acid interaction site prediction model built by enhanced graph neural networks and prioritized radial basis function neural networks. \r\nThis project serves users to use our software to directly predict protein binding sites or train our model on a new database.  \r\nIdentification of protein-protein and protein-nucleic acid binding sites provides insights into biological processes related to protein functions and technical guidance for disease diagnosis and drug design. However, accurate predictions by computational approaches remain highly challenging due to the limited knowledge of residue binding patterns. The binding pattern of a residue should be characterized by the spatial distribution of its neighboring residues combined with their physicochemical information interaction, which yet can not be achieved by previous methods. Here, we design GraphRBF, a hierarchical geometric deep learning model to learn residue binding patterns from big data. To achieve it, GraphRBF describes physicochemical information interactions by designing an enhanced graph neural network and characterizes residue spatial distributions by introducing a prioritized radial basis function neural network. After training and testing, GraphRBF shows great improvements over existing state-of-the-art methods and strong interpretability of its learned representations. \r\n",
        "doi": "10.48546/workflowhub.workflow.1107.1",
        "edam_operation": [
            "Binding site prediction"
        ],
        "edam_topic": [
            "Protein binding sites"
        ],
        "filtered_on": "binn* in name",
        "id": "1107",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1107?version=1",
        "name": "Protein-protein and protein-nucleic acid binding site prediction via interpretable hierarchical geometric deep learning",
        "number_of_steps": 0,
        "projects": [
            "Protein-protein and protein-nucleic acid binding site prediction research"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics"
        ],
        "tools": [],
        "type": "BioCompute Object",
        "update_time": "2024-08-23",
        "versions": 1
    },
    {
        "create_time": "2024-08-23",
        "creators": [
            "Mahesh Binzer-Panchal",
            "Martin Pippel"
        ],
        "description": "# Swedish Earth Biogenome Project - Genome Assembly Workflow\r\n\r\nThe primary genome assembly workflow for the Earth Biogenome Project at NBIS.\r\n\r\n## Workflow overview\r\n\r\nGeneral aim:\r\n\r\n```mermaid\r\nflowchart LR\r\n    hifi[/ HiFi reads /] --> data_inspection\r\n    ont[/ ONT reads /] -->  data_inspection\r\n    hic[/ Hi-C reads /] --> data_inspection\r\n    data_inspection[[ Data inspection ]] --> preprocessing\r\n    preprocessing[[ Preprocessing ]] --> assemble\r\n    assemble[[ Assemble ]] --> validation\r\n    validation[[ Assembly validation ]] --> curation\r\n    curation[[ Assembly curation ]] --> validation\r\n```\r\n\r\nCurrent implementation:\r\n\r\n```mermaid\r\nflowchart TD\r\n    input[/ Input file/] --> hifi\r\n    input --> hic\r\n    input --> taxonkit[[ TaxonKit name2taxid/reformat ]]\r\n    taxonkit --> goat_taxon[[ GOAT taxon search ]]\r\n    goat_taxon --> busco\r\n    goat_taxon --> dtol[[ DToL lookup ]]\r\n    hifi --> samtools_fa[[ Samtools fasta ]]\r\n    samtools_fa --> fastk_hifi\r\n    samtools_fa --> mash_screen\r\n    hifi[/ HiFi reads /] --> fastk_hifi[[ FastK - HiFi ]]\r\n    hifi --> meryl_hifi[[ Meryl - HiFi ]]\r\n    hic[/ Hi-C reads /] --> fastk_hic[[ FastK - Hi-C ]]\r\n    hifi --> meryl_hic[[ Meryl - Hi-C ]]\r\n    assembly[/ Assembly /] --> quast[[ Quast ]]\r\n    fastk_hifi --> histex[[ Histex ]]\r\n    histex --> genescopefk[[ GeneScopeFK ]]\r\n    fastk_hifi --> ploidyplot[[ PloidyPlot ]]\r\n    fastk_hifi --> katgc[[ KatGC ]]\r\n    fastk_hifi --> merquryfk[[ MerquryFK ]]\r\n    assembly --> merquryfk\r\n    meryl_hifi --> merqury[[ Merqury ]]\r\n    assembly --> merqury\r\n    fastk_hifi --> katcomp[[ KatComp ]]\r\n    fastk_hic --> katcomp\r\n    assembly --> busco[[ Busco ]]\r\n    refseq_sketch[( RefSeq sketch )] --> mash_screen[[ Mash Screen ]]\r\n    hifi --> mash_screen\r\n    fastk_hifi --> hifiasm[[ HiFiasm ]]\r\n    hifiasm --> assembly\r\n    assembly --> purgedups[[ Purgedups ]]\r\n    input --> mitoref[[ Mitohifi - Find reference ]]\r\n    assembly --> mitohifi[[ Mitohifi ]]\r\n    assembly --> fcsgx[[ FCS GX ]]\r\n    fcs_fetchdb[( FCS fetchdb )] --> fcsgx\r\n    mitoref --> mitohifi\r\n    genescopefk --> quarto[[ Quarto ]]\r\n    goat_taxon --> multiqc[[ MultiQC ]]\r\n    quarto --> multiqc\r\n    dtol --> multiqc\r\n    katgc --> multiqc\r\n    ploidyplot --> multiqc\r\n    busco --> multiqc\r\n    quast --> multiqc\r\n```\r\n\r\n## Usage\r\n\r\n```bash\r\nnextflow run -params-file <params.yml> \\\r\n    [ -c <custom.config> ] \\\r\n    [ -profile <profile> ] \\\r\n    NBISweden/Earth-Biogenome-Project-pilot\r\n```\r\n\r\nwhere:\r\n- `params.yml` is a YAML formatted file containing workflow parameters\r\n    such as input paths to the assembly specification, and settings for tools within the workflow.\r\n\r\n    Example:\r\n\r\n    ```yml\r\n    input: 'assembly_spec.yml'\r\n    outdir: results\r\n    fastk: # Optional\r\n      kmer_size: 31 # default 31\r\n    genescopefk: # Optional\r\n      kmer_size: 31 # default 31\r\n    hifiasm: # Optional, default = no extra options: Key (e.g. 'opts01') is used in assembly build name (e.g., 'hifiasm-raw-opts01').\r\n      opts01: \"--opts A\"\r\n      opts02: \"--opts B\"\r\n    busco: # Optional, default: retrieved from GOAT\r\n      lineages: 'auto' # comma separated string of lineages or auto.\r\n    ```\r\n\r\n    Alternatively parameters can be provided on the\r\n    command-line using the `--parameter` notation (e.g., `--input <path>` ).\r\n- `<custom.config>` is a Nextflow configuration file which provides\r\n    additional configuration. This is used to customise settings other than\r\n    workflow parameters, such as cpus, time, and command-line options to tools.\r\n\r\n    Example:\r\n    ```nextflow\r\n    process {\r\n        withName: 'BUSCO' {  // Selects the process to apply settings.\r\n            cpus     = 6     // Overrides cpu settings defined in nextflow.config\r\n            time     = 4.d   // Overrides time settings defined in nextflow.config to 4 days. Use .h for hours, .m for minutes.\r\n            memory   = '20GB'  // Overrides memory settings defined in nextflow.config to 20 GB.\r\n            // ext.args supplies command-line options to the process tool\r\n            // overrides settings found in configs/modules.config\r\n            ext.args = '--long'  // Supplies these as command-line options to Busco\r\n        }\r\n    }\r\n    ```\r\n- `<profile>` is one of the preconfigured execution profiles\r\n    (`uppmax`, `singularity_local`, `docker_local`, etc: see nextflow.config). Alternatively,\r\n    you can provide a custom configuration to configure this workflow\r\n    to your execution environment. See [Nextflow Configuration](https://www.nextflow.io/docs/latest/config.html#scope-executor)\r\n    for more details.\r\n\r\n\r\n### Workflow parameter inputs\r\n\r\nMandatory:\r\n\r\n- `input`: A YAML formatted input file.\r\n    Example `assembly_spec.yml` (See also [test profile input](assets/test_hsapiens.yml) TODO:: Update test profile):\r\n\r\n    ```yml\r\n    sample:                          # Required: Meta data\r\n      name: 'Laetiporus sulphureus'  # Required: Species name. Correct spelling is important to look up species information.\r\n      ploidy: 2                      # Optional: Estimated ploidy (default: retrieved from GOAT)\r\n      genome_size: 2345              # Optional: Estimated genome size (default: retrieved from GOAT)\r\n      haploid_number: 13             # Optional: Estimated haploid chromosome count (default: retrieved from GOAT)\r\n      taxid: 5630                    # Optional: Taxon ID (default: retrieved with Taxonkit)\r\n      kingdom: Eukaryota             #\u00a0Optional: (default: retrived with Taxonkit)\r\n    assembly:                        # Optional: List of assemblies to curate and validate.\r\n      - assembler: hifiasm           # For each entry, the assembler,\r\n        stage: raw                   # stage of assembly,\r\n        id: uuid                     # unique id,\r\n        pri_fasta: /path/to/primary_asm.fasta # and paths to sequences are required.\r\n        alt_fasta: /path/to/alternate_asm.fasta\r\n        pri_gfa: /path/to/primary_asm.gfa\r\n        alt_gfa: /path/to/alternate_asm.gfa\r\n      - assembler: ipa\r\n        stage: raw\r\n        id: uuid\r\n        pri_fasta: /path/to/primary_asm.fasta\r\n        alt_fasta: /path/to/alternate_asm.fasta\r\n    hic:                             # Optional: List of hi-c reads to QC and use for scaffolding\r\n      - read1: '/path/to/raw/data/hic/LS_HIC_R001_1.fastq.gz'\r\n        read2: '/path/to/raw/data/hic/LS_HIC_R001_2.fastq.gz'\r\n    hifi:                            # Required: List of hifi-reads to QC and use for assembly/validation\r\n      - reads: '/path/to/raw/data/hifi/LS_HIFI_R001.bam'\r\n    rnaseq:                          # Optional: List of Rna-seq reads to use for validation\r\n      - read1: '/path/to/raw/data/rnaseq/LS_RNASEQ_R001_1.fastq.gz'\r\n        read2: '/path/to/raw/data/rnaseq/LS_RNASEQ_R001_2.fastq.gz'\r\n    isoseq:                          # Optional: List of Isoseq reads to use for validation\r\n      - reads: '/path/to/raw/data/isoseq/LS_ISOSEQ_R001.bam'\r\n    ```\r\n\r\n\r\nOptional:\r\n\r\n- `outdir`: The publishing path for results (default: `results`).\r\n- `publish_mode`: (values: `'symlink'` (default), `'copy'`) The file\r\npublishing method from the intermediate results folders\r\n(see [Table of publish modes](https://www.nextflow.io/docs/latest/process.html#publishdir)).\r\n- `steps`: The workflow steps to execute (default is all steps). Choose from:\r\n\r\n    - `inspect`: 01 - Read inspection\r\n    - `preprocess`: 02 - Read preprocessing\r\n    - `assemble`: 03 - Assembly\r\n    - `purge`: 04 - Duplicate purging\r\n    - `polish`: 05 - Error polishing\r\n    - `screen`: 06 - Contamination screening\r\n    - `scaffold`: 07 - Scaffolding\r\n    - `curate`: 08 - Rapid curation\r\n    - `alignRNA`: 09 - Align RNAseq data\r\n\r\nSoftware specific:\r\n\r\nTool specific settings are provided by supplying values to specific keys or supplying an array of\r\nsettings under a tool name. The input to `-params-file` would look like this:\r\n\r\n```yml\r\ninput: assembly.yml\r\noutdir: results\r\nfastk:\r\n  kmer_size: 31\r\ngenescopefk:\r\n  kmer_size: 31\r\nhifiasm:\r\n  opts01: \"--opts A\"\r\n  opts02: \"--opts B\"\r\nbusco:\r\n  lineages: 'auto'\r\n```\r\n\r\n- `multiqc_config`: Path to MultiQC configuration file (default: `configs/multiqc_conf.yaml`).\r\n\r\nUppmax and PDC cluster specific:\r\n\r\n- `project`: NAISS Compute allocation number.\r\n\r\n### Workflow outputs\r\n\r\nAll results are published to the path assigned to the workflow parameter `results`.\r\n\r\nTODO:: List folder contents in results file\r\n### Customization for Uppmax\r\n\r\nA custom profile named `uppmax` is available to run this workflow specifically\r\non UPPMAX clusters. The process `executor` is `slurm` so jobs are\r\nsubmitted to the Slurm Queue Manager. All jobs submitted to slurm\r\nmust have a project allocation. This is automatically added to the `clusterOptions`\r\nin the `uppmax` profile. All Uppmax clusters have node local disk space to do\r\ncomputations, and prevent heavy input/output over the network (which\r\nslows down the cluster for all).\r\nThe path to this disk space is provided by the `$SNIC_TMP` variable, used by\r\nthe `process.scratch` directive in the `uppmax` profile. Lastly\r\nthe profile enables the use of Singularity so that all processes must be\r\nexecuted within Singularity containers. See [nextflow.config](nextflow.config)\r\nfor the profile specification.\r\n\r\nThe profile is enabled using the `-profile` parameter to nextflow:\r\n```bash\r\nnextflow run -profile uppmax <nextflow_script>\r\n```\r\n\r\nA NAISS compute allocation should also be supplied using the `--project` parameter.\r\n\r\n### Customization for PDC\r\n\r\nA custom profile named `dardel` is available to run this workflow specifically\r\non the PDC cluster *Dardel*. The process `executor` is `slurm` so jobs are\r\nsubmitted to the Slurm Queue Manager. All jobs submitted to slurm\r\nmust have a project allocation. This is automatically added to the `clusterOptions`\r\nin the `dardel` profile. Calculations are performed in the scratch space allocated\r\nby `PDC_TMP` which is also on the lustre file system and is not node local storage.\r\nThe path to this disk space is provided by the `$PDC_TMP` variable, used by\r\nthe `process.scratch` directive in the `dardel` profile. Lastly\r\nthe profile enables the use of Singularity so that all processes must be\r\nexecuted within Singularity containers. See [nextflow.config](nextflow.config)\r\nfor the profile specification.\r\n\r\nThe profile is enabled using the `-profile` parameter to nextflow:\r\n```bash\r\nnextflow run -profile dardel <nextflow_script>\r\n```\r\n\r\nA NAISS compute allocation should also be supplied using the `--project` parameter.\r\n\r\n## Workflow organization\r\n\r\nThe workflows in this folder manage the execution of your analyses\r\nfrom beginning to end.\r\n\r\n```\r\nworkflow/\r\n | - .github/                        Github data such as actions to run\r\n | - assets/                         Workflow assets such as test samplesheets\r\n | - bin/                            Custom workflow scripts\r\n | - configs/                        Configuration files that govern workflow execution\r\n | - dockerfiles/                    Custom container definition files\r\n | - docs/                           Workflow usage and interpretation information\r\n | - modules/                        Process definitions for tools used in the workflow\r\n | - subworkflows/                   Custom workflows for different stages of the main analysis\r\n | - tests/                          Workflow tests\r\n | - main.nf                         The primary analysis script\r\n | - nextflow.config                 General Nextflow configuration\r\n \\ - modules.json                    nf-core file which tracks modules/subworkflows from nf-core\r\n```\r\n\r\n",
        "doi": null,
        "edam_operation": [
            "Contact map calculation",
            "Sequence profile generation",
            "k-mer counting"
        ],
        "edam_topic": [
            "Sequence assembly"
        ],
        "filtered_on": "metap* in description",
        "id": "1106",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1106?version=1",
        "name": "Swedish Earth Biogenome Project Genome Assembly Workflow",
        "number_of_steps": 0,
        "projects": [
            "NBIS",
            "ERGA Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genome_assembly"
        ],
        "tools": [
            "Hifiasm",
            "GenomeScope 2.0",
            "Merqury",
            "Meryl",
            "BUSCO",
            "QUAST",
            "purge_dups",
            "SAMtools",
            "TaxonKit",
            "MitoHiFi",
            "Mash"
        ],
        "type": "Nextflow",
        "update_time": "2024-08-26",
        "versions": 1
    },
    {
        "create_time": "2024-08-21",
        "creators": [
            "Marie Joss\u00e9"
        ],
        "description": "Secondary metabolite biosynthetic gene cluster (SMBGC) Annotation using Neural Networks Trained on Interpro Signatures ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Marine biology"
        ],
        "filtered_on": "metap* in description",
        "id": "1105",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1105?version=1",
        "name": "Marine Omics identifying biosynthetic gene clusters",
        "number_of_steps": 5,
        "projects": [
            "FAIR-EASE",
            "usegalaxy-eu"
        ],
        "source": "WorkflowHub",
        "tags": [
            "earth-system",
            "ecology",
            "marine omics",
            "ocean"
        ],
        "tools": [
            "Create the protein fasta file",
            "Create TSV file for Sanntis",
            "Use of Sanntis",
            "Remove useless * in the protein fasta file"
        ],
        "type": "Galaxy",
        "update_time": "2024-08-21",
        "versions": 1
    },
    {
        "create_time": "2024-08-15",
        "creators": [
            "Ann-Kathrin D\u00f6rr"
        ],
        "description": "# RiboSnake: 16S rRNA analysis workflow with QIIME2 and Snakemake\r\n\r\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22656.10-brightgreen.svg)](https://snakemake.bitbucket.io)\r\n[![Build Status](https://travis-ci.org/snakemake-workflows/16S.svg?branch=master)](https://travis-ci.org/snakemake-workflows/16S)\r\n\r\nQiime2 workflow for 16S analysis created with snakemake.\r\n\r\n## Authors\r\n\r\n* Ann-Kathrin D\u00f6rr (@AKBrueggemann)\r\n\r\n## Usage\r\n\r\nIf you use this workflow in a paper, don't forget to give credits to the authors by citing the URL of this (original) repository and, if available, its DOI (see above).\r\n\r\n### Step 1: Obtain a copy of this workflow\r\n\r\nIf you want to use the workflow, please obtain a copy of it by either:\r\n[Cloning](https://help.github.com/en/articles/cloning-a-repository) the repository to your local system, into the place where you want to perform the data analysis or\r\nDownloading a zip-file of the repository to your local machine.\r\n\r\nWhen you have the folder structure added on your local machine, please add a \"data\" folder manually.\r\n\r\n### Step 2: Configure workflow\r\n\r\nConfigure the workflow according to your needs via editing the files in the `config/` folder. Adjust `config.yaml` to configure the workflow execution, and `metadata.txt` to specify your sample setup.\r\n\r\nSome important parameters you should check and set according to your own FASTQ-files in the `config.yaml` are primers for the forward and reverse reads, the `datatype`, that should be used by QIIME2 and the `min-seq-length`. Based on the sequencing, the length of the reads can vary.\r\n\r\nThe default parameters for filtering and truncation were validated with the help of a MOCK community and fitted to retrieve all bacteria from that community.\r\n\r\nIn addition to that, you need to fit the metadata-parameters to your data. Please change the names of the used metadata-columns according to your information.\r\nTake special care of the \"remove-columns\" information. Here you can add the columns you don't want to have analyzed or the workflow can't anlyse. This can happen when\r\nall of the values in one column are unique or all the same. You should also look out for the information under \"metadata-parameters\" and \"songbird\" as well as \"ancom\".\r\nIn every case you have to specify the column names based on your own data.\r\n\r\nIf your metadata is not containing numeric values, please use the \"reduced-analysis\" option in the config file to run the workflow, as the workflow is currently not able to run only on categorical metadata for the full analysis version. We are going to fix that in the future.\r\n\r\nThe workflow is able to perform clustering and denoising either with vsearch, leading to OTU creation, or with DADA2, creating ASVs. You can decide which modus to use by setting the variable \"DADA2\" to `True` (DADA2 usage) or `False` (vsearch).\r\n\r\nPlease make sure, that the names of your FASTQ files are correctly formatted. They should look like this:\r\n\r\n    samplename_SNumber_Lane_R1/R2_001.fastq.gz\r\n\r\nIn the config file you can also set the input and output directory. You can either create a specific directory for your input data and then put that filepath in the config file, or you can put the path to an existing directory where the data is located.\r\nThe data will then be copied to the workflow's data directory. The compressed and final file holding the results will be copied to the directory you specified in \"output\". It will also stay in the local \"results\" folder together with important intermediate results.\r\nThe \"data\" folder is also not provided by the repository. It is the folder the fastq files are copied to before being used in the workflow. It is best if you create the folder inside the workflows folder structure. It must definitely be created on the machine, the workflow is running on.\r\n\r\n### Step 3: Install Snakemake\r\n\r\nCreate a snakemake environment using [mamba](https://mamba.readthedocs.io/en/latest/) via:\r\n\r\n    mamba create -c conda-forge -c bioconda -n snakemake snakemake\r\n\r\nFor installation details, see the [instructions in the Snakemake documentation](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html).\r\n\r\n### Step 4: Execute workflow\r\n\r\nActivate the conda environment:\r\n\r\n    conda activate snakemake\r\n\r\nFill up the `metadata.txt` with the information of your samples:\r\n\r\n    Please be careful to not include spaces between the commas. If there is a column, that you don't have any information about, please leave it empty and simply go on with the next column.\r\n\r\nTest your configuration by performing a dry-run via\r\n\r\n    snakemake --use-conda -n\r\n\r\nExecuting the workflow takes two steps:\r\n\r\n    Data preparation: snakemake --cores $N --use-conda data_prep\r\n    Workflow execution: snakemake --cores $N --use-conda\r\n\r\nusing `$N` cores.\r\n\r\nWhen running on snakemake > 8.0 we recommend setting the --shared-fs-usage none as well as setting the environment variable TEMP to a local directory to prevent problems with the usage of the fs-storage system.\r\nThe environment variable can be set like this:\r\n\r\n    conda activate your_environment_name\r\n    export TEMP=/path/to/local/tmp\r\n\r\nThen run the snakemake command like above with the addition of the storage flag:\r\n\r\n    snakemake --cores $N --use-conda --shared-fs-usage none\r\n\r\n### Step 5: Investigate results\r\n\r\nAfter successful execution, the workflow provides you with a compressed folder, holding all interesting results ready to decompress or to download to your local machine.\r\nThe compressed file 16S-report.tar.gz holds several qiime2-artifacts that can be inspected via qiime-view. In the zipped folder report.zip is the snakemake html\r\nreport holding graphics as well as the DAG of the executed jobs and html files leading you directly to the qiime2-results, without the need of using qiime-view.\r\n\r\nThis report can, e.g., be forwarded to your collaborators.\r\n\r\n### Step 6: Obtain updates from upstream\r\n\r\nWhenever you want to synchronize your workflow copy with new developments from upstream, do the following.\r\n\r\n1. Once, register the upstream repository in your local copy: `git remote add -f upstream git@github.com:snakemake-workflows/16S.git` or `git remote add -f upstream https://github.com/snakemake-workflows/16S.git` if you do not have setup ssh keys.\r\n2. Update the upstream version: `git fetch upstream`.\r\n3. Create a diff with the current version: `git diff HEAD upstream/master workflow > upstream-changes.diff`.\r\n4. Investigate the changes: `vim upstream-changes.diff`.\r\n5. Apply the modified diff via: `git apply upstream-changes.diff`.\r\n6. Carefully check whether you need to update the config files: `git diff HEAD upstream/master config`. If so, do it manually, and only where necessary, since you would otherwise likely overwrite your settings and samples.\r\n\r\n## Contribute back\r\n\r\nIn case you have also changed or added steps, please consider contributing them back to the original repository:\r\n\r\n### Step 1: Forking the repository\r\n\r\n[Fork](https://help.github.com/en/articles/fork-a-repo) the original repo to a personal or lab account.\r\n\r\n### Step 2: Cloning\r\n\r\n[Clone](https://help.github.com/en/articles/cloning-a-repository) the fork to your local system, to a different place than where you ran your analysis.\r\n\r\n### Step 3: Add changes\r\n\r\n1. Copy the modified files from your analysis to the clone of your fork, e.g., `cp -r workflow path/to/fork`. Make sure to **not** accidentally copy config file contents or sample sheets. Instead, manually update the example config files if necessary.\r\n2. Commit and push your changes to your fork.\r\n3. Create a [pull request](https://help.github.com/en/articles/creating-a-pull-request) against the original repository.\r\n4. If you want to add your config file and the parameters as a new default parameter sets, please do this by opening a pull request adding the file to the \"contributions\" folder.\r\n\r\n## Testing\r\n\r\nTest cases are in the subfolder `.test`. They are automatically executed via continuous integration with [Github Actions](https://github.com/features/actions).\r\nIf you want to test the RiboSnake functions yourself, you can use the same data used for the CI/CD tests. The used fastq files can be downloaded [here](https://data.qiime2.org/2022.2/tutorials/importing/casava-18-paired-end-demultiplexed.zip). They have been published by Neilson et al., mSystems, 2017.\r\n\r\n### Example\r\n\r\n1. First clone teh repository to your local machine as described above.\r\n2. Download a dataset of your liking, or the data used for testing the pipeline. The FASTQ files can be downloaded with:\r\n    curl -sL \\\r\n          \"https://data.qiime2.org/2022.2/tutorials/importing/casava-18-paired-end-demultiplexed.zip\"\r\n3. Unzip the data into a folder of your liking, it can be called \"incoming\" but it does not have to be.\r\nIf you name your folder differently, please change the \"input\" path in the config file.\r\n4. If you don't want to use the whole dataset for testing, remove some of the FASTQ files from the folder:\r\n    rm PAP*\r\n    rm YUN*\r\n    rm Rep*\r\n    rm blank*\r\n5. Use the information that can be found in [this](https://data.qiime2.org/2024.5/tutorials/atacama-soils/sample_metadata.tsv) file from the Qiime2 tutorial, to fill out your metadata.txt file for the samples starting with \"BAQ\".\r\n6. The default-parameters to be used in the config file can be found in the provided file \"PowerSoil-Illumina-soil.yaml\" in the config folder.\r\n7. With these parameters and the previous steps, you should be able to execute the workflow.\r\n\r\n## Tools\r\n\r\nA list of the tools used in this pipeline:\r\n\r\n| Tool         | Link                                              |\r\n|--------------|---------------------------------------------------|\r\n| QIIME2       | www.doi.org/10.1038/s41587-019-0209-9             |\r\n| Snakemake    | www.doi.org/10.12688/f1000research.29032.1        |\r\n| FastQC       | www.bioinformatics.babraham.ac.uk/projects/fastqc |\r\n| MultiQC      | www.doi.org/10.1093/bioinformatics/btw354         |\r\n| pandas       | pandas.pydata.org                                 |\r\n| kraken2      | www.doi.org/10.1186/s13059-019-1891-0             |\r\n| vsearch      | www.github.com/torognes/vsearch                   |\r\n| DADA2        | www.doi.org/10.1038/nmeth.3869                    |\r\n| songbird     | www.doi.org/10.1038/s41467-019-10656-5            |\r\n| bowtie2      | www.doi.org/10.1038/nmeth.1923                    |\r\n| Ancom        | www.doi.org/10.3402/mehd.v26.27663                |\r\n| cutadapt     | www.doi.org/10.14806/ej.17.1.200                  |\r\n| BLAST        | www.doi.org/10.1016/S0022-2836(05)80360-2         |\r\n| gneiss       | www.doi.org/10.1128/mSystems.00162-16             |\r\n| qurro        | www.doi.org/10.1093/nargab/lqaa023                |\r\n| Rescript     | www.doi.org/10.1371/journal.pcbi.1009581          |",
        "doi": "10.48546/workflowhub.workflow.1102.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "16S in name",
        "id": "1102",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1102?version=1",
        "name": "RiboSnake: 16S rRNA analysis workflow with QIIME2 and Snakemake",
        "number_of_steps": 0,
        "projects": [
            "16S rRNA Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-08-15",
        "versions": 1
    },
    {
        "create_time": "2024-08-02",
        "creators": [
            "Timon Schlegel"
        ],
        "description": "This workflow takes a cell-type-annotated AnnData object (processed with SnapATAC2) and performs peak calling with MACS3 on the cell types. Next, a cell-by-peak matrix is constructed and differential accessibility tests are performed for comparison of either two cell types or one cell type with a background of all other cells. \r\nLastly, differentially accessible marker regions for each cell type are identified. ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1089",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1089?version=1",
        "name": "Differential peak analysis with SnapATAC2",
        "number_of_steps": 7,
        "projects": [
            "usegalaxy-eu"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "snapatac2_plotting",
            "snapatac2_peaks_and_motif"
        ],
        "type": "Galaxy",
        "update_time": "2024-08-02",
        "versions": 1
    },
    {
        "create_time": "2024-07-17",
        "creators": [
            "Timon Schlegel"
        ],
        "description": "Workflow for Single-cell ATAC-seq standard processing with SnapATAC2.\r\nThis workflow takes a fragment file as input and performs the standard steps of scATAC-seq analysis: filtering, dimension reduction, embedding and visualization of marker genes with SnapATAC2. Finally, the clusters are manually annotated with the help of marker genes. \r\nIn an alternative step, the fragment file can also be generated from a BAM file. \r\n* newer Version: Updated SnapATAC2 version from 2.5.3 to 2.6.4",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1077",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1077?version=1",
        "name": "Workflow - Standard processing of 10X single cell ATAC-seq data with SnapATAC2",
        "number_of_steps": 29,
        "projects": [
            "usegalaxy-eu"
        ],
        "source": "WorkflowHub",
        "tags": [
            "epigenetics",
            "scatac-seq"
        ],
        "tools": [
            "snapatac2_plotting",
            "scanpy_plot",
            "scanpy_inspect",
            "anndata_ops",
            "scanpy_filter",
            "Cut1",
            "snapatac2_preprocessing",
            "replace_column_with_key_value_file",
            "scanpy_normalize",
            "anndata_inspect",
            "snapatac2_clustering",
            "anndata_manipulate"
        ],
        "type": "Galaxy",
        "update_time": "2024-08-30",
        "versions": 1
    },
    {
        "create_time": "2024-08-02",
        "creators": [
            "Ra\u00fcl Sirvent"
        ],
        "description": "**Name:** Matrix Multiplication  \r\n**Contact Person:** support-compss@bsc.es  \r\n**Access Level:** public  \r\n**License Agreement:** Apache2  \r\n**Platform:** COMPSs  \r\n\r\n# Description\r\nMatrix multiplication is a binary operation that takes a pair of matrices and produces another matrix.\r\n\r\nIf A is an n\u00d7m matrix and B is an m\u00d7p matrix, the result AB of their multiplication is an n\u00d7p matrix defined only if the number of columns m in A is equal to the number of rows m in B. When multiplying A and B, the elements of the rows in A are multiplied with corresponding columns in B.\r\n\r\nIn this implementation, A and B are square matrices (same number of rows and columns), and so it is the result matrix C. Each matrix is divided in N blocks of M doubles. The multiplication of two blocks is done by a multiply task method with a simple three-nested-loop implementation. When executed with COMPSs, the main program generates N^3^ tasks arranged as N^2^ chains of N tasks in the dependency graph.\r\n\r\nN and M have been hardcoded to 6 and 8 respectively.\r\n\r\n# Execution instructions\r\nUsage:\r\n```\r\nruncompss --classpath=application_sources/jar/matmul.jar matmul.files.Matmul inputFolder/ outputFolder/\r\n``` \r\n\r\nwhere:\r\n  * inputFolder: folder where input files are located\r\n  * outputFolder: folder where output files are located\r\n\r\n# Build\r\n## Option 1: Native java\r\n```\r\njavac src/main/java/matmul/*/*.java\r\ncd src/main/java/; jar cf matmul.jar matmul/\r\ncd ../../../; mv src/main/java/matmul.jar jar/\r\n```\r\n\r\n## Option 2: Maven\r\n```\r\nmvn clean package\r\n```\r\n",
        "doi": "10.48546/workflowhub.workflow.1088.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1088",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1088?version=1",
        "name": "Java COMPSs Matrix Multiplication, out-of-core using files, reproducible example, data persistence False, MareNostrum V",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [
            "compss",
            "example",
            "java",
            "marenostrum v",
            "supercomputer",
            "tutorial",
            "non_data_persistence"
        ],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2024-08-02",
        "versions": 1
    },
    {
        "create_time": "2024-08-02",
        "creators": [
            "Ra\u00fcl Sirvent"
        ],
        "description": "**Name:** Matrix Multiplication  \r\n**Contact Person:** support-compss@bsc.es  \r\n**Access Level:** public  \r\n**License Agreement:** Apache2  \r\n**Platform:** COMPSs  \r\n\r\n# Description\r\nMatrix multiplication is a binary operation that takes a pair of matrices and produces another matrix.\r\n\r\nIf A is an n\u00d7m matrix and B is an m\u00d7p matrix, the result AB of their multiplication is an n\u00d7p matrix defined only if the number of columns m in A is equal to the number of rows m in B. When multiplying A and B, the elements of the rows in A are multiplied with corresponding columns in B.\r\n\r\nIn this implementation, A and B are square matrices (same number of rows and columns), and so it is the result matrix C. Each matrix is divided in N blocks of M doubles. The multiplication of two blocks is done by a multiply task method with a simple three-nested-loop implementation. When executed with COMPSs, the main program generates N^3^ tasks arranged as N^2^ chains of N tasks in the dependency graph.\r\n\r\nN and M have been hardcoded to 6 and 8 respectively.\r\n\r\n# Execution instructions\r\nUsage:\r\n```\r\nruncompss --classpath=application_sources/jar/matmul.jar matmul.files.Matmul inputFolder/ outputFolder/\r\n``` \r\n\r\nwhere:\r\n  * inputFolder: folder where input files are located\r\n  * outputFolder: folder where output files are located\r\n\r\n# Build\r\n## Option 1: Native java\r\n```\r\njavac src/main/java/matmul/*/*.java\r\ncd src/main/java/; jar cf matmul.jar matmul/\r\ncd ../../../; mv src/main/java/matmul.jar jar/\r\n```\r\n\r\n## Option 2: Maven\r\n```\r\nmvn clean package\r\n```\r\n",
        "doi": "10.48546/workflowhub.workflow.1086.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1086",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1086?version=1",
        "name": "Java COMPSs Matrix Multiplication, out-of-core using files, reproducible example, data persistence True",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [
            "compss",
            "example",
            "java",
            "laptop",
            "tutorial",
            "data_persistence"
        ],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2024-08-02",
        "versions": 1
    },
    {
        "create_time": "2024-07-12",
        "creators": [],
        "description": "# deepconsensus 1.2 snakemake pipeline\r\nThis snakemake-based workflow takes in a subreads.bam and results in a deepconsensus.fastq\r\n- no methylation calls !\r\n\r\nThe metadata id of the subreads file needs to be: \"m[numeric]_[numeric]_[numeric].subreads.bam\"\r\n\r\nChunking (how many subjobs) and ccs min quality filter can be adjusted in the config.yaml\r\n\r\nthe checkpoint model for deepconsensus1.2 should be accessible like this:\r\ngsutil cp -r gs://brain-genomics-public/research/deepconsensus/models/v1.2/model_checkpoint/* \"${QS_DIR}\"/model/\r\nif that does not work, try to download all at:\r\nhttps://console.cloud.google.com/storage/browser/brain-genomics-public/research/deepconsensus/models?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false\r\n\r\nA run example is included in the run_snake.sh\r\n\r\nFeedback / pull requests welcome!\r\n\r\nDeveloped by Daniel Rickert @ WGGC D\u00fcsseldorf\r\n\r\nmore to look at:\r\n\r\nhttps://www.youtube.com/watch?v=TlWtIao2i9E\r\n\r\nhttps://www.nature.com/articles/s41587-022-01435-7\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Sequence analysis"
        ],
        "filtered_on": "metap* in description",
        "id": "1075",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1075?version=1",
        "name": "Deepconsensus for Sequel2/2e subreads",
        "number_of_steps": 0,
        "projects": [
            "WGGC"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics"
        ],
        "tools": [
            "MultiQC",
            "fastp",
            "SAMtools"
        ],
        "type": "Snakemake",
        "update_time": "2024-07-17",
        "versions": 1
    },
    {
        "create_time": "2024-06-26",
        "creators": [
            "B\u00e9r\u00e9nice Batut",
            "Engy Nasr",
            "Paul Zierep"
        ],
        "description": "Microbiome - QC and Contamination Filtering",
        "doi": "10.48546/workflowhub.workflow.1061.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiome in description",
        "id": "1061",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1061?version=1",
        "name": "nanopore-pre-processing/main",
        "number_of_steps": 25,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "nanoplot",
            "fastp",
            "Add_a_column1",
            "regexColumn1",
            "krakentools_extract_kraken_reads",
            "fastqc",
            "minimap2",
            "porechop",
            "Cut1",
            "collection_column_join",
            "samtools_fastx",
            "kraken2",
            "collapse_dataset",
            "__FILTER_FAILED_DATASETS__",
            "bamtools_split_mapped",
            "Grep1",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2024-06-26",
        "creators": [
            "Engy Nasr",
            "B\u00e9r\u00e9nice Batut",
            "Paul Zierep"
        ],
        "description": "Microbiome - Taxonomy Profiling",
        "doi": "10.48546/workflowhub.workflow.1059.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in name",
        "id": "1059",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1059?version=1",
        "name": "taxonomy-profiling-and-visualization-with-krona/main",
        "number_of_steps": 3,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "taxonomy_krona_chart",
            "kraken2",
            "krakentools_kreport2krona"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2024-06-25",
        "creators": [
            "Arnau Soler Costa",
            "Amy Curwin",
            "Jordi Rambla"
        ],
        "description": "# ![IMPaCT program](https://github.com/EGA-archive/sarek-IMPaCT-data-QC/blob/master/impact_qc/docs/png/impact_data_logo_pink_horitzontal.png)\r\n\r\n[![IMPaCT](https://img.shields.io/badge/Web%20-IMPaCT-blue)](https://impact.isciii.es/)\r\n[![IMPaCT-isciii](https://img.shields.io/badge/Web%20-IMPaCT--isciii-red)](https://www.isciii.es/QueHacemos/Financiacion/IMPaCT/Paginas/default.aspx)\r\n[![IMPaCT-Data](https://img.shields.io/badge/Web%20-IMPaCT--Data-1d355c.svg?labelColor=000000)](https://impact-data.bsc.es/)\r\n\r\n## Introduction of the project\r\n\r\nIMPaCT-Data is the IMPaCT program that aims to support the development of a common, interoperable and integrated system for the collection and analysis of clinical and molecular data by providing the knowledge and resources available in the Spanish Science and Technology System. This development will make it possible to answer research questions based on the different clinical and molecular information systems available. Fundamentally, it aims to provide researchers with a population perspective based on individual data.\r\n\r\nThe IMPaCT-Data project is divided into different work packages (WP). In the context of IMPaCT-Data WP3 (Genomics), a working group of experts worked on the generation of a specific quality control (QC) workflow for germline exome samples.\r\n\r\nTo achieve this, a set of metrics related to human genomic data was decided upon, and the toolset or software to extract these metrics was implemented in an existing variant calling workflow called Sarek, part of the nf-core community. The final outcome is a Nextflow subworkflow, called IMPaCT-QC implemented in the Sarek pipeline.\r\n\r\nBelow you can find the explanation of this workflow (raw pipeline), the link to the documentation of the IMPaCT QC subworkflow and a linked documentation associated to the QC metrics added in the mentioned workflow.\r\n\r\n- [IMPaCT-data subworkflow documentation](https://github.com/EGA-archive/sarek-IMPaCT-data-QC/tree/master/impact_qc)\r\n\r\n- [Metrics documentation](https://github.com/EGA-archive/sarek-IMPaCT-data-QC/blob/master/impact_qc/docs/QC_Sarek_supporing_documentation.pdf)\r\n\r\n<h1>\r\n  <picture>\r\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-sarek_logo_dark.png\">\r\n    <img alt=\"nf-core/sarek\" src=\"docs/images/nf-core-sarek_logo_light.png\">\r\n  </picture>\r\n</h1>\r\n\r\n[![GitHub Actions CI Status](https://github.com/nf-core/sarek/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/sarek/actions/workflows/ci.yml)\r\n[![GitHub Actions Linting Status](https://github.com/nf-core/sarek/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/sarek/actions/workflows/linting.yml)\r\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/sarek/results)\r\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\r\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.3476425-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.3476425)\r\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/nf-core/sarek)\r\n\r\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23sarek-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/sarek)\r\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)\r\n[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)\r\n[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\r\n\r\n## Introduction\r\n\r\n**nf-core/sarek** is a workflow designed to detect variants on whole genome or targeted sequencing data. Initially designed for Human, and Mouse, it can work on any species with a reference genome. Sarek can also handle tumour / normal pairs and could include additional relapses.\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/sarek/results).\r\n\r\nIt's listed on [Elixir - Tools and Data Services Registry](https://bio.tools/nf-core-sarek) and [Dockstore](https://dockstore.org/workflows/github.com/nf-core/sarek).\r\n\r\n<p align=\"center\">\r\n    <img title=\"Sarek Workflow\" src=\"docs/images/sarek_workflow.png\" width=30%>\r\n</p>\r\n\r\n## Pipeline summary\r\n\r\nDepending on the options and samples provided, the pipeline can currently perform the following:\r\n\r\n- Form consensus reads from UMI sequences (`fgbio`)\r\n- Sequencing quality control and trimming (enabled by `--trim_fastq`) (`FastQC`, `fastp`)\r\n- Map Reads to Reference (`BWA-mem`, `BWA-mem2`, `dragmap` or `Sentieon BWA-mem`)\r\n- Process BAM file (`GATK MarkDuplicates`, `GATK BaseRecalibrator` and `GATK ApplyBQSR` or `Sentieon LocusCollector` and `Sentieon Dedup`)\r\n- Summarise alignment statistics (`samtools stats`, `mosdepth`)\r\n- Variant calling (enabled by `--tools`, see [compatibility](https://nf-co.re/sarek/latest/docs/usage#which-variant-calling-tool-is-implemented-for-which-data-type)):\r\n  - `ASCAT`\r\n  - `CNVkit`\r\n  - `Control-FREEC`\r\n  - `DeepVariant`\r\n  - `freebayes`\r\n  - `GATK HaplotypeCaller`\r\n  - `Manta`\r\n  - `mpileup`\r\n  - `MSIsensor-pro`\r\n  - `Mutect2`\r\n  - `Sentieon Haplotyper`\r\n  - `Strelka2`\r\n  - `TIDDIT`\r\n- Variant filtering and annotation (`SnpEff`, `Ensembl VEP`, `BCFtools annotate`)\r\n- Summarise and represent QC (`MultiQC`)\r\n\r\n<p align=\"center\">\r\n    <img title=\"Sarek Workflow\" src=\"docs/images/sarek_subway.png\" width=60%>\r\n</p>\r\n\r\n## Usage\r\n\r\n> [!NOTE]\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\r\n\r\nFirst, prepare a samplesheet with your input data that looks as follows:\r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\npatient,sample,lane,fastq_1,fastq_2\r\nID1,S1,L002,ID1_S1_L002_R1_001.fastq.gz,ID1_S1_L002_R2_001.fastq.gz\r\n```\r\n\r\nEach row represents a pair of fastq files (paired end).\r\n\r\nNow, you can run the pipeline using:\r\n\r\n```bash\r\nnextflow run nf-core/sarek \\\r\n   -profile <docker/singularity/.../institute> \\\r\n   --input samplesheet.csv \\\r\n   --outdir <OUTDIR>\r\n```\r\n\r\n> [!WARNING]\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\r\n\r\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/sarek/usage) and the [parameter documentation](https://nf-co.re/sarek/parameters).\r\n\r\n## Pipeline output\r\n\r\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/sarek/results) tab on the nf-core website pipeline page.\r\nFor more details about the output files and reports, please refer to the\r\n[output documentation](https://nf-co.re/sarek/output).\r\n\r\n## Benchmarking\r\n\r\nOn each release, the pipeline is run on 3 full size tests:\r\n\r\n- `test_full` runs tumor-normal data for one patient from the SEQ2C consortium\r\n- `test_full_germline` runs a WGS 30X Genome-in-a-Bottle(NA12878) dataset\r\n- `test_full_germline_ncbench_agilent` runs two WES samples with 75M and 200M reads (data available [here](https://github.com/ncbench/ncbench-workflow#contributing-callsets)). The results are uploaded to Zenodo, evaluated against a truth dataset, and results are made available via the [NCBench dashboard](https://ncbench.github.io/report/report.html#).\r\n\r\n## Credits\r\n\r\nSarek was originally written by Maxime U Garcia and Szilveszter Juhos at the [National Genomics Infastructure](https://ngisweden.scilifelab.se) and [National Bioinformatics Infastructure Sweden](https://nbis.se) which are both platforms at [SciLifeLab](https://scilifelab.se), with the support of [The Swedish Childhood Tumor Biobank (Barntum\u00f6rbanken)](https://ki.se/forskning/barntumorbanken).\r\nFriederike Hanssen and Gisela Gabernet at [QBiC](https://www.qbic.uni-tuebingen.de/) later joined and helped with further development.\r\n\r\nThe Nextflow DSL2 conversion of the pipeline was lead by Friederike Hanssen and Maxime U Garcia.\r\n\r\nMaintenance is now lead by Friederike Hanssen and Maxime U Garcia (now at [Seqera Labs](https://seqera/io))\r\n\r\nMain developers:\r\n\r\n- [Maxime U Garcia](https://github.com/maxulysse)\r\n- [Friederike Hanssen](https://github.com/FriederikeHanssen)\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n- [Abhinav Sharma](https://github.com/abhi18av)\r\n- [Adam Talbot](https://github.com/adamrtalbot)\r\n- [Adrian L\u00e4rkeryd](https://github.com/adrlar)\r\n- [Alexander Peltzer](https://github.com/apeltzer)\r\n- [Alison Meynert](https://github.com/ameynert)\r\n- [Anders Sune Pedersen](https://github.com/asp8200)\r\n- [arontommi](https://github.com/arontommi)\r\n- [BarryDigby](https://github.com/BarryDigby)\r\n- [Bekir Erg\u00fcner](https://github.com/berguner)\r\n- [bjornnystedt](https://github.com/bjornnystedt)\r\n- [cgpu](https://github.com/cgpu)\r\n- [Chela James](https://github.com/chelauk)\r\n- [David Mas-Ponte](https://github.com/davidmasp)\r\n- [Edmund Miller](https://github.com/edmundmiller)\r\n- [Francesco Lescai](https://github.com/lescai)\r\n- [Gavin Mackenzie](https://github.com/GCJMackenzie)\r\n- [Gisela Gabernet](https://github.com/ggabernet)\r\n- [Grant Neilson](https://github.com/grantn5)\r\n- [gulfshores](https://github.com/gulfshores)\r\n- [Harshil Patel](https://github.com/drpatelh)\r\n- [James A. Fellows Yates](https://github.com/jfy133)\r\n- [Jesper Eisfeldt](https://github.com/J35P312)\r\n- [Johannes Alneberg](https://github.com/alneberg)\r\n- [Jos\u00e9 Fern\u00e1ndez Navarro](https://github.com/jfnavarro)\r\n- [J\u00falia Mir Pedrol](https://github.com/mirpedrol)\r\n- [Ken Brewer](https://github.com/kenibrewer)\r\n- [Lasse Westergaard Folkersen](https://github.com/lassefolkersen)\r\n- [Lucia Conde](https://github.com/lconde-ucl)\r\n- [Malin Larsson](https://github.com/malinlarsson)\r\n- [Marcel Martin](https://github.com/marcelm)\r\n- [Nick Smith](https://github.com/nickhsmith)\r\n- [Nicolas Schcolnicov](https://github.com/nschcolnicov)\r\n- [Nilesh Tawari](https://github.com/nilesh-tawari)\r\n- [Nils Homer](https://github.com/nh13)\r\n- [Olga Botvinnik](https://github.com/olgabot)\r\n- [Oskar Wacker](https://github.com/WackerO)\r\n- [pallolason](https://github.com/pallolason)\r\n- [Paul Cantalupo](https://github.com/pcantalupo)\r\n- [Phil Ewels](https://github.com/ewels)\r\n- [Sabrina Krakau](https://github.com/skrakau)\r\n- [Sam Minot](https://github.com/sminot)\r\n- [Sebastian-D](https://github.com/Sebastian-D)\r\n- [Silvia Morini](https://github.com/silviamorins)\r\n- [Simon Pearce](https://github.com/SPPearce)\r\n- [Solenne Correard](https://github.com/scorreard)\r\n- [Susanne Jodoin](https://github.com/SusiJo)\r\n- [Szilveszter Juhos](https://github.com/szilvajuhos)\r\n- [Tobias Koch](https://github.com/KochTobi)\r\n- [Winni Kretzschmar](https://github.com/winni2k)\r\n\r\n## Acknowledgements\r\n\r\n|      [![Barntum\u00f6rbanken](docs/images/BTB_logo.png)](https://ki.se/forskning/barntumorbanken)      |            [![SciLifeLab](docs/images/SciLifeLab_logo.png)](https://scilifelab.se)             |\r\n| :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |\r\n| [![National Genomics Infrastructure](docs/images/NGI_logo.png)](https://ngisweden.scilifelab.se/) | [![National Bioinformatics Infrastructure Sweden](docs/images/NBIS_logo.png)](https://nbis.se) |\r\n|              [![QBiC](docs/images/QBiC_logo.png)](https://www.qbic.uni-tuebingen.de)              |                   [![GHGA](docs/images/GHGA_logo.png)](https://www.ghga.de/)                   |\r\n|                     [![DNGC](docs/images/DNGC_logo.png)](https://eng.ngc.dk/)                     |                                                                                                |\r\n\r\n## Contributions & Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#sarek` channel](https://nfcore.slack.com/channels/sarek) (you can join with [this invite](https://nf-co.re/join/slack)), or contact us: [Maxime U Garcia](mailto:maxime.garcia@seqera.io?subject=[GitHub]%20nf-core/sarek), [Friederike Hanssen](mailto:friederike.hanssen@qbic.uni-tuebingen.de?subject=[GitHub]%20nf-core/sarek)\r\n\r\n## Citations\r\n\r\nIf you use `nf-core/sarek` for your analysis, please cite the `Sarek` article as follows:\r\n\r\n> Friederike Hanssen, Maxime U Garcia, Lasse Folkersen, Anders Sune Pedersen, Francesco Lescai, Susanne Jodoin, Edmund Miller, Oskar Wacker, Nicholas Smith, nf-core community, Gisela Gabernet, Sven Nahnsen **Scalable and efficient DNA sequencing analysis on different compute infrastructures aiding variant discovery** _NAR Genomics and Bioinformatics_ Volume 6, Issue 2, June 2024, lqae031, [doi: 10.1093/nargab/lqae031](https://doi.org/10.1093/nargab/lqae031).\r\n\r\n> Garcia M, Juhos S, Larsson M et al. **Sarek: A portable workflow for whole-genome sequencing analysis of germline and somatic variants [version 2; peer review: 2 approved]** _F1000Research_ 2020, 9:63 [doi: 10.12688/f1000research.16665.2](http://dx.doi.org/10.12688/f1000research.16665.2).\r\n\r\nYou can cite the sarek zenodo record for a specific version using the following [doi: 10.5281/zenodo.3476425](https://doi.org/10.5281/zenodo.3476425)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n\r\n## CHANGELOG\r\n\r\n- [CHANGELOG](CHANGELOG.md)\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "1030",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1030?version=3",
        "name": "IMPaCT-Data quality control workflow implementation in nf-core/Sarek",
        "number_of_steps": 0,
        "projects": [
            "EGA"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "ega-archive",
            "ngs",
            "nextflow",
            "wgs",
            "quality control",
            "variant calling",
            "wes"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-06-25",
        "versions": 3
    },
    {
        "create_time": "2024-06-25",
        "creators": [
            "Usman Rashid",
            "Chen Wu",
            "Jason Shiller",
            "Ken Smith",
            "Ross Crowhurst",
            "Marcus Davy",
            "Ting-Hsuan Chen",
            "Susan Thomson",
            "Cecilia Deng"
        ],
        "description": "[![GitHub Actions CI Status](https://github.com/plant-food-research-open/assemblyqc/actions/workflows/ci.yml/badge.svg)](https://github.com/plant-food-research-open/assemblyqc/actions/workflows/ci.yml)\r\n[![GitHub Actions Linting Status](https://github.com/plant-food-research-open/assemblyqc/actions/workflows/linting.yml/badge.svg)](https://github.com/plant-food-research-open/assemblyqc/actions/workflows/linting.yml)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.10647870-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.10647870)\r\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda \u274c](http://img.shields.io/badge/run%20with-conda%20\u274c-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/plant-food-research-open/assemblyqc)\r\n\r\n## Introduction\r\n\r\n**plant-food-research-open/assemblyqc** is a [NextFlow](https://www.nextflow.io/docs/latest/index.html) pipeline which evaluates assembly quality with multiple QC tools and presents the results in a unified html report. The tools are shown in the [Pipeline Flowchart](#pipeline-flowchart) and their references are listed in [CITATIONS.md](./CITATIONS.md).\r\n\r\n## Pipeline Flowchart\r\n\r\n```mermaid\r\n%%{init: {\r\n    'theme': 'base',\r\n    'themeVariables': {\r\n    'fontSize': '52px\",\r\n    'primaryColor': '#9A6421',\r\n    'primaryTextColor': '#ffffff',\r\n    'primaryBorderColor': '#9A6421',\r\n    'lineColor': '#B180A8',\r\n    'secondaryColor': '#455C58',\r\n    'tertiaryColor': '#ffffff'\r\n  }\r\n}}%%\r\nflowchart LR\r\n  forEachTag(Assembly) ==> VALIDATE_FORMAT[VALIDATE FORMAT]\r\n\r\n  VALIDATE_FORMAT ==> ncbiFCS[NCBI FCS\\nADAPTOR]\r\n  ncbiFCS ==> Check{Check}\r\n\r\n  VALIDATE_FORMAT ==> ncbiGX[NCBI FCS GX]\r\n  ncbiGX ==> Check\r\n  Check ==> |Clean|Run(Run)\r\n\r\n  Check ==> |Contamination|Skip(Skip All)\r\n  Skip ==> REPORT\r\n\r\n  VALIDATE_FORMAT ==> GFF_STATS[GENOMETOOLS GT STAT]\r\n\r\n  Run ==> ASS_STATS[ASSEMBLATHON STATS]\r\n  Run ==> BUSCO\r\n  Run ==> TIDK\r\n  Run ==> LAI\r\n  Run ==> KRAKEN2\r\n  Run ==> HIC_CONTACT_MAP[HIC CONTACT MAP]\r\n  Run ==> MUMMER\r\n  Run ==> MINIMAP2\r\n  Run ==> MERQURY\r\n\r\n  MUMMER ==> CIRCOS\r\n  MUMMER ==> DOTPLOT\r\n\r\n  MINIMAP2 ==> PLOTSR\r\n\r\n  ASS_STATS ==> REPORT\r\n  GFF_STATS ==> REPORT\r\n  BUSCO ==> REPORT\r\n  TIDK ==> REPORT\r\n  LAI ==> REPORT\r\n  KRAKEN2 ==> REPORT\r\n  HIC_CONTACT_MAP ==> REPORT\r\n  CIRCOS ==> REPORT\r\n  DOTPLOT ==> REPORT\r\n  PLOTSR ==> REPORT\r\n  MERQURY ==> REPORT\r\n```\r\n\r\n- [FASTA VALIDATOR](https://github.com/linsalrob/fasta_validator) + [SEQKIT RMDUP](https://github.com/shenwei356/seqkit): FASTA validation\r\n- [GENOMETOOLS GT GFF3VALIDATOR](https://genometools.org/tools/gt_gff3validator.html): GFF3 validation\r\n- [ASSEMBLATHON STATS](https://github.com/PlantandFoodResearch/assemblathon2-analysis/blob/a93cba25d847434f7eadc04e63b58c567c46a56d/assemblathon_stats.pl): Assembly statistics\r\n- [GENOMETOOLS GT STAT](https://genometools.org/tools/gt_stat.html): Annotation statistics\r\n- [NCBI FCS ADAPTOR](https://github.com/ncbi/fcs): Adaptor contamination pass/fail\r\n- [NCBI FCS GX](https://github.com/ncbi/fcs): Foreign organism contamination pass/fail\r\n- [BUSCO](https://gitlab.com/ezlab/busco): Gene-space completeness estimation\r\n- [TIDK](https://github.com/tolkit/telomeric-identifier): Telomere repeat identification\r\n- [LAI](https://github.com/oushujun/LTR_retriever/blob/master/LAI): Continuity of repetitive sequences\r\n- [KRAKEN2](https://github.com/DerrickWood/kraken2): Taxonomy classification\r\n- [HIC CONTACT MAP](https://github.com/igvteam/juicebox.js): Alignment and visualisation of HiC data\r\n- [MUMMER](https://github.com/mummer4/mummer) \u2192 [CIRCOS](http://circos.ca/documentation/) + [DOTPLOT](https://plotly.com) & [MINIMAP2](https://github.com/lh3/minimap2) \u2192 [PLOTSR](https://github.com/schneebergerlab/plotsr): Synteny analysis\r\n- [MERQURY](https://github.com/marbl/merqury): K-mer completeness, consensus quality and phasing assessment\r\n\r\n## Usage\r\n\r\nRefer to [usage](./docs/usage.md), [parameters](./docs/parameters.md) and [output](./docs/output.md) documents for details.\r\n\r\n> [!NOTE]\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\r\n\r\nPrepare an `assemblysheet.csv` file with following columns representing target assemblies and associated meta-data.\r\n\r\n- `tag:` A unique tag which represents the target assembly throughout the pipeline and in the final report\r\n- `fasta:` FASTA file\r\n\r\nNow, you can run the pipeline using:\r\n\r\n```bash\r\nnextflow run plant-food-research-open/assemblyqc \\\r\n   -profile <docker/singularity/.../institute> \\\r\n   --input assemblysheet.csv \\\r\n   --outdir <OUTDIR>\r\n```\r\n\r\n> [!WARNING]\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\r\n\r\n### Plant&Food Users\r\n\r\nDownload the pipeline to your `/workspace/$USER` folder. Change the parameters defined in the [pfr/params.json](./pfr/params.json) file. Submit the pipeline to SLURM for execution.\r\n\r\n```bash\r\nsbatch ./pfr_assemblyqc\r\n```\r\n\r\n## Credits\r\n\r\nplant-food-research-open/assemblyqc was originally written by Usman Rashid ([@gallvp](https://github.com/gallvp)) and Ken Smith ([@hzlnutspread](https://github.com/hzlnutspread)).\r\n\r\nRoss Crowhurst ([@rosscrowhurst](https://github.com/rosscrowhurst)), Chen Wu ([@christinawu2008](https://github.com/christinawu2008)) and Marcus Davy ([@mdavy86](https://github.com/mdavy86)) generously contributed their QC scripts.\r\n\r\nMahesh Binzer-Panchal ([@mahesh-panchal](https://github.com/mahesh-panchal)) helped port the pipeline modules and sub-workflows to [nf-core](https://nf-co.re) schema.\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n- [Cecilia Deng](https://github.com/CeciliaDeng)\r\n- [Ignacio Carvajal](https://github.com/ignacio3437)\r\n- [Jason Shiller](https://github.com/jasonshiller)\r\n- [Sarah Bailey](https://github.com/SarahBailey1998)\r\n- [Susan Thomson](https://github.com/cflsjt)\r\n- [Ting-Hsuan Chen](https://github.com/ting-hsuan-chen)\r\n\r\nThe pipeline uses nf-core modules contributed by following authors:\r\n\r\n<a href=\"https://github.com/gallvp\"><img src=\"https://github.com/gallvp.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/drpatelh\"><img src=\"https://github.com/drpatelh.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/midnighter\"><img src=\"https://github.com/midnighter.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/mahesh-panchal\"><img src=\"https://github.com/mahesh-panchal.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/jfy133\"><img src=\"https://github.com/jfy133.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/adamrtalbot\"><img src=\"https://github.com/adamrtalbot.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/maxulysse\"><img src=\"https://github.com/maxulysse.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/matthdsm\"><img src=\"https://github.com/matthdsm.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/joseespinosa\"><img src=\"https://github.com/joseespinosa.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/ewels\"><img src=\"https://github.com/ewels.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/sofstam\"><img src=\"https://github.com/sofstam.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/sateeshperi\"><img src=\"https://github.com/sateeshperi.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/priyanka-surana\"><img src=\"https://github.com/priyanka-surana.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/phue\"><img src=\"https://github.com/phue.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/muffato\"><img src=\"https://github.com/muffato.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/lescai\"><img src=\"https://github.com/lescai.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/kevinmenden\"><img src=\"https://github.com/kevinmenden.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/jvhagey\"><img src=\"https://github.com/jvhagey.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/joon-klaps\"><img src=\"https://github.com/joon-klaps.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/jeremy1805\"><img src=\"https://github.com/jeremy1805.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/heuermh\"><img src=\"https://github.com/heuermh.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/grst\"><img src=\"https://github.com/grst.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/friederikehanssen\"><img src=\"https://github.com/friederikehanssen.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/felixkrueger\"><img src=\"https://github.com/felixkrueger.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/erikrikarddaniel\"><img src=\"https://github.com/erikrikarddaniel.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/edmundmiller\"><img src=\"https://github.com/edmundmiller.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/d4straub\"><img src=\"https://github.com/d4straub.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/charles-plessy\"><img src=\"https://github.com/charles-plessy.png\" width=\"50\" height=\"50\"></a>\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\n## Citations\r\n\r\nIf you use plant-food-research-open/assemblyqc for your analysis, please cite it as:\r\n\r\n> Rashid, U., Wu, C., Shiller, J., Smith, K., Crowhurst, R., Davy, M., Chen, T.-H., Thomson, S., & Deng, C. (2024). AssemblyQC: A NextFlow pipeline for evaluating assembly quality (2.0.0). Zenodo. https://doi.org/10.5281/zenodo.10647870\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Data quality management",
            "Genomics"
        ],
        "filtered_on": "metap* in description",
        "id": "1058",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1058?version=1",
        "name": "AssemblyQC: A NextFlow pipeline for evaluating assembly quality",
        "number_of_steps": 0,
        "projects": [
            "Plant-Food-Research-Open"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "busco",
            "hi-c",
            "merqury",
            "report",
            "statistics",
            "adaptor",
            "contamination",
            "fcs",
            "genome",
            "k-mer",
            "n50",
            "phasing",
            "quality control",
            "repeat",
            "synteny",
            "taxonomy",
            "telomere"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-06-25",
        "versions": 1
    },
    {
        "create_time": "2024-06-20",
        "creators": [
            "Luke Silver",
            "Anna Syme"
        ],
        "description": "This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \r\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \r\n\r\nThe workflows can be run in this order: \r\n* Repeat masking\r\n* RNAseq QC and read trimming\r\n* Find transcripts\r\n* Combine transcripts\r\n* Extract transcripts\r\n* Convert formats\r\n* Fgenesh annotation\r\n\r\n****\r\n\r\nWorkflow information:\r\n* Input = genome.fasta.\r\n* Outputs = soft_masked_genome.fasta, hard_masked_genome.fasta, and table of repeats found. \r\n* Runs RepeatModeler with default settings, uses the output of this (repeat library) as input into RepeatMasker. \r\n* Runs RepeatMasker with default settings except for: Skip masking of simple tandem repeats and low complexity regions. (-nolow) : default set to yes.  Perform softmasking instead of hardmasking - set to yes. \r\n* Converts the soft-masked genome to hard-masked for for use in other tools if required. \r\n* Workflow report displays an edited table of repeats found. Note: a known bug is that sometimes the workflow report text resets to default text. To restore, look for an earlier workflow version with correct workflow report text, and copy and paste report text into current version.\r\n",
        "doi": "10.48546/workflowhub.workflow.875.3",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "875",
        "keep": true,
        "latest_version": 3,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/875?version=3",
        "name": "Repeat masking - TSI",
        "number_of_steps": 2,
        "projects": [
            "Australian BioCommons",
            "Galaxy Australia"
        ],
        "source": "WorkflowHub",
        "tags": [
            "tsi-annotation"
        ],
        "tools": [
            "repeatmodeler",
            "repeatmasker_wrapper"
        ],
        "type": "Galaxy",
        "update_time": "2024-06-20",
        "versions": 3
    },
    {
        "create_time": "2024-10-10",
        "creators": [
            "Lucille Delisle"
        ],
        "description": "This workflow takes a collection of BAM (output of STAR) and a gtf. It extends the input gtf using de novo annotation.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "1051",
        "keep": true,
        "latest_version": 2,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1051?version=2",
        "name": "brew3r/main",
        "number_of_steps": 5,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "map_param_value",
            "stringtie_merge",
            "stringtie",
            "brew3r_r"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-07-31",
        "creators": [
            "ABRomics None",
            "Pierre Marin",
            "Clea Siguret"
        ],
        "description": "Annotation of an assembled bacterial genomes to detect genes, potential plasmids, integrons and Insertion sequence (IS) elements.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1050",
        "keep": true,
        "latest_version": 12,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1050?version=12",
        "name": "bacterial_genome_annotation/main",
        "number_of_steps": 6,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "abromics",
            "annotation",
            "genomics",
            "bacterial-genomics",
            "fasta",
            "genome-annotation"
        ],
        "tools": [
            "tooldistillator",
            "isescan",
            "bakta",
            "integron_finder",
            "plasmidfinder",
            "tooldistillator_summarize"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 12
    },
    {
        "create_time": "2025-09-18",
        "creators": [
            "ABRomics None",
            "Pierre Marin",
            "Clea Siguret"
        ],
        "description": "Antimicrobial resistance gene detection from assembled bacterial genomes",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "antimicrobial in tags",
        "id": "1049",
        "keep": true,
        "latest_version": 8,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1049?version=8",
        "name": "amr_gene_detection/main",
        "number_of_steps": 5,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "abromics",
            "amr",
            "amr-detection",
            "genomics",
            "antibiotic-resistance",
            "antimicrobial resistance",
            "antimicrobial-resistance-genes",
            "bacterial-genomics",
            "fasta"
        ],
        "tools": [
            "tooldistillator",
            "abricate",
            "amrfinderplus",
            "tooldistillator_summarize",
            "staramr_search"
        ],
        "type": "Galaxy",
        "update_time": "2025-09-18",
        "versions": 8
    },
    {
        "create_time": "2024-06-18",
        "creators": [
            "Ra\u00fcl Sirvent"
        ],
        "description": "**Name:** SparseLU \r\n**Contact Person:** support-compss@bsc.es \r\n**Access Level:** public \r\n**License Agreement:** Apache2 \r\n**Platform:** COMPSs \r\n\r\n# Description\r\nThe Sparse LU application computes an LU matrix factorization on a sparse blocked matrix. The matrix size (number of blocks) and the block size are parameters of the application. \r\n\r\nAs the algorithm progresses, the area of the matrix that is accessed is smaller; concretely, at each iteration, the 0th row and column of the current matrix are discarded. On the other hand, due to the sparseness of the matrix, some of its blocks might not be allocated and, therefore, no work is generated for them.\r\n\r\nWhen executed with COMPSs, Sparse LU produces several types of task with different granularity and numerous dependencies between them.\r\n\r\n# Versions\r\nThere are three versions of Sparse LU, depending on the data types used to store the blocks.\r\n## Version 1\r\n''files'', where the matrix blocks are stored in files.\r\n## Version 2\r\n''objects'', where the matrix blocks are represented by objects.\r\n## Version 3\r\n''arrays'', where the matrix blocks are stored in arrays.\r\n\r\n\r\n# Execution instructions\r\nUsage:\r\n```\r\nruncompss sparseLU.files.SparseLU numberOfBlocks blockSize\r\nruncompss sparseLU.objects.SparseLU numberOfBlocks blockSize\r\nruncompss sparseLU.arrays.SparseLU numberOfBlocks blockSize\r\n```\r\n\r\nwhere:\r\n  * numberOfBlocks: Number of blocks inside each matrix\r\n  * blockSize: Size of each block\r\n\r\n\r\n# Execution Example\r\n```\r\nruncompss sparseLU.objects.SparseLU 16 4 \r\nruncompss sparseLU.files.SparseLU 16 4\r\nruncompss sparseLU.arrays.SparseLU 16 4 \r\n```\r\n\r\n\r\n# Build\r\n## Option 1: Native java\r\n```\r\ncd application_sources/; javac src/main/java/sparseLU/*/*.java\r\ncd src/main/java/; jar cf sparseLU.jar sparseLU/\r\ncd ../../../; mv src/main/java/sparseLU.jar jar/\r\n```\r\n\r\n## Option 2: Maven\r\n```\r\ncd application_sources/\r\nmvn clean package\r\n```\r\n",
        "doi": "10.48546/workflowhub.workflow.1047.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "1047",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1047?version=1",
        "name": "Java COMPSs LU Factorization for Sparse Matrices, MareNostrum V, 3 nodes, no data persistence",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2024-06-18",
        "versions": 1
    },
    {
        "create_time": "2021-06-21",
        "creators": [],
        "description": "Variant Interpretation Pipeline (VIP) that annotates, filters and reports prioritized causal variants in humans, see https://github.com/molgenis/vip for more information.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "DNA mutation",
            "DNA polymorphism",
            "Genetic variation",
            "Structural variation"
        ],
        "filtered_on": "annot* in tags",
        "id": "125",
        "keep": true,
        "latest_version": 1,
        "license": "LGPL-3.0",
        "link": "https:/workflowhub.eu/workflows/125?version=1",
        "name": "MOLGENIS/VIP: Variant Interpretation Pipeline",
        "number_of_steps": 0,
        "projects": [
            "MOLGENIS"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "bioinformatics",
            "classification",
            "genomics",
            "java",
            "nextflow",
            "pipeline",
            "report",
            "snps",
            "sv",
            "vcf",
            "workflows",
            "variation"
        ],
        "tools": [],
        "type": "Unrecognized workflow type",
        "update_time": "2024-06-12",
        "versions": 1
    },
    {
        "create_time": "2024-06-10",
        "creators": [
            "Carlos Classen"
        ],
        "description": "# CNVand\r\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22658.0.0-brightgreen.svg?style=flat-square)](https://snakemake.bitbucket.io)\r\n[![Conda](https://img.shields.io/badge/conda-\u226523.11.0-brightgreen.svg?style=flat-square)](https://anaconda.org/conda-forge/mamba)\r\n![Docker](https://img.shields.io/badge/docker-\u226526.1.4-brightgreen.svg?style=flat-square)\r\n![License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square)\r\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](code_of_conduct.md) \r\n\r\nCNVand is a snakemake workflow for CNV analysis, tailored for preparing data used by the [CNVizard](https://github.com/IHGGM-Aachen/CNVizard) CNV visualization tool. Given a set of BAM and VCF files, it utilizes the tools `CNVkit` and `AnnotSV` to analyze and annotate copy number variations.\r\n\r\n## General Settings and Samplesheet\r\nTo configure this pipeline, modify the config under `config/config.yaml` as needed. Detailed explanations for each setting are provided within the file.\r\n\r\nAdd samples to the pipeline by completing `config/samplesheet.tsv`. Each `sample` should be associated with a `path` to the corresponding BAM and VCF file.\r\n\r\nFor detailed instructions on how to configure CNVand see `config/README.md`.\r\n\r\n## Reference Files\r\nTo use CNVand some external reference files are needed alongside your sample data.\r\n\r\n### Genome\r\n\r\nFor `cnvkit_fix` to work, you need to specify a reference genome in the config file. Take care to use the same reference file for your entire workflow!\r\n\r\n### Annotations\r\n\r\nFor AnnotSV to work, the annotation files must be downloaded separately and be referenced in the config file under the respective key. For human annotations, this can be done [here](https://www.lbgi.fr/~geoffroy/Annotations/Annotations_Human_3.4.2.tar.gz). In case this link is not working, check the original [AnnotSV](https://github.com/lgmgeo/AnnotSV/tree/master) repository for updates on how to obtain the annotations.\r\n\r\n## Pipeline Setup\r\nCNVand can be executed using mamba environments or a pre-built docker container.\r\n\r\n### Mamba (Snakedeploy)\r\nFor a one-click installation, snakedeploy can be used. For further information, see the entry for CNVand in the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog/?repo=IHGGM-Aachen/CNVand)\r\n\r\n### Mamba (Manual)\r\nThis workflow can easily setup manually with the given environment file. Install Snakemake and dependencies using the command:\r\n\r\n```bash\r\nmamba env create -f environment.yml\r\n```\r\n\r\nThen activate the newly created environment with: \r\n\r\n```bash\r\nmamba activate cnvand\r\n```\r\n\r\nNow configure the pipeline and download the needed annotation and refenrece files. When everything is set up, Execute the pipeline with:\r\n\r\n```bash\r\nsnakemake --cores all --use-conda\r\n```\r\n\r\nGenerate a comprehensive execution report by running:\r\n\r\n```bash\r\nsnakemake --report report.zip\r\n```\r\n\r\n\r\n### Docker\r\n\r\nCNVand can also be used inside a Docker container. To do so, first pull the Docker image with:\r\n\r\n```bash\r\ndocker pull ghcr.io/ihggm-aachen/cnvand:latest\r\n```\r\n\r\nThen run the container with the bind mounts needed in your setup:\r\n\r\n```bash\r\ndocker run -it -v /path/to/your/data:/data ghcr.io/ihggm-aachen/cnvand:latest /bin/bash\r\n```\r\n\r\nThis command opens an interactive shell inside the Docker container. Once inside the container, you are placed inside the `/cnvand` the directory. From there then run the pipeline once you set an appropriate configuration:\r\n\r\n```bash\r\nsnakemake --cores all --use-conda\r\n```\r\n\r\n## Contributing\r\n\r\nWe welcome contributions to improve CNVand. Please see our [CONTRIBUTING.md](CONTRIBUTING.md) for details on how to get started.\r\n\r\n## Code of Conduct\r\n\r\nWe are committed to fostering an open and welcoming environment. Please see our [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md) for our community guidelines.\r\n\r\n## Documentation\r\n\r\nDetailed documentation for the workflow can be found in `workflow/documentation.md`.\r\n\r\n## Testing\r\n\r\nTo ensure the pipeline runs correctly, we have set up both unit and integration tests. Unit tests are generated from successful workflow runs, and integration tests are configured to run the entire workflow with test data.\r\n\r\n### Integration Tests\r\n\r\nThe integration test can be run using the data and config provided. Remember to download the correct reference/annotations (GRCh38 in case of the bundled NIST data) by yourself and adjust your local paths as necessary!\r\n\r\n### Unit Tests\r\n\r\nRun the unit tests with:\r\n\r\n```bash\r\npytest -v .tests/unit\r\n```\r\n\r\nThis will check for the correct CNVand output per rule.\r\n\r\n## License\r\n\r\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE.md) file for details.\r\n",
        "doi": "10.48546/workflowhub.workflow.1039.1",
        "edam_operation": [
            "Copy number variation detection",
            "Gene functional annotation"
        ],
        "edam_topic": [
            "Biomedical science",
            "Copy number variation",
            "Data visualisation"
        ],
        "filtered_on": "annot* in tags",
        "id": "1039",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1039?version=1",
        "name": "CNVand",
        "number_of_steps": 0,
        "projects": [
            "Institute for Human Genetics and Genomic Medicine Aachen"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "cnvkit",
            "copy number variation",
            "magic",
            "snakemake",
            "annotsv"
        ],
        "tools": [
            "Snakemake",
            "CNVkit",
            "AnnotSV"
        ],
        "type": "Snakemake",
        "update_time": "2024-06-10",
        "versions": 1
    },
    {
        "create_time": "2024-06-06",
        "creators": [
            "Zargham Ahmad",
            "Helge Hecht",
            "Elliott J. Price"
        ],
        "description": "# Galaxy Workflow Documentation: MS Finder Pipeline\r\n\r\nThis document outlines a MSFinder Galaxy workflow designed for peak annotation. The workflow consists of several steps aimed at preprocessing MS data, filtering, enhancing, and running MSFinder.\r\n\r\n## Step 1: Data Collection and Preprocessing\r\nCollect if the inchi and smiles are missing from the dataset, and subsequently filter out the spectra which are missing inchi and smiles.\r\n\r\n### 1.1 MSMetaEnhancer: Collect InChi, Isomeric_smiles, and Nominal_mass\r\n- Utilizes MSMetaEnhancer to collect InChi and Isomeric_smiles using PubChem and IDSM databases.\r\n- Utilizes MSMetaEnhancer to collect MW using RDkit (For GOLM).\r\n\r\n### 1.2 replace key\r\n- replace isomeric_smiles key to smiles using replace text tool\r\n- replace MW key to parent_mass using replace text tool (For GOLM)\r\n\r\n### 1.3 Matchms Filtering\r\n- Filters out invalid SMILES and InChi from the dataset using Matchms filtering.\r\n\r\n## Step 2: Complex Removal and Subsetting Dataset\r\nRemoves coordination complexes from the dataset.\r\n\r\n### 2.1 Remove Complexes and Subset Data\r\n- Removes complexes from the dataset.\r\n- Exports metadata using Matchms metadata export, cuts the SMILES column, removes complexes using Rem_Complex tool, and updates the dataset using Matchms subsetting.\r\n\r\n## Step 3: Data Key Manipulation\r\nAdd missing metadata required by the MSFinder for annotation.\r\n\r\n### 3.1 Matchms Remove Key\r\n- Removes existing keys such as adduct, charge, and ionmode from the dataset.\r\n\r\n### 3.2 Matchms Add Key\r\n- Adds necessary keys like charge, ionmode, and adduct to the dataset.\r\n\r\n### 3.3 Matchms Filtering\r\n- Derives precursor m/z using parent mass and adduct information using matchms filtering.\r\n\r\n### 3.4 Matchms Convert\r\n- Converts the dataset to Riken format for compatibility with MSFinder using matchms convert.\r\n\r\n## Step 4: Peak Annotation\r\n### 4.1 Recetox-MSFinder\r\n- Executes MSFinder with a 0.5 Da tolerance for both MS1 and MS2, including all element checks and an extended range for peak annotation.\r\n\r\n## Step 5: Error Handling and Refinement\r\nCheck the MSFinder output to see if the output is the results or the log file. If the output is log file remove the smile from the dataset using matchms subsetting tool and rerun MSFinder.\r\n\r\n### 5.1 Error Handling\r\n- Handles errors in peak annotation by removing SMILES that are not accepted by MSFinder.\r\n- Reruns MSFinder after error correction or with different parameter (if applicable).\r\n\r\n## Step 6: High-res Annotation\r\n### 6.1 High-Res Peak Overwriting\r\n- Utilizes the Use_Theoretical_mz_Annotations tool to Overwrite experimentally measured mz values for peaks with theoretical values from peak comments.\r\n",
        "doi": "10.48546/workflowhub.workflow.888.2",
        "edam_operation": [
            "Annotation",
            "Data filtering",
            "Information retrieval"
        ],
        "edam_topic": [
            "Compound libraries and screening",
            "Computational chemistry",
            "Data mining",
            "Small molecules"
        ],
        "filtered_on": "metap* in tags",
        "id": "888",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/888?version=2",
        "name": "Theoretical fragment substructure generation and in silico mass spectral library high-resolution upcycling workflow",
        "number_of_steps": 22,
        "projects": [
            "RECETOX SpecDatRI"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "cheminformatics",
            "metabolomics"
        ],
        "tools": [
            "csv_to_tabular",
            "column_remove_by_header",
            "matchms_convert",
            "rem_complex",
            "matchms_remove_key",
            "matchms_add_key",
            "matchms_filtering",
            "tabular_to_csv",
            "matchms_subsetting",
            "recetox_msfinder",
            "tp_find_and_replace",
            "matchms_metadata_export",
            "use_theoretical_mz_annotations",
            "msmetaenhancer"
        ],
        "type": "Galaxy",
        "update_time": "2024-06-19",
        "versions": 2
    },
    {
        "create_time": "2024-06-11",
        "creators": [
            "Sarai Varona and Sara Monzon",
            "Patel H",
            "Varona S and Monzon S"
        ],
        "description": "Assembly and intrahost/low-frequency variant calling for viral samples",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "1027",
        "keep": true,
        "latest_version": 11,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1027?version=11",
        "name": "nf-core/viralrecon",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "artic",
            "amplicon",
            "assembly",
            "metagenomics",
            "ont",
            "sars-cov-2",
            "virus",
            "covid-19",
            "covid19",
            "illumina",
            "long-read-sequencing",
            "nanopore",
            "oxford-nanopore",
            "variant-calling",
            "viral"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-06-11",
        "versions": 11
    },
    {
        "create_time": "2025-02-19",
        "creators": [
            "Maxime Garcia",
            "Szilveszter Juhos"
        ],
        "description": "An open-source analysis pipeline to detect germline or somatic variants from whole genome or targeted sequencing",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "1020",
        "keep": true,
        "latest_version": 28,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1020?version=28",
        "name": "nf-core/sarek",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "gatk4",
            "genomics",
            "germline",
            "somatic",
            "cancer",
            "pre-processing",
            "target-panels",
            "variant-calling",
            "whole-exome-sequencing",
            "whole-genome-sequencing"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-02-19",
        "versions": 28
    },
    {
        "create_time": "2024-09-06",
        "creators": [
            "S\u00e9bastien Guizard (@sguizard)"
        ],
        "description": "Genes and transcripts annotation with Isoseq using uLTRA and TAMA",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "993",
        "keep": true,
        "latest_version": 8,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/993?version=8",
        "name": "nf-core/isoseq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "isoseq",
            "isoseq-3",
            "rna",
            "tama",
            "ultra"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-09-06",
        "versions": 8
    },
    {
        "create_time": "2024-06-11",
        "creators": [
            "Harshil Patel"
        ],
        "description": "Pipeline to fetch metadata and raw FastQ files from public databases",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "986",
        "keep": true,
        "latest_version": 14,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/986?version=14",
        "name": "nf-core/fetchngs",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "fastq",
            "ddbj",
            "download",
            "ena",
            "geo",
            "sra",
            "synapse"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-06-11",
        "versions": 14
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Alexander Peltzer",
            "The nf-core/eager community",
            "Stephen Clayton",
            "James A Fellows-Yates"
        ],
        "description": "A fully reproducible and state-of-the-art ancient DNA analysis pipeline",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "983",
        "keep": true,
        "latest_version": 30,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/983?version=30",
        "name": "nf-core/eager",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "adna",
            "ancient-dna-analysis",
            "ancientdna",
            "genome",
            "pathogen-genomics",
            "population-genetics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-26",
        "versions": 30
    },
    {
        "create_time": "2024-06-11",
        "creators": [
            "Daniel Schreyer"
        ],
        "description": "Pipeline for the identification of circular DNAs",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in tags",
        "id": "972",
        "keep": true,
        "latest_version": 8,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/972?version=8",
        "name": "nf-core/circdna",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dna",
            "genomics",
            "ampliconarchitect",
            "ampliconsuite",
            "circle-seq",
            "circular",
            "eccdna",
            "ecdna",
            "extrachromosomal-circular-dna"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-06-11",
        "versions": 8
    },
    {
        "create_time": "2024-06-11",
        "creators": [
            "Payam Emami"
        ],
        "description": "Pre-processing of mass spectrometry-based metabolomics data",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in tags",
        "id": "57",
        "keep": true,
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/57?version=4",
        "name": "nf-core/metaboigniter",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metabolomics",
            "identification",
            "mass-spectrometry",
            "ms1",
            "ms2",
            "quantification"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-06-11",
        "versions": 4
    },
    {
        "create_time": "2024-11-27",
        "creators": [
            "Debjyoti Ghosh"
        ],
        "description": "Use DADA2 for sequence quality control. DADA2 is a pipeline for detecting and correcting (where possible) Illumina amplicon sequence data. As implemented in the q2-dada2 plugin, this quality control process will additionally filter any phiX reads (commonly present in marker gene Illumina sequence data) that are identified in the sequencing data, and will filter chimeric sequences.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in description",
        "id": "892",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/892?version=3",
        "name": "qiime2-II-denoising/IIa-denoising-se",
        "number_of_steps": 4,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "qiime2__feature_table__summarize",
            "qiime2__feature_table__tabulate_seqs",
            "qiime2__metadata__tabulate",
            "qiime2__dada2__denoise_single"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 3
    },
    {
        "create_time": "2024-05-13",
        "creators": [
            "Thomas Roetzer-Pejrimovsky"
        ],
        "description": "# GBMatch_CNN\r\nWork in progress...\r\nPredicting TS &amp; risk from glioblastoma whole slide images\r\n\r\n# Reference\r\nUpcoming paper: stay tuned...\r\n\r\n# Dependencies\r\npython 3.7.7\r\n\r\nrandaugment by Khrystyna Faryna: https://github.com/tovaroe/pathology-he-auto-augment\r\n\r\ntensorflow 2.1.0\r\n\r\nscikit-survival 0.13.1\r\n\r\npandas 1.0.3\r\n\r\nlifelines 0.25.0\r\n\r\n# Description\r\nThe pipeline implemented here predicts transcriptional subtypes and survival of glioblastoma patients based on H&E stained whole slide scans. Sample data is provided in this repository. To test the basic functionality with 5-fold-CV simply run train_model_OS.py (for survival) or train_model_TS.py (for transcriptional subtypes). Please note that this will not reproduce the results from the manuscript, as only a small fraction of the image data can be provided in this repository due to size constraints. In order to reproduce the results from the manuscript, please refer to the step by step guide below. The whole dataset can be accessed at https://www.medical-epigenomics.org/papers/GBMatch/.\r\nIf you wish to adopt this pipeline for your own use, please be sure to set the correct parameters in config.py.\r\n\r\nMoreover, we provide a fully trained model in gbm_predictor.py for predicting new samples (supported WSI formats are ndpi and svs). To use GBMPredictor, simply initialize by calling \r\n`gbm_predictor = GBMPredictor()`\r\nand predict your sample by calling\r\n`(predicted_TS, risk_group, median_riskscore) = gbm_predictor.predict(*path_to_slidescan*)`\r\nHeatmaps and detailed results will be automatically saved in a subfolder in your sample path.\r\n\r\n# Reproducing the manuscript results - step by step guide\r\n\r\n## Training the CNN model\r\n1. Clone this repository and install the dependencies in your environment. Make sure that the path for randaugment is correctly set in the config.py (should be correct by default).\r\n2. Download all included image tiles at https://doi.org/10.5281/zenodo.8358673 and replace the data/training/image_tiles folder with the image_tiles folder from zenodo.\r\n3. Run train_model_OS.py and/or train_model_TS.py to reproduce the training with 5-fold cross validation. Models and results will be saved in the data/models folder.\r\n4. Run train_final_model_OS.py and/or train_final_model_TS.py to train the final model on the whole training dataset.\r\n\r\n## Validate the CNN model on TCGA data\r\n1. Download scans and clinical data of the TCGA glioblastoma cohort from https://www.cbioportal.org/ and/or https://portal.gdc.cancer.gov/\r\n2. Copy tumor segmentations from GBMatch_CNN/data/validation/segmentation into the same folder as the TCGA slide scans\r\n3. Predict TCGA samples with gbm_predictor (see above).\r\n(You can also find all prediction results in GBMatch_CNN/data/validation/TCGA_annotation_prediction.csv.)\r\n\r\n## Evaluation of the tumor microenvironment\r\n1. Install qupath 0.3.0 (newer versions should also work): https://qupath.github.io/.\r\n2. Download immunohistochemical slides from https://www.medical-epigenomics.org/papers/GBMatch/.\r\n3. Download annotation (IHC_geojsons) from https://doi.org/10.5281/zenodo.8358673.\r\n4. Create a new project and import all immunohistochemical slides & annotations.\r\n5. Copy the CD34 and HLA-DR thresholder from GBMatch_CNN/qupath into your project.\r\n6. Run GBMatch_CNN/qupath/IHC_eval.groovy for all slides - immunohistochemistry results will be saved to a IHC_results-folder.\r\n7. Create a new project and import all HE image tiles.\r\n8. Run GBMatch_CNN/qupath/cellularity.groovy for all slides - cellularity results will be saved to a HE-results-folder.\r\n",
        "doi": "10.48546/workflowhub.workflow.883.1",
        "edam_operation": [
            "Classification"
        ],
        "edam_topic": [
            "Biomedical science",
            "Machine learning",
            "Oncology",
            "Pathology",
            "Transcriptomics"
        ],
        "filtered_on": "annot* in description",
        "id": "883",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/883?version=1",
        "name": "Training a CNN model for classification of transcriptional subtypes and survival prediction in glioblastoma",
        "number_of_steps": 0,
        "projects": [
            "BRAIN - Biomedical Research on Adult Intracranial Neoplasms"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "pathology"
        ],
        "tools": [
            "GBMatch - GBMPredictor"
        ],
        "type": "Python",
        "update_time": "2024-05-13",
        "versions": 1
    },
    {
        "create_time": "2021-11-11",
        "creators": [
            "Michael Roach"
        ],
        "description": "A hecatomb is a great sacrifice or an extensive loss. Heactomb the software empowers an analyst to make data driven decisions to 'sacrifice' false-positive viral reads from metagenomes to enrich for true-positive viral reads. This process frequently results in a great loss of suspected viral sequences / contigs.\r\n\r\nFor information about installation, usage, tutorial etc please refer to the documentation: https://hecatomb.readthedocs.io/en/latest/\r\n\r\n### Quick start guide\r\n\r\nInstall Hecatomb from Bioconda\r\n```bash\r\n# create an env called hecatomb and install Hecatomb in it\r\nconda create -n hecatomb -c conda-forge -c bioconda hecatomb\r\n\r\n# activate conda env\r\nconda activate hecatomb\r\n\r\n# check the installation\r\nhecatomb -h\r\n\r\n# download the databases - you only have to do this once\r\nhecatomb install\r\n\r\n# Run the test dataset\r\nhecatomb run --test\r\n```",
        "doi": "10.48546/workflowhub.workflow.235.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "235",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/235?version=1",
        "name": "Hecatomb",
        "number_of_steps": 0,
        "projects": [
            "HecatombDevelopment"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-05-13",
        "versions": 1
    },
    {
        "create_time": "2024-05-08",
        "creators": [
            "Luke Silver",
            "Anna Syme"
        ],
        "description": "This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \r\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \r\n\r\nThe workflows can be run in this order: \r\n* Repeat masking\r\n* RNAseq QC and read trimming\r\n* Find transcripts\r\n* Combine transcripts\r\n* Extract transcripts\r\n* Convert formats\r\n* Fgenesh annotation\r\n\r\n****\r\n\r\nAbout this workflow:\r\n\r\n* Inputs: transdecoder-peptides.fasta, transdecoder-nucleotides.fasta\r\n* Runs many steps to convert outputs into the formats required for Fgenesh - .pro, .dat and .cdna",
        "doi": "10.48546/workflowhub.workflow.880.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "880",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/880?version=1",
        "name": "Convert formats - TSI",
        "number_of_steps": 47,
        "projects": [
            "Australian BioCommons",
            "Galaxy Australia"
        ],
        "source": "WorkflowHub",
        "tags": [
            "tsi-annotation"
        ],
        "tools": [
            "seqtk_seq",
            "fasta2tab",
            "fasta_compute_length",
            "tp_awk_tool",
            "tp_replace_in_line",
            "tp_grep_tool",
            "tab2fasta",
            "Cut1",
            "tp_sed_tool",
            "Paste1",
            "seqtk_subseq",
            "tp_sort_header_tool"
        ],
        "type": "Galaxy",
        "update_time": "2024-05-09",
        "versions": 1
    },
    {
        "create_time": "2024-05-08",
        "creators": [
            "Luke Silver",
            "Anna Syme"
        ],
        "description": "This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \r\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \r\n\r\nThe workflows can be run in this order: \r\n* Repeat masking\r\n* RNAseq QC and read trimming\r\n* Find transcripts\r\n* Combine transcripts\r\n* Extract transcripts\r\n* Convert formats\r\n* Fgenesh annotation\r\n\r\n****\r\n\r\nAbout this workflow:\r\n\r\n* Input: merged_transcriptomes.fasta. \r\n* Runs TransDecoder to produce longest_transcripts.fasta\r\n* (Runs both the LongOrfs and Predict parts together. Default settings except Long Orfs options: -m =20)\r\n* Runs Busco on output. ",
        "doi": "10.48546/workflowhub.workflow.879.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "879",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/879?version=1",
        "name": "Extract transcripts - TSI",
        "number_of_steps": 3,
        "projects": [
            "Australian BioCommons",
            "Galaxy Australia"
        ],
        "source": "WorkflowHub",
        "tags": [
            "tsi-annotation"
        ],
        "tools": [
            "transdecoder",
            "tp_sed_tool",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2024-05-09",
        "versions": 1
    },
    {
        "create_time": "2024-05-08",
        "creators": [
            "Luke Silver",
            "Anna Syme"
        ],
        "description": "This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \r\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \r\n\r\nThe workflows can be run in this order: \r\n* Repeat masking\r\n* RNAseq QC and read trimming\r\n* Find transcripts\r\n* Combine transcripts\r\n* Extract transcripts\r\n* Convert formats\r\n* Fgenesh annotation\r\n\r\n****\r\n\r\nAbout this workflow:\r\n\r\n* Inputs: multiple transcriptome.gtfs from different tissues, genome.fasta, coding_seqs.fasta, non_coding_seqs.fasta \r\n* Runs StringTie merge to combine transcriptomes, with default settings except for -m = 30 and -F = 0.1, to produce a merged_transcriptomes.gtf. \r\n* Runs Convert GTF to BED12 with default settings, to produce a merged_transcriptomes.bed. \r\n* Runs bedtools getfasta with default settings except for -name = yes, -s = yes, -split - yes, to produce a merged_transcriptomes.fasta\r\n* Runs CPAT to generate seqs with high coding probability. \r\n* Filters out non-coding seqs from the merged_transcriptomes.fasta\r\n* Output: filtered_merged_transcriptomes.fasta",
        "doi": "10.48546/workflowhub.workflow.878.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "878",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/878?version=1",
        "name": "Combine transcripts - TSI",
        "number_of_steps": 8,
        "projects": [
            "Australian BioCommons",
            "Galaxy Australia"
        ],
        "source": "WorkflowHub",
        "tags": [
            "tsi-annotation"
        ],
        "tools": [
            "bedtools_getfastabed",
            "seq_filter_by_id",
            "gtftobed12",
            "cpat",
            "tp_sed_tool",
            "stringtie_merge",
            "skipping 1 header line\nFilter1",
            "tp_cut_tool"
        ],
        "type": "Galaxy",
        "update_time": "2024-05-09",
        "versions": 1
    },
    {
        "create_time": "2024-05-08",
        "creators": [
            "Luke Silver",
            "Anna Syme"
        ],
        "description": "This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \r\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \r\n\r\nThe workflows can be run in this order: \r\n* Repeat masking\r\n* RNAseq QC and read trimming\r\n* Find transcripts\r\n* Combine transcripts\r\n* Extract transcripts\r\n* Convert formats\r\n* Fgenesh annotation\r\n\r\n****\r\n\r\nAbout this workflow:\r\n\r\n* Run this workflow per tissue. \r\n* Inputs: masked_genome.fasta and the trimmed RNAseq reads (R1 and R2) from one type of tissue. \r\n* Index genome and align reads to genome with HISAT2, with default settings except for: Advanced options: spliced alignment options: specify options: Transcriptome assembly reporting: selected option: Report alignments tailored for transcript assemblers including StringTie (equivalent to -dta flag). \r\n* Runs samtools sort to sort bam by coordinate. \r\n* Runs StringTie to generate gtf from sorted bam. \r\n* Output: transcripts.gtf from a single tissue.",
        "doi": "10.48546/workflowhub.workflow.877.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "877",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/877?version=1",
        "name": "Find transcripts - TSI",
        "number_of_steps": 2,
        "projects": [
            "Australian BioCommons",
            "Galaxy Australia"
        ],
        "source": "WorkflowHub",
        "tags": [
            "tsi-annotation"
        ],
        "tools": [
            "hisat2",
            "stringtie"
        ],
        "type": "Galaxy",
        "update_time": "2024-05-09",
        "versions": 1
    },
    {
        "create_time": "2024-05-08",
        "creators": [
            "Luke Silver",
            "Anna Syme"
        ],
        "description": "This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \r\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \r\n\r\nThe workflows can be run in this order: \r\n* Repeat masking\r\n* RNAseq QC and read trimming\r\n* Find transcripts\r\n* Combine transcripts\r\n* Extract transcripts\r\n* Convert formats\r\n* Fgenesh annotation\r\n\r\n****\r\n\r\nAbout this workflow:\r\n\r\n* Repeat this workflow separately for datasets from different tissues. \r\n* Inputs = collections of R1 files, and R2 files (all from a single tissue type). \r\n* Runs FastQC with default settings, separately for raw reads R1 and R2 collections; all output to MultiQC. \r\n* Runs Trimmomatic with initial ILLUMINACLIP step (using standard adapter sequence for TruSeq3 paired-ended), uses settings SLIDINGWINDOW:4:5 LEADING:5 TRAILING:5 MINLEN:25, retain paired (not unpaired) outputs. User can modify at runtime. \r\n* Runs FastQC with default settings, separately for trimmed R1 and R2 collections; all output to MultiQC. \r\n* From Trimmomatic output: concatenate all R1 reads; concatenate all R2 reads. \r\n* Outputs = trimmed merged R1 file, trimmed merged R2 file. \r\n* Log files from Trimmomatic to MultiQC, to summarise trimming results. \r\n* Note: a known bug with MultiQC html output is that plot is labelled as \"R1\" reads, when it actually contains information from both R1 and R2 read sets - this is under investigation (and is due to a Trimmomatic output file labelling issue). \r\n* MultiQC results table formatted to show % of reads retained after trimming, table included in workflow report. \r\n* Note: a known bug is that sometimes the workflow report text resets to default text. To restore, look for an earlier workflow version with correct workflow report text, and copy and paste report text into current version. ",
        "doi": "10.48546/workflowhub.workflow.876.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "876",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/876?version=1",
        "name": "QC and trimming of RNAseq reads - TSI",
        "number_of_steps": 14,
        "projects": [
            "Australian BioCommons",
            "Galaxy Australia"
        ],
        "source": "WorkflowHub",
        "tags": [
            "tsi-annotation"
        ],
        "tools": [
            "__EXTRACT_DATASET__",
            "fastqc",
            "Cut1",
            "tp_replace_in_column",
            "Remove beginning1",
            "trimmomatic",
            "tp_cat",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2024-05-09",
        "versions": 1
    },
    {
        "create_time": "2024-05-03",
        "creators": [],
        "description": "Complete workflow for TANGO as reported in Lecomte et al (2024),\r\n\"Revealing the dynamics and mechanisms of bacterial interactions in\r\ncheese production with metabolic modelling\", Metabolic Eng. 83:24-38\r\nhttps://doi.org/10.1016/j.ymben.2024.02.014\r\n\r\n1. Parameters for individual models are obtained by optimization\r\n2. Individual dynamics and community dynamics are simulated\r\n3. Figures for the manuscript are assembled from the results.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Microbial ecology"
        ],
        "filtered_on": "metap* in description",
        "id": "873",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/873?version=1",
        "name": "Tango: Numerical reconciliation of bacterial fermentation in cheese production",
        "number_of_steps": 6,
        "projects": [
            "MISTIC"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2024-05-03",
        "versions": 1
    },
    {
        "create_time": "2024-04-30",
        "creators": [
            "Ra\u00fcl Sirvent"
        ],
        "description": "**Name:** Matrix multiplication with Files, reproducibility example, without data persistence\r\n**Contact Person**: support-compss@bsc.es  \r\n**Access Level**: public  \r\n**License Agreement**: Apache2  \r\n**Platform**: COMPSs  \r\n\r\n# Description\r\nMatrix multiplication is a binary operation that takes a pair of matrices and produces another matrix.\r\n\r\nIf A is an n\u00d7m matrix and B is an m\u00d7p matrix, the result AB of their multiplication is an n\u00d7p matrix defined only if the number of columns m in A is equal to the number of rows m in B. When multiplying A and B, the elements of the rows in A are multiplied with corresponding columns in B.\r\n\r\nIn this implementation, A and B are square matrices (same number of rows and columns), and so it is the result matrix C. Each matrix is divided in N blocks of M doubles (N hardcoded to 2, and M hardcoded to 8). The multiplication of two blocks is done by a multiply task method with a simple three-nested-loop implementation. When executed with COMPSs, the main program generates N^3^ tasks arranged as N^2^ chains of N tasks in the dependency graph.\r\n\r\n# Reproducibility\r\nTo reproduce the exact results of this example, follow the instructions at the [Workflow Provenance section at COMPSs User Manual](https://compss-doc.readthedocs.io/en/stable/Sections/05_Tools/04_Workflow_Provenance.html), WITHOUT data persistence, PyCOMPSs application.\r\n\r\n# Execution instructions\r\nUsage:\r\n```\r\nruncompss --lang=python src/matmul_files.py inputs_folder/ outputs_folder/\r\n```\r\n\r\nwhere:\r\n* inputs_folder/: Folder where A and B matrices are located\r\n* outputs_folder/: Folder with the resulting C matrix\r\n\r\n\r\n# Execution Examples\r\n```\r\nruncompss --lang=python src/matmul_files.py inputs/ outputs/\r\nruncompss src/matmul_files.py inputs/ outputs/\r\npython -m pycompss src/matmul_files.py inputs/ outputs/\r\n```\r\n\r\n# Build\r\nNo build is required\r\n",
        "doi": "10.48546/workflowhub.workflow.839.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "839",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/839?version=1",
        "name": "PyCOMPSs Matrix Multiplication, out-of-core using files, MareNostrum V, reproducibility example, without data persistence",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [
            "marenostrum v",
            "pycompss",
            "supercomputer",
            "non_data_persistence"
        ],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2024-06-18",
        "versions": 1
    },
    {
        "create_time": "2024-04-30",
        "creators": [
            "Ra\u00fcl Sirvent"
        ],
        "description": "**Name:** Matrix multiplication with Files, reproducibility example  \r\n**Contact Person**: support-compss@bsc.es  \r\n**Access Level**: public  \r\n**License Agreement**: Apache2  \r\n**Platform**: COMPSs  \r\n\r\n# Description\r\nMatrix multiplication is a binary operation that takes a pair of matrices and produces another matrix.\r\n\r\nIf A is an n\u00d7m matrix and B is an m\u00d7p matrix, the result AB of their multiplication is an n\u00d7p matrix defined only if the number of columns m in A is equal to the number of rows m in B. When multiplying A and B, the elements of the rows in A are multiplied with corresponding columns in B.\r\n\r\nIn this implementation, A and B are square matrices (same number of rows and columns), and so it is the result matrix C. Each matrix is divided in N blocks of M doubles (N hardcoded to 2, and M hardcoded to 8). The multiplication of two blocks is done by a multiply task method with a simple three-nested-loop implementation. When executed with COMPSs, the main program generates N^3^ tasks arranged as N^2^ chains of N tasks in the dependency graph.\r\n\r\n# Reproducibility\r\nTo reproduce the exact results of this example, follow the instructions at the [Workflow Provenance section at COMPSs User Manual](https://compss-doc.readthedocs.io/en/stable/Sections/05_Tools/04_Workflow_Provenance.html), WITH data persistence, PyCOMPSs application\r\n\r\n# Execution instructions\r\nUsage:\r\n```\r\nruncompss --lang=python src/matmul_files.py inputs_folder/ outputs_folder/\r\n```\r\n\r\nwhere:\r\n* inputs_folder/: Folder where A and B matrices are located\r\n* outputs_folder/: Folder with the resulting C matrix\r\n\r\n\r\n# Execution Examples\r\n```\r\nruncompss --lang=python src/matmul_files.py inputs/ outputs/\r\nruncompss src/matmul_files.py inputs/ outputs/\r\npython -m pycompss src/matmul_files.py inputs/ outputs/\r\n```\r\n\r\n# Build\r\nNo build is required\r\n",
        "doi": "10.48546/workflowhub.workflow.838.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "838",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/838?version=1",
        "name": "PyCOMPSs Matrix Multiplication, out-of-core using files, reproducibility example",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [
            "laptop",
            "pycompss",
            "data_persistence"
        ],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2024-08-02",
        "versions": 1
    },
    {
        "create_time": "2024-04-26",
        "creators": [
            "Dominik Lux",
            "Julian Uszkoreit"
        ],
        "description": "# ProGFASTAGen\r\n\r\nThe ProGFASTAGen (**Pro**tein-**G**raph-**FASTA**-**Gen**erator or **Pro**t**G**raph-**FASTA**-**Gen**erator) repository contains workflows to generate so-called precursor-specific-FASTAs (using the precursors from MGF-files) including feature-peptides, like VARIANTs or CONFLICTs if desired, or global-FASTAs (as described in [ProtGraph](https://github.com/mpc-bioinformatics/ProtGraph)). The single workflow scripts have been implemented with [Nextflow-DSL-2](https://www.nextflow.io/docs/latest/dsl2.html) and are independent to each other. Each of these workflows can be used on their own or can be imported to other workflows for other use-cases. Further, we included three main-workflows, to show how the single workflows can be chained together. The `main_workflow_protein_fasta.nf`-workflow converts Thermo-RAW-files into MGF, searches with Comet (and Percolator) and the identification results are then further summarized. The workflows `main_workflow_global_fasta.nf` and `main_workflow_precursor_specific_fasta.nf` generate specific FASTA-files before search-engine-identification. Below are example nextflow-calls, which can be used.\r\n\r\nRegarding the precursor-specific-FASTA-generation: The source-code of the C++ implementation for traversal can be found in `bin`. There, four implementations are present: `Float/Int`-Versions as well as `DryRun/VarLimitter`-Versions of the traversal. The `Float/Int`-Versions can be faster/slower depending on th processor-architecture and can be used via a flag in the `create_precursor_specific_fasta.nf`-workflow. The `DryRun`-Version does not generate a FASTA but tests the used system (depending on a query-timeout) to determine the maximum number of variants which can be used, while not timing out. The actual FASTA-generation happens in the `VarLimitter`-Version using the generated protein-graphs at hand.\r\n\r\nin **Prerequisites** a small description of dependencies and how to set up the host system is given. **Individual steps** describes the single workflows and how they can be called, while **Main Workflow Scripts** shows example-calls of the main workflows. In **Regenerate Results from Publication**, the calls and parameters are shown, which were used in the publication. Using the same FASTA or UniProt flat file format with a similar server-setting should yield similar results as used in the publication.\r\n\r\n## Prerequisites\r\n\r\n### Executing on Linux\r\n\r\nThis workflow can be only executed on linux (tested on Ubuntu 22.04 and ArchLinux). Before setting up the `bin`-folder, some requiered binaries need to be present on the OS. (Focusing on Ubuntu:) The following packages need to be installed on Ubuntu (via `apt`), if not already:\r\n\r\n```text\r\nbuild-essential\r\nwget\r\ncurl\r\nunzip\r\ncmake\r\nmono-complete\r\npython3-pip (or any environment with Python3, where pip is available)\r\npython-is-python3 (needed for ubuntu, so that python points to python3)\r\n```\r\n\r\nIf all packages are installed (and the python environment is set up), the setup-script needs to be executed, which downloads needed dependencies and compiles the source-code located in the `bin`-folder:\r\n\r\n```shell\r\nchmod +x compile_and_setup_depencies.sh  # In case this file is not executable\r\n./compile_and_setup_depencies.sh  # Downloads dependencies, compiles the C++-implementation and sets all binaries in the bin-folder as executable\r\n```\r\n\r\nIf the script exits without errors, the provided workflows can be executed with the command `nextflow`.\r\n\r\n### Executing in Docker\r\n\r\nAlternatively, docker can be used. For this, please follow the [installation guide](https://docs.docker.com/engine/install/ubuntu/) for docker. After installing docker, a local docker-container can be build with all needed dependencies for the workflows. We provide a `Dockerfile` in the `docker`-folder. To build it, execute (while beeing with a shell in the root-folder of this repository) the following:\r\n\r\n```shell\r\ndocker build -t progfastagen:local . -f docker/Dockerfile\r\n```\r\n\r\nThis command builds a local docker container, tagging it with `progfastagen:local`, which can be later used by nextflow. To use it with nextflow, make sure that `nextflow` is installed on the host-system. For each of the workflow example calls below, the `-with-docker progfastagen:local` then needs to be appended, to let `nextflow` know to use the local docker-container.\r\n\r\n## Individual Steps\r\n\r\nEach step has been implemented in such a way, that it can be executed on its own. Each subsection below, provides a brief overview and an example call of the required parameters to demonstrate how the workflow can be called. If you are interested for all the available parameters within a workflow and want modify or tune them, then please refer to the source of the workflows, where each parameter is described briefly.\r\n\r\n### Converting RAW-files to MGF\r\n\r\nThe workflow `convert_to_mgf.nf` is a wrapper around the ThermoRawFileParser and converts RAW-files to the MGF-format. The `ctm_raws` parameter needs to be set, in order to generate the MGF-files:\r\n\r\n```text\r\nnextflow run convert_to_mgf.nf \\\r\n    --ctm_raws < Folder containing RAW-files > \\\r\n    --ctm_outdir < Output-Folder, where the MGFs should be stored >\r\n```\r\n\r\n### Generating a Precursor-Specific-FASTA\r\n\r\nThe workflow `create_precursor_specific_fasta.nf` generates a precursor-specific-FASTA-file, tailored to a set of MGF-files. Here, Protein-Graphs are generated, using the UniProt flat file format (which can be downloaded from [UniProt](https://www.uniprot.org/) by selecting `Text` as format) and a python script prepares the queries, by extracting the MS2-precursors from the MGF-files (using a tolerance, in ppm). Using the Protein-Graphs and a `DryRun`-Version of the traversal, the maximum-variant-limits are determined for each Protein-Graph (and mass-query-range) using a binary-search. These limits are then used for the actual ms2-specific-FASTA-generation in conjunction with the extracted MS2-precursors and a compacted FASTA is returned, which is tailored to the MGF-files.\r\n\r\nAltough of the complexity, the workflow only requires the following parameters to generate such a FASTA:\r\n\r\n```text\r\nnextflow run create_precursor_specific_fasta.nf \\\r\n    --cmf_mgf_files < Folder containing MGF-files > \\\r\n    --cmf_sp_embl_file < Path to a SP-EMBL-File (UniProt flat file format) > \\\r\n    --cmf_outdir <The Output-Folder where the traversal-limits are saved and the ms2-specific-FASTA is stored >\r\n```\r\n\r\nThe optional parameter: `cmf_pg_additional_params` is added to ProtGraph directly, allowing every parameter, ProtGraph provides to be set there (e.g. useful if the digestion should be changed or features/PTMs should be included/excluded, etc...), allowing arbitrary settings to generate Protein-Graphs if desired. It defaults to use all features, ProtGraph can parse.\r\n\r\n**Note regarding PTMs/Tolerance**: The FASTA is tailored to the MS2-precursors, therefore variable and fixed modifications need to be set to the same settings as for the actual identification. This workflow defaults to carbamidomethylation (C, fixed) and oxidation (M, variable). See ProtGraph (and the workflow-parameter `cmf_pg_additional_params`) to set the PTMs accordingly in the Protein-Graphs. The same applies for the MS2-precursor-tolereance which can be set with `cmf_query_ppm` and defaults to `5ppm`.\r\n\r\n**Note regarding Limits**: This workflows defaults to allow up to 5 seconds per query and limits peptides to contain at most 5 variants (with a maximum of 5000 Da per peptide), resulting into FASTA-files which can be 15-200GB large (depending on dataset and species). Changing these settings can drastically increase/decrease the runtime/memory usage/disk usage. We advise to change those settings slightly and to pay attention on the runtime/memory usage/disk usage if run with the newly set limits (and dataset + species) the first time.\r\n\r\n**Note regarding identification**: If digestion is enabled (default is `Trypsin`), the resulting FASTA contains already digested entries, thus searching with a search-engine, the digestion should be set to `off/no_cut`.\r\n\r\n### Generating a Global-FASTA\r\n\r\nThis workflow generates a so called global-FASTA, using ProtGraph, the UniProt flat file format and some global limits for writing out peptides/proteins. Global-FASTAs can be generated with the `create_global_fasta.nf`-workflow. To generate a global-FASTA, only a path to a single SP-EMBL-file (UniProt flat file format) is required. Such a file can be downloaded from [UniProt](https://www.uniprot.org/) directly, by selecting `Text` instead of `FASTA` as the download format.\r\n\r\n```text\r\nnextflow run create_global_fasta.nf \\\r\n    --cgf_sp_embl_file < Path to a SP-EMBL-File (UniProt flat file format) > \\\r\n    --cgf_outdir < The output-folder, where the gloabl-FASTA and some Protein-Graph-statistics should be saved >\r\n```\r\n\r\nPer default, this workflow does not export feature-peptides and is set to only export peptides with up to 5000 Da mass and maximum of two miscleavages. It is possible to generate global-FASTA with some specific features (like containing, `SIGNAL`, `PEPTIDE` or others) and other limits. The parameters `cgf_features_in_graphs` and `cgf_peptide_limits` can be set accordingly. These are added to ProtGraph directly, hence every parameter ProtGraph provides, can be set here (including different digestion settings).\r\n\r\n**Note**: A dry run with ProtGraph to generate statistics how many peptide would be theoretically exported is advised prior for testing. Some Protein-Graphs with some features (e.g. P53 using variants) can contain to many peptides, which could result to very long runtimes and huge FASTAs.\r\n\r\n**Note regarding identification**: If digestion is enabled (default is `Trypsin`), the resulting FASTA contains already digested entries, thus searching with a search-engine, the digestion should be set to `off/no_cut`.\r\n\r\n### Identification via Coment (and Percolator)\r\n\r\nWe provide an identification workflow to showcase, that the generated FASTAs can be used with search-engines. The workflow `identification_via_comet.nf` identifies MGF-files individually, using custom search-settings for Comet (and if desired rescores the results with Percolator), applies an FDR-cutoff using the q-value (for each file) and exposes the identification results into an output-folder.\r\n\r\nThree parameters are required, to execute the workflow:\r\n\r\n1. The MGFs which should be identified\r\n2. The Comet-Parameter file to set the search-settings\r\n3. The FASTA-file which should be used for identification\r\n\r\nBelow is an example call with all required parameters (Percolator is enabled by default):\r\n\r\n```text\r\nnextflow run identification_via_comet.nf \\\r\n    --idc_mgf_folder < Folder containing MGF-files > \\\r\n    --idc_fasta_file < The FASTA which should be used for identification > \\\r\n    --idc_search_parameter_file < The Comet-Parameters file (Search Configuration) > \\\r\n    --idc_outdir < Output-Folder where the results of the identification files are stored >\r\n```\r\n\r\nHere is another example call with all required parameters (this time, turning Percolator off):\r\n\r\n```text\r\nnextflow run identification_via_comet.nf \\\r\n    --idc_mgf_folder < Folder containing MGF-files > \\\r\n    --idc_fasta_file < The FASTA which should be used for identification > \\\r\n    --idc_search_parameter_file < The Comet-Parameters file (Search Configuration) > \\\r\n    --idc_outdir < Output-Folder where the results of the identification files are stored > \\\r\n    --idc_use_percolator 0\r\n```\r\n\r\n**Note**: This identification-workflow defaults to an FDR-cutoff (q-value) of `--idc_fdr \"0.01\"`, reporting only 1% filtered PSMs. Arbitrary and multiple FDR-cutoffs can be set and can be changed to the desired value.\r\n\r\n### Summarization of results\r\n\r\nThe `summarize_ident_results.nf`-workflow genereates convenient summarization of the identification results. Here, the identification-results are binned into 4 groups:\r\n\r\n1. Unique PSMs (a match, which can only originate from one protein)\r\n2. Shared PSMs (a match, which can originate from multiple proteins)\r\n3. Unique Feature PSMs (as 1., but only containing peptides, which can be explained by a features)\r\n4. Shared Feature PSMs (as 2., but only can be explained by features from all originating proteins)\r\n\r\nFurthermore, heatmaps are generated to provide an overview of found peptides across all MGFs/RAW-files.\r\n\r\nTo call this method, a `glob` needs to be specified in this workflow:\r\n\r\n```text\r\nnextflow run summarize_ident_results.nf \\\r\n    --sir_identified_files_glob < The glob matching the desired output from the identification results >\r\n    --sir_outdir < The output directory where the summarized results should be saved >\r\n```\r\n\r\nIn case, the identification workflow was executed using an FDR of 0.01, you could use the following `glob`:\r\n\r\n```text\r\nnextflow run summarize_ident_results.nf \\\r\n    --sir_identified_files_glob \"<Path_to_folder>/*qvalue_no_decoys_fdr_0.01.tsv\"\r\n    --sir_outdir < The output directory where the summarized results should be saved >\r\n```\r\n\r\n**Note**: This step can be used only if specific columns are present in the tables. Furthermore, it distinguishes between the identification results from a FASTA by UniProt or by ProtGraph. The additional parameters control, whether to bin results in group 3 and 4, decide if variable modifications should be considered as unique, as well as if a peptide, which originates multiple times to the same protein should be considered as unique. The main-workflows set these parameters accordingly and can be used as an example.\r\n\r\n## Main Workflow Scripts\r\n\r\nEach individual step described above, is also imported and chained into three main-workflows:\r\n\r\n1. `main_workflow_protein_fasta.nf` (UniProt-FASTA-search)\r\n2. `main_workflow_global_fasta.nf` (Generation of a global-FASTA and search)\r\n3. `main_workflow_precursor_specific_fasta.nf` (Generation of a precursor-specific-FASTA and search)\r\n\r\ngenerating summarized identification results across multiple RAW-files.\r\n\r\nIn each of these workflows, it is possible to modify the parameters of the imported subworkflows, by using the imported subworkflows parameters directly (as shown in the **Individual Steps** above).\r\n\r\nFor protein-FASTA identification, only three parameters are required:\r\n\r\n```text\r\nnextflow run main_workflow_protein_fasta.nf \\\r\n    --main_fasta_file < The FASTA-file, to be used for identification > \\\r\n    --main_raw_files_folder < The folder containing RAW-files > \\\r\n    --main_comet_params < The parameters file for comet (for identification) > \\\r\n    --main_outdir < Output-Folder where all the results from the workflows should be saved >\r\n```\r\n\r\nThis is also true for the other two workflows, where instead of a FASTA-file, the UniProt flat file format needs to be provided. Such a file can be downloaded from [UniProt](https://www.uniprot.org/) directly, by selecting the format `Text` instead of the format `FASTA`.\r\n\r\nHere are the correpsonding calls for global-FASTA and precurosr-specific-FASTA generation and identification:\r\n\r\n```text\r\n# global-FASTA\r\nnextflow run main_workflow_global_fasta.nf \\\r\n    --main_sp_embl_file < The SP-EMBL-file used for Protein-Graph- and FASTA-generation (UniProt flat file format) > \\\r\n    --main_raw_files_folder < The folder containing RAW-files > \\\r\n    --main_comet_params< The parameters file for comet (for identification) > \\\r\n    --main_outdir < Output-Folder where all the results from the workflows should be saved >\r\n\r\n# precursor-specific-FASTA\r\nnextflow run main_workflow_precursor_specific_fasta.nf \\\r\n    --main_sp_embl_file < The SP-EMBL-file used for Protein-Graph- and FASTA-generation (UniProt flat file format) > \\\r\n    --main_raw_files_folder < The folder containing RAW-files > \\\r\n    --main_comet_params < The parameters file for comet (for identification) > \\\r\n    --main_outdir < Output-Folder where all the results from the workflows should be saved >\r\n```\r\n\r\n**Note**: Only defining the required parameters, uses the default parameters for every other setting. For all workflows, this would mean, that the FDR-cutoff (q-value) is set to `0.01` resulting into both FDRs considered. Furthermore, the global-FASTA and precursor-specific-FASTA workflows assume Trypsin digestion. For the global-FASTA-workflow, no features are exported by default, which may not be desired, if someone whishes to search for peptide-features (like `SIGNAL`, etc..). For the precursor-specific-FASTA-workflow, the PTMs carbamidomethylation (C, fixed) and oxidation (M, variable) are assumed, which may need to be modified.\r\n\r\n**Note regarding example calls**: Further below you can find the calls as used in the publication. These set the most minimal parameters for a correct execution on custom datasets and can be used as an example.\r\n\r\n## Regenerate Results from Publication\r\n\r\nIn this subsection you can find the nextflow-calls which were used to execute the 3 workflows. Executing this with the same UniProt flat file/FASTA-file should yield the similar/same results. For generated precursor-specific-FASTAs it may happen, that these are generated with slightly different variant-limits, therefore a slightly different FASTA to search with and slightly different identification results.\r\n\r\nThe FASTA/UniProt flat file used for identification can be found [here](https://cloud.mpc.rub.de/s/LJ2bgGNmsxzSaod). The Comet configuration files are provided in the `example_configuration`-folder. The datasets can be retrieved from [PRIDE](https://www.ebi.ac.uk/pride/).\r\n\r\n### PXD002171\r\n\r\n```shell\r\n# PXD002171 Precursor-Specific\r\nnextflow run main_workflow_precursor_specific_fasta.nf \\\r\n    -with-report \"PXD002171_results_precursor_specific/nextflow_report.html\" \\\r\n    -with-timeline \"PXD002171_results_precursor_specific/nextflow_timeline.html\" \\\r\n    --main_sp_embl_file 20230619_homo_sapiens_proteome.txt \\\r\n    --main_raw_files_folder PXD002171 \\\r\n    --main_comet_params example_configurations/PXD002171_no_dig.txt \\\r\n    --main_outdir PXD002171_results_precursor_specific \\\r\n    --cmf_max_precursor_da 5000 \\\r\n    --cmf_query_ppm 5 \\\r\n    --cmf_timeout_for_single_query 5 \\\r\n    --cmf_maximum_variant_limit 5 \\\r\n    --cmf_pg_additional_params \"-ft VARIANT -ft SIGNAL -ft INIT_MET -ft CONFLICT -ft VAR_SEQ -ft PEPTIDE -ft PROPEP -ft CHAIN -vm 'M:15.994915' -vm 'C:71.037114'\" \\\r\n    --idc_fdr \"0.01\"\r\n    \r\n# PXD002171 Global digested FASTA\r\nnextflow run main_workflow_global_fasta.nf \\\r\n    -with-report \"PXD002171_global_fasta/nextflow_report.html\" \\\r\n    -with-timeline \"PXD002171_global_fasta/nextflow_timeline.html\" \\\r\n    --main_sp_embl_file 20230619_homo_sapiens_proteome.txt \\\r\n    --main_raw_files_folder PXD002171 \\\r\n    --main_comet_params example_configurations/PXD002171_no_dig.txt \\\r\n    --main_outdir PXD002171_global_fasta \\\r\n    --cgf_features_in_graphs \"-ft None\" \\\r\n    --cgf_peptide_limits \"--pep_miscleavages 2 --pep_min_pep_length 5\" \\\r\n    --idc_fdr \"0.01\"\r\n\r\n# PXD002171 Protein FASTA\r\nnextflow run main_workflow_protein_fasta.nf \\\r\n    -with-report \"PXD002171_protein_fasta/nextflow_report.html\" \\\r\n    -with-timeline \"PXD002171_protein_fasta/nextflow_timeline.html\" \\\r\n    --main_fasta_file 20230619_homo_sapiens_proteome.fasta \\\r\n    --main_raw_files_folder PXD002171 \\\r\n    --main_comet_params example_configurations/PXD002171_trypsin_dig.txt \\\r\n    --main_outdir PXD002171_protein_fasta \\\r\n    --idc_fdr \"0.01\"\r\n```\r\n\r\n### PXD028605\r\n\r\n```shell\r\n# PXD028605 Precursor-Specific\r\nnextflow run main_workflow_precursor_specific_fasta.nf \\\r\n    -with-report \"PXD028605_results_precursor_specific/nextflow_report.html\" \\\r\n    -with-timeline \"PXD028605_results_precursor_specific/nextflow_timeline.html\" \\\r\n    --main_sp_embl_file 20230619_homo_sapiens_proteome.txt \\\r\n    --main_raw_files_folder PXD028605 \\\r\n    --main_comet_params example_configurations/PXD028605_no_dig.txt \\\r\n    --main_outdir PXD028605_results_precursor_specific \\\r\n    --cmf_max_precursor_da 5000 \\\r\n    --cmf_query_ppm 20 \\\r\n    --cmf_timeout_for_single_query 5 \\\r\n    --cmf_maximum_variant_limit 5 \\\r\n    --cmf_pg_additional_params \"-ft VARIANT -ft SIGNAL -ft INIT_MET -ft CONFLICT -ft VAR_SEQ -ft PEPTIDE -ft PROPEP -ft CHAIN -fm 'C:57.021464' -vm 'M:15.9949'\" \\\r\n    --idc_fdr \"0.01\"\r\n\r\n# PXD028605 Global digested FASTA\r\nnextflow run main_workflow_global_fasta.nf \\\r\n    -with-report \"PXD028605_global_fasta/nextflow_report.html\" \\\r\n    -with-timeline \"PXD028605_global_fasta/nextflow_timeline.html\" \\\r\n    --main_sp_embl_file 20230619_homo_sapiens_proteome.txt \\\r\n    --main_raw_files_folder PXD028605 \\\r\n    --main_comet_params example_configurations/PXD028605_no_dig.txt \\\r\n    --main_outdir PXD028605_global_fasta \\\r\n    --cgf_features_in_graphs \"-ft None\" \\\r\n    --cgf_peptide_limits \"--pep_miscleavages 2 --pep_min_pep_length 5\" \\\r\n    --idc_fdr \"0.01\"\r\n\r\n# PXD028605 Protein FASTA\r\nnextflow run main_workflow_protein_fasta.nf \\\r\n    -with-report \"PXD028605_protein_fasta/nextflow_report.html\" \\\r\n    -with-timeline \"PXD028605_protein_fasta/nextflow_timeline.html\" \\\r\n    --main_fasta_file 20230619_homo_sapiens_proteome.fasta \\\r\n    --main_raw_files_folder PXD028605 \\\r\n    --main_comet_params example_configurations/PXD028605_trypsin_dig.txt \\\r\n    --main_outdir PXD028605_protein_fasta \\\r\n    --idc_fdr \"0.01\"\r\n```\r\n",
        "doi": "10.48546/workflowhub.workflow.837.1",
        "edam_operation": [
            "Peptide identification",
            "Protein identification"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Protein variants",
            "Proteomics"
        ],
        "filtered_on": "binn* in description",
        "id": "837",
        "keep": true,
        "latest_version": 1,
        "license": "BSD-3-Clause",
        "link": "https:/workflowhub.eu/workflows/837?version=1",
        "name": "ProGFASTAGen - Protein-Graph FASTA Generation (and Identification) Workflows",
        "number_of_steps": 0,
        "projects": [
            "Medizinisches Proteom-Center, Medical Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "proteomics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-04-26",
        "versions": 1
    },
    {
        "create_time": "2024-04-25",
        "creators": [
            "Georgina Samaha"
        ],
        "description": "Parabricks-Genomics-nf is a GPU-enabled pipeline for alignment and germline short variant calling for short read sequencing data. The pipeline utilises [NVIDIA's Clara Parabricks](https://docs.nvidia.com/clara/parabricks/4.2.0/index.html) toolkit to dramatically speed up the execution of best practice bioinformatics tools. Currently, this pipeline is **configured specifically for [NCI's Gadi HPC](https://nci.org.au/our-systems/hpc-systems)**. \r\n\r\nNVIDIA's Clara Parabricks can deliver a significant speed improvement over traditional CPU-based methods, and is designed to be used only with NVIDIA GPUs. This pipeline is suitable for population screening projects as it executes Parabrick's implementations of BWA mem for short read alignment and Google's DeepVariant for short variant calling. Additionally, it uses standard CPU implementations of data quality evaluation tools [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) and [MultiQC](https://multiqc.info/) and [DNAnexus' GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) for scalable gVCF merging and joint variant calling. Optionally, [Variant Effect Predictor (VEP)](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0974-4) can be run for variant annotation. \r\n",
        "doi": "10.48546/workflowhub.workflow.836.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "836",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/836?version=1",
        "name": "Parabricks-Genomics-nf",
        "number_of_steps": 0,
        "projects": [
            "Sydney Informatics Hub"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "bioinformatics",
            "genomics",
            "indels",
            "snps",
            "gpu",
            "mapping",
            "variant calling",
            "whole genome sequencing"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-04-25",
        "versions": 1
    },
    {
        "create_time": "2024-04-24",
        "creators": [
            "Rutger Vos",
            "Fabian Deister",
            "Ben Price"
        ],
        "description": "![Perl CI](https://github.com/FabianDeister/Library_curation_BOLD/actions/workflows/ci.yml/badge.svg)\r\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10975576.svg)](https://doi.org/10.5281/zenodo.10975576)\r\n\r\n# Library curation BOLD\r\n\r\n![alt text](https://github.com/FabianDeister/Library_curation_BOLD/blob/main/doc/IBOL_LOGO_TRANSPARENT.png?raw=true)\r\n\r\nThis repository contains scripts and synonymy data for pipelining the \r\nautomated curation of [BOLD](https://boldsystems.org) data dumps in \r\nBCDM TSV format. The goal is to implement the classification of barcode \r\nreference sequences as is being developed by the \r\n[BGE](https://biodiversitygenomics.eu) consortium. A living document\r\nin which these criteria are being developed is located\r\n[here](https://docs.google.com/document/d/18m-7UnoJTG49TbvTsq_VncKMYZbYVbau98LE_q4rQvA/edit).\r\n\r\nA further goal of this project is to develop the code in this repository\r\naccording to the standards developed by the community in terms of automation,\r\nreproducibility, and provenance. In practice, this means including the\r\nscripts in a pipeline system such as [snakemake](https://snakemake.readthedocs.io/),\r\nadopting an environment configuration system such as\r\n[conda](https://docs.conda.io/), and organizing the folder structure\r\nin compliance with the requirements of\r\n[WorkFlowHub](https://workflowhub.eu/). The latter will provide it with \r\na DOI and will help generate [RO-crate](https://www.researchobject.org/ro-crate/)\r\ndocuments, which means the entire tool chain is FAIR compliant according\r\nto the current state of the art.\r\n\r\n## Install\r\nClone the repo:\r\n```{shell}\r\ngit clone https://github.com/FabianDeister/Library_curation_BOLD.git\r\n```\r\nChange directory: \r\n```{shell}\r\ncd Library_curation_BOLD\r\n```\r\nThe code in this repo depends on various tools. These are managed using\r\nthe `mamba` program (a drop-in replacement of `conda`). The following\r\nsets up an environment in which all needed tools are installed:\r\n\r\n```{shell}\r\nmamba env create -f environment.yml\r\n```\r\n\r\nOnce set up, this is activated like so:\r\n\r\n```{shell}\r\nmamba activate bold-curation\r\n```\r\n\r\n## How to run\r\n### Bash\r\nAlthough the aim of this project is to integrate all steps of the process\r\nin a simple snakemake pipeline, at present this is not implemented. Instead,\r\nthe steps are executed individually on the command line as perl scripts\r\nwithin the conda/mamba environment. Because the current project has its own\r\nperl modules in the `lib` folder, every script needs to be run with the \r\nadditional include flag to add the module folder to the search path. Hence,\r\nthe invocation looks like the following inside the scripts folder:\r\n\r\n```{shell}\r\nperl -I../../lib scriptname.pl -arg1 val1 -arg2 val2\r\n```\r\n### snakemake\r\n\r\nFollow the installation instructions above.\r\n\r\nUpdate config/config.yml to define your input data.\r\n\r\nNavigate to the directory \"workflow\" and type:\r\n```{shell}\r\nsnakemake -p -c {number of cores} target\r\n```\r\n\r\nIf running on an HPC cluster with a SLURM scheduler you could use a bash script like this one:\r\n```{shell}\r\n#!/bin/bash\r\n#SBATCH --partition=hour\r\n#SBATCH --output=job_curate_bold_%j.out\r\n#SBATCH --error=job_curate_bold_%j.err\r\n#SBATCH --mem=24G\r\n#SBATCH --cpus-per-task=2\r\n\r\nsource activate bold-curation\r\n\r\nsnakemake -p -c 2 target\r\n\r\necho Complete!\r\n```\r\n",
        "doi": "10.48546/workflowhub.workflow.833.1",
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Data mining"
        ],
        "filtered_on": "binn* in description",
        "id": "833",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/833?version=1",
        "name": "Library curation BOLD",
        "number_of_steps": 0,
        "projects": [
            "Biodiversity Genomics Europe (general)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dna barcoding"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-04-24",
        "versions": 1
    },
    {
        "create_time": "2024-04-16",
        "creators": [],
        "description": "The input to this workflow is a data matrix of gene expression that was collected from a pediatric patient tumor patient from the KidsFirst Common Fund program [1]. The RNA-seq samples are the columns of the matrix, and the rows are the raw expression gene count for all human coding genes (Table 1). This data matrix is fed into TargetRanger [2] to screen for targets which are highly expressed in the tumor but lowly expressed across most healthy human tissues based on gene expression data collected from postmortem patients with RNA-seq by the GTEx Common Fund program [3]. Based on this analysis the gene IMP U3 small nucleolar ribonucleoprotein 3 (IMP3) was selected because it was the top candidate returned from the TargetRanger analysis (Tables 2-3). IMP3 is also commonly called insulin-like growth factor 2 mRNA-binding protein 3 (IGF2BP3). Next, we leverage unique knowledge from various other Common Fund programs to examine various functions and knowledge related to IMP3. First, we queried the LINCS L1000 data [4] from the LINCS program [5] converted into RNA-seq-like LINCS L1000 Signatures [6] using the SigCom LINCS API [7] to identify mimicker or reverser small molecules that maximally impact the expression of IMP3 in human cell lines (Fig. 1, Table 4). In addition, we also queried the LINCS L1000 data to identify single gene CRISPR knockouts that down-regulate the expression of IMP3 (Fig. 1, Table 5). These potential drug targets were filtered using the Common Fund IDG program's list of understudied proteins [8] to produce a set of additional targets (Table 6). Next, IMP3 was searched for knowledge provided by the with the Metabolomics Workbench MetGENE tool [9]. MetGENE aggregates knowledge about pathways, reactions, metabolites, and studies from the Metabolomics Workbench Common Fund supported resource [10]. The Metabolomics Workbench was searched to find associated metabolites linked to IMP3 [10]. Furthermore, we leveraged the Linked Data Hub API [11] to list knowledge about regulatory elements associated with IMP3 (Table 6). Finally, the GlyGen database [12] was queried to identify relevant sets of proteins that are the product of the IMP3 genes, as well as known post-translational modifications discovered on IMP3.\r\n\r\n1. Lonsdale, J. et al. The Genotype-Tissue Expression (GTEx) project. Nature Genetics vol. 45 580\u2013585 (2013). doi:10.1038/ng.2653\r\n2. Evangelista, J. E. et al. SigCom LINCS: data and metadata search engine for a million gene expression signatures. Nucleic Acids Research vol. 50 W697\u2013W709 (2022). doi:10.1093/nar/gkac328\r\n3. IDG Understudied Proteins, https://druggablegenome.net/AboutIDGProteinList\r\n4. MetGENE, https://sc-cfdewebdev.sdsc.edu/MetGENE/metGene.php\r\n5. The Metabolomics Workbench, https://www.metabolomicsworkbench.org/\r\n6. Linked Data Hub, https://ldh.genome.network/cfde/ldh/\r\n7. York, W. S. et al. GlyGen: Computational and Informatics Resources for Glycoscience. Glycobiology vol. 30 72\u201373 (2019). doi:10.1093/glycob/cwz080",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "814",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-NC-SA-4.0",
        "link": "https:/workflowhub.eu/workflows/814?version=1",
        "name": "Use Case 13: Novel Cell Surface Targets for Individual Cancer Patients Analyzed with Common Fund Datasets",
        "number_of_steps": 13,
        "projects": [
            "NIH CFDE Playbook Workflow Partnership"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Ensure a file contains a gene count matrix, load it into a standard format",
            "Select one Gene",
            "Based on IDG understudied proteins list",
            "Identify RNA-seq-like LINCS L1000 Signatures which reverse the expression of the gene.",
            "Extract Metabolomics reactions for the gene from MetGENE",
            "Identify RNA-seq-like LINCS L1000 Chemical Perturbagen Signatures which reverse the expression of the gene.",
            "Identify gene-centric information from Metabolomics.",
            "Find protein product records in GlyGen for the gene",
            "Identify RNA-seq-like LINCS L1000 CRISPR KO Signatures which reverse the expression of the gene.",
            "Upload a Data File",
            "Identify significantly overexpressed genes when compared to normal tissue in GTEx",
            "Resolve regulatory elements from gene with Linked Data Hub",
            "Extract Metabolomics metabolites for the gene from MetGENE"
        ],
        "type": "Common Workflow Language",
        "update_time": "2024-04-23",
        "versions": 1
    },
    {
        "create_time": "2024-04-23",
        "creators": [
            "Andrii Neronov",
            "Th\u00e9o Boyer"
        ],
        "description": "This workflow provides a calculaiton of the power spectrum of Stochastic Gravitational Wave Backgorund (SGWB) from a first-order cosmological phase transition based on the parameterisations of Roper Pol et al. (2023). The power spectrum includes two components: from the sound waves excited by collisions of bubbles of the new phase and from the turbulence that is induced by these collisions.\r\n\r\nThe cosmological epoch of the phase transition is described by the temperature, T_star and by the number(s) of relativistic degrees of freedom, g_star that should be specified as parameters.\r\n\r\nThe phase transition itself is characterised by phenomenological parameters, alpha, beta_H and epsilon_turb, the latent heat, the ratio of the Hubble radius to the bubble size at percolation and the fraction of the energy otuput of the phase transition that goes into turbulence.\r\n\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.831.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "OTU in description",
        "id": "831",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/831?version=1",
        "name": "SGWB model spectrum",
        "number_of_steps": 1,
        "projects": [
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "astronomy"
        ],
        "tools": [
            "sgwb_astro_tool"
        ],
        "type": "Galaxy",
        "update_time": "2024-04-23",
        "versions": 1
    },
    {
        "create_time": "2024-04-18",
        "creators": [
            "Andrii Neronov",
            "Th\u00e9o Boyer",
            "Denys Savchenko",
            "Volodymyr Savchenko"
        ],
        "description": "The tool provides a calculation of the power spectrum of Stochastic Gravitational Wave Backgorund (SGWB) from a first-order cosmological phase transition based on the parameterisations of Roper Pol et al. (2023). The power spectrum includes two components: from the sound waves excited by collisions of bubbles of the new phase and from the turbulence that is induced by these collisions.\r\n\r\nThe cosmological epoch of the phase transition is described by the temperature, T_star and by the number(s) of relativistic degrees of freedom, g_star that should be specified as parameters.\r\n\r\nThe phase transition itself is characterised by phenomenological parameters, alpha, beta_H and epsilon_turb, the latent heat, the ratio of the Hubble radius to the bubble size at percolation and the fraction of the energy otuput of the phase transition that goes into turbulence.\r\n\r\nThe product Model spectrum outputs the power spectrum for fixed values of these parameters. The product Phase transition parameters reproduces the constraints on the phase transition parameters from the Pulsar Timing Array gravitational wave detectors, reported by Boyer & Neronov (2024), including the estimate of the cosmological magnetic field induced by turbulence.\r\n",
        "doi": "10.48546/workflowhub.workflow.815.2",
        "edam_operation": [],
        "edam_topic": [
            "Physics"
        ],
        "filtered_on": "OTU in description",
        "id": "815",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/815?version=2",
        "name": "Stochastic Gravitational Wave Backgorund (SGWB) tool",
        "number_of_steps": 2,
        "projects": [
            "EuroScienceGateway",
            "ODA"
        ],
        "source": "WorkflowHub",
        "tags": [
            "astronomy"
        ],
        "tools": [
            "sgwb_astro_tool"
        ],
        "type": "Galaxy",
        "update_time": "2024-04-18",
        "versions": 2
    },
    {
        "create_time": "2024-04-09",
        "creators": [
            "Damon-Lee Pointon",
            "William Eagles",
            "Ying Sims"
        ],
        "description": "[![Cite with Zenodo](https://zenodo.org/badge/509096312.svg)](https://zenodo.org/doi/10.5281/zenodo.10047653)\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.10.1-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/sanger-tol/treeval)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/treeval [1.1.0 - Ancient Aurora]** is a bioinformatics best-practice analysis pipeline for the generation of data supplemental to the curation of reference quality genomes. This pipeline has been written to generate flat files compatible with [JBrowse2](https://jbrowse.org/jb2/) as well as HiC maps for use in Juicebox, PretextView and HiGlass.\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nYou can also set up and attempt to run the pipeline here: https://gitpod.io/#https://github.com/BGAcademy23/treeval-curation\r\nThis is a gitpod set up for BGA23 with a version of TreeVal, although for now gitpod will not run a nextflow pipeline die to issues with using singularity. We will be replacing this with an AWS instance soon.\r\n\r\nThe treeval pipeline has a sister pipeline currently named [curationpretext](https://github.com/sanger-tol/curationpretext) which acts to regenerate the pretext maps and accessory files during genomic curation in order to confirm interventions. This pipeline is sufficiently different to the treeval implementation that it is written as it's own pipeline.\r\n\r\n1. Parse input yaml ( YAML_INPUT )\r\n2. Generate my.genome file ( GENERATE_GENOME )\r\n3. Generate insilico digests of the input assembly ( INSILICO_DIGEST )\r\n4. Generate gene alignments with high quality data against the input assembly ( GENE_ALIGNMENT )\r\n5. Generate a repeat density graph ( REPEAT_DENSITY )\r\n6. Generate a gap track ( GAP_FINDER )\r\n7. Generate a map of self complementary sequence ( SELFCOMP )\r\n8. Generate syntenic alignments with a closely related high quality assembly ( SYNTENY )\r\n9. Generate a coverage track using PacBio data ( LONGREAD_COVERAGE )\r\n10. Generate HiC maps, pretext and higlass using HiC cram files ( HIC_MAPPING )\r\n11. Generate a telomere track based on input motif ( TELO_FINDER )\r\n12. Run Busco and convert results into bed format ( BUSCO_ANNOTATION )\r\n13. Ancestral Busco linkage if available for clade ( BUSCO_ANNOTATION:ANCESTRAL_GENE )\r\n14. Count KMERs with FastK and plot the spectra using MerquryFK ( KMER )\r\n15. Generate a coverge track using KMER data ( KMER_READ_COVERAGE )\r\n\r\n## Usage\r\n\r\n> **Note**\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how\r\n> to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline)\r\n> with `-profile test` before running the workflow on actual data.\r\n\r\nCurrently, it is advised to run the pipeline with docker or singularity as a small number of major modules do not currently have a conda env associated with them.\r\n\r\nNow, you can run the pipeline using:\r\n\r\n```bash\r\n# For the FULL pipeline\r\nnextflow run main.nf -profile singularity --input treeval.yaml --outdir {OUTDIR}\r\n\r\n# For the RAPID subset\r\nnextflow run main.nf -profile singularity --input treeval.yaml -entry RAPID --outdir {OUTDIR}\r\n```\r\n\r\nAn example treeval.yaml can be found [here](assets/local_testing/nxOscDF5033.yaml).\r\n\r\nFurther documentation about the pipeline can be found in the following files: [usage](https://pipelines.tol.sanger.ac.uk/treeval/dev/usage), [parameters](https://pipelines.tol.sanger.ac.uk/treeval/dev/parameters) and [output](https://pipelines.tol.sanger.ac.uk/treeval/dev/output).\r\n\r\n> **Warning:**\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\r\n> provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\r\n\r\n## Credits\r\n\r\nsanger-tol/treeval has been written by Damon-Lee Pointon (@DLBPointon), Yumi Sims (@yumisims) and William Eagles (@weaglesBio).\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n<ul>\r\n  <li>@gq1 - For building the infrastructure around TreeVal and helping with code review</li>\r\n  <li>@ksenia-krasheninnikova - For help with C code implementation and YAML parsing</li>\r\n  <li>@mcshane - For guidance on algorithms </li>\r\n  <li>@muffato - For code reviews and code support</li>\r\n  <li>@priyanka-surana - For help with the majority of code reviews and code support</li>\r\n</ul>\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\n## Citations\r\n\r\n<!--TODO: Citation-->\r\n\r\nIf you use sanger-tol/treeval for your analysis, please cite it using the following doi: [10.5281/zenodo.10047653](https://doi.org/10.5281/zenodo.10047653).\r\n\r\n### Tools\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [
            "Genome alignment",
            "Genome assembly",
            "Sequence assembly visualisation"
        ],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "813",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/813?version=1",
        "name": "sanger-tol/treeval v1.1.0 - Ancient Aurora",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "genome_assembly"
        ],
        "tools": [
            "SAMtools",
            "BEDTools",
            "Minimap2",
            "pyfastaq",
            "Nextflow",
            "BUSCO",
            "seqtk",
            "UCSC Genome Browser Utilities",
            "PretextView",
            "JBrowse 2",
            "Merqury",
            "miniprot",
            "tabix",
            "MUMmer"
        ],
        "type": "Nextflow",
        "update_time": "2024-04-09",
        "versions": 1
    },
    {
        "create_time": "2024-03-12",
        "creators": [],
        "description": "# skim2rrna\r\n\r\n**skim2rrna** is a snakemake pipeline for the batch assembly, annotation, and phylogenetic analysis of ribosomal genes from low coverage genome skims. The pipeline was designed to work with sequence data from museum collections. However, it should also work with genome skims from recently collected samples.\r\n\r\n## Contents\r\n - [Setup](#setup)\r\n - [Example data](#example-data)\r\n - [Input](#input)\r\n - [Output](#output)\r\n - [Filtering contaminants](#filtering-contaminants)\r\n - [Assembly and annotation only](#assembly-and-annotation-only)\r\n - [Running your own data](#running-your-own-data)\r\n - [Getting help](#getting-help)\r\n - [Citations](#citations)\r\n\r\n## Setup\r\n\r\nThe pipeline is written in Snakemake and uses conda and singularity to install the necessary tools.\r\n\r\nIt is *strongly recommended* to install conda using Mambaforge. See details here https://snakemake.readthedocs.io/en/stable/getting_started/installation.html\r\n\r\nOnce conda is installed, you can pull the github repo and set up the base conda environment.\r\n\r\n```\r\n# get github repo\r\ngit clone https://github.com/o-william-white/skim2rrna\r\n\r\n# change dir\r\ncd skim2rrna\r\n\r\n# setup conda env\r\nconda env create -n snakemake -f workflow/envs/conda_env.yaml\r\n```\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2rrna\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Example data\r\n\r\nBefore you run your own data, it is recommended to run the example datasets provided . This will confirm there are no user-specific issues with the setup and it also installs all the dependencies. The example data includes simulated ribosomal data from 25 different butterfly species. \r\n\r\nTo run the example data, use the code below. **Note that you need to change the user email to your own address**. The email is required by the Bio Entrez package to fetch reference sequences. The first time you run the pipeline, it will take some time to install each of the conda environments, so it is a good time to take a tea break :).\r\n```\r\nconda activate snakemake\r\n\r\nsnakemake \\\r\n   --cores 4 \\\r\n   --use-conda \\\r\n   --use-singularity \\ \r\n   --config user_email=user@example_email.com\r\n```\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2rrna\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Input\r\n\r\nSnakemake requires a `config.yaml` and `samples.csv` to define input parameters and sequence data for each sample. \r\n\r\nFor the example data provided, the config file is located here `config/config.yaml` and it looks like this:\r\n```\r\n# path to sample sheet csv with columns for ID,forward,reverse,taxid,seed,gene\r\nsamples: config/samples.csv\r\n\r\n# user email\r\nuser_email: user@example_email.com\r\n\r\n# getorganelle reference (go_fetch, custom)\r\ngo_reference: go_fetch\r\n\r\n# forward adapter\r\nforward_adapter: AGATCGGAAGAGCACACGTCTGAACTCCAGTCA\r\n\r\n# reverse adapter\r\nreverse_adapter: AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\r\n\r\n# fastp deduplication (True/False)\r\nfastp_dedup: True\r\n\r\n# barrnap kindgom (Bacteria:bac, Archaea:arc, Eukaryota:euk, None:NA)\r\nbarrnap_kingdom: euk\r\n\r\n# alignment trimming method to use (gblocks or clipkit)\r\nalignment_trim: gblocks\r\n\r\n# alignment missing data threshold for alignment (0.0 - 1.0)\r\nmissing_threshold: 0.5\r\n\r\n# name of outgroup sample (optional)\r\n# use \"NA\" if there is no obvious outgroup\r\n# if more than one outgroup use a comma separated list i.e. \"sampleA,sampleB\"\r\noutgroup: Eurema_blanda\r\n\r\n# plot dimensions (cm)\r\nplot_height: 20\r\nplot_width: 20\r\n```\r\n\r\nThe example samples.csv file is located here `config/samples.csv` and it looks like this (note that the seed and gene columns are only required if the custom getorganelle database option is specified in the config file):\r\n\r\n\r\n ID | forward | reverse | taxid | seed | gene \r\n----|---------|---------|-------|------|------\r\nAdelpha_iphiclus | .test/reads/Adelpha_iphiclus_1.fq.gz | .test/reads/Adelpha_iphiclus_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nAnartia_jatrophae_saturata | .test/reads/Anartia_jatrophae_saturata_1.fq.gz | .test/reads/Anartia_jatrophae_saturata_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nAraschnia_levana | .test/reads/Araschnia_levana_1.fq.gz | .test/reads/Araschnia_levana_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nAuzakia_danava | .test/reads/Auzakia_danava_1.fq.gz | .test/reads/Auzakia_danava_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nBaeotus_beotus | .test/reads/Baeotus_beotus_1.fq.gz | .test/reads/Baeotus_beotus_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nCatacroptera_cloanthe | .test/reads/Catacroptera_cloanthe_1.fq.gz | .test/reads/Catacroptera_cloanthe_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nChalinga_pratti | .test/reads/Chalinga_pratti_1.fq.gz | .test/reads/Chalinga_pratti_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nDiaethria_gabaza_eupepla | .test/reads/Diaethria_gabaza_eupepla_1.fq.gz | .test/reads/Diaethria_gabaza_eupepla_2.fq.gz | 127268 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nDoleschallia_melana | .test/reads/Doleschallia_melana_1.fq.gz | .test/reads/Doleschallia_melana_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nEurema_blanda | .test/reads/Eurema_blanda_1.fq.gz | .test/reads/Eurema_blanda_2.fq.gz | 42450 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nHypolimnas_usambara | .test/reads/Hypolimnas_usambara_1.fq.gz | .test/reads/Hypolimnas_usambara_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nJunonia_villida | .test/reads/Junonia_villida_1.fq.gz | .test/reads/Junonia_villida_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nKallima_paralekta | .test/reads/Kallima_paralekta_1.fq.gz | .test/reads/Kallima_paralekta_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nKallimoides_rumia | .test/reads/Kallimoides_rumia_1.fq.gz | .test/reads/Kallimoides_rumia_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nLitinga_cottini | .test/reads/Litinga_cottini_1.fq.gz | .test/reads/Litinga_cottini_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nMallika_jacksoni | .test/reads/Mallika_jacksoni_1.fq.gz | .test/reads/Mallika_jacksoni_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nModuza_procris | .test/reads/Moduza_procris_1.fq.gz | .test/reads/Moduza_procris_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nParasarpa_zayla | .test/reads/Parasarpa_zayla_1.fq.gz | .test/reads/Parasarpa_zayla_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nPhaedyma_columella | .test/reads/Phaedyma_columella_1.fq.gz | .test/reads/Phaedyma_columella_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nPrecis_pelarga | .test/reads/Precis_pelarga_1.fq.gz | .test/reads/Precis_pelarga_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nProtogoniomorpha_temora | .test/reads/Protogoniomorpha_temora_1.fq.gz | .test/reads/Protogoniomorpha_temora_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nSalamis_cacta | .test/reads/Salamis_cacta_1.fq.gz | .test/reads/Salamis_cacta_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nSmyrna_blomfildia | .test/reads/Smyrna_blomfildia_1.fq.gz | .test/reads/Smyrna_blomfildia_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nTacola_larymna | .test/reads/Tacola_larymna_1.fq.gz | .test/reads/Tacola_larymna_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\nYoma_algina | .test/reads/Yoma_algina_1.fq.gz | .test/reads/Yoma_algina_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\r\n\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2rrna\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Output\r\n\r\nAll output files are saved to the `results` direcotry. Below is a table summarising all of the output files generated by the pipeline.\r\n\r\n| Directory             | Description               |\r\n|-----------------------|---------------------------|\r\n| fastqc_raw            | Fastqc reports for raw input reads |\r\n| fastp                 | Fastp reports from quality control of raw reads |\r\n| fastqc_qc             | Fastqc reports for quality controlled reads |\r\n| go_fetch              | Optional output containing reference databases used by GetOrganelle |\r\n| getorganelle          | GetOrganelle output with a directory for each sample |\r\n| assembled_sequence    | Assembled sequences selected from GetOrganelle output and renamed |\r\n| seqkit                | Seqkit summary of each assembly |\r\n| blastn                | Blastn output of each assembly |\r\n| minimap               | Mapping output of quality filtered reads against each assembly |\r\n| blobtools             | Blobtools assembly summary collating blastn and mapping output |\r\n| annotations           | Annotation outputs of mitos |\r\n| summary               | Summary per sample (seqkit stats), contig (GC content, length, coverage, taxonomy and annotations) and annotated gene counts |\r\n| annotated_genes  | Unaligned fasta files of annotated genes identified across all samples |\r\n| mafft                 | Mafft aligned fasta files of annotated genes identified across all samples |\r\n| mafft_filtered        | Mafft aligned fasta files after the removal of sequences based on a missing data threshold |\r\n| alignment_trim        | Ambiguous parts of alignment removed using either gblocks or clipkit |\r\n| iqtree                | Iqtree phylogenetic analysis of annotated genes |\r\n| plot_tree             | Plots of phylogenetic trees |\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2rrna\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Filtering contaminants\r\n\r\nIf you are working with museum collections, it is possible that you may assemble and annotate sequences from contaminant/non-target species. *Contaminant sequences can be identified based on the blast search output or unusual placement in the phylogenetic trees* (see blobtools and plot_tree outputs). \r\n\r\nA supplementary python script `format_alignments.py `is provided to remove putative contaminants from alignments, and format the alignments for downstream phylogenetic analysis.\r\n\r\nFor example, let's say we wanted to remove all sequences from the sample \"Kallima_paralekta\" and 5.8S ribosomal sequence, you could run the script as shown below. The script works by identifying and removing sequences that have names with  `Kallima_paralekta` or `5_8S` in the sequence names. The filtered alignments are written to a new output directory `filter_alignments_output`.\r\n\r\n```\r\npython workflow/scripts/format_alignments.py  \\\r\n   --input results/mafft_filtered/ \\\r\n   --cont Kallima_paralekta 5_8S \\\r\n   --output filter_alignments_output\r\n```\r\n\r\n*Note that the output fasta files have been reformatted so each alignment file is named after the gene and each sequence is named after the sample.* This is useful if you would like to run our related pipeline **gene2phylo** for further phylogenetic analyses.\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2rrna\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Assembly and annotation only\r\n\r\nIf you are only interested in the assembly of ribosomal sequences and annotation of genes without the phylogenetic analysis, you can stop the pipeline from running the gene alignment and phylogenetic analyses using the `--omit-from` parameter.\r\n```\r\nsnakemake \\\r\n   --cores 4 \\\r\n   --use-conda \\\r\n   --use-singularity \\\r\n   --config user_email=user@example_email.com \\\r\n   --omit-from mafft \r\n```\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2rrna\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Running your own data\r\n\r\nThe first thing you need to do is generate your own config.yaml and samples.csv files, using the files provided as a template.\r\n\r\nGetOrganelle requires reference data in the format of seed and gene reference fasta files. By default the pipeline uses a basic python script called go_fetch.py https://github.com/o-william-white/go_fetch to download and format reference data formatted for GetOrganelle. \r\n\r\ngo_fetch.py works by searching NCBI based on the NCBI taxonomy specified by the taxid column in the samples.csv file. Note that the seed and gene columns in the samples.csv file are only required if you want to provide your own custom GetOrganelle seed and gene reference databases. \r\n\r\nYou can use the default reference data for GetOrganelle, but I would recommend using custom reference databases where possible. See here for details of how to set up your own databases https://github.com/Kinggerm/GetOrganelle/wiki/FAQ#how-to-assemble-a-target-organelle-genome-using-my-own-reference\r\n\r\n## Getting help\r\n\r\nIf you have any questions, please do get in touch in the issues or by email o.william.white@gmail.com\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2rrna\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n\r\n## Citations\r\n\r\nIf you use the pipeline, please cite our bioarxiv preprint: https://doi.org/10.1101/2023.08.11.552985\r\n\r\nSince the pipeline is a wrapper for several other bioinformatic tools we also ask that you cite the tools used by the pipeline:\r\n - Fastqc https://github.com/s-andrews/FastQC\r\n - Fastp https://doi.org/10.1093/bioinformatics/bty560\r\n - GetOrganelle https://doi.org/10.1186/s13059-020-02154-5\r\n - Blastn https://doi.org/10.1186/1471-2105-10-421\r\n - Minimap2 https://doi.org/10.1093/bioinformatics/bty191\r\n - Blobtools https://doi.org/10.12688/f1000research.12232.1\r\n - Seqkit https://doi.org/10.1371/journal.pone.0163962\r\n - MITOS2 https://doi.org/10.1016/j.ympev.2012.08.023\r\n - Gblocks (default) https://doi.org/10.1093/oxfordjournals.molbev.a026334\r\n - Clipkit (optional) https://doi.org/10.1371/journal.pbio.3001007\r\n - Mafft https://doi.org/10.1093/molbev/mst010\r\n - Iqtree https://doi.org/10.1093/molbev/msu300\r\n - ete3 https://doi.org/10.1093/molbev/msw046\r\n - ggtree https://doi.org/10.1111/2041-210X.12628\r\n\r\n<br/>\r\n<div align=\"right\">\r\n    <b><a href=\"#skim2rrna\">\u21a5 back to top</a></b>\r\n</div>\r\n<br/>\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "792",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/792?version=1",
        "name": "skim2rrna",
        "number_of_steps": 0,
        "projects": [
            "NHM Clark group"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-03-12",
        "versions": 1
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Matthias Bernt"
        ],
        "description": "dada2 amplicon analysis for paired end data\n\nThe workflow has three main outputs: \n- the sequence table (output of makeSequenceTable)\n- the taxonomy (output of assignTaxonomy)\n- the counts which allow to track the number of sequences in the samples through the steps (output of sequence counts)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in tags",
        "id": "790",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/790?version=3",
        "name": "dada2/main",
        "number_of_steps": 14,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "name:amplicon"
        ],
        "tools": [
            "dada2_removeBimeraDenovo",
            "__UNZIP_COLLECTION__",
            "dada2_plotQualityProfile",
            "dada2_dada",
            "dada2_seqCounts",
            "__APPLY_RULES__",
            "dada2_mergePairs",
            "dada2_filterAndTrim",
            "dada2_assignTaxonomyAddspecies",
            "dada2_learnErrors",
            "dada2_makeSequenceTable"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 3
    },
    {
        "create_time": "2024-03-06",
        "creators": [
            "Peter Menzel"
        ],
        "description": "# score-assemblies\r\n\r\nA Snakemake-wrapper for evaluating *de novo* bacterial genome assemblies, e.g. from Oxford Nanopore (ONT) or Illumina sequencing.\r\n\r\nThe workflow includes the following programs:\r\n* [pomoxis](https://github.com/nanoporetech/pomoxis) assess_assembly and assess_homopolymers\r\n* dnadiff from the [mummer](https://mummer4.github.io/index.html) package\r\n* [NucDiff](https://github.com/uio-cels/NucDiff/)\r\n* [QUAST](http://quast.sourceforge.net/quast)\r\n* [BUSCO](https://busco.ezlab.org/)\r\n* [ideel](https://github.com/mw55309/ideel/), which uses [prodigal](https://github.com/hyattpd/Prodigal) and [diamond](https://github.com/bbuchfink/diamond)\r\n* [bakta](https://github.com/oschwengers/bakta)\r\n\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.786.1",
        "edam_operation": [],
        "edam_topic": [
            "Sequence analysis"
        ],
        "filtered_on": "annot* in tags",
        "id": "786",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/786?version=1",
        "name": "score-assemblies",
        "number_of_steps": 0,
        "projects": [
            "Peter Menzel's Team"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genome-annotation",
            "genome_assembly"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-03-06",
        "versions": 1
    },
    {
        "create_time": "2024-03-06",
        "creators": [
            "Peter Menzel"
        ],
        "description": "# ont-assembly-snake\r\n\r\nA Snakemake wrapper for easily creating *de novo* bacterial genome assemblies from Oxford Nanopore (ONT) sequencing data, and optionally Illumina data,\r\nusing any combination of read filtering, assembly, long and short read polishing, and reference-based polishing.\r\n\r\n## Included programs\r\n\r\n| read filtering | assembly | long read polishing | short read polishing | reference-based polishing |\r\n| --- | --- | --- | --- | --- |\r\n| [Filtlong](https://github.com/rrwick/Filtlong)<br/> [Rasusa](https://github.com/mbhall88/rasusa) | [Flye](https://github.com/fenderglass/Flye)<br/> [raven](https://github.com/lbcb-sci/raven)<br/> [miniasm](https://github.com/lh3/miniasm)<br/> [Unicycler](https://github.com/rrwick/Unicycler)<br/> [Canu](https://github.com/marbl/canu)  | [racon](https://github.com/lbcb-sci/racon)<br/> [medaka](https://github.com/nanoporetech/medaka) | [pilon](https://github.com/broadinstitute/pilon/wiki)<br/> [Polypolish](https://github.com/rrwick/Polypolish)<br/> [POLCA](https://github.com/alekseyzimin/masurca#polca) | [Homopolish](https://github.com/ythuang0522/homopolish)<br/> [proovframe](https://github.com/thackl/proovframe) | \r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.787.1",
        "edam_operation": [
            "Genome assembly"
        ],
        "edam_topic": [
            "Sequence assembly"
        ],
        "filtered_on": "binn* in description",
        "id": "787",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/787?version=1",
        "name": "ont-assembly-snake",
        "number_of_steps": 0,
        "projects": [
            "Peter Menzel's Team"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genome_assembly",
            "name:illumina",
            "name:ont"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-03-06",
        "versions": 1
    },
    {
        "create_time": "2024-02-21",
        "creators": [
            "Volodymyr Savchenko",
            "Denys Savchenko",
            "Andrii Neronov"
        ],
        "description": "Protype demonstrator of a workflow reducing HESS and INTEGRAL/SPI-ACS data to common Light Curve format and combining the lightcurves into a multi-wavelength observation.",
        "doi": "10.48546/workflowhub.workflow.766.1",
        "edam_operation": [
            "Correlation"
        ],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "766",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/766?version=1",
        "name": "Example Multi-Wavelength Light-Curve Analysis",
        "number_of_steps": 5,
        "projects": [
            "EuroScienceGateway",
            "ODA"
        ],
        "source": "WorkflowHub",
        "tags": [
            "astronomy"
        ],
        "tools": [
            "lightcurve_analysis_astro_tool_pr91",
            "grb-detection_astro_tool",
            "hess_astro_tool"
        ],
        "type": "Galaxy",
        "update_time": "2025-11-04",
        "versions": 1
    },
    {
        "create_time": "2024-02-14",
        "creators": [
            "Anthony Bretaudeau"
        ],
        "description": "Refining Genome Annotations with Apollo",
        "doi": "10.48546/workflowhub.workflow.749.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "749",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/749?version=1",
        "name": "Refining Genome Annotations with Apollo (prokaryotes)",
        "number_of_steps": 5,
        "projects": [
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genome-annotation"
        ],
        "tools": [
            "iframe",
            "list_organism",
            "jbrowse",
            "create_account",
            "create_or_update"
        ],
        "type": "Galaxy",
        "update_time": "2025-11-04",
        "versions": 1
    },
    {
        "create_time": "2024-02-15",
        "creators": [
            "Anthony Bretaudeau"
        ],
        "description": "Functional annotation of protein sequences",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "755",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/755?version=1",
        "name": "Functional protein annotation using EggNOG-mapper and InterProScan",
        "number_of_steps": 2,
        "projects": [
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genome-annotation"
        ],
        "tools": [
            "eggnog_mapper",
            "interproscan"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-31",
        "versions": 1
    },
    {
        "create_time": "2024-02-15",
        "creators": [
            "Anthony Bretaudeau"
        ],
        "description": "Structural and functional genome annotation with Funannotate",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "754",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/754?version=1",
        "name": "Genome annotation with Funannotate",
        "number_of_steps": 9,
        "projects": [
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genome-annotation"
        ],
        "tools": [
            "interproscan",
            "funannotate_predict",
            "funannotate_annotate",
            "jbrowse",
            "aegean_parseval",
            "rna_star",
            "eggnog_mapper",
            "funannotate_compare",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-31",
        "versions": 1
    },
    {
        "create_time": "2024-02-15",
        "creators": [
            "Anthony Bretaudeau"
        ],
        "description": "Masking repeats in a genome using RepeatMasker",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "753",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/753?version=1",
        "name": "Masking repeats with RepeatMasker",
        "number_of_steps": 2,
        "projects": [
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genome-annotation"
        ],
        "tools": [
            "red",
            "repeatmasker_wrapper"
        ],
        "type": "Galaxy",
        "update_time": "2025-11-04",
        "versions": 1
    },
    {
        "create_time": "2024-02-13",
        "creators": [],
        "description": "## EBP-Nor Genome Assembly pipeline\r\n\r\nThis repository contains the EBP-Nor genome assembly pipeline. This pipeline is implemented in snakemake.\r\nThis pipeline is developed to create haplotype-resolved genome assemblies from PacBio HiFi reads and HiC reads,\r\nand is primarly designed for diploid eukaryotic organisms. The pipeline is designed to work on a linux cluster with slurm as workload manager.\r\n\r\n## Requirements & Setup\r\n\r\nSome software need to be configured/installed before the pipeline can be run\r\n\r\n### Conda setup\r\n\r\nMost required software, including snakemake itself, can be installed using [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html).\r\n\r\nOnce conda is installed, you can create a new environment containing most necessary software from the provided asm_pipeline.yaml file as follows:\r\n\r\n```shell\r\nconda create -n asm_pipeline --file=worfklow/envs/asm_pipeline.yaml\r\n```\r\n\r\n### Other software setup\r\n\r\nThe following software need to be installed manually:\r\n\r\n- KMC v3.1.1 (https://github.com/tbenavi1/KMC)\r\n- HiFiAdapterFilt (https://github.com/sheinasim/HiFiAdapterFilt)\r\n- Oatk (https://github.com/c-zhou/oatk)\r\n- OatkDB (https://github.com/c-zhou/OatkDB)\r\n- NCBI FCS-Adaptor (https://github.com/ncbi/fcs/wiki/FCS-adaptor)\r\n- NCBI FCS-GX (https://github.com/ncbi/fcs/wiki/FCS-GX)\r\n\r\nPlease refer to their respective installation instructions to properly install them. You will need to privide the installation paths of these software to the config file (see Parameter section).\r\n\r\n### BUSCO database setup\r\n\r\nAs in general, computing nodes are not connected to the internet, BUSCO lineage datasets need to be downloaded manually before running the pipeline.\r\nThis can easily be done by running\r\n\r\n```shell\r\nbusco --download eukaryota\r\n```\r\n\r\nYou will need to specify the folder where you downloaded the busco lineages in the config file (see Parameter section).\r\n\r\n### Data\r\n\r\nThis pipeline is created for using PacBio HiFi reads together with paired-end Hi-C data.\r\nYou will need to specify the absolute paths to these files in the config file (see Parameters section).\r\n\r\n### Parameters\r\n\r\nThe necessary config files for running the pipeline can be found in the config folder.\r\n\r\nGeneral snakemake and cluster submission parameters are defined in ```config/config.yaml```, \r\ndata- and software-specfic parameters are defined in ```config/asm_params.yaml```.\r\n\r\nFirst, define the paths of the input files you want to use:\r\n- pacbio: path to the location of the PacBio HiFi reads (```.fastq.gz```)\r\n- hicF and hicR: path to the forward and reverse HiC reads respectively\r\n\r\nFor software not installed by conda, the installation path needs to be provided to the Snakemake pipeline by editing following parameters in the ```config/asm_params.yaml```:\r\n\r\n- Set the \"adapterfilt_install_dir\" parameter to the installation path of HiFiAdapterFilt\r\n- Set the \"KMC_path\" parameter to the installation path of KMC\r\n- Set the \"oatk_dir\" parameter to the installation path of oatk\r\n- Set the \"oatk_db\" parameter to the directory where you downloaded the oatk_db files\r\n- Set the \"fcs_path\" parameter to the location of the ```run_fcsadaptor.sh``` and ```fcs.py``` scripts\r\n- Set the \"fcs_adaptor_image\" and \"fcs_gx_image\" parameters to the paths to the ```fcs-adaptor.sif``` and ```fcs-gx.sif``` files respectively\r\n- Set the \"fcs_gx_db\" parameter to the path of the fcs-gx database\r\n\r\nA couple of other parameters need to be verified as well in the config/asm_params.yaml file before running the pipeline:\r\n\r\n- The location of the input data (```input_dir```) should be set to the folder containing the input data.\r\n- The location of the downloaded busco lineages (```busco_db_dir```) should be set to the folder containing the busco lineages files downloaded earlier\r\n- The required BUSCO lineage for running the BUSCO analysis needs to set (```busco_lineage``` parameter). Run ```busco --list-datasets``` to get an overview of all available datasets.\r\n- The required oatk lineage for running organelle genome assembly (```oatk_lineage``` parameter). Check https://github.com/c-zhou/OatkDB for an overview of available lineages.\r\n- A boolean value wether the species is plant (for plastid prediction) or not (```oatk_isPlant```; set to either True or False)\r\n- The NCBI taxid of your species, required for the decontamination step (```taxid``` parameter)\r\n\r\n## Usage and run modes\r\n\r\nBefore running, make sure to activate the conda environment containing the necessary software: ```conda activate asm_assembly```.\r\nTo run the pipeline, run the following command:\r\n\r\n```\r\nsnakemake --profile config/ --configfile config/asm_params.yaml --snakefile workflow/Snakefile {run_mode}\r\n```\r\n\r\nIf you invoke the snakemake command in another directory than the one containing the ```workflow``` and ```config``` folders, \r\nor if the config files (```config.yaml``` and ```asm_params.yaml```) are in another location, you need to specify their correct paths on the command line.\r\n\r\nThe workflow parameters can be modified in 3 ways:\r\n- Directly modifying the ```config/asm_parameters.yaml``` file\r\n- Overriding the default parameters on the command line: ```--config parameter=new_value```\r\n- Overriding the default parameters using a different yaml file: ```--configfile path_to_parameters.yaml```\r\n\r\nThe pipeline has different runing modes, and the run mode should always be the last argument on the command line:\r\n\r\n- \"all\" (default): will run the full workflow including pre-assembly (genomescope & smudgeplot), assembly, scaffolding, decontamination, and organelle assembly\r\n- \"pre_assembly\": will run only the pre-assembly steps (genomescope & smudgeplot)\r\n- \"assembly\": will filter the HiFi reads and assemble them using hifiasm (also using the Hi-C reads), and run busco\r\n- \"scaffolding\": will run all steps necessary for scaffolding (filtering, assembly, HiC filtering, scaffolding, busco), but without pre-assembly\r\n- \"decontamination\": will run assembly, scaffolding, and decontamination, but without pre-assembly and busco analyses\r\n- \"organelles\": will run only organnelle genome assembly\r\n\r\n## Output\r\n\r\nAll generated output will be present in the \"results\" directory, which will be created in the folder from where you invoke the snakemake command.\r\nThis results directory contains different subdirectories related to the different steps in the assembly:\r\n- results/pre_assembly: genomescope and smudgeplot output (each in its own subfolder)\r\n- results/assembly: Hifiasm assembly output and corresponding busco results\r\n- results/scaffolding: scaffolding output, separated in two folders:\r\n  - meryl: meryl databases used for filtering HiC reads\r\n  - yahs: scaffolding output, including final scaffolds and their corresponding busco results\r\n- results/decontamination: decontamination output of the final scaffolded assembly\r\n- results/organelles: assembled organellar genomes\r\n\r\nAdditionally, a text file containing all software versions will be created in the specified input directory.\r\nThe log files of the different steps in the workflow can be found in the ```logs``` directory that will be created.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "740",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/740?version=1",
        "name": "EBP-Nor Genome Assembly Pipeline",
        "number_of_steps": 0,
        "projects": [
            "EBP-Nor"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-02-13",
        "versions": 1
    },
    {
        "create_time": "2024-01-24",
        "creators": [],
        "description": "![workflow](https://github.com/naturalis/barcode-constrained-phylogeny/actions/workflows/python-package-conda.yml/badge.svg)\r\n[![License: Apache-2.0](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\r\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10519081.svg)](https://doi.org/10.5281/zenodo.10519081)\r\n\r\n![Logo](https://github.com/naturalis/barcode-constrained-phylogeny/blob/main/doc/logo-small.png?raw=true)\r\n\r\n# Bactria: BarCode TRee Inference and Analysis\r\nThis repository contains code and data for building very large, topologically-constrained \r\nbarcode phylogenies through a divide-and-conquer strategy. Such trees are useful as \r\nreference materials for curating barcode data by detecting rogue terminals (indicating\r\nincorrect taxonomic annotation) and in the comparable calculation of alpha and beta \r\nbiodiversity metrics across metabarcoding assays. \r\n\r\nThe input data for the approach we develop here currently comes from BOLD data dumps. \r\nThe international database [BOLD Systems](https://www.boldsystems.org/index.php) \r\ncontains DNA barcodes for hundreds of thousands of species, with multiple barcodes per \r\nspecies. The data dumps we use here are TSV files whose columns conform to the nascent\r\nBCDM (barcode data model) vocabulary. As such, other data sources that conform to this\r\nvocabulary could in the future be used as well, such as [UNITE](https://unite.ut.ee/).\r\n\r\nTheoretically, such data could be filtered and aligned per DNA marker to make \r\nphylogenetic trees. However, there are two limiting factors: building very large \r\nphylogenies is computationally intensive, and barcodes are not considered ideal for \r\nbuilding big trees because they are short (providing insufficient signal to resolve large \r\ntrees) and because they tend to saturate across large patristic distances.\r\n\r\n![concept](https://github.com/naturalis/barcode-constrained-phylogeny/blob/main/doc/concept.png)\r\n\r\nBoth problems can be mitigated by using the \r\n[Open Tree of Life](https://tree.opentreeoflife.org/opentree/argus/opentree13.4@ott93302) \r\nas a further source of phylogenetic signal. The BOLD data can be split into chunks that \r\ncorrespond to Open Tree of Life clades. These chunks can be made into alignments and \r\nsubtrees. The OpenTOL can be used as a constraint in the algorithms to make these. The \r\nchunks are then combined in a large synthesis by grafting them on a backbone made from \r\nexemplar taxa from the subtrees. Here too, the OpenTOL is a source of phylogenetic \r\nconstraint.\r\n\r\nIn this repository this concept is developed for both animal species and plant species.\r\n\r\n## Installation\r\n\r\nThe pipeline and its dependencies are managed using conda. On a linux or osx system, you \r\ncan follow these steps to set up the `bactria` Conda environment using an `environment.yml` \r\nfile and a `requirements.txt` file:\r\n\r\n1. **Clone the Repository:**  \r\n   Clone the repository containing the environment files to your local machine:\r\n   ```bash\r\n   git clone https://github.com/naturalis/barcode-constrained-phylogeny.git\r\n   cd barcode-constrained-phylogeny\r\n   ```\r\n2. **Create the Conda Environment:**\r\n   Create the bactria Conda environment using the environment.yml file with the following \r\n   command:\r\n   ```bash\r\n   conda env create -f workflow/envs/environment.yml\r\n   ```\r\n   This command will create a new Conda environment named bactria with the packages \r\n   specified in the environment.yml file. This step is largely a placeholder because\r\n   most of the dependency management is handled at the level of individual pipeline\r\n   steps, which each have their own environment specification.\r\n3. **Activate the Environment:**\r\n   After creating the environment, activate it using the conda activate command:\r\n   ```bash\r\n   conda activate bactria\r\n   ```\r\n4. **Verify the Environment:**\r\n   Verify that the bactria environment was set up correctly and that all packages were \r\n   installed using the conda list command:\r\n   ```bash\r\n   conda list\r\n   ```\r\n   This command will list all packages installed in the active conda environment. You should \r\n   see all the packages specified in the environment.yml file and the requirements.txt file.\r\n\r\n## How to run\r\n\r\nThe pipeline is implemented using snakemake, which is available within the conda \r\nenvironment that results from the installation. Important before running the snakemake pipeline \r\nis to change in [config/config.yaml](config/config.yaml) the number of threads available on your \r\ncomputer. Which marker gene is used in the pipeline is also specified in the config.yaml (default \r\nCOI-5P). Prior to execution, the BOLD data package to use (we used the \r\n[release of 30 December 2022](https://www.boldsystems.org/index.php/datapackage?id=BOLD_Public.30-Dec-2022)) \r\nmust be downloaded manually and stored in the [resources/](resources/) directory. If a BOLD release \r\nfrom another date is used the file names in config.yaml need to be updated. \r\n\r\nHow to run the entire pipeline:\r\n\r\n```bash \r\nsnakemake -j {number of threads} --use-conda\r\n```\r\n\r\nSnakemake rules can be performed separately:\r\n```bash \r\nsnakemake -R {Rule} -j {number of threads} --use-conda\r\n```\r\n\r\nEnter the same number at {number of threads} as you filled in previously in src/config.yaml.\r\nIn {Rule} insert the rule to be performed.\r\n\r\nHere is an overview of all the rules in the Snakefile:\r\n\r\n![graphviz (1)](https://github.com/naturalis/barcode-constrained-phylogeny/blob/main/doc/dag.svg)\r\n(zoomed view is available [here](https://raw.githubusercontent.com/naturalis/barcode-constrained-phylogeny/main/doc/dag.svg))\r\n\r\n## Repository layout\r\n\r\nBelow is the top-level layout of the repository. This layout is in line with \r\n[community standards](https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html) and must be adhered to.\r\nAll of these subfolders contains further explanatory READMEs to explain their contents in more detail.\r\n\r\n- [config](config/) - configuration files\r\n- [doc](doc/) - documentation and background literature\r\n- [logs](logs/) - where log files are written during pipeline runtime\r\n- [resources](resources/) - external data resources (from BOLD and OpenTree) are downloaded here\r\n- [results](results/) - intermediate and final results are generated here\r\n- [workflow](workflow/) - script source code and driver snakefile \r\n\r\n## License\r\n\r\n&copy; 2023 Naturalis Biodiversity Center\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except \r\nin compliance with the License. You may obtain a copy of the License at\r\n\r\n[http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\r\n   \r\nUnless required by applicable law or agreed to in writing, software distributed under the License \r\nis distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express \r\nor implied. See the License for the specific language governing permissions and limitations under \r\nthe License.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "706",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/706?version=1",
        "name": "Bactria: BarCode TRee Inference and Analysis",
        "number_of_steps": 0,
        "projects": [
            "Biodiversity Genomics Europe (general)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "python",
            "snakemake",
            "phylogenetics"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-02-05",
        "versions": 1
    },
    {
        "create_time": "2024-02-02",
        "creators": [
            "Jessica Gomez-Garrido"
        ],
        "description": "# CLAWS (CNAG's Long-read Assembly Workflow in Snakemake)\r\n Snakemake Pipeline used for de novo genome assembly @CNAG. It has been developed for Snakemake v6.0.5.\r\n\r\nIt accepts Oxford Nanopore Technologies (ONT) reads, PacBio HFi reads, illumina paired-end data, illumina 10X data and Hi-C reads. It does the preprocessing of the reads, assembly, polishing, purge_dups, scaffodling and different evaluation steps. By default it will preprocess the reads, run Flye + Hypo + purge_dups + yahs and evaluate the resulting assemblies with BUSCO, MERQURY, Nseries and assembly_stats. It needs a config file and a spec file (json file with instructions on which resources should slurm use for each of the jobs). Both files are created by the script \"create_config_assembly.py\" that is located in the bin directory. To check all the options accepted by the script, do:\r\n\r\n```\r\nbin/create_config_assembly.py -h\r\n```\r\n\r\nOnce the 2 config files are produced, the pipeline can be launched using snakemake like this:\r\n\r\n``snakemake --notemp -j 999 --snakefile assembly_pipeline.smk --configfile assembly.config --is --cluster-conf assembly.spec --use-conda --use-envmodules``\r\n\r\nIf you are using an HPC cluster, please check how should you run snakemake to launch the jobs to the cluster. \r\n\r\nMost of the tools used will be installed via conda using the environments of the \"envs\" directory after providing the \"--use-conda\" option to snakemake. However, a few tools cannot be installed via conda and will have to be available in your PATH, or as a module in the cluster. Those tools are:\r\n\r\n- NextDenovo/2.5.0\r\n- NextPolish/1.4.1\r\n\r\n# How to provide input data:\r\n\r\nThere are several ways of providing the reads.\r\n\r\n### 1- ONT reads\r\n\r\n1.1 Using the option ``--ont-dir {DIR}`` in create_config_assembly.py.\r\n\r\nIf you do so, it will look for all the files in the directory that end in '.fastq.gz' and will add the basenames to \"ONT_wildcards\". These wildcards will be processed by the pipeline that will:\r\n\r\n- Concatenate all the files into a single file\r\n\r\n- Run filtlong with the default or specified parameters. \r\n\r\n- Use the resulting file for assembly, polishing and/or purging.\r\n\r\nYou can also specify the basenames of the files that you want to use with the ``--ont-list `` option. In this case, the pipeline will use the wildcards that you're providing instead of merging all the files in the directory.\r\n\r\n1.2 Using the option ```--ont-reads {FILE}``` in create_config_assembly.py.\r\n\r\nIf you do so, it will consider that you already have all the reads in one file and will:  \r\n\r\n- Run filtlong with the default or specified parameters.\r\n\r\n- Use the resulting file for assembly, polishing and/or purging.\r\n\r\n1.3 Using the option ```--ont-filt {FILE}```. It will use this file as the output from filtlong. Hence, it will skip the preprocessing steps and directly use it for assembly, polishing and/or purging. \r\n\r\n\r\n\r\n### 2-Illumina 10X-linked data\r\n\r\n2.1 Using the  ```--raw-10X {DIR:list}``` option. \r\n\r\nDictionary with 10X raw read directories, it has to be the mkfastq dir. You must specify as well the sampleIDs from this run. Example: '{\"mkfastq-                        dir\":\"sample1,sample2,sample3\"}'...\r\n\r\nIt will take each basename in the list to get the fastqs from the corresponding directory and run longranger on each sample. Afterwards, it will build meryldbs for each \"barcoded\" file. Finally, it will concatenate all the meryldbs and \"barcoded\" files. Resulting \"barcoded\" file will be used for polishing. \r\n\r\n2.2 Using the ``--processed-10X {DIR}`` parameter. \r\n\r\nThis directory can already be there or be produced by the pipeline as described in step 2.1. Once all the \"barcoded\" fastq files are there, meryldbs will be built for each \"barcoded\" file.  Finally, it will concatenate all the meryldbs and \"barcoded\" files. Resulting \"barcoded\" file will be used for polishing. \r\n\r\n2.3 Using the ``--10X`` option. \r\n\r\nThe argument to this is the path to the concatenated \".barcoded\" file that needs to be used for polishing. If the pre-concatenated files are not given, meryldbs will be directly generated with this file, but it may run out of memory. \r\n\r\n### 3- Illumina short-read data\r\n\r\n3.1 Using the ``--illumina-dir {DIR}`` option, that will look for all the files in the directory that end in '.1.fastq.gz' and will add the basenames to \"illumina_wildcards\". These wildcards will be processed by the pipeline that will: \r\n\r\n- Trim adaptors with Trimgalore\r\n\r\n- Concatenate all the trimmed *.1.fastq.gz and the *2.fastq.gz in one file per pair. \r\n\r\n- The resulting reads will be used for building meryldbs and polishing. \r\n\r\n3.2 Using the ``--processed-illumina`` option. If the directory exists and contains files, the pipeline will look for all the files in the directory that end in '.1.fastq.gz' and will add the basenames to \"illumina_wildcards\". These wildcards will be processed by the pipeline that will:\r\n\r\n- Concatenate all the trimmed *.1.fastq.gz and the *2.fastq.gz in one file per pair. \r\n\r\n- The resulting reads will be used for building meryldbs and polishing. \r\n\r\n3.3 Using the ``--pe1 {FILE} and --pe2 {FILE}`` options. That will consider that these are the paired files containing all the illumina reads ready to be used and will build meryldbs and polish with them.\r\n\r\n### 4- Input assemblies\r\n\r\nIf you want to polish an already assembled assembly, you can give it to the pipeline by using the option ``--assembly-in ASSEMBLY_IN [ASSEMBLY_IN ...]\r\n                        Dictionary with assemblies that need to be polished but not assembled and directory where they should\r\n                        be polished. Example: '{\"assembly1\":\"polishing_dir1\"}' '{\"assembly2\"=\"polishing_dir2\"}' ...``\r\n\t\t\t\r\nIf you want to start the pipeline after polishing on an already existing assembly, you can give it to the pipeline by using the option ``--postpolish-assemblies POSTPOLISH_ASSEMBLIES [POSTPOLISH_ASSEMBLIES ...]\r\n                        Dictionary with assemblies for which postpolishing steps need to be run but that are not assembled and\r\n                        base step for the directory where the first postpolishing step should be run. Example:\r\n                        '{\"assembly1\":\"s04.1_p03.1\"}' '{\"assembly2\"=\"s04.2_p03.2\"}' ...``\r\n\r\nTo evaluate and produce the final pretext file on a curated assembly, use ``--curated-assemblies CURATED_ASSEMBLIES [CURATED_ASSEMBLIES ...]\r\n                        Dictionary with assemblies that have already been curated. Evaluations and read alignment will be perforder. Example:\r\n                        '{\"assembly1\":\"s04.1_p03.1\"}' '{\"assembly2\":\"s04.2_p03.2\"}' ...``\r\n\r\n\r\n\r\n# Description of implemented rules\r\n\r\n1- Preprocessing:\r\n\t\r\n- **Read concatenation:**\r\n\r\n``zcat {input.fastqs} | pigz -p {threads} -c  > {output.final_fastq}``\r\n\t\r\n- **Longranger for 10X reads**: it uses the Longranger version installed in the path specified in the configfile\r\n\r\n``longranger basic --id={params.sample} --sample={params.sample} --fastqs={input.mkfastq_dir} --localcores={threads}``\r\n\r\n- **Trimgalore:** By default it gives the ``--max_n 0 --gzip -q 20 --paired --retain_unpaired`` options, but it can be changed with the ``--trim-galore-opts `` argument. \r\n\r\n``trim_galore -j {threads} {params.opts} {input.read1} {input.read2}``\r\n\r\n- **Filtlong:** it uses the Filtlong version installed in the path specified in the configfile. By default it gives the min_length and min_mean_q parameters, but extra parameters can be added with the ``--filtlong-opts`` option.\r\n\r\n``filtlong --min_length {params.minlen} --min_mean_q {params.min_mean_q} {params.opts} {input.reads} | pigz -p {threads} -c > {output.outreads}``\r\n\t\r\n- **Build meryldb**: it uses the merqury conda environment specified in the configfile. It takes as argument the `--mery-k` value that needs to be estimated first for the genome size. It can run either on the illumina reads, the ont reads or both, default behaviour is both. \r\n\r\n``meryl k={params.kmer} count output {output.out_dir} {input.fastq}``\r\n\t\r\n- Concat meryldbs: with the merqury conda environment specified in the configfile\r\n\r\n``meryl union-sum output {output.meryl_all} {input.input_run}``\r\n\t\r\n- **Align ONT (Minimap2):** it aligns the reads using minimap2 and outputs the alignment either in bam or in paf.gz formats. It uses the minimap2 conda environment specified in the configfile\r\n\r\n``minimap2 -{params.align_opts} -t {threads} {input.genome} {input.reads} ``\r\n\r\n- **Align Illumina (BWA-MEM):** it aligns the reads with BWA-mem and outputs a bam file\r\n\r\n``bwa mem -Y {params.options} -t {threads} {input.genome} {input.reads} | samtools view -Sb - | samtools sort -@ {threads} -o {output.mapping} -``\r\n\r\n2- Assembly\r\n\r\n- **Flye (default)**. It is run by default, if you don't want the pipeline to run it, you can give `--no-flye` option when creating the config. It uses the conda environment specified in the config. By default it is set to 2 polishing iterations and gives the genome-size estimate that has been given when creating the config. Extra options can be provided with the `--flye-opts`.\r\n\r\n``flye --{params.readtype} {input.reads} -o {params.outdir}out -t {threads} -i {params.pol_iterations} {params.other_flye_opts} ``\r\n\t\r\n- **Nextdenovo (if ``run-nextdenovo``):** It uses the cluster module specified in the config. If nextdenovo option is turned on, the create_config script will also create the nextdenovo config file. Check the create_config help to see which options can be modified on it. \r\n\r\n``nextDenovo {input.config}``\r\n\r\n3- Polishing\r\n\r\n- **Hypo (default):** It is the polisher that the pipeline uses by default, it can be turned off specifying ``--no-hypo`` when creating the config. If selected, the reads will be aligned in previous rules and then hypo will be run, it requires illumina data. It uses the conda environment specified in the config. \r\n\r\n``hypo -r @short_reads.list.txt -d {input.genome} -b {input.sr_bam} -c {coverage} -s {params.genome_size} -B {input.lr_bam} -t {threads} -o {output.polished} -p {params.proc} {params.opts} ``\r\n\t\r\n- **Nextpolish ont (if turned on):** to run nextpolish with ONT reads, specify ``--nextpolish-ont-rounds`` and the number of rounds you want to run of it. \r\n\r\n``\"python /apps/NEXTPOLISH/1.3.1/lib/nextpolish2.py -g {input.genome} -p {threads} -l lgs.fofn -r {params.lrtype} > {output.polished}``\r\n\t\r\n- **Nextpolish illumina (if turned on):** to run nextpolish with ONT reads, specify ``--nextpolish-ill-rounds`` and the number of rounds you want to run of it. \r\n\r\n``\"python /apps/NEXTPOLISH/1.3.1/lib/nextpolish1.py -g {input.genome}  -p {threads} -s {input.bam} -t {params.task} > {output.polished}``\r\n\r\n4- Post-assembly\r\n\r\n- **Purge_dups (by default):** select ``--no-purgedups`` if you don't want to run it. If no manual cutoffs are given, it'll run purgedups with automatic cutoffs and then will rerun it selecting the mean cutoff as 0.75\\*cov. It uses the version installed in the cluster module specified in the config. \r\n\r\n5- Evaluations\r\n\t\r\n- **Merqury:** It runs on each 'terminal' assembly. This is, the base assembly and the resulting assembly from each branch of the pipeline. \r\n\t\r\n- **Busco:** It can be run only in the terminal assemblies or on all the assemblies produced by the pipeline. It uses the conda environment specified in the config as well as the parameters specified. \r\n\t\r\n- **Nseries:** This is run during the *finalize* on all the assemblies that are evaluated. After it, that rule combines the statistics produced by all the evaluation rules. \r\n\r\n# Description of all options\r\n```\r\n bin/create_config_assembly.py -h\r\nusage: create_configuration_file [-h] [--configFile configFile] [--specFile specFile] [--ndconfFile ndconfFile] [--concat-cores concat_cores]\r\n                                 [--genome-size genome_size] [--lr-type lr_type] [--basename base_name] [--species species] [--keep-intermediate]\r\n                                 [--preprocess-lr-step PREPROCESS_ONT_STEP] [--preprocess-10X-step PREPROCESS_10X_STEP]\r\n                                 [--preprocess-illumina-step PREPROCESS_ILLUMINA_STEP] [--preprocess-hic-step PREPROCESS_HIC_STEP]\r\n                                 [--flye-step FLYE_STEP] [--no-flye] [--nextdenovo-step NEXTDENOVO_STEP] [--run-nextdenovo]\r\n                                 [--nextpolish-cores nextpolish_cores] [--minimap2-cores minimap2_cores] [--bwa-cores bwa_cores]\r\n                                 [--hypo-cores hypo_cores] [--pairtools-cores pairtools_cores] [--busco-cores busco_cores]\r\n                                 [--nextpolish-ont-rounds nextpolish_ont_rounds] [--nextpolish-ill-rounds nextpolish_ill_rounds]\r\n                                 [--hypo-rounds hypo_rounds] [--longranger-cores longranger_cores] [--longranger-path longranger_path]\r\n                                 [--genomescope-opts genomescope_additional] [--no-purgedups] [--ploidy ploidy] [--run-tigmint] [--run-kraken2]\r\n                                 [--no-yahs] [--scripts-dir SCRIPTS_DIR] [--ont-reads ONT_READS] [--ont-dir ONT_DIR] [--ont-filt ONT_FILTERED]\r\n                                 [--pe1 PE1] [--pe2 PE2] [--processed-illumina PROCESSED_ILLUMINA] [--raw-10X RAW_10X [RAW_10X ...]]\r\n                                 [--processed-10X PROCESSED_10X] [--10X R10X] [--illumina-dir ILLUMINA_DIR]\r\n                                 [--assembly-in ASSEMBLY_IN [ASSEMBLY_IN ...]]\r\n                                 [--postpolish-assemblies POSTPOLISH_ASSEMBLIES [POSTPOLISH_ASSEMBLIES ...]]\r\n                                 [--curated-assemblies CURATED_ASSEMBLIES [CURATED_ASSEMBLIES ...]] [--hic-dir HIC_DIR]\r\n                                 [--pipeline-workdir PIPELINE_WORKDIR] [--filtlong-dir FILTLONG_DIR] [--concat-hic-dir CONCAT_HIC_DIR]\r\n                                 [--flye-dir FLYE_DIR] [--nextdenovo-dir NEXTDENOVO_DIR] [--flye-polishing-dir POLISH_FLYE_DIR]\r\n                                 [--nextdenovo-polishing-dir POLISH_NEXTDENOVO_DIR] [--eval-dir eval_dir] [--stats-out stats_out]\r\n                                 [--hic-qc-dir hic_qc_dir] [--filtlong-minlen filtlong_minlen] [--filtlong-min-mean-q filtlong_min_mean_q]\r\n                                 [--filtlong-opts filtlong_opts] [--kraken2-db kraken2_db] [--kraken2-kmer kraken2_kmers]\r\n                                 [--kraken2-opts additional_kraken2_opts] [--kraken2-cores kraken2_threads] [--trim-galore-opts trim_galore_opts]\r\n                                 [--trim-Illumina-cores Trim_Illumina_cores] [--flye-cores flye_cores] [--flye-polishing-iterations flye_pol_it]\r\n                                 [--other-flye-opts other_flye_opts] [--nextdenovo-cores nextdenovo_cores] [--nextdenovo-jobtype nextdenovo_type]\r\n                                 [--nextdenovo-task nextdenovo_task] [--nextdenovo-rewrite nextdenovo_rewrite]\r\n                                 [--nextdenovo-parallel_jobs nextdenovo_parallel_jobs] [--nextdenovo-minreadlen nextdenovo_minreadlen]\r\n                                 [--nextdenovo-seeddepth nextdenovo_seeddepth] [--nextdenovo-seedcutoff nextdenovo_seedcutoff]\r\n                                 [--nextdenovo-blocksize nextdenovo_blocksize] [--nextdenovo-pa-correction  nextdenovo_pa_correction]\r\n                                 [--nextdenovo-minimap_raw nextdenovo_minimap_raw] [--nextdenovo-minimap_cns nextdenovo_minimap_cns]\r\n                                 [--nextdenovo-minimap_map nextdenovo_minimap_map] [--nextdenovo-sort nextdenovo_sort]\r\n                                 [--nextdenovo-correction_opts nextdenovo_correction_opts] [--nextdenovo-nextgraph_opt nextdenovo_nextgraph_opt]\r\n                                 [--sr-cov ill_cov] [--hypo-proc hypo_processes] [--hypo-no-lr] [--hypo-opts hypo_opts]\r\n                                 [--purgedups-cores purgedups_cores] [--purgedups-calcuts-opts calcuts_opts] [--tigmint-cores tigmint_cores]\r\n                                 [--tigmint-opts tigmint_opts] [--hic-qc] [--no-pretext] [--assembly-qc assembly_qc] [--yahs-cores yahs_cores]\r\n                                 [--yahs-mq yahs_mq] [--yahs-opts yahs_opts] [--hic-map-opts hic_map_opts] [--mq mq [mq ...]]\r\n                                 [--hic-qc-assemblylen hic_qc_assemblylen] [--blast-cores blast_cores] [--hic-blastdb blastdb]\r\n                                 [--hic-readsblast hic_readsblast] [--no-final-evals] [--busco-lin busco_lineage] [--merqury-db merqury_db]\r\n                                 [--merqury-plot-opts merqury_plot_opts] [--meryl-k meryl_k] [--meryl-threads meryl_threads]\r\n                                 [--meryl-reads meryl_reads [meryl_reads ...]] [--ont-list ONT_wildcards] [--illumina-list illumina_wildcards]\r\n                                 [--r10X-list r10X_wildcards] [--hic-list hic_wildcards]\r\n\r\nCreate a configuration json file for the assembly pipeline.\r\n\r\noptions:\r\n  -h, --help            show this help message and exit\r\n\r\nGeneral Parameters:\r\n  --configFile configFile\r\n                        Configuration JSON to be generated. Default assembly.config\r\n  --specFile specFile   Cluster specifications JSON fileto be generated. Default assembly.spec\r\n  --ndconfFile ndconfFile\r\n                        Name pf the nextdenovo config file. Default nextdenovo.config\r\n  --concat-cores concat_cores\r\n                        Number of threads to concatenate reads and to run filtlong. Default 4\r\n  --genome-size genome_size\r\n                        Approximate genome size. Example: 615m or 2.6g. Default None\r\n  --lr-type lr_type     Type of long reads (options are flye read-type options). Default nano-hq\r\n  --basename base_name  Base name for the project. Default None\r\n  --species species     Name of the species to be assembled. Default None\r\n  --keep-intermediate   Set this to True if you do not want intermediate files to be removed. Default False\r\n  --preprocess-lr-step PREPROCESS_ONT_STEP\r\n                        Step for preprocessing long-reads. Default 02.1\r\n  --preprocess-10X-step PREPROCESS_10X_STEP\r\n                        Step for preprocessing 10X reads. Default 02.2\r\n  --preprocess-illumina-step PREPROCESS_ILLUMINA_STEP\r\n                        Step for preprocessing illumina reads. Default 02.2\r\n  --preprocess-hic-step PREPROCESS_HIC_STEP\r\n                        Step for preprocessing hic reads. Default 02.3\r\n  --flye-step FLYE_STEP\r\n                        Step for running flye. Default 03.1\r\n  --no-flye             Give this option if you do not want to run Flye.\r\n  --nextdenovo-step NEXTDENOVO_STEP\r\n                        Step for running nextdenovo. Default 03.2\r\n  --run-nextdenovo      Give this option if you do want to run Nextdenovo.\r\n  --nextpolish-cores nextpolish_cores\r\n                        Number of threads to run the nextpolish step. Default 24\r\n  --minimap2-cores minimap2_cores\r\n                        Number of threads to run the alignment with minimap2. Default 32\r\n  --bwa-cores bwa_cores\r\n                        Number of threads to run the alignments with BWA-Mem2. Default 16\r\n  --hypo-cores hypo_cores\r\n                        Number of threads to run the hypo step. Default 24\r\n  --pairtools-cores pairtools_cores\r\n                        Number of threads to run the pairtools step. Default 100\r\n  --busco-cores busco_cores\r\n                        Number of threads to run BUSCO. Default 32\r\n  --nextpolish-ont-rounds nextpolish_ont_rounds\r\n                        Number of rounds to run the Nextpolish with ONT step. Default 0\r\n  --nextpolish-ill-rounds nextpolish_ill_rounds\r\n                        Number of rounds to run the Nextpolish with illumina step. Default 0\r\n  --hypo-rounds hypo_rounds\r\n                        Number of rounds to run the Hypostep. Default 1\r\n  --longranger-cores longranger_cores\r\n                        Number of threads to run longranger. Default 16\r\n  --longranger-path longranger_path\r\n                        Path to longranger executable. Default /scratch/project/devel/aateam/src/10X/longranger-2.2.2\r\n  --genomescope-opts genomescope_additional\r\n                        Additional options to run Genomescope2 with. Default -m 10000\r\n  --no-purgedups        Give this option if you do not want to run Purgedups.\r\n  --ploidy ploidy       Expected ploidy. Default 2\r\n  --run-tigmint         Give this option if you want to run the scaffolding with 10X reads step.\r\n  --run-kraken2         Give this option if you want to run Kraken2 on the input reads.\r\n  --no-yahs             Give this option if you do not want to run yahs.\r\n\r\nInputs:\r\n  --scripts-dir SCRIPTS_DIR\r\n                        Directory with the different scripts for the pipeline. Default\r\n                        /software/assembly/pipelines/Assembly_pipeline/CLAWSv2.2/bin/../scripts/\r\n  --ont-reads ONT_READS\r\n                        File with all the ONT reads. Default None\r\n  --ont-dir ONT_DIR     Directory where the ONT fastqs are stored. Default None\r\n  --ont-filt ONT_FILTERED\r\n                        File with the ONT reads after running filtlong on them. Default None\r\n  --pe1 PE1             File with the illumina paired-end fastqs, already trimmed, pair 1.\r\n  --pe2 PE2             File with the illumina paired-end fastqs, already trimmed, pair 2.\r\n  --processed-illumina PROCESSED_ILLUMINA\r\n                        Directory to Processed illumina reads. Already there or to be produced by the pipeline.\r\n  --raw-10X RAW_10X [RAW_10X ...]\r\n                        Dictionary with 10X raw read directories, it has to be the mkfastq dir. You must specify as well the sampleIDs from this run.\r\n                        Example: '{\"mkfastq-dir\":\"sample1,sample2,sample3\"}'...\r\n  --processed-10X PROCESSED_10X\r\n                        Directory to Processed 10X reads. Already there or to be produced by the pipeline.\r\n  --10X R10X            File with barcoded 10X reads in fastq.gz format, concatenated.\r\n  --illumina-dir ILLUMINA_DIR\r\n                        Directory where the raw illumina fastqs are stored. Default None\r\n  --assembly-in ASSEMBLY_IN [ASSEMBLY_IN ...]\r\n                        Dictionary with assemblies that need to be polished but not assembled and directory where they should be polished. Example:\r\n                        '{\"assembly1\":\"polishing_dir1\"}' '{\"assembly2\"=\"polishing_dir2\"}' ...\r\n  --postpolish-assemblies POSTPOLISH_ASSEMBLIES [POSTPOLISH_ASSEMBLIES ...]\r\n                        Dictionary with assemblies for whic postpolishing steps need to be run but that are not assembled and base step for the\r\n                        directory where the first postpolishing step should be run. Example: '{\"assembly1\":\"s04.1_p03.1\"}'\r\n                        '{\"assembly2\":\"s04.2_p03.2\"}' ...\r\n  --curated-assemblies CURATED_ASSEMBLIES [CURATED_ASSEMBLIES ...]\r\n                        Dictionary with assemblies that have already been curated. Evaluations and read alignment will be perforder. Example:\r\n                        '{\"assembly1\":\"s04.1_p03.1\"}' '{\"assembly2\":\"s04.2_p03.2\"}' ...\r\n  --hic-dir HIC_DIR     Directory where the HiC fastqs are stored. Default None\r\n\r\nOutputs:\r\n  --pipeline-workdir PIPELINE_WORKDIR\r\n                        Base directory for the pipeline run. Default /scratch_isilon/groups/assembly/jgomez/test_CLAWSv2/ilErePala/assembly/\r\n  --filtlong-dir FILTLONG_DIR\r\n                        Directory to process the ONT reads with filtlong. Default s02.1_p01.1_Filtlong\r\n  --concat-hic-dir CONCAT_HIC_DIR\r\n                        Directory to concatenate the HiC reads. Default s02.3_p01.1_Concat_HiC\r\n  --flye-dir FLYE_DIR   Directory to run flye. Default s03.1_p02.1_flye/\r\n  --nextdenovo-dir NEXTDENOVO_DIR\r\n                        Directory to run nextdenovo. Default s03.2_p02.1_nextdenovo/\r\n  --flye-polishing-dir POLISH_FLYE_DIR\r\n                        Directory to polish the flye assembly. Default s04.1_p03.1_polishing/\r\n  --nextdenovo-polishing-dir POLISH_NEXTDENOVO_DIR\r\n                        Directory to run nextdenovo. Default s04.2_p03.2_polishing/\r\n  --eval-dir eval_dir   Base directory for the evaluations. Default evaluations/\r\n  --stats-out stats_out\r\n                        Path to the file with the final statistics.\r\n  --hic-qc-dir hic_qc_dir\r\n                        Directory to run the hic_qc. Default hic_qc/\r\n\r\nFiltlong:\r\n  --filtlong-minlen filtlong_minlen\r\n                        Minimum read length to use with Filtlong. Default 1000\r\n  --filtlong-min-mean-q filtlong_min_mean_q\r\n                        Minimum mean quality to use with Filtlong. Default 80\r\n  --filtlong-opts filtlong_opts\r\n                        Extra options to run Filtlong (eg. -t 4000000000)\r\n\r\nKraken2:\r\n  --kraken2-db kraken2_db\r\n                        Database to be used for running Kraken2. Default None\r\n  --kraken2-kmer kraken2_kmers\r\n                        Database to be used for running Kraken2. Default None\r\n  --kraken2-opts additional_kraken2_opts\r\n                        Optional parameters for the rule Kraken2. Default\r\n  --kraken2-cores kraken2_threads\r\n                        Number of threads to run the Kraken2 step. Default 16\r\n\r\nTrim_Galore:\r\n  --trim-galore-opts trim_galore_opts\r\n                        Optional parameters for the rule trim_galore. Default --max_n 0 --gzip -q 20 --paired --retain_unpaired\r\n  --trim-Illumina-cores Trim_Illumina_cores\r\n                        Number of threads to run the Illumina trimming step. Default 8\r\n\r\nFlye:\r\n  --flye-cores flye_cores\r\n                        Number of threads to run FLYE. Default 128\r\n  --flye-polishing-iterations flye_pol_it\r\n                        Number of polishing iterations to use with FLYE. Default 2\r\n  --other-flye-opts other_flye_opts\r\n                        Additional options to run Flye. Default --scaffold\r\n\r\nNextdenovo:\r\n  --nextdenovo-cores nextdenovo_cores\r\n                        Number of threads to run nextdenovo. Default 2\r\n  --nextdenovo-jobtype nextdenovo_type\r\n                        Job_type for nextdenovo. Default slurm\r\n  --nextdenovo-task nextdenovo_task\r\n                        Task need to run. Default all\r\n  --nextdenovo-rewrite nextdenovo_rewrite\r\n                        Overwrite existing directory. Default yes\r\n  --nextdenovo-parallel_jobs nextdenovo_parallel_jobs\r\n                        Number of tasks used to run in parallel. Default 50\r\n  --nextdenovo-minreadlen nextdenovo_minreadlen\r\n                        Filter reads with length < minreadlen. Default 1k\r\n  --nextdenovo-seeddepth nextdenovo_seeddepth\r\n                        Expected seed depth, used to calculate seed_cutoff, co-use with genome_size, you can try to set it 30-45 to get a better\r\n                        assembly result. Default 45\r\n  --nextdenovo-seedcutoff nextdenovo_seedcutoff\r\n                        Minimum seed length, <=0 means calculate it automatically using bin/seq_stat. Default 0\r\n  --nextdenovo-blocksize nextdenovo_blocksize\r\n                        Block size for parallel running, split non-seed reads into small files, the maximum size of each file is blocksize. Default 1g\r\n  --nextdenovo-pa-correction  nextdenovo_pa_correction\r\n                        number of corrected tasks used to run in parallel, each corrected task requires ~TOTAL_INPUT_BASES/4 bytes of memory usage,\r\n                        overwrite parallel_jobs only for this step. Default 100\r\n  --nextdenovo-minimap_raw nextdenovo_minimap_raw\r\n                        minimap2 options, used to find overlaps between raw reads, see minimap2-nd for details. Default -t 30\r\n  --nextdenovo-minimap_cns nextdenovo_minimap_cns\r\n                        minimap2 options, used to find overlaps between corrected reads. Default -t 30\r\n  --nextdenovo-minimap_map nextdenovo_minimap_map\r\n                        minimap2 options, used to map reads back to the assembly. Default -t 30 --no-kalloc\r\n  --nextdenovo-sort nextdenovo_sort\r\n                        sort options, see ovl_sort for details. Default -m 400g -t 20\r\n  --nextdenovo-correction_opts nextdenovo_correction_opts\r\n                        Correction options. Default -p 30 -dbuf\r\n  --nextdenovo-nextgraph_opt nextdenovo_nextgraph_opt\r\n                        nextgraph options, see nextgraph for details. Default -a 1\r\n\r\nHypo:\r\n  --sr-cov ill_cov      Approximate short read coverage for hypo Default 0\r\n  --hypo-proc hypo_processes\r\n                        Number of contigs to be processed in parallel by HyPo. Default 6\r\n  --hypo-no-lr          Set this to false if you don\u00a1t want to run hypo with long reads. Default True\r\n  --hypo-opts hypo_opts\r\n                        Additional options to run Hypo. Default None\r\n\r\nPurge_dups:\r\n  --purgedups-cores purgedups_cores\r\n                        Number of threads to run purgedups. Default 8\r\n  --purgedups-calcuts-opts calcuts_opts\r\n                        Adjusted values to run calcuts for purgedups. Default None\r\n\r\nScaffold_with_10X:\r\n  --tigmint-cores tigmint_cores\r\n                        Number of threads to run the 10X scaffolding step. Default 12\r\n  --tigmint-opts tigmint_opts\r\n                        Adjusted values to run the scaffolding with 10X reads. Default None\r\n\r\nHiC:\r\n  --hic-qc              Give this option if only QC of the HiC data needs to be done.\r\n  --no-pretext          Give this option if you do not want to generate the pretext file\r\n  --assembly-qc assembly_qc\r\n                        Path to the assembly to be used perfom the QC of the HiC reads.\r\n  --yahs-cores yahs_cores\r\n                        Number of threads to run YAHS. Default 48\r\n  --yahs-mq yahs_mq     Mapping quality to use when running yahs.Default 40\r\n  --yahs-opts yahs_opts\r\n                        Additional options to give to YAHS.Default\r\n  --hic-map-opts hic_map_opts\r\n                        Options to use with bwa mem when aligning the HiC reads. Deafault -5SP -T0\r\n  --mq mq [mq ...]      Mapping qualities to use for processing the hic mappings. Default [0, 40]\r\n  --hic-qc-assemblylen hic_qc_assemblylen\r\n                        Lentgh of the assembly to be used for HiC QC\r\n  --blast-cores blast_cores\r\n                        Number of threads to run blast with the HiC unmapped reads.Default 8\r\n  --hic-blastdb blastdb\r\n                        BLAST Database to use to classify the hic unmapped reads. Default /scratch_isilon/groups/assembly/data/blastdbs\r\n  --hic-readsblast hic_readsblast\r\n                        Number of unmapped hic reads to classify with blast. Default 100\r\n\r\nFinalize:\r\n  --no-final-evals      If specified, do not run evaluations on final assemblies. Default True\r\n  --busco-lin busco_lineage\r\n                        Path to the lineage directory to run Busco with. Default None\r\n  --merqury-db merqury_db\r\n                        Meryl database. Default None\r\n  --merqury-plot-opts merqury_plot_opts\r\n                        Meryl database. Default None\r\n  --meryl-k meryl_k     Merqury plot additional options, for example \" -m 200 -n 6000|\". Default None\r\n  --meryl-threads meryl_threads\r\n                        Number of threads to run meryl and merqury. Default 4\r\n  --meryl-reads meryl_reads [meryl_reads ...]\r\n                        Type of reads to be used to build the meryldb. Default ont illumina\r\n\r\nWildcards:\r\n  --ont-list ONT_wildcards\r\n                        List with basename of the ONT fastqs that will be used. Default None\r\n  --illumina-list illumina_wildcards\r\n                        List with basename of the illumina fastqs. Default None\r\n  --r10X-list r10X_wildcards\r\n                        List with basename of the raw 10X fastqs. Default None\r\n  --hic-list hic_wildcards\r\n                        List with basename of the raw hic fastqs. Default None\r\n```\r\n# Changes made to v2.2: \r\n\r\n1. General: \r\n\r\n\tNow default read_type is nano-hq \r\n\r\n2. Rule trim_galore: \r\n\r\n\t\"--max_n 0\" has been added to the default behaviour of \"--trim-galore-opts\" \r\n\r\n3. Meryl: \r\n\r\n\tNew option \"--meryl-reads\" has been added to the config. Default is \"Illumina ont\" to build the meryl database using both type of reads, it can be changed to one or the other \r\n\r\n4. Merqury: \r\n\r\n\tOption \"--merqury-plot-opts\" has been added to config file. It can be used to modify the x and y axis maximum values (eg. --merqury-plot-opts \" -m 200 -n 6000\") \r\n\r\n5. Genomescope: \r\n\r\n\t\"-m 10000\" is now part of the default behavior of \"--genomescope-opts\" \r\n\r\n6. Hic_statistics: \r\n\r\n\tThis is now running for each assembly and mq for which a pretext file is generated \r\n\r\n7. Assembly inputs for different steps: \r\n\r\n\ta. \"--assembly-in\" to start after assembly step (eg. Evaluation, polishing, purging and scaffolding) \r\n\r\n\tb. \"--postpolish-assemblies\" to start after polishing step (eg. Evaluation, purging and scaffolding) \r\n\r\n\tc. \"--curated-assemblies\" to start after scaffolding step (eg. Evaluation and pretext generation) \r\n",
        "doi": "10.48546/workflowhub.workflow.567.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "567",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/567?version=2",
        "name": "CLAWS (CNAG's long-read assembly workflow in Snakemake)",
        "number_of_steps": 0,
        "projects": [
            "ERGA Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-02-02",
        "versions": 2
    },
    {
        "create_time": "2024-01-09",
        "creators": [
            "Rafael Terra",
            "Diego Carvalho"
        ],
        "description": "# Framework for construction of phylogenetic networks on High Performance Computing (HPC) environment\r\n\r\n## Introduction\r\n\r\nPhylogeny refers to the evolutionary history and relationship between biological lineages related by common descent. Reticulate evolution refers to the origination of lineages through the complete or partial merging of ancestor lineages. Networks may be used to represent lineage independence events in non-treelike phylogenetic processes.\r\n\r\nThe methodology for reconstructing networks is still in development. Here we explore two methods for reconstructing rooted explicit phylogenetic networks, PhyloNetworks and Phylonet, which employ computationally expensive and time consuming algorithms. The construction of phylogenetic networks follows a coordinated processing flow of data sets analyzed and processed by the coordinated execution of a set of different programs, packages, libraries or pipelines, called workflow activities. \r\n\r\nIn view of the complexity in modeling network experiments, the present work introduces a workflow for phylogenetic network analyses coupled to be executed in High-Performance Computing (HPC) environments. The workflow aims to integrate well-established software, pipelines and scripts, implementing a challenging task since these tools do not consistently profit from the HPC environment, leading to an increase in the expected makespan and idle computing resources.\r\n\r\n## Requirements\r\n\r\n1. Python >= 3.8\r\n   1. Biopython >= 1.75\r\n   2. Pandas >= 1.3.2\r\n   3. Parsl >= 1.0\r\n3. Raxml >= 8.2.12\r\n4. Astral  >= 5.7.1\r\n5. SnaQ (PhyloNetworks) >= 0.13.0\r\n6. MrBayes >= 3.2.7a\r\n7. BUCKy >=  1.4.4\r\n8. Quartet MaxCut >= 2.10\r\n9. PhyloNet >= 3.8.2\r\n10. Julia >= 1.4.1\r\n11. IQTREE >= 2.0\r\n\r\n\r\n## How to use\r\n\r\n### Setting up the framework\r\n\r\nThe framework uses a file to get all the needed parameters. For default it loads the file *default.ini* in the config folder, but you can explicitly load other files using the argument ``-s name_of_the_file``, *e.g.* ``-s config/test.ini``.\r\n\r\n* Edit *parl.env* with the environment variables you may need, such as modules loadeds in SLURM\r\n* Edit *work.config* with the directories of your phylogeny studies (the framework receives as input a set of homologous gene alignments of species in the nexus format).\r\n* Edit *default.ini* with the path for each of the needed softwares and the parameters of the execution provider.\r\n\r\nFor default, the execution logs are created in the ``runinfo`` folder. To change it you can use the `-r folder_path` parameter.\r\n\r\n#### Contents of the configuration file\r\n\r\n* General settings\r\n\r\n```ini\r\n[GENERAL]\r\nExecutionProvider = SLURM\r\nScriptDir \t\t= ./scripts\r\nEnviron\t\t\t= config/parsl.env\r\nWorkload\t\t= config/work.config\r\nNetworkMethod   = MP\r\nTreeMethod      = RAXML\r\nBootStrap       = 1000\r\n```\r\n\r\n1. The framework can be executed in a HPC environment using the Slurm resource manager using the parameter ``ExecutionProvider`` equals to ``SLURM`` or locally with ``LOCAL``. \r\n2. The path of the scripts folder is assigned  in ``ScriptDir``. It's recommended to use the absolute path to avoid errors.\r\n3. The ``Environ`` parameter contains the path of the file used to set environment variables. More details can be seen below.\r\n4. In ``Workload`` is the path of the experiments that will be performed.\r\n5. ``NetworkMethod`` and ``TreeMethod`` are the default network and tree methods that will be used to perform the workloads' studies.\r\n6. ``Bootstrap`` is the parameter used in all the software that use bootstrap (RAxML, IQTREE and ASTRAL)\r\n\r\n* Workflow execution settings\r\n \r\n  When using SLURM, these are the needed parameters:\r\n  ```ini\r\n  [WORKFLOW]\r\n  Monitor\t\t\t= False\r\n  PartCore\t= 24\r\n  PartNode\t= 1\r\n  Walltime\t= 00:20:00\r\n  ```\r\n\r\n  1. ``Monitor`` is a parameter to use parsl's monitor module in HPC environment. It can be *true* or *false*. If you want to use it, it's necessary to set it as *true* and manually change the address in ``infra_manager.py``\r\n  2. If you are using it in a HPC environment (using SLURM), the framework is going to submit in a job. ``PartCore`` is the number of cores of the node; ``PartNode`` is the number of nodes of the partition; and the ``Walltime`` parameter is the maximum amount of time the job will be able to run.\r\n\r\n  However, if the the desired execution method is the LocalProvider, _i.e._ the execution is being performed in your own machine, only these parameters are necessary:\r\n\r\n  ```ini\r\n  [WORKFLOW]\r\n  Monitor\t\t\t= False\r\n  MaxCore\t= 6\r\n  CoresPerWorker\t= 1\r\n\r\n  ```\r\n\r\n* RAxML settings\r\n\r\n  ```ini\r\n  [RAXML]\r\n  RaxmlExecutable = raxmlHPC-PTHREADS\r\n  RaxmlThreads \t= 6\r\n  RaxmlEvolutionaryModel = GTRGAMMA --HKY85\r\n  ```\r\n\r\n* IQTREE settings\r\n\r\n  ```ini\r\n  [IQTREE]\r\n  IqTreeExecutable = iqtree2\r\n  IqTreeEvolutionaryModel = TIM2+I+G \r\n  IqTreeThreads = 6\r\n  ```\r\n\r\n* ASTRAL settings\r\n\r\n  ```ini\r\n  [ASTRAL]\r\n  AstralExecDir \t= /opt/astral/5.7.1\r\n  AstralJar \t\t= astral.jar\r\n  ```\r\n\r\n* PhyloNet settings\r\n\r\n  ```ini\r\n  [PHYLONET]\r\n  PhyloNetExecDir \t= /opt/phylonet/3.8.2/\r\n  PhyloNetJar \t\t= PhyloNet.jar\r\n  PhyloNetThreads     = 6\r\n  PhyloNetHMax        = 3\r\n  PhyloNetRuns        = 5\r\n  ```\r\n\r\n* SNAQ settings\r\n\r\n  ```ini\r\n  [SNAQ]\r\n  SnaqThreads\t\t= 6\r\n  SnaqHMax        = 3\r\n  SnaqRuns        = 3\r\n  ```\r\n\r\n* Mr. Bayes settings\r\n\r\n  ```ini\r\n  [MRBAYES]\r\n  MBExecutable\t= mb\r\n  MBParameters\t= set usebeagle=no beagledevice=cpu beagleprecision=double; mcmcp ngen=100000 burninfrac=.25 samplefreq=50 printfreq=10000 diagnfreq=10000 nruns=2 nchains=2 temp=0.40 swapfreq=10\r\n  ```\r\n\r\n* Bucky settings\r\n\r\n  ```ini\r\n  [BUCKY]\r\n  BuckyExecutable = bucky\r\n  MbSumExecutable = mbsum\r\n  ```\r\n\r\n* Quartet MaxCut\r\n\r\n  ```ini\r\n  QUARTETMAXCUT]\r\n  QmcExecDir       = /opt/quartet/\r\n  QmcExecutable    = find-cut-Linux-64\r\n  ```\r\n\r\n#### Workload file\r\n\r\nFor default the workload file is ``work.config`` in the *config* folder. The file contains the absolute paths of the experiment's folders.\r\n\r\n```\r\n/home/rafael.terra/Biocomp/data/Denv_1\r\n```\r\n\r\nYou can comment folders using the # character in the beginning of the path. *e. g.* ``#/home/rafael.terra/Biocomp/data/Denv_1``. That way the framework won't read this path.\r\n\r\nYou can also run a specific flow for a path using ``@TreeMethod|NetworkMethod`` in the end of a path. Where *TreeMethod* can be RAXML, IQTREE or MRBAYES and *NetworkMethod* can be MPL or MP (case sensitive). The supported flows are: ``RAXML|MPL``, ``RAXML|MP``, ``IQTREE|MPL``, ``IQTREE|MP`` and ``MRBAYES|MPL``. For example:\r\n\r\n```\r\n/home/rafael.terra/Biocomp/data/Denv_1@RAXML|MPL\r\n```\r\n\r\n#### Environment file\r\n\r\nThe environment file contains all the environment variables (like module files used in SLURM) used during the framework execution. Example:\r\n\r\n```sh\r\nmodule load python/3.8.2\r\nmodule load raxml/8.2_openmpi-2.0_gnu\r\nmodule load java/jdk-12\r\nmodule load iqtree/2.1.1\r\nmodule load bucky/1.4.4\r\nmodule load mrbayes/3.2.7a-OpenMPI-4.0.4\r\nsource /scratch/app/modulos/julia-1.5.1.sh\r\n```\r\n\r\n#### Experiment folder\r\n\r\nEach experiment folder needs to have a *input folder* containing a *.tar.gz* compressed file and a *.json* with the following content. **The framework considers that there is only one file of each extension in the input folder**.\r\n\r\n```json\r\n{\r\n\t\"Mapping\":\"\",\r\n\t\"Outgroup\":\"\"\r\n}\r\n```\r\n\r\nWhere ``Mapping`` is a direct mapping of the taxon, when there are multiple alleles per species, in the format ``species1:taxon1,taxon2;species2:taxon3,taxon4`` *(white spaces are not supported)* and ``Outgroup`` is the taxon used to root the network. The Mapping parameter is optional (although it has to be in the json file without value), but the outgroup is obligatory. It's important to say that the flow *MRBAYES|MPL* doesn't support multiple alleles per species. Example:\r\n\r\n```json\r\n{\r\n  \"Mapping\": \"dengue_virus_type_2:FJ850082,FJ850088,JX669479,JX669482,JX669488,KP188569;dengue_virus_type_3:FJ850079,FJ850094,JN697379,JX669494;dengue_virus_type_1:FJ850073,FJ850084,FJ850093,JX669465,JX669466,JX669475,KP188545,KP188547;dengue_virus_type_4:JN559740,JQ513337,JQ513341,JQ513343,JQ513344,JQ513345,KP188563,KP188564;Zika_virus:MH882543\", \r\n  \"Outgroup\": \"MH882543\"\r\n}\r\n```\r\n\r\n\r\n## Running the framework\r\n\r\n* In a local machine:\r\n\r\n  After setting up the framework, just run ``python3 parsl_workflow.py``.\r\n  \r\n* In a SLURM environment:\r\n\r\n  Create an submition script that inside contains: ``python3 parsl_workflow.py``.\r\n\r\n  ```sh\r\n  #!/bin/bash\r\n  #SBATCH --time=15:00:00\r\n  #SBATCH -e slurm-%j.err\r\n  #SBATCH -o slurm-%j.out\r\n  module load python/3.9.6\r\n  cd /path/to/biocomp\r\n  python3 parsl_workflow.py\r\n  ```\r\n\r\nThe framework is under heavy development. If you notice any bug, please create an issue here on GitHub.\r\n\r\n### Running in a DOCKER container\r\n\r\nThe framework is also available to be used in Docker. It can be built from source or pushed from DockerHub.\r\n\r\n#### Building it from the source code\r\n\r\nAdapt the default settings file ``config/default.ini`` according to your machine, setting the number of threads and bootstrap. After that, run ``docker build -t hp2net .`` in the project's root folder.\r\n\r\n#### Downloading it from Dockerhub\r\n\r\nThe docker image can also be downloaded from [Docker hub](https://hub.docker.com/repository/docker/rafaelstjf/hp2net/general). To do that, just run the command ``docker pull rafaelstjf/hp2net:main``\r\n\r\n#### Running\r\n\r\nThe first step to run the framework is to setup your dataset. To test if the framework is running without problems in your machine, you can use the [example datasets](example_data).\r\n\r\n![Alt text](docs/example_data.png)\r\n\r\nExtracting the ``example_data.zip`` file, a new folder called ``with_outgroup`` is created. This folder contain four datasets of DENV sequences.\r\n\r\nThe next step is the creation of the settings and workload files. For the settings file, download the [default.ini](config/default.ini) from this repository and change it to you liking (the path of all software are already configured to run on docker). The workload file is a text file containing the absolute path of the datasets, followed by the desired pipeline, as shown before in this document. Here for example purposes, the ``input.txt`` file was created.\r\n\r\n![Alt text](docs/example_files.png)\r\n\r\nWith all the files prepared, the framework can be executed from the ``example_data`` folder as following:\r\n\r\n``docker run --rm -v $PWD:$PWD rafaelstjf/hp2net:main -s $PWD/default.ini -w $PWD/input.txt``\r\n\r\n**Important:** the docker doesn't save your logs, for that add the parameter: ``-r $PWD/name_of_your_log_folder``.\r\n\r\n---\r\nIf you are running it on **Santos Dumont Supercomputer**, both downloading and execution of the docker container need to be performed from a submission script and executed using ``sg docker -c \"sbatch script.sh\"``. The snippet below shows an example of submission script.\r\n\r\n```sh\r\n#!/bin/bash\r\n#SBATCH --nodes=1\r\n#SBATCH --ntasks-per-node=24\r\n#SBATCH -p cpu_small\r\n#SBATCH -J Hp2NET\r\n#SBATCH --exclusive\r\n#SBATCH --time=02:00:00\r\n#SBATCH -e slurm-%j.err\r\n#SBATCH -o slurm-%j.out\r\n\r\nDIR='/scratch/pcmrnbio2/rafael.terra/WF_parsl/example_data'\r\ndocker  pull rafaelstjf/hp2net:main\r\n\r\ndocker run --rm -v $DIR:$DIR rafaelstjf/hp2net:main -s ${DIR}/sdumont.ini -w ${DIR}/entrada.txt -r ${DIR}/logs\r\n```\r\n\r\n## If you use it, please cite\r\n\r\nTerra, R., Coelho, M., Cruz, L., Garcia-Zapata, M., Gadelha, L., Osthoff, C., ... & Ocana, K. (2021, July). Ger\u00eancia e An\u00e1lises de Workflows aplicados a Redes Filogen\u00e9ticas de Genomas de Dengue no Brasil. In *Anais do XV Brazilian e-Science Workshop* (pp. 49-56). SBC.\r\n\r\n**Also cite all the coupled software!**\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.703.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "703",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/703?version=1",
        "name": "HP2NET - Framework for Construction of Phylogenetic Networks on High Performance Computing (HPC) Environment",
        "number_of_steps": 0,
        "projects": [
            "HP2NET - Framework for construction of phylogenetic networks on High Performance Computing (HPC) environment"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "hpc",
            "parsl",
            "phylogenetics"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2024-01-18",
        "versions": 1
    },
    {
        "create_time": "2024-01-10",
        "creators": [
            "Michael Hall"
        ],
        "description": "# Pangenome databases provide superior host removal and mycobacteria classification from clinical metagenomic data\r\n\r\n> Hall, M, Coin, L., Pangenome databases provide superior host removal and mycobacteria classification from clinical metagenomic data. bioRxiv 2023. doi: [10.1101/2023.09.18.558339][doi]\r\n\r\nBenchmarking different ways of doing read (taxonomic) classification, with a focus on\r\nremoval of contamination and classification of _M. tuberculosis_ reads.\r\n\r\nThis repository contains the code and snakemake pipeline to build/download the\r\ndatabases, obtain all results from [the paper][doi], along with accompanying configuration\r\nfiles.\r\n\r\nCustom databases have all been uploaded to Zenodo, along with the simulated reads:\r\n\r\n- Nanopore simulated metagenomic reads - <https://doi.org/10.5281/zenodo.8339788>\r\n- Illumina simulated metagenomic reads - <https://doi.org/10.5281/zenodo.8339790>\r\n- Nanopore and Illumina artificial real reads - <https://doi.org/10.5281/zenodo.10472796>\r\n- Kraken2 database built from the Human Pangenome Reference Consortium\r\n  genomes - <https://doi.org/10.5281/zenodo.8339731>\r\n- Kraken2 database built from the kraken2 Human\r\n  library - <https://doi.org/10.5281/zenodo.8339699>\r\n- Kraken2 database built from a *Mycobacterium* representative set of\r\n  genomes - <https://doi.org/10.5281/zenodo.8339821>\r\n- A (fasta) database of representative genomes from the *Mycobacterium*\r\n  genus - <https://doi.org/10.5281/zenodo.8339940>\r\n- A (fasta) database of *M. tuberculosis* genomes from a variety of\r\n  lineages - <https://doi.org/10.5281/zenodo.8339947>\r\n- The fasta file built from the [Clockwork](https://github.com/iqbal-lab-org/clockwork)\r\n  decontamination pipeline - <https://doi.org/10.5281/zenodo.8339802>\r\n\r\n## Example usage\r\n\r\nWe provide some usage examples showing how to download the databases and then use them\r\non your reads.\r\n\r\n### Human read removal\r\n\r\nThe method we found to give the best balance of runtime, memory usage, and precision and\r\nrecall was kraken2 with a database built from the Human Pangenome Reference Consortium\r\ngenomes.\r\n\r\nThis example has been wrapped into a standalone tool called [`nohuman`](https://github.com/mbhall88/nohuman/) which takes a fastq as input and returns a fastq with human reads removed.\r\n\r\n#### Download human database\r\n\r\n```\r\nmkdir HPRC_db/\r\ncd HPRC_db\r\nURL=\"https://zenodo.org/record/8339732/files/k2_HPRC_20230810.tar.gz\"\r\nwget \"$URL\"\r\ntar -xzf k2_HPRC_20230810.tar.gz\r\nrm k2_HPRC_20230810.tar.gz\r\n```\r\n\r\n#### Run kraken2 with HPRC database\r\n\r\nYou'll need [kraken2](https://github.com/DerrickWood/kraken2) installed for this step.\r\n\r\n```\r\nkraken2 --threads 4 --db HPRC_db/ --output classifications.tsv reads.fq\r\n```\r\n\r\nIf you are using Illumina reads, a slight adjustment is needed\r\n\r\n```\r\nkraken2 --paired --threads 4 --db HPRC_db/ --output classifications.tsv reads_1.fq reads_2.fq\r\n```\r\n\r\n#### Extract non-human reads\r\n\r\nYou'll need [seqkit](https://github.com/shenwei356/seqkit) installed for this step\r\n\r\nFor Nanopore data\r\n\r\n```\r\nawk -F'\\t' '$1==\"U\" {print $2}' classifications.tsv | \\\r\n  seqkit grep -f - -o reads.depleted.fq reads.fq\r\n```\r\n\r\nFor Illumina data\r\n\r\n```\r\nawk -F'\\t' '$1==\"U\" {print $2}' classifications.tsv > ids.txt\r\nseqkit grep --id-regexp '^(\\S+)/[12]' -f ids.txt -o reads_1.depleted.fq reads_1.fq\r\nseqkit grep --id-regexp '^(\\S+)/[12]' -f ids.txt -o reads_2.depleted.fq reads_2.fq\r\n```\r\n\r\n### *M. tuberculosis* classification/enrichment\r\n\r\nFor this step we recommend either [minimap2](https://github.com/lh3/minimap2) or kraken2\r\nwith a *Mycobacterium* genus database. We leave it to the user to decide which approach\r\nthey prefer based on the results in our manuscript.\r\n\r\n#### Download databases\r\n\r\n```\r\nmkdir Mycobacterium_db\r\ncd Mycobacterium_db\r\n# download database for use with minimap2\r\nURL=\"https://zenodo.org/record/8339941/files/Mycobacterium.rep.fna.gz\"\r\nwget \"$URL\"\r\nIDS_URL=\"https://zenodo.org/record/8343322/files/mtb.ids\"\r\nwget \"$IDS_URL\"\r\n# download kraken database\r\nURL=\"https://zenodo.org/record/8339822/files/k2_Mycobacterium_20230817.tar.gz\"\r\nwget \"$URL\"\r\ntar -xzf k2_Mycobacterium_20230817.tar.gz\r\nrm k2_Mycobacterium_20230817.tar.gz\r\n```\r\n\r\n#### Classify reads\r\n\r\n**minimap2**\r\n\r\n```\r\n# nanopore\r\nminimap2 --secondary=no -c -t 4 -x map-ont -o reads.aln.paf Mycobacterium_db/Mycobacterium.rep.fna.gz reads.depleted.fq\r\n# illumina\r\nminimap2 --secondary=no -c -t 4 -x sr -o reads.aln.paf Mycobacterium_db/Mycobacterium.rep.fna.gz reads_1.depleted.fq reads_2.depleted.fq\r\n```\r\n\r\n**kraken2**\r\n\r\n```\r\n# nanopore\r\nkraken2 --db Mycobacterium_db --threads 4 --report myco.kreport --output classifications.myco.tsv reads.depleted.fq\r\n# illumina\r\nkraken2 --db Mycobacterium_db --paired --threads 4 --report myco.kreport --output classifications.myco.tsv reads_1.depleted.fq reads_2.depleted.fq\r\n```\r\n\r\n#### Extract *M. tuberculosis* reads\r\n\r\n**minimap2**\r\n\r\n```\r\n# nanopore\r\ngrep -Ff Mycobacterium_db/mtb.ids reads.aln.paf | cut -f1 | \\\r\n  seqkit grep -f - -o reads.enriched.fq reads.depleted.fq\r\n# illumina\r\ngrep -Ff Mycobacterium_db/mtb.ids reads.aln.paf | cut -f1 > keep.ids\r\nseqkit grep -f keep.ids -o reads_1.enriched.fq reads_1.depleted.fq\r\nseqkit grep -f keep.ids -o reads_2.enriched.fq reads_2.depleted.fq\r\n```\r\n\r\n**kraken2**\r\n\r\nWe'll use\r\nthe [`extract_kraken_reads.py` script](https://github.com/jenniferlu717/KrakenTools#extract_kraken_readspy)\r\nfor this\r\n\r\n```\r\n# nanopore\r\npython extract_kraken_reads.py -k classifications.myco.tsv -1 reads.depleted.fq -o reads.enriched.fq -t 1773 -r myco.kreport --include-children\r\n# illumina\r\npython extract_kraken_reads.py -k classifications.myco.tsv -1 reads_1.depleted.fq -2 reads_2.depleted.fq -o reads_1.enriched.fq -o2 reads_2.enriched.fq -t 1773 -r myco.kreport --include-children\r\n```\r\n\r\n[doi]: https://doi.org/10.1101/2023.09.18.558339 \r\n",
        "doi": "10.48546/workflowhub.workflow.700.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in name",
        "id": "700",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/700?version=2",
        "name": "Pangenome databases provide superior host removal and mycobacteria classification from clinical metagenomic data",
        "number_of_steps": 0,
        "projects": [
            "Pangenome database project"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-01-10",
        "versions": 2
    },
    {
        "create_time": "2024-01-08",
        "creators": [
            "Diego De Panis"
        ],
        "description": "The workflow takes a trimmed Illumina paired-end reads collection, runs Meryl to create a K-mer database, Genomescope2 to estimate genome properties and Smudgeplot to estimate ploidy. The main results are K-mer ddatabase and genome profiling plots, tables, and values useful for downstream analysis. Default K-mer length and ploidy for Genomescope are 21 and 2, respectively. ",
        "doi": null,
        "edam_operation": [
            "Sequencing quality control"
        ],
        "edam_topic": [
            "Whole genome sequencing"
        ],
        "filtered_on": "profil* in tags",
        "id": "698",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/698?version=1",
        "name": "ERGA Profiling Illumina v2311 (WF1)",
        "number_of_steps": 17,
        "projects": [
            "ERGA Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [
            "erga",
            "illumina",
            "name:profiling"
        ],
        "tools": [
            "Add_a_column1",
            "tp_grep_tool",
            "genomescope",
            "Convert characters1",
            "smudgeplot",
            "Cut1",
            "tp_cut_tool",
            "tp_find_and_replace",
            "param_value_from_file",
            "meryl"
        ],
        "type": "Galaxy",
        "update_time": "2024-01-08",
        "versions": 1
    },
    {
        "create_time": "2024-01-09",
        "creators": [],
        "description": "# BACPAGE\r\n\r\nThis repository contains an easy-to-use pipeline for the assembly and analysis of bacterial genomes using ONT long-read or Illumina short-read technology. \r\nRead the complete documentation and instructions for bacpage and each of its functions [here](https://cholgen.github.io/sequencing-resources/bacpage-command.html)\r\n\r\n# Introduction\r\nAdvances in sequencing technology during the COVID-19 pandemic has led to massive increases in the generation of sequencing data. Many bioinformatics tools have been developed to analyze this data, but very few tools can be utilized by individuals without prior bioinformatics training.\r\n\r\nThis pipeline was designed to encapsulate pre-existing tools to automate analysis of whole genome sequencing of bacteria. \r\nInstallation is fast and straightfoward. \r\nThe pipeline is easy to setup and contains rationale defaults, but is highly modular and configurable by more advance users.\r\nBacpage has individual commands to generate consensus sequences, perform *de novo* assembly, construct phylogenetic tree, and generate quality control reports.\r\n\r\n# Features\r\nWe anticipate the pipeline will be able to perform the following functions:\r\n- [x] Reference-based assembly of Illumina paired-end reads\r\n- [x] *De novo* assembly of Illumina paired-end reads\r\n- [ ] *De novo* assembly of ONT long reads\r\n- [x] Run quality control checks\r\n- [x] Variant calling using [bcftools](https://github.com/samtools/bcftools)\r\n- [x] Maximum-likelihood phylogenetic inference of processed samples and background dataset using [iqtree](https://github.com/iqtree/iqtree2) \r\n- [x] MLST profiling and virulence factor detection\r\n- [x] Antimicrobial resistance genes detection\r\n- [ ] Plasmid detection\r\n\r\n# Installation\r\n1. Install `mamba` by running the following two command:\r\n```commandline\r\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\r\nbash Mambaforge-$(uname)-$(uname -m).sh\r\n```\r\n\r\n2. Clone the bacpage repository:\r\n```commandline\r\ngit clone https://github.com/CholGen/bacpage.git\r\n```\r\n\r\n3. Switch to the development branch of the pipeline:\r\n```commandline\r\ncd bacpage/\r\ngit checkout -b split_into_command\r\n```\r\n\r\n3. Install and activate the pipeline's conda environment:\r\n```commandline\r\nmamba env create -f environment.yaml\r\nmamba activate bacpage\r\n```\r\n\r\n4. Install the `bacpage` command:\r\n```commandline\r\npip install .\r\n```\r\n\r\n5. Test the installation:\r\n```commandline\r\nbacpage -h\r\nbacpage version\r\n```\r\nThese command should print the help and version of the program. Please create an issue if this is not the case.\r\n\r\n# Updating\r\n\r\n1. Navigate to the directory where you cloned the bacpage repository on the command line:\r\n```commandline\r\ncd bacpage/\r\n```\r\n2. Activate the bacpage conda environment:\r\n```commandline\r\nmamba activate bacpage\r\n```\r\n3. Pull the lastest changes from GitHub:\r\n```commandline\r\ngit pull\r\n```\r\n4. Update the bacpage conda environemnt:\r\n```commandline\r\nmamba env update -f environment.yaml\r\n```\r\n5. Reinstall the `bacpage` command:\r\n```commandline\r\npip install .\r\n```\r\n\r\n# Usage\r\n0. Activate the bacpage conda environment:\r\n```commandline\r\nmamba activate bacpage\r\n```\r\n1. Create a directory specifically for the batch of samples you would like to analyze (called a project directory).\r\n```commandline\r\nbacpage setup [your-project-directory-name]\r\n```\r\n2. Place paired sequencing reads in the `input/` directory of your project directory.\r\n3. From the pipeline's directory, run the reference-based assembly pipeline on your samples using the following command:\r\n```commandline\r\nbacpage assemble [your-project-directory-name]\r\n```\r\nThis will generate a consensus sequence in FASTA format for each of your samples and place them in \r\n`<your-project-directory-name>/results/consensus_sequences/<sample>.masked.fasta`. An HTML report containing alignment \r\nand quality metrics for your samples can be found at `<your-project-directory-name>/results/reports/qc_report.html`.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "plasmid* in description",
        "id": "695",
        "keep": true,
        "latest_version": 2,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/695?version=2",
        "name": "Phylogeny reconstruction using bacpage",
        "number_of_steps": 0,
        "projects": [
            "CholGen"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Workflow Description Language",
        "update_time": "2024-01-09",
        "versions": 1
    },
    {
        "create_time": "2023-12-20",
        "creators": [],
        "description": "![bacpage](https://raw.githubusercontent.com/CholGen/bacpage/split_into_command/.github/logo_dark.png){width=500}\r\n\r\nThis repository contains an easy-to-use pipeline for the assembly and analysis of bacterial genomes using ONT long-read or Illumina short-read technology.\r\n\r\n# Introduction\r\nAdvances in sequencing technology during the COVID-19 pandemic has led to massive increases in the generation of sequencing data. Many bioinformatics tools have been developed to analyze this data, but very few tools can be utilized by individuals without prior bioinformatics training.\r\n\r\nThis pipeline was designed to encapsulate pre-existing tools to automate analysis of whole genome sequencing of bacteria. Installation is fast and straightfoward. The pipeline is easy to setup and contains rationale defaults, but is highly modular and configurable by more advance users.\r\nA successful run generates consensus sequences, typing information, phylogenetic tree, and quality control report.\r\n\r\n# Features\r\nWe anticipate the pipeline will be able to perform the following functions:\r\n- [x] Reference-based assembly of Illumina paired-end reads\r\n- [x] *De novo* assembly of Illumina paired-end reads\r\n- [ ] *De novo* assembly of ONT long reads\r\n- [x] Run quality control checks\r\n- [x] Variant calling using [bcftools](https://github.com/samtools/bcftools)\r\n- [x] Maximum-likelihood phylogenetic inference of processed samples and background dataset using [iqtree](https://github.com/iqtree/iqtree2) \r\n- [x] MLST profiling and virulence factor detection\r\n- [x] Antimicrobial resistance genes detection\r\n- [ ] Plasmid detection\r\n\r\n# Installation\r\n1. Install `miniconda` by running the following two command:\r\n```commandline\r\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\r\nbash Mambaforge-$(uname)-$(uname -m).sh\r\n```\r\n\r\n2. Clone the repository:\r\n```commandline\r\ngit clone https://github.com/CholGen/bacpage.git\r\n```\r\n\r\n3. Install and activate the pipeline's conda environment:\r\n```commandline\r\ncd bacpage/\r\nmamba env create -f environment.yaml\r\nmamba activate bacpage\r\n```\r\n\r\n4. Install the `bacpage` command:\r\n```commandline\r\npip install .\r\n```\r\n\r\n5. Test the installation:\r\n```commandline\r\nbacpage -h\r\nbacpage version\r\n```\r\nThese command should print the help and version of the program. Please create an issue if this is not the case.\r\n\r\n# Usage\r\n0. Navigate to the pipeline's directory.\r\n1. Copy the `example/` directory to create a directory specifically for each batch of samples.\r\n```commandline\r\ncp example/ <your-project-directory-name>\r\n```\r\n2. Place raw sequencing reads in the `input/` directory of your project directory.\r\n3. Record the name and absolute path of raw sequencing reads in the `sample_data.csv` found within your project directory.\r\n4. Replace the values `<your-project-directory-name>` and `<sequencing-directory>` in `config.yaml` found within your project directory, with the absolute path of your project directory and pipeline directory, respectively.\r\n5. Determine how many cores are available on your computer:\r\n```commandline\r\ncat /proc/cpuinfo | grep processor\r\n```\r\n6. From the pipeline's directory, run the entire pipeline on your samples using the following command:\r\n```commandline\r\nsnakemake --configfile <your-project-directory-name>/config.yaml --cores <cores>\r\n```\r\nThis will generate a consensus sequence in FASTA format for each of your samples and place them in `<your-project-directory-name>/results/consensus_sequences/<sample>.masked.fasta`. An HTML report containing alignment and quality metrics for your samples can be found at `<your-project-directory-name>/results/reports/qc_report.html`. A phylogeny comparing your sequences to the background dataset can be found at `<your-project-directory-name>/results/phylogeny/phylogeny.tree`\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "plasmid* in description",
        "id": "693",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/693?version=1",
        "name": "Reference-based assembly with bacpage",
        "number_of_steps": 0,
        "projects": [
            "CholGen"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Workflow Description Language",
        "update_time": "2023-12-20",
        "versions": 1
    },
    {
        "create_time": "2023-01-31",
        "creators": [
            "Georgina Samaha",
            "Marina Kennerson",
            "Tracy Chew",
            "Sarah Beecroft"
        ],
        "description": "\r\n\r\n\r\nGermlineStructuralV-nf is a pipeline for identifying structural variant events in human Illumina short read whole genome sequence data. GermlineStructuralV-nf identifies structural variant and copy number events from BAM files using [Manta](https://github.com/Illumina/manta/blob/master/docs/userGuide/README.md#de-novo-calling), [Smoove](https://github.com/brentp/smoove), and [TIDDIT](https://github.com/SciLifeLab/TIDDIT). Variants are then merged using [SURVIVOR](https://github.com/fritzsedlazeck/SURVIVOR), and annotated by [AnnotSV](https://pubmed.ncbi.nlm.nih.gov/29669011/). The pipeline is written in Nextflow and uses Singularity/Docker to run containerised tools.",
        "doi": "10.48546/workflowhub.workflow.431.1",
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "DNA mutation",
            "Genomics",
            "Structural genomics",
            "Whole genome sequencing"
        ],
        "filtered_on": "annot* in tags",
        "id": "431",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/431?version=1",
        "name": "GermlineStructuralV-nf",
        "number_of_steps": 0,
        "projects": [
            "Sydney Informatics Hub",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "bioinformatics",
            "genomics",
            "nextflow",
            "annotsv",
            "manta",
            "rare diseases",
            "smoove",
            "structural variants",
            "survivor",
            "tiddit",
            "variant_calling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-12-18",
        "versions": 1
    },
    {
        "create_time": "2023-12-15",
        "creators": [
            "Javier Conejero"
        ],
        "description": "**Name:** Word Count  \r\n**Contact Person**: support-compss@bsc.es  \r\n**Access Level**: public  \r\n**License Agreement**: Apache2  \r\n**Platform**: COMPSs  \r\n\r\n# Description\r\nWordcount is an application that counts the number of words for a given set of files.\r\n\r\nTo allow parallelism the file is divided in blocks that are treated separately and merged afterwards.\r\n\r\nResults are printed to a Pickle binary file, so they can be checked using: python -mpickle result.txt\r\n\r\nThis example also shows how to manually add input or output datasets to the workflow provenance recording (using the 'input' and 'output' terms in the ro-crate-info.yaml file).\r\n\r\n# Execution instructions\r\nUsage:\r\n```\r\nruncompss --lang=python $(pwd)/application_sources/src/wordcount_blocks.py filePath resultPath blockSize\r\n```\r\n\r\nwhere:\r\n* filePath: Absolute path of the file to parse\r\n* resultPath: Absolute path to the result file\r\n* blockSize: Size of each block. The lower the number, the more tasks will be generated in the workflow\r\n\r\n# Execution Examples\r\n```\r\nruncompss --lang=python $(pwd)/application_sources/src/wordcount_blocks.py $(pwd)/dataset/data/compss.txt result.txt 300\r\nruncompss $(pwd)/application_sources/src/wordcount_blocks.py $(pwd)/dataset/data/compss.txt result.txt 300\r\npython -m pycompss $(pwd)/application_sources/src/wordcount.py $(pwd)/dataset/data/compss.txt result.txt 300\r\n```\r\n\r\n# Build\r\nNo build is required\r\n",
        "doi": "10.48546/workflowhub.workflow.687.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "687",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/687?version=1",
        "name": "PyCOMPSs Wordcount test, dividing input file in blocks, only Python dictionaries used as task parameters (run at MareNostrum IV)",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [
            "example",
            "marenostrum iv",
            "pycompss",
            "supercomputer",
            "tutorial",
            "data_persistence"
        ],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2023-12-15",
        "versions": 1
    },
    {
        "create_time": "2023-12-14",
        "creators": [
            "Felix M\u00f6lder",
            "David L\u00e4hnemann",
            "Johannes K\u00f6ster"
        ],
        "description": "# Snakemake workflow: dna-seq-varlociraptor\r\n\r\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22656.3.0-brightgreen.svg)](https://snakemake.github.io)\r\n[![GitHub actions status](https://github.com/snakemake-workflows/dna-seq-varlociraptor/workflows/Tests/badge.svg?branch=master)](https://github.com/snakemake-workflows/dna-seq-varlociraptor/actions?query=branch%3Amaster+workflow%3ATests)\r\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4675661.svg)](https://doi.org/10.5281/zenodo.4675661)\r\n\r\n\r\nA Snakemake workflow for calling small and structural variants under any kind of scenario (tumor/normal, tumor/normal/relapse, germline, pedigree, populations) via the unified statistical model of [Varlociraptor](https://varlociraptor.github.io).\r\n\r\n\r\n## Usage\r\n\r\nThe usage of this workflow is described in the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog/?usage=snakemake-workflows%2Fdna-seq-varlociraptor).\r\n\r\nIf you use this workflow in a paper, don't forget to give credits to the authors by citing the URL of this (original) repository and its DOI (see above).\r\n",
        "doi": null,
        "edam_operation": [
            "Annotation",
            "Variant calling",
            "Variant classification",
            "Variant filtering"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Biomedical science",
            "Molecular genetics"
        ],
        "filtered_on": "ITS in description",
        "id": "686",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/686?version=2",
        "name": "dna-seq-varlociraptor",
        "number_of_steps": 0,
        "projects": [
            "Snakemake-Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics"
        ],
        "tools": [
            "varlociraptor",
            "BWA",
            "FreeBayes",
            "Delly2",
            "GATK",
            "Variant Effect Predictor (VEP)",
            "BCFtools"
        ],
        "type": "Snakemake",
        "update_time": "2023-12-14",
        "versions": 2
    },
    {
        "create_time": "2023-11-23",
        "creators": [
            "Phuong Doan"
        ],
        "description": "# ANNOTATO - Annotation workflow To Annotate Them Oll\r\n\r\n- [ANNOTATO - Annotation workflow To Annotate Them Oll](#annotato---annotation-workflow-to-annotate-them-oll)\r\n  - [Overview of the workflow](#overview-of-the-workflow)\r\n    - [Input data](#input-data)\r\n    - [Pipeline steps](#pipeline-steps)\r\n    - [Output data](#output-data)\r\n  - [Prerequisites](#prerequisites)\r\n  - [Installation](#installation)\r\n  - [Running ANNOTATO](#running-annotato)\r\n    - [Before running the pipeline (IMPORTANT)](#before-running-the-pipeline-important)\r\n    - [Without RNASeq and protein data](#without-rnaseq-and-protein-data)\r\n    - [Running ANNOTATO with RNASeq data](#running-annotato-with-rnaseq-data)\r\n    - [Running ANNOTATO with protein data](#running-annotato-with-protein-data)\r\n    - [Running ANNOTATO with both protein and RNASeq data](#running-annotato-with-both-protein-and-rnaseq-data)\r\n    - [Running ANNOTATO with params.json](#running-annotato-with-paramsjson)\r\n    - [Other parameters for running the analysis](#other-parameters-for-running-the-analysis)\r\n  - [Evaluating output GFF to the exon level](#evaluating-output-gff-to-the-exon-level)\r\n  - [Performance of the workflow on annotating difference eukaryote genomes](#performance-of-the-workflow-on-annotating-difference-eukaryote-genomes)\r\n  - [Future work](#future-work)\r\n\r\n## Overview of the workflow\r\n\r\nThe pipeline is based on `Funannotate` or `BRAKER` and was initially developed and tested on the two datasets:\r\n- Drosophila melanogaster: [https://doi.org/10.5281/zenodo.8013373](https://doi.org/10.5281/zenodo.8013373)\r\n- *Pocillopora* cf. *effusa*: [https://www.ncbi.nlm.nih.gov/biosample/26809107](https://www.ncbi.nlm.nih.gov/biosample/26809107)\r\n\r\nThen, it was further tested on these species during the [BioHackathon 2023 - project 20](https://github.com/elixir-europe/biohackathon-projects-2023/tree/main/20)\r\n\r\n- Helleia helle\r\n- Homo sapiens chrom 19\r\n- Melampus jaumei\r\n- Phakellia ventilabrum\r\n- Trifolium dubium\r\n\r\n### Input data\r\n\r\n- Reference genome `genome.[.fna, .fa, .fasta][.gz]`\r\n- RNAseq data listed in a metadata csv file. Input type can be mixed between long and short reads, with the option of single-end read. The input file should follow the format below:\r\n\r\n```\r\nsample_id,R1_path,R2_path,read_type\r\nSAM1,/path/to/R1,,long             # For long reads\r\nSAM2,/path/to/R1,/path/to/R2,short # For PE reads\r\nSAM3,/path/to/R1,,short            # For SE reads\r\n```\r\n\r\n- Protein sequence data in fasta format, could be gzip or not\r\n\r\n### Pipeline steps\r\n\r\n![Pipeline](./assets/images/annotato-workflow.drawio.svg)\r\n\r\nThe main pipeline is divided into five different subworkflows.\r\n- `Preprocess RNA` is where the input RNASeq data are QC and trimmed.\r\n- `Process RNA Minimap` is triggered when long reads FastQ are in the input CSV file.\r\n- `Process RNA STAR` will run when short reads FastQ are in the input CSV.\r\n- `Genome Masking` runs by default if not skipped. It assumes the input genome fasta is not masked and will run Denovo repeat masking with RepeatModeler and RepeatMasker.\r\n- `Filter Repeat` whenever there is a Denovo masking step, this sub-workflow will be triggered to remove the repeat sequences that appeared in the Uniprot Swissprot protein data. \r\n\r\n### Output data\r\n\r\n- MultiQC report for the RNASeq data, before and after trimming, mapping rate of short reads, and the BUSCO results of predicted genes.\r\n- RepeatMasker report containing quantity of masked sequence and distribution among TE families\r\n- Protein-coding gene annotation file in gff3 format\r\n- BUSCO summary of annotated sequences\r\n\r\n## Prerequisites\r\n\r\nThe following programs are required to run the workflow and the listed version were tested. \r\n\r\n`nextflow v23.04.0 or higher`\r\n\r\n`singularity`\r\n\r\n`conda` and `mamba` (currently, having problem with Funannotate and BRAKER installation)\r\n\r\n`docker` (have not been tested but in theory should work fine)\r\n\r\n## Installation\r\n\r\nSimply get the code from github or workflowhub and directly use it for the analysis with `nextflow`.\r\n\r\n```\r\ngit clone https://github.com/ERGA-consortium/pipelines/tree/main/annotation/nextflow\r\n```\r\n\r\n## Running ANNOTATO\r\n\r\n### Before running the pipeline (IMPORTANT)\r\n\r\nOne thing with Nextflow is that it is running off a Java Virtual Machine (JVM), and it will try to use all available memory for Nextflow even though it is unnecessary (for workflow management and job control). This will cause much trouble if you run a job on an HPC cluster. Thus, to minimize the effect of it, we need to limit the maximum memory the JVM can use.\r\n\r\n```\r\nexport NFX_OPTS=\"-Xms=512m -Xmx=3g\"\r\n```\r\n\r\n`-Xms` is the lower limit, which is set as 512 MB.\r\n`-Xmx` is the upper limit, which in this case is set as 3 GB.\r\nPlease modify this according to your situation.\r\n\r\n### Without RNASeq and protein data\r\n\r\nPerform the analysis with only the draft genome and busco database.\r\n\r\n```\r\nnextflow run main.nf --genome /path/to/genome.fasta --species \"Abc def\" --buscodb 'metazoa' \r\n```\r\n\r\nThe workflow will run Denovo repeat masking on the draft genome, then softmask the repeat region and use the genome to run `funannotate`. Add `--run_braker` to run the genome prediction using `BRAKER` instead.\r\n\r\n### Running ANNOTATO with RNASeq data\r\n\r\nWhen you want to let the workflow run the mapping by itself, uses `input.csv` as input with the link to all `FASTQ` file.\r\n\r\n```\r\nnextflow run main.nf --genome /path/to/genome.fasta[.gz] --rnaseq /path/to/input.csv --species \"Abc def\" --buscodb 'metazoa' \r\n```\r\n\r\nBased on the content of the `input.csv` file to trigger different RNASeq processing workflows. The output `bam` file will then be used for genome prediction.\r\n\r\nWhen reads are mapped to the reference genome, the aligned `bam` file can be used as input to the pipeline instead of the raw `FASTQ`\r\n\r\n```\r\nnextflow run main.nf --genome /path/to/genome.fasta[.gz] --short_rna_bam /path/to/shortreads.bam [--long_rna_bam /path/to/longreads.bam] --species \"Abc def\" --buscodb 'metazoa' \r\n```\r\n\r\n**ATTENTION**: One major drawback of the current workflow is that the input genome will be sorted and renamed by the `funannotate sort` function. This is because `AUGUSTUS` and `Funannotate` won't work normally when the header of the input genome is too long and contains weird characters. Therefore, if you want to provide a `bam` file as input instead of the raw `FASTQ`, please run `funannotate sort` on the genome fasta first and then use it as the reference for running alignment. Or in case your genome headers are already shorter than 16 character, please add `--skip_rename` when running the pipeline.\r\n\r\n### Running ANNOTATO with protein data\r\n\r\n```\r\nnextflow run main.nf --genome /path/to/genome.fasta[.gz] --protein /path/to/protein.fasta[.gz] --species \"Abc def\" --buscodb 'metazoa' \r\n```\r\n\r\nWhen only protein data is provided, the workflow will run denovo masking then repeat filter with the additional protein data. The masked genome and protein fasta will then be used for gene prediction.\r\n\r\n### Running ANNOTATO with both protein and RNASeq data\r\n\r\nThe full pipeline is triggered when both RNASeq data and protein fasta is provided.\r\n\r\n```\r\nnextflow run main.nf --genome /path/to/genome.fasta[.gz] --protein /path/to/protein.fasta[.gz] --rnaseq /path/to/input.csv --species \"Abc def\" --buscodb 'metazoa' \r\n```\r\n\r\n### Running ANNOTATO with params.json\r\n\r\nOne plus side with Nextflow is that it can use a parameter JSON file called `params.json` to start the analysis pipeline with all required parameters. Please modify the content of the `params.json` according to your need then run the following command.\r\n\r\n```\r\nnextflow run main.nf -params-file params.json\r\n```\r\n\r\n### Other parameters for running the analysis\r\n\r\n```\r\nCompulsory input:\r\n--genome                       Draft genome fasta file contain the assembled contigs/scaffolds\r\n--species                      Species name for the annotation pipeline, e.g. \"Drosophila melanogaster\"\r\n\r\nOptional input:\r\n--protein                      Fasta file containing known protein sequences used as an additional information for gene prediction pipeline.\r\n                               Ideally this should come from the same species and/or closely related species. [default: null]\r\n--rnaseq                       A CSV file following the pattern: sample_id,R1_path,R2_path,read_type.\r\n                               This could be generated using gen_input.py. Run `python gen_input.py --help` for more information. \r\n                               [default: null]\r\n--long_rna_bam                 A BAM file for the alignment of long reads (if any) to the draft genome. Noted that the header of the draft\r\n                               genome need to be renamed first before alignment otherwise it will causes trouble for AUGUSTUS and funannotate. \r\n                               [default: null]\r\n--short_rna_bam                A BAM file for the alignment of short reads (if any) to the draft genome. Noted that the header of the draft \r\n                               genome need to be renamed first before alignment otherwise it will causes trouble for AUGUSTUS and funannotate. \r\n                               [default: null]\r\n--knownrepeat                  Fasta file containing known repeat sequences of the species, this will be used directly for masking \r\n                               (if --skip_denovo_masking) or in combination with the denovo masking. [default: null]\r\n\r\nOutput option:\r\n--outdir                       Output directory. \r\n--tracedir                     Pipeline information. \r\n--publish_dir_mode             Option for nextflow to move data to the output directory. [default: copy]\r\n--tmpdir                       Database directory. \r\n\r\nFunannotate params:\r\n--run_funannotate              Whether to use funannotate for gene prediction. [default: true]\r\n--organism                     Fungal-specific option. Should be change to \"fungus\" if the annotated organism is fungal. [default: other]\r\n--ploidy                       Set the ploidy for gene prediction, in case of haploid, a cleaning step will be performed by funannotate to remove\r\n                               duplicated contigs/scaffold. [default: 2]\r\n--buscodb                      BUSCO database used for AUGUSTUS training and evaluation. [default: eukaryota]\r\n--buscoseed                    AUGUSTUS pre-trained species to start BUSCO. Will be override if rnaseq data is provided. [default: null]\r\n\r\nBraker params:\r\n--run_braker                   Whether to use BRAKER for gene prediction. [default: false]\r\n\r\nSkipping options:\r\n--skip_rename                  Skip renaming genome fasta file by funannotate sort. \r\n--skip_all_masking             Skip all masking processes, please be sure that your --genome input is soft-masked before triggering this \r\n                               parameter. [default: false]\r\n--skip_denovo_masking          Skip denovo masking using RepeatModeler, this option can only be run when --knownrepeat fasta is provided. \r\n                               [default: false]\r\n--skip_functional_annotation   Skip functional annotation step. [default: false]\r\n--skip_read_preprocessing      Skip RNASeq preprocessing step. [default: false]\r\n\r\nExecution/Engine profiles:\r\nThe pipeline supports profiles to run via different Executers and Engines e.g.: -profile local,conda\r\n\r\nExecuter (choose one):\r\n  local\r\n  slurm\r\n\r\nEngines (choose one):\r\n  conda\r\n  mamba\r\n  docker\r\n  singularity\r\n\r\nPer default: -profile slurm,singularity is executed.\r\n```\r\n\r\n## Evaluating output GFF to the exon level\r\n\r\nWe provided a script to analyze the output GFF of ANNOTATO (which also could be applied to the GFF file output of other pipelines) to report the number of exons per mRNA/tRNA. To run this, simply use:\r\n\r\n```\r\npython bin/analyze_exons.py -f ${GFF}\r\n```\r\n\r\nBelow is the sample output of this script\r\n\r\n```\r\nINFORMATION REGARDING mRNA\r\nNumber of transcripts: 33086\r\nLargest number of exons in all transcripts: 128\r\nMonoexonic transcripts: 4085\r\nMultiexonic transcripts: 29001\r\nMono:Mult Ratio: 0.14\r\nBoxplot of number of exons per transcript:\r\nMin: 1\r\n25%: 2\r\n50%: 4\r\n75%: 8\r\nMax: 128\r\nMean: 6.978812790908542\r\n==================================================\r\nINFORMATION REGARDING tRNA\r\nNumber of transcripts: 2017\r\nLargest number of exons in all transcripts: 1\r\nMonoexonic transcripts: 2017\r\nMultiexonic transcripts: 0\r\nNo multiexonic transcripts, unable to calculate Mono:Mult Ratio\r\nBoxplot of number of exons per transcript:\r\nMin: 1\r\n25%: 1\r\n50%: 1\r\n75%: 1\r\nMax: 1\r\nMean: 1.0\r\n==================================================\r\n```\r\n\r\nThis script was originally written by [Katharina Hoff](https://github.com/Gaius-Augustus/GALBA/blob/main/scripts/analyze_exons.py) and was modified accordingly to suit the analysis of GFF file.\r\n\r\n## Performance of the workflow on annotating difference eukaryote genomes\r\n\r\nThe following table is the result predicted by ANNOTATO on difference species during the [Europe BioHackathon 2023](https://github.com/elixir-europe/biohackathon-projects-2023/tree/main/20).\r\n\r\n| Species                    | Genome size | N.Genes | N.Exons | N.mRNA | BUSCO lineage | BUSCO score                             | OMArk Completeness                                                 | OMArk Consistency                                                                       |\r\n| :---:                      | :---:       | :---:   | :---:   | :---:  | :---:         | :---:                                   | :---:                                                              | :---:                                                                                   |\r\n| Drosophila melanogaster    | 143M        | 14,753  | 57,343  | 14,499 | diptera       | C:96.1%[S:95.6%,D:0.5%],F:1.2%,M:2.7%   | melanogaster subgroup, C:90.38%[S:84.32%,D:6.06%],M:9.62%,,n:12442 | A:94.21%[P:4.05%,F:7.28%],I:1.61%[P:0.5%,F:0.42%],C:0.00%[P:0.00%,F:0.00%],U:4.19%      |\r\n| Helleia helle              | 547M        | 37,367  | 139,302 | 28,445 | lepidoptera   | C:74.6%[S:73.4%,D:1.2%],F:5.4%,M:20.0%  | Papilionidea, C:82.04%[S:66.12%,D:15.92%],M:17.96%, n:7939         | A:44.78%[P:14.41%,F:6.02%],I:3.53%[P:2.1%,F:0.7%],C:0.00%[P:0.00%,F:0.00%],U:51.69%     |\r\n| Homo sapiens chrom 19      | 58M         | 1,872   | 11,937  | 1,862  | primates      | C:5.0%[S:4.8%,D:0.2%],F:0.5%,M:94.5%    | Hominidae, C:8.57%[S:7.74%,D:0.83%],M:91.43%, n=17843              | A:87.54%[P:12.73%,F:13.1%],I:4.78%[P:1.5%,F:2.04%],C:0.00%[P:0.00%,F:0.00%],U:7.68%     |\r\n| Melampus jaumei            | 958M        | 61,128  | 335,483 | 60,720 | mollusca      | C:80.4%[S:67.2%,D:13.2%],F:3.8%,M:15.8% | Lophotrochozoa, C: 92.5%[S: 66.29%, D: 26.21%], M:7.5%, n:2373     | A:41.45%[P:15.72%,F:9.97%],I:15.97%[P:10.68%,F:3.07%],C:0.00%[P:0.00%,F:0.00%],U:42.57% |\r\n| Phakellia ventilabrum      | 186M        | 19,073  | 157,441 | 18,855 | metazoa       | C:80.9%[S:79.2%,D:1.7%],F:6.5%,M:12.6%  | Metazoa, C:86.79%[S:76.9%,D:9.9%],M:13.21% , n:3021                | A:53.81%[P:18.92%,F:5.06%],I:5.0%[P:2.7%,F:0.68%],C:0.00%[P:0.00%,F:0.00%],U:41.19%     |\r\n| *Pocillopora* cf. *effusa* | 347M        | 35,103  | 230,901 | 33,086 | metazoa       | C:95.1%[S:92.2%,D:2.9%],F:1.7%,M:3.2%   | Eumetazoa, C:94.16%[S:84.3%,D:9.86%],M:5.84%,n:3255                | A:52.94%[P:22.30%,F:3.69%],I:3.44%[P:2.08%,F:0.28%],C:0.00%[P:0.00%,F:0.00%],U:43.62%   |\r\n| Trifolium dubium           | 679M        | 78,810  | 354,662 | 77,763 | fabales       | C:95.1%[S:19.5%,D:75.6%],F:1.5%,M:3.4%  | NPAAA clade, C:94.58%[S:19.21%,D:75.38%],M:5.42%,n:15412           | A:71.99%[P:11.03%,F:6.63%],I:2.77%[P:1.66%,F:0.52%],C:0.00%[P:0.00%,F:0.00%],U:25.23%   |\r\n\r\n## Future work\r\n- Python wrapper function to remove intermediate files\r\n- Adding functional annotation with `Interproscan` and `eggnog`\r\n- Adding PASA results to further improve the accuracy of the training\r\n- Adding custom parameter for both `BRAKER` and `funannotate`",
        "doi": "10.48546/workflowhub.workflow.654.2",
        "edam_operation": [],
        "edam_topic": [
            "Gene structure",
            "Gene transcripts",
            "Genomics"
        ],
        "filtered_on": "annot* in tags",
        "id": "654",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/654?version=2",
        "name": "ANNOTATO - ERGA Genome Annotation Workflow in Nextflow",
        "number_of_steps": 0,
        "projects": [
            "ERGA Annotation",
            "Bioinformatics Laboratory for Genomics and Biodiversity (LBGB)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "bge",
            "biodiversity",
            "bioinformatics",
            "de_novo",
            "erga",
            "genomics",
            "nextflow",
            "transcriptomics",
            "workflows",
            "rna-seq"
        ],
        "tools": [
            "funannotate",
            "BRAKER1",
            "Minimap2",
            "STAR",
            "RepeatMasker",
            "RepeatModeler",
            "BLAST",
            "StringTie",
            "FastQC",
            "fastp",
            "BUSCO",
            "MultiQC",
            "protexcluder"
        ],
        "type": "Nextflow",
        "update_time": "2023-11-24",
        "versions": 2
    },
    {
        "create_time": "2023-11-21",
        "creators": [
            "Zafran Hussain Shah"
        ],
        "description": "# Evaluation of Swin Transformer and knowledge transfer for denoising of super-resolution structured illumination microscopy data\r\n\r\nIn recent years, convolutional neural network (CNN)-based methods have shown remarkable performance in the denoising and reconstruction of super-resolved structured illumination microscopy (SR-SIM) data. Therefore, CNN-based architectures have been the main focus of existing studies. Recently, however, an alternative and highly\r\ncompetitive deep learning architecture, Swin Transformer, has been proposed for image restoration tasks. In this work, we present SwinT-fairSIM, a novel method for restoring SR-SIM images with low signal-to-noise ratio (SNR) based on Swin Transformer. The experimental results show that SwinT-fairSIM outperforms previous CNN-based denoising methods. Furthermore, the generalization capabilities of deep learning methods for image restoration tasks on real fluorescence microscopy data have not been fully explored yet, i.e., the extent to which trained artificial neural networks are limited to specific types of cell structures and noise. Therefore, as a second contribution, we benchmark two types of transfer learning, i.e., direct transfer and fine-tuning, in combination with SwinT-fairSIM and two CNN-based methods for denoising SR-SIM data. Direct transfer does not prove to be a viable strategy, but fine-tuning achieves results comparable to conventional training from scratch while saving computational time and potentially reducing the amount of required training data. As a third contribution, we published four datasets of raw SIM images and already reconstructed SR-SIM images. These datasets cover two types of cell structures, tubulin filaments and vesicle structures. Different noise levels are available for the tubulin filaments. These datasets are structured in such a way that they can be easily used by the research community for research on denoising, super-resolution, and transfer learning strategies.\r\n\r\nThe SIM microscopy datasets that were used during this work can be downloaded through this link: http://dx.doi.org/10.5524/102461\r\n\r\n\r\n## Installation:\r\n\r\nThis implementation requires the Tensorflow-GPU2.5 version. To avoid package conflicts, we recommend you create a new environment by using our provided environment.yml file. To create a new environment please run the following script:\r\n\r\n>  conda env create -f environment.yml\r\n\r\n## How to use this code:\r\n\r\nThis code can be used to train a denoising model from scratch or to fine-tune a pretrained model. After the installation of the Python environment from the yml file, the next step is to set the input parameters in the JSON parameter file (i.e., ParameterFile.json). Most of the input parameters are self-explanatory but below we will discuss some of the important input parameters from the JSON file:\r\n\r\n- TrainNetworkfromScratch: This input parameter will train the model from scratch If set to True, otherwise, for fine-tuning, It should be False.\r\n- ActivateTrainandTestModel: This parameter will be set to False If you want to use this code for evaluation of the trained model or the reproducibility of the results by using pretrained models.\r\n- PretrainedmodelPath: This parameter is mandatory in case of fine-tuning or evaluation of a pretrained model.\r\n- FineTuneStartingpoint and FineTuneEndingpoint: These two input parameters are essential in the fine-tuning of a pretrained model. All the layers between the starting and ending points will be frozen during the fine-tuning of the pretrained model.\r\n\r\nAfter the assignment of the input parameters. You can run the following script from the command line to start training the model:\r\n\r\n> python MainModule.py 'ParameterFile.json'\r\n\r\n## Reproducibility and evaluation:\r\n\r\nTo reproduce the results of the paper all the trained models used in this work are available in the 'Models' directory at [zenodo](https://doi.org/10.5281/zenodo.7626173). This code is capable of performing all the necessary steps for the training and test phases. It will automatically evaluate the model and generate a result directory to write all the results. Similarly, during the training process, It will also create a model directory and save the trained model along with the best checkpoints in the model directory.   \r\n\r\n## Important Note:\r\n\r\nThis code will work with at least one GPU.\r\n\r\n## Reference:\r\n\r\nPlease cite our paper in case you use this code for any scientific publication. We will soon upload the citation index!\r\n\r\n\r\n\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.675.1",
        "edam_operation": [],
        "edam_topic": [
            "Biomedical science"
        ],
        "filtered_on": "binn* in description",
        "id": "675",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/675?version=1",
        "name": "Evaluation of Swin Transformer and knowledge transfer for denoising of super-resolution structured illumination microscopy data",
        "number_of_steps": 0,
        "projects": [
            "Evaluation of Swin Transformer and knowledge transfer for denoising of super-resolution structured illumination microscopy data"
        ],
        "source": "WorkflowHub",
        "tags": [
            "deep learning",
            "machine learning",
            "python",
            "sim",
            "image processing",
            "microscopy"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2023-11-21",
        "versions": 1
    },
    {
        "create_time": "2023-11-16",
        "creators": [
            "Javier Conejero"
        ],
        "description": "**Contact Person:** support-compss@bsc.es  \r\n**Access Level:** public  \r\n**License Agreement:** Apache2  \r\n**Platform:** COMPSs  \r\n\r\n# Description\r\n\r\nSimple is an application that takes one value and increases it by five units. The purpose of this application is to show how tasks are managed by COMPSs.\r\n\r\n# Execution instructions\r\nUsage:\r\n```\r\nruncompss --lang=python src/simple.py initValue\r\n```\r\n\r\nwhere:\r\n* initValue: Initial value for counter\r\n\r\n# Execution Examples\r\n```\r\nruncompss --lang=python src/simple.py 1\r\nruncompss src/simple.py 1\r\npython -m pycompss src/simple.py 1\r\n```\r\n\r\n# Build\r\nNo build is required\r\n",
        "doi": "10.48546/workflowhub.workflow.673.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "673",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/673?version=1",
        "name": "PyCOMPSs simple example (ran on macOS laptop, input generated by the code, INOUT file example)",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [
            "example",
            "laptop",
            "pycompss",
            "tutorial",
            "data_persistence"
        ],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2023-11-16",
        "versions": 1
    },
    {
        "create_time": "2023-10-26",
        "creators": [
            "Valentine Murigneux",
            "Mike Thang",
            "Saskia Hiltemann",
            "B\u00e9r\u00e9nice Batut"
        ],
        "description": "The aim of this workflow is to handle the routine part of shotgun metagenomics data processing on Galaxy Australia. \r\n\r\nThe workflow is using the tools MetaPhlAn2 for taxonomy classification and HUMAnN2 for functional profiling of the metagenomes. The workflow is based on the Galaxy Training tutorial 'Analyses of metagenomics data - The global picture' (Saskia Hiltemann, B\u00e9r\u00e9nice Batut) https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/general-tutorial/tutorial.html#shotgun-metagenomics-data. \r\n\r\nThe how-to guide is available here: https://vmurigneu.github.io/shotgun_howto_ga_workflows/\r\n",
        "doi": "10.48546/workflowhub.workflow.624.1",
        "edam_operation": [
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metagenomic sequencing",
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "624",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/624?version=1",
        "name": "Analyses of shotgun metagenomics data with MetaPhlAn2",
        "number_of_steps": 17,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gucfg2galaxy",
            "metagenomics",
            "shotgun"
        ],
        "tools": [
            "",
            "metaphlan2",
            "humann2_regroup_table",
            "Cut1",
            "merge_metaphlan_tables",
            "taxonomy_krona_chart",
            "humann2",
            "metaphlan2krona",
            "humann2_renorm_table"
        ],
        "type": "Galaxy",
        "update_time": "2024-04-05",
        "versions": 1
    },
    {
        "create_time": "2023-11-14",
        "creators": [
            "Damon-Lee Pointon",
            "Will Eagles",
            "Ying Sims",
            "Matthieu Muffato",
            "Priyanka Surana"
        ],
        "description": "[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.10.1-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/sanger-tol/treeval)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/treeval** is a bioinformatics best-practice analysis pipeline for the generation of data supplemental to the curation of reference quality genomes. This pipeline has been written to generate flat files compatible with [JBrowse2](https://jbrowse.org/jb2/).\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nThe treeval pipeline has a sister pipeline currently named [curationpretext](https://github.com/sanger-tol/curationpretext) which acts to regenerate the pretext maps and accessory files during genomic curation in order to confirm interventions. This pipeline is sufficiently different to the treeval implementation that it is written as it's own pipeline.\r\n\r\n1. Parse input yaml ( YAML_INPUT )\r\n2. Generate my.genome file ( GENERATE_GENOME )\r\n3. Generate insilico digests of the input assembly ( INSILICO_DIGEST )\r\n4. Generate gene alignments with high quality data against the input assembly ( GENE_ALIGNMENT )\r\n5. Generate a repeat density graph ( REPEAT_DENSITY )\r\n6. Generate a gap track ( GAP_FINDER )\r\n7. Generate a map of self complementary sequence ( SELFCOMP )\r\n8. Generate syntenic alignments with a closely related high quality assembly ( SYNTENY )\r\n9. Generate a coverage track using PacBio data ( LONGREAD_COVERAGE )\r\n10. Generate HiC maps, pretext and higlass using HiC cram files ( HIC_MAPPING )\r\n11. Generate a telomere track based on input motif ( TELO_FINDER )\r\n12. Run Busco and convert results into bed format ( BUSCO_ANNOTATION )\r\n13. Ancestral Busco linkage if available for clade ( BUSCO_ANNOTATION:ANCESTRAL_GENE )\r\n\r\n## Usage\r\n\r\n> **Note**\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how\r\n> to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline)\r\n> with `-profile test` before running the workflow on actual data.\r\n\r\nCurrently, it is advised to run the pipeline with docker or singularity as a small number of major modules do not currently have a conda env associated with them.\r\n\r\nNow, you can run the pipeline using:\r\n\r\n```bash\r\n# For the FULL pipeline\r\nnextflow run main.nf -profile singularity --input treeval.yaml --outdir {OUTDIR}\r\n\r\n# For the RAPID subset\r\nnextflow run main.nf -profile singularity --input treeval.yaml -entry RAPID --outdir {OUTDIR}\r\n```\r\n\r\nAn example treeval.yaml can be found [here](assets/local_testing/nxOscDF5033.yaml).\r\n\r\nFurther documentation about the pipeline can be found in the following files: [usage](https://pipelines.tol.sanger.ac.uk/treeval/dev/usage), [parameters](https://pipelines.tol.sanger.ac.uk/treeval/dev/parameters) and [output](https://pipelines.tol.sanger.ac.uk/treeval/dev/output).\r\n\r\n> **Warning:**\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\r\n> provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\r\n\r\n## Credits\r\n\r\nsanger-tol/treeval has been written by Damon-Lee Pointon (@DLBPointon), Yumi Sims (@yumisims) and William Eagles (@weaglesBio).\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n<ul>\r\n  <li>@gq1 - For building the infrastructure around TreeVal and helping with code review</li>\r\n  <li>@ksenia-krasheninnikova - For help with C code implementation and YAML parsing</li>\r\n  <li>@mcshane - For guidance on algorithms </li>\r\n  <li>@muffato - For code reviews and code support</li>\r\n  <li>@priyanka-surana - For help with the majority of code reviews and code support</li>\r\n</ul>\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\n## Citations\r\n\r\n<!--TODO: Citation-->\r\n\r\nIf you use sanger-tol/treeval for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX).\r\n\r\n### Tools\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "668",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/668?version=1",
        "name": "sanger-tol/treeval v1.0 - Ancient Atlantis",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-14",
        "versions": 1
    },
    {
        "create_time": "2023-11-14",
        "creators": [
            "Matthieu Muffato",
            "Priyanka Surana"
        ],
        "description": "# ![sanger-tol/ensemblrepeatdownload](docs/images/sanger-tol-ensemblrepeatdownload_logo.png)\r\n\r\n[![GitHub Actions CI Status](https://github.com/sanger-tol/ensemblrepeatdownload/workflows/nf-core%20CI/badge.svg)](https://github.com/sanger-tol/ensemblrepeatdownload/actions?query=workflow%3A%22nf-core+CI%22)\r\n\r\n<!-- [![GitHub Actions Linting Status](https://github.com/sanger-tol/ensemblrepeatdownload/workflows/nf-core%20linting/badge.svg)](https://github.com/sanger-tol/ensemblrepeatdownload/actions?query=workflow%3A%22nf-core+linting%22) -->\r\n\r\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7183380-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7183380)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n\r\n[![Get help on Slack](http://img.shields.io/badge/slack-SangerTreeofLife%20%23pipelines-4A154B?labelColor=000000&logo=slack)](https://SangerTreeofLife.slack.com/channels/pipelines)\r\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40sangertol-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/sangertol)\r\n[![Watch on YouTube](http://img.shields.io/badge/youtube-tree--of--life-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/channel/UCFeDpvjU58SA9V0ycRXejhA)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/ensemblrepeatdownload** is a pipeline that downloads repeat annotations from Ensembl into a Tree of Life directory structure.\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the GitHub CI infrastructure. This ensures that the pipeline runs in a third-party environment, and has sensible resource allocation defaults set to run on real-world datasets.\r\n\r\n## Pipeline summary\r\n\r\n## Overview\r\n\r\nThe pipeline takes a CSV file that contains assembly accession number, Ensembl species names (as they may differ from Tree of Life ones !), output directories.\r\nAssembly accession numbers are optional too. If missing, the pipeline assumes it can be retrieved from files named `ACCESSION` in the standard location on disk.\r\nThe pipeline downloads the repeat annotation as the masked Fasta file and a BED file.\r\nAll files are compressed with `bgzip`, and indexed with `samtools faidx` or `tabix`.\r\n\r\nSteps involved:\r\n\r\n- Download the masked fasta file from Ensembl.\r\n- Extract the coordinates of the masked regions into a BED file.\r\n- Compress and index the BED file with `bgzip` and `tabix`.\r\n\r\n## Quick Start\r\n\r\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=22.04.0`)\r\n\r\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/) (you can follow [this tutorial](https://singularity-tutorial.github.io/01-installation/)), [`Podman`](https://podman.io/), [`Shifter`](https://nersc.gitlab.io/development/shifter/how-to-use/) or [`Charliecloud`](https://hpc.github.io/charliecloud/) for full pipeline reproducibility _(you can use [`Conda`](https://conda.io/miniconda.html) both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_.\r\n\r\n3. Download the pipeline and test it on a minimal dataset with a single command:\r\n\r\n   ```bash\r\n   nextflow run sanger-tol/ensemblrepeatdownload -profile test,YOURPROFILE --outdir <OUTDIR>\r\n   ```\r\n\r\n   Note that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (`YOURPROFILE` in the example command above). You can chain multiple config profiles in a comma-separated string.\r\n\r\n   > - The pipeline comes with config profiles called `docker`, `singularity`, `podman`, `shifter`, `charliecloud` and `conda` which instruct the pipeline to use the named tool for software management. For example, `-profile test,docker`.\r\n   > - Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\r\n   > - If you are using `singularity`, please use the [`nf-core download`](https://nf-co.re/tools/#downloading-pipelines-for-offline-use) command to download images first, before running the pipeline. Setting the [`NXF_SINGULARITY_CACHEDIR` or `singularity.cacheDir`](https://www.nextflow.io/docs/latest/singularity.html?#singularity-docker-hub) Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\r\n   > - If you are using `conda`, it is highly recommended to use the [`NXF_CONDA_CACHEDIR` or `conda.cacheDir`](https://www.nextflow.io/docs/latest/conda.html) settings to store the environments in a central location for future pipeline runs.\r\n\r\n4. Start running your own analysis!\r\n\r\n   ```console\r\n   nextflow run sanger-tol/ensemblrepeatdownload --input $PWD/assets/samplesheet.csv --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\r\n   ```\r\n\r\n## Documentation\r\n\r\nThe sanger-tol/ensemblrepeatdownload pipeline comes with documentation about the pipeline [usage](docs/usage.md) and [output](docs/output.md).\r\n\r\n## Credits\r\n\r\nsanger-tol/ensemblrepeatdownload was originally written by @muffato.\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#pipelines` channel](https://sangertreeoflife.slack.com/channels/pipelines). Please [create an issue](https://github.com/sanger-tol/ensemblrepeatdownload/issues/new/choose) on GitHub if you are not on the Sanger slack channel.\r\n\r\n## Citations\r\n\r\nIf you use sanger-tol/ensemblrepeatdownload for your analysis, please cite it using the following doi: [10.5281/zenodo.7183380](https://doi.org/10.5281/zenodo.7183380)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "667",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/667?version=1",
        "name": "sanger-tol/ensemblrepeatdownload v1.0.0 - Gwaihir the Windlord",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-14",
        "versions": 1
    },
    {
        "create_time": "2023-11-14",
        "creators": [
            "Matthieu Muffato",
            "Priyanka Surana"
        ],
        "description": "# ![sanger-tol/ensemblgenedownload](docs/images/sanger-tol-ensemblgenedownload_logo.png)\r\n\r\n[![GitHub Actions CI Status](https://github.com/sanger-tol/ensemblgenedownload/workflows/nf-core%20CI/badge.svg)](https://github.com/sanger-tol/ensemblgenedownload/actions?query=workflow%3A%22nf-core+CI%22)\r\n\r\n<!-- [![GitHub Actions Linting Status](https://github.com/sanger-tol/ensemblgenedownload/workflows/nf-core%20linting/badge.svg)](https://github.com/sanger-tol/ensemblgenedownload/actions?query=workflow%3A%22nf-core+linting%22) -->\r\n\r\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7183206-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7183206)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n\r\n[![Get help on Slack](http://img.shields.io/badge/slack-SangerTreeofLife%20%23pipelines-4A154B?labelColor=000000&logo=slack)](https://SangerTreeofLife.slack.com/channels/pipelines)\r\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40sangertol-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/sangertol)\r\n[![Watch on YouTube](http://img.shields.io/badge/youtube-tree--of--life-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/channel/UCFeDpvjU58SA9V0ycRXejhA)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/ensemblgenedownload** is a pipeline that downloads gene annotations from Ensembl into the Tree of Life directory structure.\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the GitHub CI infrastructure. This ensures that the pipeline runs in a third-party environment, and has sensible resource allocation defaults set to run on real-world datasets.\r\n\r\n## Pipeline summary\r\n\r\n## Overview\r\n\r\nThe pipeline takes a CSV file that contains assembly accession number, Ensembl species names (as they may differ from Tree of Life ones !), output directories, and geneset versions.\r\nAssembly accession numbers are optional. If missing, the pipeline assumes it can be retrieved from files named `ACCESSION` in the standard location on disk.\r\nThe pipeline downloads the Fasta files of the genes (cdna, cds, and protein sequences) as well as the GFF3 file.\r\nAll files are compressed with `bgzip`, and indexed with `samtools faidx` or `tabix`.\r\n\r\nSteps involved:\r\n\r\n- Download from Ensembl the GFF3 file, and the sequences of the genes in\r\n  Fasta format.\r\n- Compress and index all Fasta files with `bgzip`, `samtools faidx`, and\r\n  `samtools dict`.\r\n- Compress and index the GFF3 file with `bgzip` and `tabix`.\r\n\r\n## Quick Start\r\n\r\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=22.04.0`)\r\n\r\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/) (you can follow [this tutorial](https://singularity-tutorial.github.io/01-installation/)), [`Podman`](https://podman.io/), [`Shifter`](https://nersc.gitlab.io/development/shifter/how-to-use/) or [`Charliecloud`](https://hpc.github.io/charliecloud/) for full pipeline reproducibility _(you can use [`Conda`](https://conda.io/miniconda.html) both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_.\r\n\r\n3. Download the pipeline and test it on a minimal dataset with a single command:\r\n\r\n   ```bash\r\n   nextflow run sanger-tol/ensemblgenedownload -profile test,YOURPROFILE --outdir <OUTDIR>\r\n   ```\r\n\r\n   Note that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (`YOURPROFILE` in the example command above). You can chain multiple config profiles in a comma-separated string.\r\n\r\n   > - The pipeline comes with config profiles called `docker`, `singularity`, `podman`, `shifter`, `charliecloud` and `conda` which instruct the pipeline to use the named tool for software management. For example, `-profile test,docker`.\r\n   > - Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\r\n   > - If you are using `singularity`, please use the [`nf-core download`](https://nf-co.re/tools/#downloading-pipelines-for-offline-use) command to download images first, before running the pipeline. Setting the [`NXF_SINGULARITY_CACHEDIR` or `singularity.cacheDir`](https://www.nextflow.io/docs/latest/singularity.html?#singularity-docker-hub) Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\r\n   > - If you are using `conda`, it is highly recommended to use the [`NXF_CONDA_CACHEDIR` or `conda.cacheDir`](https://www.nextflow.io/docs/latest/conda.html) settings to store the environments in a central location for future pipeline runs.\r\n\r\n4. Start running your own analysis!\r\n\r\n   ```console\r\n   nextflow run sanger-tol/ensemblgenedownload --input $PWD/assets/samplesheet.csv --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\r\n   ```\r\n\r\n## Documentation\r\n\r\nThe sanger-tol/ensemblgenedownload pipeline comes with documentation about the pipeline [usage](docs/usage.md) and [output](docs/output.md).\r\n\r\n## Credits\r\n\r\nsanger-tol/ensemblgenedownload was originally written by @muffato.\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#pipelines` channel](https://sangertreeoflife.slack.com/channels/pipelines). Please [create an issue](https://github.com/sanger-tol/ensemblgenedownload/issues/new/choose) on GitHub if you are not on the Sanger slack channel.\r\n\r\n## Citations\r\n\r\nIf you use sanger-tol/ensemblgenedownload for your analysis, please cite it using the following doi: [10.5281/zenodo.7183206](https://doi.org/10.5281/zenodo.7183206)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "666",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/666?version=1",
        "name": "sanger-tol/insdcdownload v1.0.1 - Hefty m\u00fbmakil",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-14",
        "versions": 1
    },
    {
        "create_time": "2023-11-02",
        "creators": [
            "Matthieu Muffato",
            "Priyanka Surana"
        ],
        "description": "# ![sanger-tol/insdcdownload](docs/images/sanger-tol-insdcdownload_logo.png)\r\n\r\n[![GitHub Actions CI Status](https://github.com/sanger-tol/insdcdownload/workflows/nf-core%20CI/badge.svg)](https://github.com/sanger-tol/insdcdownload/actions?query=workflow%3A%22nf-core+CI%22)\r\n\r\n<!-- [![GitHub Actions Linting Status](https://github.com/sanger-tol/insdcdownload/workflows/nf-core%20linting/badge.svg)](https://github.com/sanger-tol/insdcdownload/actions?query=workflow%3A%22nf-core+linting%22) -->\r\n\r\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7155119-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7155119)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n\r\n[![Get help on Slack](http://img.shields.io/badge/slack-SangerTreeofLife%20%23pipelines-4A154B?labelColor=000000&logo=slack)](https://SangerTreeofLife.slack.com/channels/pipelines)\r\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40sangertol-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/sangertol)\r\n[![Watch on YouTube](http://img.shields.io/badge/youtube-tree--of--life-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/channel/UCFeDpvjU58SA9V0ycRXejhA)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/insdcdownload** is a pipeline that downloads assemblies from INSDC into a Tree of Life directory structure.\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the GitHub CI infrastructure. This ensures that the pipeline runs in a third-party environment, and has sensible resource allocation defaults set to run on real-world datasets.\r\n\r\n## Pipeline summary\r\n\r\n## Overview\r\n\r\nThe pipeline takes an assembly accession number, as well as the assembly name, and downloads it. It also builds a set of common indices (such as `samtools faidx`), and extracts the repeat-masking performed by the NCBI.\r\n\r\nSteps involved:\r\n\r\n- Download from the NCBI the genomic sequence (Fasta) and the assembly\r\n  stats and reports files.\r\n- Turn the masked Fasta file into an unmasked one.\r\n- Compress and index all Fasta files with `bgzip`, `samtools faidx`, and\r\n  `samtools dict`.\r\n- Generate the `.sizes` file usually required for conversion of data\r\n  files to UCSC's \"big\" formats, e.g. bigBed.\r\n- Extract the coordinates of the masked regions into a BED file.\r\n- Compress and index the BED file with `bgzip` and `tabix`.\r\n\r\n## Quick Start\r\n\r\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=22.04.0`)\r\n\r\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/) (you can follow [this tutorial](https://singularity-tutorial.github.io/01-installation/)), [`Podman`](https://podman.io/), [`Shifter`](https://nersc.gitlab.io/development/shifter/how-to-use/) or [`Charliecloud`](https://hpc.github.io/charliecloud/) for full pipeline reproducibility _(you can use [`Conda`](https://conda.io/miniconda.html) both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_.\r\n\r\n3. Download the pipeline and test it on a minimal dataset with a single command:\r\n\r\n   ```bash\r\n   nextflow run sanger-tol/insdcdownload -profile test,YOURPROFILE --outdir <OUTDIR>\r\n   ```\r\n\r\n   Note that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (`YOURPROFILE` in the example command above). You can chain multiple config profiles in a comma-separated string.\r\n\r\n   > - The pipeline comes with config profiles called `docker`, `singularity`, `podman`, `shifter`, `charliecloud` and `conda` which instruct the pipeline to use the named tool for software management. For example, `-profile test,docker`.\r\n   > - Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\r\n   > - If you are using `singularity`, please use the [`nf-core download`](https://nf-co.re/tools/#downloading-pipelines-for-offline-use) command to download images first, before running the pipeline. Setting the [`NXF_SINGULARITY_CACHEDIR` or `singularity.cacheDir`](https://www.nextflow.io/docs/latest/singularity.html?#singularity-docker-hub) Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\r\n   > - If you are using `conda`, it is highly recommended to use the [`NXF_CONDA_CACHEDIR` or `conda.cacheDir`](https://www.nextflow.io/docs/latest/conda.html) settings to store the environments in a central location for future pipeline runs.\r\n\r\n4. Start running your own analysis!\r\n\r\n   ```console\r\n   nextflow run sanger-tol/insdcdownload --assembly_accession GCA_927399515.1 --assembly_name gfLaeSulp1.1 --outdir results\r\n   ```\r\n\r\n## Documentation\r\n\r\nThe sanger-tol/insdcdownload pipeline comes with documentation about the pipeline [usage](docs/usage.md) and [output](docs/output.md).\r\n\r\n## Credits\r\n\r\nsanger-tol/insdcdownload was mainly written by @muffato, with major borrowings from @priyanka-surana's [read-mapping](https://github.com/sanger-tol/readmapping) pipeline, e.g. the script to remove the repeat-masking, and the overall structure and layout of the sub-workflows.\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#pipelines` channel](https://sangertreeoflife.slack.com/channels/pipelines). Please [create an issue](https://github.com/sanger-tol/insdcdownload/issues/new/choose) on GitHub if you are not on the Sanger slack channel.\r\n\r\n## Citations\r\n\r\nIf you use sanger-tol/insdcdownload for your analysis, please cite it using the following doi: [10.5281/zenodo.7155119](https://doi.org/10.5281/zenodo.7155119)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "638",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/638?version=1",
        "name": "sanger-tol/insdcdownload v1.1.0 - Deciduous ent",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-14",
        "versions": 1
    },
    {
        "create_time": "2023-11-14",
        "creators": [
            " Priyanka Surana"
        ],
        "description": "# ![sanger-tol/readmapping](docs/images/sanger-tol-readmapping_logo.png)\r\n\r\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.6563577-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.6563577)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.10.1-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/sanger-tol/readmapping)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/readmapping** is a bioinformatics best-practice analysis pipeline for mapping reads generated using Illumina, HiC, PacBio and Nanopore technologies against a genome assembly.\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nOn merge to `dev` and `main` branch, automated continuous integration tests run the pipeline on a full-sized dataset on the Wellcome Sanger Institute HPC farm using the Nextflow Tower infrastructure. This ensures that the pipeline runs on full sized datasets, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources.\r\n\r\n## Pipeline summary\r\n\r\n<img src=\"https://raw.githubusercontent.com/sanger-tol/readmapping/976525ad7b5327607a049aa85bbca36a48c6ba48/docs/images/sanger-tol-readmapping_workflow.png\" height=\"700\">\r\n\r\n## Quick Start\r\n\r\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=22.10.1`)\r\n\r\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/) (you can follow [this tutorial](https://singularity-tutorial.github.io/01-installation/)), [`Podman`](https://podman.io/), [`Shifter`](https://nersc.gitlab.io/development/shifter/how-to-use/) or [`Charliecloud`](https://hpc.github.io/charliecloud/) for full pipeline reproducibility _(you can use [`Conda`](https://conda.io/miniconda.html) both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_.\r\n\r\n3. Download the pipeline and test it on a minimal dataset with a single command:\r\n\r\n   ```bash\r\n   nextflow run sanger-tol/readmapping -profile test,YOURPROFILE --outdir <OUTDIR>\r\n   ```\r\n\r\n   Note that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (`YOURPROFILE` in the example command above). You can chain multiple config profiles in a comma-separated string.\r\n\r\n   > - The pipeline comes with config profiles called `docker`, `singularity`, `podman`, `shifter`, `charliecloud` and `conda` which instruct the pipeline to use the named tool for software management. For example, `-profile test,docker`.\r\n   > - Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\r\n   > - If you are using `singularity`, please use the [`nf-core download`](https://nf-co.re/tools/#downloading-pipelines-for-offline-use) command to download images first, before running the pipeline. Setting the [`NXF_SINGULARITY_CACHEDIR` or `singularity.cacheDir`](https://www.nextflow.io/docs/latest/singularity.html?#singularity-docker-hub) Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\r\n   > - If you are using `conda`, it is highly recommended to use the [`NXF_CONDA_CACHEDIR` or `conda.cacheDir`](https://www.nextflow.io/docs/latest/conda.html) settings to store the environments in a central location for future pipeline runs.\r\n\r\n4. Start running your own analysis!\r\n\r\n   ```bash\r\n   nextflow run sanger-tol/readmapping --input samplesheet.csv --fasta genome.fa.gz --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\r\n   ```\r\n\r\n## Credits\r\n\r\nsanger-tol/readmapping was originally written by [Priyanka Surana](https://github.com/priyanka-surana).\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n- [Matthieu Muffato](https://github.com/muffato) for the text logo\r\n- [Guoying Qi](https://github.com/gq1) for being able to run tests using Nf-Tower and the Sanger HPC farm\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#pipelines` channel](https://sangertreeoflife.slack.com/channels/pipelines). Please [create an issue](https://github.com/sanger-tol/readmapping/issues/new/choose) on GitHub if you are not on the Sanger slack channel.\r\n\r\n## Citations\r\n\r\nIf you use sanger-tol/readmapping for your analysis, please cite it using the following doi: [10.5281/zenodo.6563577](https://doi.org/10.5281/zenodo.6563577)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "665",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/665?version=1",
        "name": "sanger-tol/readmapping v1.1.0 - Hebridean Black",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-14",
        "versions": 1
    },
    {
        "create_time": "2021-02-15",
        "creators": [
            "Krisztian Papp"
        ],
        "description": "A pipeline for mapping, calling, and annotation of SARS-CoV2 variants.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "105",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/105?version=1",
        "name": "ENA SARS-CoV2 Variant Calling",
        "number_of_steps": 0,
        "projects": [
            "SARS-CoV-2 Data Hubs"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-24",
        "versions": 1
    },
    {
        "create_time": "2021-02-12",
        "creators": [
            "David F. Nieuwenhuijse",
            " Alexey Sokolov"
        ],
        "description": "A workflow for mapping and consensus generation of SARS-CoV2 whole genome amplicon nanopore data implemented in the Nextflow framework. Reads are mapped to a reference genome using Minimap2 after trimming the amplicon primers with a fixed length at both ends of the amplicons using Cutadapt. The consensus is called using Pysam based on a majority read support threshold per position of the Minimap2 alignment and positions with less than 30x coverage are masked using \u2018N\u2019.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in name",
        "id": "104",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/104?version=1",
        "name": "ENA SARS-CoV-2 Nanopore Amplicon Sequencing Analysis Workflow",
        "number_of_steps": 0,
        "projects": [
            "SARS-CoV-2 Data Hubs"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-24",
        "versions": 1
    },
    {
        "create_time": "2023-11-14",
        "creators": [
            "David Yuan"
        ],
        "description": "# covid-sequence-analysis-workflow\r\n\r\nThis is the official repository of the SARS-CoV-2 variant surveillance pipeline developed by Danish Technical University (DTU), Eotvos Lorand University (ELTE), EMBL-EBI, Erasmus Medical Center (EMC) under the [Versatile Emerging infectious disease Observatory (VEO)](https://www.globalsurveillance.eu/projects/veo-versatile-emerging-infectious-disease-observatory) project. The project consists of 20 European partners. It is funded by the European Commission.\r\n\r\nThe pipeline has been integrated on EMBL-EBI infrastructure to automatically process raw SARS-CoV-2 read data, presenting in the COVID-19 Data Portal: https://www.covid19dataportal.org/sequences?db=sra-analysis-covid19&size=15&crossReferencesOption=all#search-content.\r\n\r\n## Architecture\r\n\r\nThe pipeline supports sequence reads from both Illumina and Nanopore platforms. It is designed to be highly portable for both Google Cloud Platform and High Performance Computing cluster with IBM Spectrum LSF. We have performed secondary and tertiary analysis on millions of public samples. The pipeline shows good performance for large scale production. \r\n\r\n![Component diagram](doc/img/pipeline.components.png)\r\n\r\nThe pipeline takes SRA from the public FTP from ENA. It submits analysis objects back to ENA on the fly. The intermediate results and logs are stored in the cloud storage buckets or high performance local POSIX file system. The metadata is stored in Google BigQuery for metadata and status tracking and analysis. The runtime is created with Docker / Singularity containers and NextFlow. \r\n\r\n## Process to run the pipelines\r\n\r\nThe pipeline requires the Nextflow Tower for the application level monitoring. A free test account can be created for evaluation purposes at https://tower.nf/.\r\n\r\n### Preparation\r\n\r\n1. Store `export TOWER_ACCESS_TOKEN='...'` in `$HOME/.bash_profile`. Restart the current session or source the updated `$HOME/.bash_profile`.\r\n2. Run `git clone https://github.com/enasequence/covid-sequence-analysis-workflow`.\r\n3. Create `./covid-sequence-analysis-workflow/data/projects_accounts.csv` with submission_account_id and submission_passwor, for example:\r\n>  project_id,center_name,meta_key,submission_account_id,submission_password,ftp_password\r\n>  PRJEB45555,\"European Bioinformatics Institute\",public,,,\r\n\r\n### Running pipelines\r\n\r\n1. Run `./covid-sequence-analysis-workflow/init.sra_index.sh` to initialize or reinitialize the metadata in BigQuery.\r\n2. Run `./covid-sequence-analysis-workflow/./start.lsf.jobs.sh` with proper parameters to start the batch jobs on LSF or `./covid-sequence-analysis-workflow/./start.gls.jobs.sh` with proper parameters to start the batch jobs on GCP.\r\n\r\n### Error handling\r\n\r\nIf a job is killed or died, run the following to update the metadata to avoid reprocessing samples completed successfully.\r\n\r\n1. Run `./covid-sequence-analysis-workflow/update.receipt.sh <batch_id>` to collect the submission receipts and to update submission metadata. The script can be run at anytime. It needs to be run if a batch job is killed instead of completed for any reason.\r\n2. Run `./covid-sequence-analysis-workflow/set.archived.sh` to update stats for analyses submitted. The script can be run at anytime. It needs to be run at least once before ending a snapshot to make sure that the stats are up-to-date.\r\n\r\nTo reprocess the samples failed, delete the record in `sra_processing`.\r\n",
        "doi": "10.48546/workflowhub.workflow.664.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "664",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/664?version=1",
        "name": "covid-sequence-analysis-workflow",
        "number_of_steps": 0,
        "projects": [
            "SARS-CoV-2 Data Hubs"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genomics",
            "sars-cov-2",
            "pathogen"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-14",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Yvan Le Bras",
            "Coline Royaux"
        ],
        "description": "Galaxy Workflow created on Galaxy-E european instance, ecology.usegalaxy.eu, related to the Galaxy training tutorial \"[Metabarcoding/eDNA through Obitools](https://training.galaxyproject.org/training-material/topics/ecology/tutorials/Obitools-metabarcoding/tutorial.html)\" .\r\n\r\nThis workflow allows to analyze DNA metabarcoding / eDNA data produced on Illumina sequencers using the OBITools.",
        "doi": "10.48546/workflowhub.workflow.655.1",
        "edam_operation": [
            "DNA barcoding"
        ],
        "edam_topic": [
            "Biodiversity",
            "Ecology",
            "Genetics",
            "Phylogenetics"
        ],
        "filtered_on": "metap* in name",
        "id": "655",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/655?version=1",
        "name": "Obitools eDNA metabarcoding",
        "number_of_steps": 23,
        "projects": [
            "PNDB"
        ],
        "source": "WorkflowHub",
        "tags": [
            "biodiversity",
            "ecology"
        ],
        "tools": [
            "obi_grep",
            "obi_ngsfilter",
            "obi_annotate",
            "seq_filter_by_id",
            "obi_illumina_pairend",
            "obi_uniq",
            "ncbi_blastn_wrapper",
            "fastqc",
            "Cut1",
            "obi_tab",
            "obi_stat",
            "fastq_groomer",
            "join1",
            "Filter1",
            "obi_clean",
            "wc_gnu",
            "unzip"
        ],
        "type": "Galaxy",
        "update_time": "2024-12-09",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Saskia Hiltemann",
            "B\u00e9r\u00e9nice Batut",
            "Dave Clements",
            "Ahmed Mehdi"
        ],
        "description": "The workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, B\u00e9r\u00e9nice Batut, Dave Clements), adapted for pipeline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \r\n\r\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "653",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/653?version=1",
        "name": "Workflow 7 : Beta Diversity [16S Microbial Analysis With Mothur]",
        "number_of_steps": 4,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "mothur_tree_shared",
            "mothur_dist_shared",
            "newick_display",
            "mothur_heatmap_sim"
        ],
        "type": "Galaxy",
        "update_time": "2023-11-09",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Saskia Hiltemann",
            "B\u00e9r\u00e9nice Batut",
            "Dave Clements",
            "Ahmed Mehdi"
        ],
        "description": "The workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, B\u00e9r\u00e9nice Batut, Dave Clements), adapted for pipeline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \r\n\r\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "652",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/652?version=1",
        "name": "Workflow 6: Alpha Diversity [16S Microbial Analysis With Mothur]",
        "number_of_steps": 3,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "mothur_summary_single",
            "mothur_rarefaction_single",
            "XY_Plot_1"
        ],
        "type": "Galaxy",
        "update_time": "2023-11-09",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Saskia Hiltemann",
            "B\u00e9r\u00e9nice Batut",
            "Dave Clements",
            "Ahmed Mehdi"
        ],
        "description": "The workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, B\u00e9r\u00e9nice Batut, Dave Clements), adapted for pipeline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \r\n\r\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "651",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/651?version=1",
        "name": "Workflow 5: OTU Clustering [16S Microbial Analysis With Mothur]",
        "number_of_steps": 5,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "mothur_sub_sample",
            "mothur_count_groups",
            "mothur_classify_otu",
            "mothur_cluster_split",
            "mothur_make_shared"
        ],
        "type": "Galaxy",
        "update_time": "2023-11-09",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Saskia Hiltemann",
            "B\u00e9r\u00e9nice Batut",
            "Dave Clements",
            "Ahmed Mehdi"
        ],
        "description": "The workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, B\u00e9r\u00e9nice Batut, Dave Clements), adapted for pipeline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \r\n\r\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "650",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/650?version=1",
        "name": "Workflow 3: Classification [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 2,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "mothur_remove_lineage",
            "mothur_classify_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2023-11-09",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Saskia  Hiltemann",
            "B\u00e9r\u00e9nice Batut",
            "Dave Clements",
            "Ahmed Mehdi"
        ],
        "description": "16S Microbial Analysis with mothur (short)\r\n\r\nThe workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, B\u00e9r\u00e9nice Batut, Dave Clements), adapted for piepline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \r\n\r\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "648",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/648?version=1",
        "name": "Workflow 1: Further Quality Control [16S Microbial Analysis With Mothur]",
        "number_of_steps": 5,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "mothur_summary_seqs",
            "mothur_screen_seqs",
            "mothur_count_seqs",
            "mothur_unique_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2023-11-09",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Saskia Hiltemann",
            "B\u00e9r\u00e9nice Batut",
            "Dave Clements",
            "Ahmed Mehdi"
        ],
        "description": "The workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, B\u00e9r\u00e9nice Batut, Dave Clements), adapted for piepline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \r\n\r\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "649",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/649?version=1",
        "name": "Workflow 2: Data Cleaning And Chimera Removal [16S Microbial Analysis With Mothur]",
        "number_of_steps": 9,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "mothur_screen_seqs",
            "mothur_filter_seqs",
            "mothur_chimera_vsearch",
            "mothur_remove_seqs",
            "mothur_summary_seqs",
            "mothur_pre_cluster",
            "mothur_unique_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2023-11-09",
        "versions": 1
    },
    {
        "create_time": "2023-06-23",
        "creators": [
            "Jorge Ejarque"
        ],
        "description": "**Name:** SparseLU  \r\n**Contact Person:** support-compss@bsc.es  \r\n**Access Level:** public  \r\n**License Agreement:** Apache2  \r\n**Platform:** COMPSs  \r\n\r\n# Description\r\nThe Sparse LU application computes an LU matrix factorization on a sparse blocked matrix. The matrix size (number of blocks) and the block size are parameters of the application. \r\n\r\nAs the algorithm progresses, the area of the matrix that is accessed is smaller; concretely, at each iteration, the 0th row and column of the current matrix are discarded. On the other hand, due to the sparseness of the matrix, some of its blocks might not be allocated and, therefore, no work is generated for them.\r\n\r\nWhen executed with COMPSs, Sparse LU produces several types of task with different granularity and numerous dependencies between them.\r\n\r\n# Versions\r\nThere are three versions of Sparse LU, depending on the data types used to store the blocks.\r\n## Version 1\r\n''files'', where the matrix blocks are stored in files.\r\n## Version 2\r\n''objects'', where the matrix blocks are represented by objects.\r\n## Version 3\r\n''arrays'', where the matrix blocks are stored in arrays.\r\n\r\n\r\n# Execution instructions\r\nUsage:\r\n```\r\nruncompss sparseLU.files.SparseLU numberOfBlocks blockSize\r\nruncompss sparseLU.objects.SparseLU numberOfBlocks blockSize\r\nruncompss sparseLU.arrays.SparseLU numberOfBlocks blockSize\r\n```\r\n\r\nwhere:\r\n  * numberOfBlocks: Number of blocks inside each matrix\r\n  * blockSize: Size of each block\r\n\r\n\r\n# Execution Example\r\n```\r\nruncompss sparseLU.objects.SparseLU 16 4 \r\nruncompss sparseLU.files.SparseLU 16 4\r\nruncompss sparseLU.arrays.SparseLU 16 4 \r\n```\r\n\r\n\r\n# Build\r\n## Option 1: Native java\r\n```\r\ncd application_sources/; javac src/main/java/sparseLU/*/*.java\r\ncd src/main/java/; jar cf sparseLU.jar sparseLU/\r\ncd ../../../; mv src/main/java/sparseLU.jar jar/\r\n```\r\n\r\n## Option 2: Maven\r\n```\r\ncd application_sources/\r\nmvn clean package\r\n```\r\n",
        "doi": "10.48546/workflowhub.workflow.515.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "515",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/515?version=1",
        "name": "Java COMPSs LU Factorization for Sparse Matrices",
        "number_of_steps": 0,
        "projects": [
            "Cluster Emergent del Cervell Hum\u00e0",
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [
            "compss",
            "example",
            "java",
            "marenostrum iv",
            "supercomputer",
            "tutorial",
            "data_persistence"
        ],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2023-11-24",
        "versions": 1
    },
    {
        "create_time": "2023-10-27",
        "creators": [
            "Wolfgang Maier"
        ],
        "description": "A variation of the Cancer variant annotation (hg38 VEP-based) workflow at https://doi.org/10.48546/workflowhub.workflow.607.1.\r\n\r\nLike that other workflow it takes a list of tumor/normal sample pair variants in VCF format (see the other workflow for details about the expected format) and\r\n\r\n1. annotates them using the ENSEMBL Variant Effect Predictor and custom annotation data\r\n2. turns the annotated VCF into a MAF file for import into cBioPortal\r\n3. generates human-readable variant- and gene-centric reports\r\n\r\nIn addition, this worklfow exports the resulting MAF dataset to a WebDAV-enabled remote folder for subsequent import into cBioPortal.\r\nWebDAV access details can be configured in the Galaxy user preferences.",
        "doi": "10.48546/workflowhub.workflow.629.1",
        "edam_operation": [
            "Annotation",
            "SNP annotation"
        ],
        "edam_topic": [
            "Biomedical science",
            "Genetic variation",
            "Oncology"
        ],
        "filtered_on": "annot* in name",
        "id": "629",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/629?version=1",
        "name": "Cancer variant annotation (hg38 VEP-based) with MAF export",
        "number_of_steps": 85,
        "projects": [
            "usegalaxy-eu",
            "EOSC4Cancer"
        ],
        "source": "WorkflowHub",
        "tags": [
            "eosc4cancer"
        ],
        "tools": [
            "snpSift_filter",
            "Add_a_column1",
            "tp_easyjoin_tool",
            "Cut1",
            "bg_column_arrange_by_header",
            "ensembl_vep",
            "vcfanno",
            "param_value_from_file",
            "__BUILD_LIST__",
            "split_file_to_collection",
            "tp_text_file_with_recurring_lines",
            "export_remote",
            "datamash_ops",
            "vcf2maf",
            "__SORTLIST__",
            "add_line_to_file",
            "Filter1",
            "tp_find_and_replace",
            "__FILTER_FROM_FILE__",
            "__MERGE_COLLECTION__",
            "tp_tail_tool",
            "__EXTRACT_DATASET__",
            "compose_text_param",
            "__RELABEL_FROM_FILE__",
            "tp_replace_in_column",
            "collapse_dataset",
            "bcftools_plugin_split_vep",
            "datamash_transpose",
            "snpSift_extractFields"
        ],
        "type": "Galaxy",
        "update_time": "2025-05-08",
        "versions": 1
    },
    {
        "create_time": "2023-10-27",
        "creators": [
            "Wolfgang Maier"
        ],
        "description": "Call somatic, germline and LoH event variants from PE Illumina sequencing data obtained from matched pairs of tumor and normal tissue samples.\r\n\r\nThis workflow can be used with whole-genome and whole-exome sequencing data as input. For WES data, parts of the analysis can be restricted to the exome capture kits target regions by providing the optional \"Regions of Interest\" bed dataset.\r\n\r\nThe current version uses bwa-mem for read mapping and varscan somatic for variant calling and somatic status classification.",
        "doi": "10.48546/workflowhub.workflow.628.1",
        "edam_operation": [
            "Variant calling"
        ],
        "edam_topic": [
            "Biomedical science",
            "Genetic variation",
            "Oncology"
        ],
        "filtered_on": "ITS in description",
        "id": "628",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/628?version=1",
        "name": "Variant calling from matched tumor/normal sample pair (hg38 version)",
        "number_of_steps": 47,
        "projects": [
            "usegalaxy-eu",
            "EOSC4Cancer"
        ],
        "source": "WorkflowHub",
        "tags": [
            "eosc4cancer"
        ],
        "tools": [
            "samtools_rmdup",
            "qualimap_bamqc",
            "__BUILD_LIST__",
            "pick_value",
            "bwa_mem",
            "split_file_to_collection",
            "tp_text_file_with_recurring_lines",
            "bamleftalign",
            "samtools_calmd",
            "multiqc",
            "varscan_somatic",
            "fastqc",
            "__EXTRACT_DATASET__",
            "compose_text_param",
            "tp_replace_in_line",
            "samtools_view",
            "__RELABEL_FROM_FILE__",
            "Convert characters1",
            "__APPLY_RULES__",
            "collapse_dataset",
            "trimmomatic",
            "Grep1"
        ],
        "type": "Galaxy",
        "update_time": "2025-05-08",
        "versions": 1
    },
    {
        "create_time": "2023-10-27",
        "creators": [
            "Javier Conejero"
        ],
        "description": "**Name:** Matrix multiplication with Objects  \r\n**Contact Person**: support-compss@bsc.es  \r\n**Access Level**: public  \r\n**License Agreement**: Apache2  \r\n**Platform**: COMPSs  \r\n\r\n# Description\r\nMatrix multiplication is a binary operation that takes a pair of matrices and produces another matrix.\r\n\r\nIf A is an n\u00d7m matrix and B is an m\u00d7p matrix, the result AB of their multiplication is an n\u00d7p matrix defined only if the number of columns m in A is equal to the number of rows m in B. When multiplying A and B, the elements of the rows in A are multiplied with corresponding columns in B.\r\n\r\nIn this implementation, A and B are square matrices (same number of rows and columns), and so it is the result matrix C. Each matrix is divided in N blocks of M doubles. The multiplication of two blocks is done by a multiply task method with a simple three-nested-loop implementation. When executed with COMPSs, the main program generates N^3^ tasks arranged as N^2^ chains of N tasks in the dependency graph.\r\n\r\n# Execution instructions\r\nUsage:\r\n```\r\nruncompss --lang=python src/matmul_objects.py numberOfBlocks blockSize\r\n```\r\n\r\nwhere:\r\n* numberOfBlocks: Number of blocks inside each matrix\r\n* blockSize: Size of each block\r\n\r\n\r\n# Execution Examples\r\n```\r\nruncompss --lang=python src/matmul_objects.py 16 4\r\nruncompss src/matmul_objects.py 16 4\r\npython -m pycompss src/matmul_objects.py 16 4\r\n```\r\n\r\n# Build\r\nNo build is required\r\n",
        "doi": "10.48546/workflowhub.workflow.627.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "627",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/627?version=1",
        "name": "PyCOMPSs Matrix Multiplication with Objects (inputs generated by the code)",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [
            "example",
            "laptop",
            "pycompss",
            "tutorial",
            "data_persistence"
        ],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2023-11-24",
        "versions": 1
    },
    {
        "create_time": "2023-05-30",
        "creators": [
            "Jorge Ejarque"
        ],
        "description": "**Name:** Matrix Multiplication  \r\n**Contact Person:** support-compss@bsc.es  \r\n**Access Level:** public  \r\n**License Agreement:** Apache2  \r\n**Platform:** COMPSs  \r\n\r\n# Description\r\nMatrix multiplication is a binary operation that takes a pair of matrices and produces another matrix.\r\n\r\nIf A is an n\u00d7m matrix and B is an m\u00d7p matrix, the result AB of their multiplication is an n\u00d7p matrix defined only if the number of columns m in A is equal to the number of rows m in B. When multiplying A and B, the elements of the rows in A are multiplied with corresponding columns in B.\r\n\r\nIn this implementation, A and B are square matrices (same number of rows and columns), and so it is the result matrix C. Each matrix is divided in N blocks of M doubles. The multiplication of two blocks is done by a multiply task method with a simple three-nested-loop implementation. When executed with COMPSs, the main program generates N^3^ tasks arranged as N^2^ chains of N tasks in the dependency graph.\r\n\r\n# Versions\r\nThere are three versions of Matrix Multiplication, depending on the data types used to store the blocks.\r\n## Version 1\r\n''files'', where the matrix blocks are stored in files.\r\n## Version 2\r\n''objects'', where the matrix blocks are represented by objects.\r\n## Version 3\r\n''arrays'', where the matrix blocks are stored in arrays.\r\n\r\n# Execution instructions\r\nUsage:\r\n```\r\nruncompss matmul.files.Matmul numberOfBlocks blockSize\r\nruncompss matmul.objects.Matmul numberOfBlocks blockSize\r\nruncompss matmul.arrays.Matmul numberOfBlocks blockSize\r\n``` \r\n\r\nwhere:\r\n  * numberOfBlocks: Number of blocks inside each matrix\r\n  * blockSize: Size of each block\r\n\r\n\r\n# Execution Example\r\n```\r\nruncompss matmul.objects.Matmul 16 4\r\nruncompss matmul.files.Matmul 16 4\r\nruncompss matmul.arrays.Matmul 16 4  \r\n```\r\n\r\n# Build\r\n## Option 1: Native java\r\n```\r\ncd ~/tutorial_apps/java/matmul/; javac src/main/java/matmul/*/*.java\r\ncd src/main/java/; jar cf matmul.jar matmul/\r\ncd ../../../; mv src/main/java/matmul.jar jar/\r\n```\r\n\r\n## Option 2: Maven\r\n```\r\ncd ~/tutorial_apps/java/matmul/\r\nmvn clean package\r\n```\r\n",
        "doi": "10.48546/workflowhub.workflow.484.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "484",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/484?version=1",
        "name": "Java COMPSs Matrix Multiplication, out-of-core, using files",
        "number_of_steps": 0,
        "projects": [
            "Cluster Emergent del Cervell Hum\u00e0",
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [
            "compss",
            "example",
            "java",
            "laptop",
            "tutorial",
            "data_persistence"
        ],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2023-10-28",
        "versions": 1
    },
    {
        "create_time": "2023-05-30",
        "creators": [
            "Javier Conejero"
        ],
        "description": "**Name:** Matrix multiplication with Files  \r\n**Contact Person**: support-compss@bsc.es  \r\n**Access Level**: public  \r\n**License Agreement**: Apache2  \r\n**Platform**: COMPSs  \r\n\r\n# Description\r\nMatrix multiplication is a binary operation that takes a pair of matrices and produces another matrix.\r\n\r\nIf A is an n\u00d7m matrix and B is an m\u00d7p matrix, the result AB of their multiplication is an n\u00d7p matrix defined only if the number of columns m in A is equal to the number of rows m in B. When multiplying A and B, the elements of the rows in A are multiplied with corresponding columns in B.\r\n\r\nIn this implementation, A and B are square matrices (same number of rows and columns), and so it is the result matrix C. Each matrix is divided in N blocks of M doubles. The multiplication of two blocks is done by a multiply task method with a simple three-nested-loop implementation. When executed with COMPSs, the main program generates N^3^ tasks arranged as N^2^ chains of N tasks in the dependency graph.\r\n\r\n# Execution instructions\r\nUsage:\r\n```\r\nruncompss --lang=python src/matmul_files.py numberOfBlocks blockSize\r\n```\r\n\r\nwhere:\r\n* numberOfBlocks: Number of blocks inside each matrix\r\n* blockSize: Size of each block\r\n\r\n\r\n# Execution Examples\r\n```\r\nruncompss --lang=python src/matmul_files.py 4 4\r\nruncompss src/matmul_files.py 4 4\r\npython -m pycompss src/matmul_files.py 4 4\r\n```\r\n\r\n# Build\r\nNo build is required\r\n",
        "doi": "10.48546/workflowhub.workflow.485.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "485",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/485?version=1",
        "name": "PyCOMPSs Matrix Multiplication, out-of-core, using files",
        "number_of_steps": 0,
        "projects": [
            "Cluster Emergent del Cervell Hum\u00e0",
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [
            "example",
            "laptop",
            "pycompss",
            "tutorial",
            "data_persistence"
        ],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2023-10-27",
        "versions": 1
    },
    {
        "create_time": "2023-10-22",
        "creators": [
            "Yasmmin Martins"
        ],
        "description": "## Summary\r\n\r\nThe data preparation pipeline contains tasks for two distinct scenarios: [leukaemia](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE425) that contains microarray data for 119 patients and [ovarian](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE140082) cancer that contains next generation sequencing data for 380 patients.\r\n\r\nThe disease outcome prediction pipeline offers two strategies for this task:\r\n\r\n**Graph kernel method**: It starts generating personalized networks for each patient using the interactome file provided and generate the patient network checking if each PPI of the interactome has both proteins up regulated or down regulated according to the gene expression table provided. The first step generate a set of graphs for the patients that are evaluated with 4 distinct kernels for graph classification, which are: Linear kernel between edge histograms, Linear kernel between vertex histograms and the Weisfeiler lehman. These kernels functions calculate a similarity matrix for the graphs and then this matrix is used by the support vector machine classifier. Then the predictions are delivered to the last task that exports a report with the accuracy reached by each kernel. It allows some customizations about the network parameters to be used, such as the DEG cutoff to determine up and down regulated based on the log2 fold change, which will determine the topology and the labels distribution in the specific sample graphs. It is also possible customize the type of node/edge attributes passed to the kernel function, which may be only label, only weight or both.\r\n\r\n**GSEA based pathway scores method**: This method is faster and do not rely on tensor inputs such as the previous method. It uses geneset enrichment analysis on the pathways from KEGG 2021 of Human, and uses the scores of the pathways found enriched for the samples to build the numerical features matrix, that is then delivered to the AdaBoost classifier. The user may choose balance the dataset using oversampling strategy provided by SMOTE.\r\n\r\n## Usage Instructions\r\n### Preparation:\r\n1. ````git clone https://github.com/YasCoMa/screendop.git````\r\n2. ````cd screendop````\r\n3. Decompress screening_ovarian/raw_expression_table.tsv.tar.xz\r\n4. Create conda environment to handle dependencies: ````conda env create -f drugresponse_env.yml````\r\n5. ````conda activate drugresponse_env````\r\n6. Setup an environment variable named \"path_workflow_screendop\" with the full path to this workflow folder\r\n\r\n### Data preparation - File ````data_preparation_for_pipeline.py```` :\r\n\r\n#### Files decompression\r\n\r\n- Decompress data_preparation/lekaemia.tar.xz\r\n- Decompress data_preparation/ovarian/GSE140082_data.tar.xz\r\n    - Put the decompressed file GSE140082_series_matrix.txt in data_preparation/ovarian/\r\n    \r\n#### Pipeline parameters\r\n\r\n- __-rt__ or __--running_type__ <br>\r\n\tUse to prepare data for the desired scenario: <br>\r\n\t1 - Run with Leukaemia data <br>\r\n\t2 - Run with Ovarian cancer data\r\n\r\n#### Running modes examples\r\n\r\n1. Run for Leukaemia data: <br>\r\n````python3 data_preparation_for_pipeline.py -rt 1 ```` \r\n\r\nIn this case, you must have [R](https://www.r-project.org/) installed and also the library [limma](https://bioconductor.org/packages/release/bioc/html/limma.html), it is used to determine DEGs from microarray data. For this dataset, the files are already prepared in the folder.\r\n\r\n2. Run for Ovarian cancer data: <br>\r\n````python3 data_preparation_for_pipeline.py -rt 2 ```` \r\n\r\nIn this case, you must have [R](https://www.r-project.org/) installed and also the library [DESeq](https://bioconductor.org/packages/release/bioc/html/DESeq.html), because this scenario treats next generation sequencing data\r\n\r\n### Disease outcome prediction execution - File ````main.py````:\r\n\r\n#### Pipeline parameters\r\n\r\n- __-rt__ or __--running_step__ <br>\r\n\tUse to prepare data for the desired scenario: <br>\r\n\t1 - Run graph kernel method <br>\r\n\t2 - Run gsea based pathway scores method\r\n\r\n- __-cf__ or __--configuration_file__ <br>\r\n\tFile with the expression values for the genes by sample/patient in tsv format<br>\r\n\t\r\n\tExample of this file: config.json\r\n\t\t\r\n#### Input configuration file\r\n\r\n- Configuration file keys (see also the example in config.json):\r\n    - **folder** (mandatory for both methods): working directory\r\n    - **identifier**: project identifier to be used in the result files\r\n    - **mask_expression_table** (mandatory for both methods): Gene expression values file with the result of the fold change normalized value of a certain gene for each sample, already pruned by the significance (p-value). \r\n    - **raw_expression_table** (mandatory for both methods): Raw gene expression values already normalized following the method pf preference of the user.\r\n    - **labels_file** (mandatory for both methods): File with the prognosis label for each sample\r\n    - **deg_cutoff_up**: Cutoff value to determine up regulated gene. Default value is 1.\r\n    - **deg_cutoff_down**: Cutoff value to determine down regulated gene. Default value is -1.\r\n    - **nodes_enrichment**: Node attributes to be used in the screening evaluation. It may be a list combining the options \"label\", \"weight\" or \"all\". Examples: [\"all\", \"weight\"], [\"label\"], [\"label\", \"weight\"]. Default value is [\"all\"].\r\n    - **edges_enrichment**: Edge attributes to be used in the screening evaluation. It may be a list combining the options \"label\", \"weight\" or \"all\". Examples: [\"all\", \"weight\"], [\"label\"], [\"label\", \"weight\"]. Default value is [\"all\"].\r\n    - **flag_balance**: Flag to indicate whether the user wants to balance the samples in each outcome class, by SMOTE oversampling. Values may be false or true. Default value is false.\r\n\r\n#### Running modes examples\r\n1. Running disease outcome prediction by graph kernel method: <br>\r\n\t````python3 main.py -rt 1 -cf config.json````\r\n\r\n2. Running disease outcome prediction by gsea enriched network method: <br>\r\n\t````python3 main.py -rt 2 -cf config.json````\r\n\r\n## Reference\r\nMartins, Y. C. (2023). Multi-task analysis of gene expression data on cancer public datasets. medRxiv, 2023-09.\r\n\r\n## Bug Report\r\nPlease, use the [Issue](https://github.com/YasCoMa/screendop/issues) tab to report any bug.",
        "doi": null,
        "edam_operation": [
            "Gene-set enrichment analysis",
            "Modelling and simulation",
            "Prediction and recognition"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Data mining",
            "Medical informatics",
            "Workflows"
        ],
        "filtered_on": "binn* in description",
        "id": "621",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/621?version=1",
        "name": "ScreenDOP - Screening of strategies for disease outcome prediction",
        "number_of_steps": 0,
        "projects": [
            "yPublish - Bioinfo tools"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "data transformation",
            "data wrangling",
            "disease outcome prediction",
            "gene set enrichment analysis",
            "personalized medicine",
            "public cancer datasets exploration"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2023-10-22",
        "versions": 1
    },
    {
        "create_time": "2023-10-22",
        "creators": [
            "Yasmmin Martins"
        ],
        "description": "## Summary\r\n\r\nThis pipeline contains the following functions: \r\n(1) Data processing to handle the tansformations needed to obtain the original pathway scores of the samples according to single sample analysis GSEA\r\n(2) Model training based on the disease and healthy sample pathway scores, to classify them\r\n(3) Scoring matrix weights optimization according to a gold standard list of drugs (those that went on clinical trials or are approved for the disease).It tests the weights in a range of 0 to 30 (you may change as you want). The evaluation function tests and try to maximize the number of approved drugs whose modified pathway scores for disease samples is changed from disease to healthy sample classification, according to the trained model.\r\n(4) Computation of the calibrated disease samples pathwa scores according to the interaction among drug and targets found in the sample pathways & Drug ranking based on the disease samples whose calibrated matrix were responsible to change the trained model decision from disease to healthy state.\r\n(5) Drug combination ranking evaluated the same way as in option (4) but adding the effects of multiple drugs in each sample while calculating the calibrated scoring matrix\r\n            \r\n## Input configuration file:\r\n* The pipeline only needs a configuration file and the step number you want to run.\r\n- Configuration file keys (see also the example in config.json):\r\n    - **identifier**: project identifier to be used in the result files\r\n    - **type_normalization**: normalization type (possible values: tpm, fpkm, tmm, cpm or fpkm_uq)\r\n    - **genome_assembly**: the supported assemblies are the 37 and 38 (values may be: g37 or g38)\r\n    - **pathway_geneset**: pathway-based gene sets, choose one identifier from the list in [genesets_available.txt](https://github.com/YasCoMa/caliscoma_pipeline/blob/master/genesets_available.txt)\r\n    - **folder**: working directory\r\n    - **expression_file**: compressed gene expression file for the desired icgc project, it must be separated by tabulation. The following columns are mandatory: submitted_file_id (sample names), raw_read_count (the read counts without normalization) and gene_id (genes in ensembl or hgnc symbol). File expected to be in {folder}.\r\n    - **labels_file** (optional for function 1): file with two columns, one named 'sample' corresponding to the unique values of submitted_sample_id; the second named 'label' corresponding to a disease (or confirmed tumour) (1) or a healthy (0) case. File expected to be in {folder}.\r\n    - **trained_model** (optional for function 1): file with the trained model to separate healthy and disease cases. Full path is expected.\r\n    - **means_table_file** (optional for function 1): file with the means table calculated when the model is trained by the function 3. Full path is expected.\r\n    - **samples_pathway_scores** (optional for function 1): file with the original model calculated pathway scores by function 1, in order to check the number of features expected by the original model. Full path is expected.\r\n    - **optimized_weights_file**: tab separated table file with two columns representing the weights (w1, w2, w3) and their respective values.\r\n    - **drug_list_file** (only mandatory for function 3): file with the gold standard drug list (one drugbank id per line), this file is expected to be in the in the experiment item folder results ({folder}/{identifier})\r\n    - **drug_combination_file** (only mandatory for function 5): file with the drug combination candidates list (drugbank ids concatenated with comma in each line). Full path is expected.\r\n\r\n- Observation:    \r\n    * The \"labels_file\" parameter is mandatory for the weights optimization, scoring matrix calculation, model traning and drug (or drug combination) ranking \r\n    * In case of transfer learning, \"labels_file\" may be ignored only if both \"trained_model\", \"means_table_file\" and \"samples_pathway_scores\" are present. This is only possible for the functions 2, 4 and 5. For weights optimization, only labels file is accepted.\r\n    * If type_normalization and/or genome_assembly are missing or empty, it will switch to the default fpkm_uq\r\n    * If pathway_geneset is missing or empty, it will switch to the default KEGG_2021_HUMAN\r\n    * If optimized_weights_file is missing or empty, it will switch to the default values (w1: 20, w2: 5, w3: 10)\r\n    \r\n## Usage Instructions\r\n### Preparation:\r\n1. ````git clone https://github.com/YasCoMa/caliscoma_pipeline.git````\r\n2. ````cd caliscoma_pipeline````\r\n3. Create conda environment to handle dependencies: ````conda env create -f drugresponse_env.yml````\r\n4. ````conda activate drugresponse_env````\r\n5. Setup an environment variable named \"path_workflow\" with the full path to this workflow folder\r\n\r\n### Getting data for the running example in the LICA-FR and LIRI-JP projects from ICGC\r\n1. Download the [expression file for LICA-FR](https://dcc.icgc.org/api/v1/download?fn=/current/Projects/LICA-FR/exp_seq.LICA-FR.tsv.gz) and put it in data_icgc folder\r\n2. Download the [expression file for LIRI-JP](https://dcc.icgc.org/api/v1/download?fn=/current/Projects/LIRI-JP/exp_seq.LIRI-JP.tsv.gz) and put it in data_icgc folder\r\n3. For the liri-jp project, the labels file is already processed, to given an example of a project that run all steps proposed by this workflow\r\n\r\n### Run analysis\r\n- Run all steps: ````python3 main.py -rt 0 -cf config.json````\r\n- Run all steps: ````python3 main.py -rt 0 -cf config_transfer_options.json````\r\n\r\n- Run only data processing: ````python3 main.py -rt 1 -cf config.json````\r\n- Run only data processing: ````python3 main.py -rt 1 -cf config_transfer_options.json````\r\n\r\n- Run only model training & modified pathway score matrix: ````python3 main.py -rt 2 -cf config.json````\r\n- Run only model training & modified pathway score matrix: ````python3 main.py -rt 2 -cf config_transfer_options.json````\r\n\r\n- Run only weights optimization: ````python3 main.py -rt 3 -cf config.json````\r\n\r\n- Run only drug ranking: ````python3 main.py -rt 4 -cf config.json````\r\n- Run only drug ranking: ````python3 main.py -rt 4 -cf config_transfer_options.json````\r\n\r\n- Run only drug combination evaluation: ````python3 main.py -rt 5 -cf config.json````\r\n- Run only drug combination evaluation: ````python3 main.py -rt 5 -cf config_transfer_options.json````\r\n\r\n## Reference\r\nMartins, Y. C. (2023). Multi-task analysis of gene expression data on cancer public datasets. medRxiv, 2023-09.\r\n\r\n## Bug Report\r\nPlease, use the [Issues](https://github.com/YasCoMa/caliscoma_pipeline/issues) tab to report any bug.",
        "doi": null,
        "edam_operation": [
            "Gene-set enrichment analysis",
            "Modelling and simulation",
            "Prediction and recognition"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Data management",
            "Data mining",
            "Drug discovery",
            "Molecular interactions, pathways and networks"
        ],
        "filtered_on": "binn* in description",
        "id": "620",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/620?version=1",
        "name": "DReCaS - Pipeline for drug ranking based on computed pathway scores of disease and healthy samples",
        "number_of_steps": 0,
        "projects": [
            "yPublish - Bioinfo tools"
        ],
        "source": "WorkflowHub",
        "tags": [
            "workflows",
            "data retrieval and transformation",
            "durg response simulation",
            "gene set enrichment analysis",
            "personalized medicine"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2023-10-22",
        "versions": 1
    },
    {
        "create_time": "2023-10-22",
        "creators": [
            "Yasmmin Martins"
        ],
        "description": "## Summary\r\n\r\nThe PPI information aggregation pipeline starts getting all the datasets in [GEO](https://www.ncbi.nlm.nih.gov/geo/) database whose material was generated using expression profiling by high throughput sequencing. From each database identifiers, it extracts the supplementary files that had the counts table. Once finishing the download step, it identifies those that were normalized or had the raw counts to normalize.  It also identify and map the gene ids to uniprot (the ids found usually were from HGNC and Ensembl). For each normalized counts table belonging to some experiment, il filters those which have the proteins (already mapped from HGNC to Uniprot identifiers) in the pairs in evaluation. Then, it calculates the correlation matrix based on Pearson method in the tables and saves the respective pairs correlation value for each table. Finally, a repor is made for each pair in descending order of correlation value with the experiment identifiers.\r\n\r\n## Requirements:\r\n* Python packages needed:\r\n\t- os\r\n\t- scipy\r\n\t- pandas\r\n\t- sklearn\r\n\t- Bio python\r\n\t- numpy\r\n\r\n## Usage Instructions\r\n* Preparation:\r\n\t1. ````git clone https://github.com/YasCoMa/PipeAggregationInfo.git````\r\n\t2. ````cd PipeAggregationInfo````\r\n\t3. ````pip3 install -r requirements.txt````\r\n\r\n### Preprocessing pipeline\r\n* Go to the ncbi [GDS database webpage](https://www.ncbi.nlm.nih.gov/gds), use the key words to filter your gds datasets of interest and save the results as file (\"Send to\" option), and choose \"Summary (text)\"\r\n* Alternatively, we already saved the results concerning protein interactions, you may use them to run preprocessing in order to obtain the necessary files for the main pipeline\r\n* Running preprocessing:\r\n    - ````cd preprocessing````\r\n    - ````python3 data_preprocessing.py ./workdir_preprocessing filter_files````\r\n    - ````cd ../````\r\n    - Copy the generated output folder \"data_matrices_count\" into the workflow folder: ````cp -R preprocessing/workdir_preprocessing/data_matrices_count .````\r\n\r\n### Main pipeline\r\n\r\n* Pipeline parameters:\r\n\t- __-rt__ or __--running_type__ <br>\r\n\t\tUse to indicate the step you want to execute (it is desirable following the order): <br>\r\n\t\t1 - Make the process of finding the experiments and ranking them by correlation <br>\r\n\t\t2 - Select pairs that were already processed and ranked making a separated folder of interest\r\n\r\n\t- __-fo__ or __--folder__ <br>\r\n\t\tFolder to store the files (use the folder where the other required file can be found)\r\n\t\r\n\t- __-if__ or __--interactome_file__ <br>\r\n\t\tFile with the pairs (two columns with uniprot identifiers in tsv format)<br>\r\n\t\t\r\n\t\tExample of this file: running_example/all_pairs.tsv\r\n\r\n\t- __-spf__ or __--selected_pairs_file__ <br>\r\n\t\tFile with PPIs of interest (two columns with uniprot identifiers in tsv format)<br>\r\n\t\t\r\n\t\tExample of this file: running_example/selected_pairs.tsv\r\n\r\n* Running modes examples:\r\n\t1. Run step 1: <br>\r\n\t````python3 pipeline_expression_pattern.py -rt 1 -fo running_example/ -if all_pairs.tsv ````\r\n\r\n\t2. Run step 2: <br>\r\n\t````python3 pipeline_expression_pattern.py -rt 2 -fo running_example/ -spf selected_pairs.tsv ````\r\n\r\n## Bug Report\r\nPlease, use the [Issue](https://github.com/YasCoMa/PipeAggregationInfo/issues) tab to report any bug.",
        "doi": null,
        "edam_operation": [
            "Data retrieval",
            "Expression correlation analysis",
            "Protein interaction network analysis"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Gene expression",
            "Protein interactions"
        ],
        "filtered_on": "profil* in description",
        "id": "619",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/619?version=1",
        "name": "PipePatExp - Pipeline to aggregate gene expression correlation information for PPI",
        "number_of_steps": 0,
        "projects": [
            "yPublish - Bioinfo tools"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "gene expression correlation",
            "gene expression data wrangling",
            "geo database mining"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2023-10-22",
        "versions": 1
    },
    {
        "create_time": "2023-10-21",
        "creators": [
            "Yasmmin Martins"
        ],
        "description": "## Summary\r\n\r\nThis pipeline has as major goal provide a tool for protein interactions (PPI) prediction data formalization and standardization using the [OntoPPI](https://link.springer.com/chapter/10.1007/978-3-030-36599-8_23) ontology. This pipeline is splitted in two parts: (i) a part to prepare data from three main sources of PPI data ([HINT](http://hint.yulab.org/), [STRING](https://string-db.org/) and [PredPrin](https://github.com/YasCoMa/PredPrin.git)) and create the standard files to be processed by the next part; (ii) the second part uses the data prepared before to semantically describe using ontologies related to the concepts of this domain. It describes the provenance information of PPI prediction experiments, datasets characteristics, functional annotations of proteins involved in the PPIs, description of the PPI detection methods (also named as evidence) used in the experiment,  and the prediction score obtained by each PPI detection method for the PPIs. This pipeline also execute data fusion to map the same protein pairs from different data sources and, finally, it creates a database of all these information in the [alegro](https://allegrograph.com/) graph triplestore.\r\n\r\n## Requirements:\r\n* Python packages needed:\r\n\t- pip3 install numpy\r\n\t- pip3 install rdflib\r\n\t- pip3 install uuid\r\n\t- pip3 install SPARQLWrapper\r\n\t- alegro graph tools (pip3 install agraph-python) <br > \r\n\t\tGo to this [site](https://franz.com/agraph/support/documentation/current/python/install.html) for the installation tutorial\r\n\r\n## Usage Instructions\r\n### Preparation:\r\n1. ````git clone https://github.com/YasCoMa/ppintegrator.git````\r\n2. ````cd ppintegrator````\r\n3. `pip3 install -r requirements.txt`\r\n**Allegrograph is a triple store, which is a database to maintain semantic descriptions. This database's server provides a web application with a user interface to run, edit and manage queries, visualize results and manipulate the data without writing codes other than SPARQL query language. The use of the Allegregraph option is not mandatory, but if you want to export and use it, you have to install the server and the client.**\r\n4. if you want to use the Allegrograph server option (this triple store has free license up to 5,000,000 triples), install allegrograph server in your machine (configure a user and password): Server - https://franz.com/agraph/support/documentation/current/server-installation.html; Client - https://franz.com/agraph/support/documentation/current/python/install.html\r\n5. Export the following environment variables to configure Allegrograph server\r\n\r\n````\r\nexport AGRAPH_HOST=127.0.0.1\r\nexport AGRAPH_PORT=10035\r\nexport AGRAPH_USER=chosen_user\r\nexport AGRAPH_PASSWORD=chosen_password\r\n````\r\n5. Start allegrograph: ````path/to/allegrograph/bin/agraph-control --config path/to/allegrograph/lib/agraph.cfg start````\r\n6. Read the file data_requirements.txt to understand which files are needed for the process\r\n\r\n### Data preparation (first part) - File ````prepare_data_triplification.py```` :\r\n* Pipeline parameters:\r\n\t- __-rt__ or __--running_type__ <br>\r\n\t\tUse to indicate from which source you want to prepare PPI data, as follows: <br>\r\n\t\t1 - Prepare data for PredPrin <br>\r\n\t\t2 - Prepare data for String <br>\r\n\t\t3 - Prepare data for HINT\r\n\t- __-fec__ or __--file_experiment_config__ <br>\r\n\t\tFile with the experiment configuration in json format<br>\r\n\t\t\r\n\t\tExamples are in these files (all the metadata are required): params_hint.json, params_predrep_5k.json e params_string.json\r\n\r\n\t- __-org__ or __--organism__ <br>\r\n\t\tPrepare data only for one organism of interest (example: homo_sapiens) <br >\r\n\r\n\t\tThis parameter is optional. If you do not specify, it will automatically use the organisms described in the experiment configuration file above\r\n\r\n\r\n* Running modes examples:\r\n\t1. Running for PPI data generated by PredPrin: <br>\r\n\t````python3 prepare_data_triplification.py -rt 1 -fec params_predrep_5k.json````\r\n\r\n\t2. Running for HINT database: <br>\r\n\t````python3 prepare_data_triplification.py -rt 3 -fec params_hint.json````\r\n\r\n\t3. Running for STRING database: <br>\r\n\t````python3 prepare_data_triplification.py -rt 2 -fec params_string.json````\r\n\r\n\tIn the file ````auxiliar_data_preparation.py```` you can run it for all the examples provided automatically, as follows: <br>\r\n\t````python3 auxiliar_data_preparation.py````\r\n\r\n\r\n### PPI data triplification (second part) - File ````triplification_ppi_data.py````:\r\n\r\n* Pipeline parameters:\r\n\t- __-rt__ or __--running_type__ <br>\r\n\t\tUse to indicate which execution step you want to run (it is desirable following the order showed): <br>\r\n\t\t0 - Generate the descriptions for all the protein interaction steps of an experiment  (run steps 1, 2 and 3) <br >\r\n\t\t1 - Generate triples just about data provenance <br >\r\n\t\t2 - Generate triples just for protein functional annotations<br >\r\n\t\t3 - Generate triples just for the score results of each evidence<br >\r\n\t\t4 - Execute data fusion<br >\r\n\t\t5 - Generate descriptions and execute data fusion (run steps 1, 2, 3 and 4)<br >\r\n\t\t6 - Export to allegrograph server\r\n\r\n\t- __-fec__ or __--file_experiment_config__ <br>\r\n\t\tFile with the experiment configuration in json format<br>\r\n\t\t\r\n\t\tExamples are in these files (all the metadata are required): params_hint.json, params_predrep_5k.json e params_string.json\r\n\r\n\t- __-fev__ or __--file_evidence_info__ <br>\r\n\t\tFile with the PPI detection methods information in json format<br>\r\n\t\t\r\n\t\tExamples are in these files (all the metadata are required): evidences_information.json, evidences_information_hint.json e evidences_information_string.json\r\n\r\n\t- __-fcv__ or __--file_config_evidence__ <br>\r\n\t\tFile with the experiment and evidence methods files addresses in tsv format<br>\r\n\t\t\r\n\t\tExample of this file: config_evidence_file.tsv\r\n\r\n* Running modes examples:\r\n\t1. Running to generate all semantic descriptions for PredPrin: <br>\r\n\t````python3 triplification_ppi_data.py -rt 0 -fec params_predrep_5k.json -fev evidences_information.json````\r\n\r\n\t2. Running to generate only triples of data provenance: <br>\r\n\t````python3 triplification_ppi_data.py -rt 1 -fec params_hint.json -fev evidences_information_hint.json````\r\n\r\n\t3. Running to generate only triples of PPI scores for each evidence: <br>\r\n\t````python3 triplification_ppi_data.py -rt 3 -fec params_hint.json -fev evidences_information_hint.json````\r\n\r\n\t4. Running to generate only triples of protein functional annotations (only PredPrin exports these annotations): <br>\r\n\t````python3 triplification_ppi_data.py -rt 2 -fec params_predrep_5k.json -fev evidences_information.json````\r\n\r\n\t5. Running to generate all semantic descrptions for STRING: <br>\r\n\t````python3 triplification_ppi_data.py -rt 0 -fec params_string.json -fev evidences_information_string.json````\r\n    \r\n    **For the next options (4, 5 and 6), it is mandatory running at least mode 1 and 3 for HINT, STRING and PredPrin**\r\n    \r\n\t6. Running to execute data fusion of different sources: <br>\r\n\t````python3 triplification_ppi_data.py -rt 4 -fcv config_evidence_file.tsv````\r\n\r\n\t7. Running to generate all semantic descriptions and execute data fusion of different sources (combines mode 0 and 4): <br>\r\n\t````python3 triplification_ppi_data.py -rt 5 -fcv config_evidence_file.tsv````\r\n\r\n\t8.  Export semantic data to allegrograph server: <br>\r\n\t````python3 triplification_ppi_data.py -rt 6 -fcv config_evidence_file.tsv````\r\n\r\n## Query Scenarios for analysis\r\nSupposing you ran all the steps showed in the section above, you can run the following options to analyse the data stored alegro graph triple store. <br>\r\nFile to use for this section: ````query_analysis_ppitriplificator.py```` <br>\r\n\r\n* Parameter:\r\n\t- __-q__ or __--query_option__ <br>\r\n\t\tUse to indicate which query you want to perform: <br>\r\n\t\t1 - Get all the different organisms whose interactions are stored in the database<br >\r\n\t\t2 - Get the interactions that have scientific papers associated and the list of these papers<br >\r\n\t\t3 - Get a list of the most frequent biological processes annotated for the interactions of Escherichia coli bacteria<br >\r\n\t\t4 - Get only the interactions belonging to a specific biological process (regulation of transcription, DNA-templated) in Escherichia coli bacteria<br >\r\n\t\t5 - Get the scores of interactions belonging to a specific biological process (regulation of transcription, DNA-templated) in Escherichia coli bacteria<br >\r\n\t\t6 - Get a list of the most frequent biological processes annotated for the interactions of human organism<br >\r\n\t\t7 - Get only the interactions belonging to a specific biological process (positive regulation of transcription by RNA polymerase II) in human organism<br >\r\n\t\t8 - Get the scores of interactions belonging to a specific biological process (positive regulation of transcription by RNA polymerase II) in human organism\r\n\r\n* Running modes examples:\r\n\t1. Running queries: <br>\r\n\t````python3 query_analysis_ppitriplificator.py -q 1 ```` <br>\r\n\t\tChange number 1 to the respective number of the query you want to perform\r\n\r\n## Reference\r\nMartins, Y. C., Ziviani, A., Cerqueira e Costa, M. D. O., Cavalcanti, M. C. R., Nicol\u00e1s, M. F., & de Vasconcelos, A. T. R. (2023). PPIntegrator: semantic integrative system for protein\u2013protein interaction and application for host\u2013pathogen datasets. Bioinformatics Advances, 3(1), vbad067.\r\n\r\n## Bug Report\r\nPlease, use the [Issues](https://github.com/YasCoMa/ppintegrator/issues) tab to report any bug.",
        "doi": null,
        "edam_operation": [
            "Data retrieval",
            "Named-entity and concept recognition",
            "Relation extraction",
            "Text annotation"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Data curation and archival",
            "Data integration and warehousing",
            "Ontology and terminology"
        ],
        "filtered_on": "annot* in tags",
        "id": "618",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/618?version=1",
        "name": "PPIntegrator - PPI Triplification Process",
        "number_of_steps": 0,
        "projects": [
            "yPublish - Bioinfo tools"
        ],
        "source": "WorkflowHub",
        "tags": [
            "data annotation",
            "data fusion",
            "protein interactin data triplification",
            "protein interactions database integration"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2023-10-21",
        "versions": 1
    },
    {
        "create_time": "2023-10-21",
        "creators": [
            "Yasmmin Martins"
        ],
        "description": "## Summary\r\n\r\nThe validation process proposed has two pipelines for filtering PPIs predicted by some _IN SILICO_  detection method, both pipelines can be executed separately. The first pipeline (i) filter according to association rules of cellular locations extracted from HINT database. The second pipeline (ii) filter according to scientific papers where both proteins in the PPIs appear in interaction context in the sentences.\r\n\r\nThe pipeline (i) starts extracting cellular component annotations from HINT PPIs building a dataset and then the Apriori algorithm is applied in this dataset in an iterative process that repeat the application of this algorithm till the rules cover 15 main locations in the cell. This process generate a database with association rules with two main columns: antecedent and consequent, meaning that a location that occurs in antecedent also occurs with the location in consequent. The filtering task evaluate the PPI checking if some location annotated for the first protein is in the antecedent column and if some location of the second protein is also in the same rule but in the consequent column. If so, the PPI passes according to the criteria.\r\n\r\nThe pipeline (ii) starts getting all papers that mention both proteins in the PPIs and extrating their content using the NCBI [API](https://www.ncbi.nlm.nih.gov/home/develop/api/). These XML files are cleaned removing hypertext markup and references to figures, tables and supplementary materials. The paragraphs of the remaining articles content are processed by Natural language processing steps to extract sentences, tokens, stopwords removal to remove words extremely common in english language and do not help to identify the context of interest, prioritizing tokens using part-of-speech tagging to keep just nouns and verbs. Then the sentences filtered goes to the task that identifies the proteins of the PPI in evaluation among the tokens and also tries to identify tokens or set of tokens that mention experimental methods. The sentences that have the proteins of interest are filtered if the nouns and verbs have some of the items of the list of words indicating interaction relation (recruit, bind, interact, signaling, etc). Finally, a report is made by pair with the article identifiers, the sentences, the proteins and interacting words found.\r\n\r\nThe figure below illustrates all the tasks of these pipelines.\r\n\r\n<div style=\"text-align: center\">\r\n\t<img src=\"pipeline.png\" alt=\"pipeline\"\r\n\ttitle=\"PPI validation process\" width=\"600px\" />\r\n</div>\r\n\r\n## Requirements:\r\n* Python packages needed:\r\n\t- pip3 install pandas\r\n\t- pip3 install rdflib\r\n\t- pip3 install mlxtend\r\n\t- pip3 install inflect\r\n\t- pip3 install nltk\r\n\t- pip3 install biopython\r\n\t- pip3 install lxml\r\n\t- pip3 install bs4 (beautiful soup)\r\n\r\n## Usage Instructions\r\n### Preparation:\r\n1. ````git clone https://github.com/YasCoMa/ppi_validation_process.git````\r\n2. `pip3 install -r requirements.txt`\r\n3. ````cd ppi_validation_process/pipe_location_assocRules/````\r\n4. ````unzip pygosemsim.zip````\r\n5. ````cd ../````\r\n\r\n### Filtering by association rules of cellular locations (first filtering part) - File ````pipe_location_assocRules/find_pattern.py```` :\r\n* Pipeline parameters:\r\n\t- __-fo__ or __--folder__ <br>\r\n\t\tFolder to store the files (use the folder where the other required file can be found)\r\n\t- __-if__ or __--interactome_file__ <br>\r\n\t\tFile with the pairs (two columns with uniprot identifiers in tsv format)<br>\r\n\r\n\t\tExample of this file: pipe_location_assocRules/running_example/all_pairs.tsv\r\n\r\n\r\n* Running modes examples:\r\n\t1. Go to the first filtering part folder: <br>\r\n\t````cd pipe_location_assocRules/````\r\n\r\n\t2. Uncompress annotation_data.zip\r\n\t\r\n\t3. Run: <br>\r\n\t````python3 find_pattern.py -fo running_example/ -if all_pairs.tsv````\r\n\r\n\r\n### Filtering by text mining on scientific papers (second filtering part) - File ````ppi_pubminer/pubmed_pmc_literature_pipeline.py````:\r\n\r\n* Pipeline parameters:\r\n\t- __-em__ or __--execution_mode__ <br>\r\n\t\tUse to indicate the execution mode desired: <br>\r\n\t\t1 - Mode using a list of protein pairs as bait <br>\r\n\t\t2 - Mode that tries to find sentences of PPI context for any protein pairs given a list of articles\r\n\t\r\n\t- __-fo__ or __--folder__ <br>\r\n\t\tFolder to store the files (use the folder where the other required file can be found)\r\n\r\n\t- __-rtm1__ or __--running_type_mode_1__ <br>\r\n\t\tUse to indicate which execution step you want to run for mode 1 (it is desirable following the order showed): <br>\r\n\t\t0 (default) - Run all steps <br>\r\n\t\t1 - Run step 1 (Get mentions of both proteins in PMC articles) <br>\r\n\t\t2 - Run step 2 (Get the PMC or Pubmed files, clean and store them) <br>\r\n\t\t3 - Run step 3 (Get the exact sentences where the proteins were found on interacting context)\r\n\r\n\t- __-rtm2__ or __--running_type_mode_2__ <br>\r\n\t\tUse to indicate which execution step you want to run for mode 2 (it is desirable following the order showed): <br>\r\n\t\t0 (default) - Run all steps <br>\r\n\t\t1 - Run step 1 (Get the PMC or Pubmed files from the given list, clean and store them) <br>\r\n\t\t2 - Run step 2 (Get the exact sentences where the proteins were found on an interacting context)\r\n\r\n\t- __-fp__ or __--file_pairs__ <br>\r\n\t\t(For mode 1) File with the pairs (two columns with uniprot identifiers in tsv format)<br>\r\n\t\t\r\n\t\tExample of this file: ppipubminer/running_example/mode_1/all_pairs.tsv\r\n\r\n\t- __-fe__ or __--file_evaluation__ <br>\r\n\t\t(For mode 1) File exported after step 1 execution in tsv format<br>\r\n\r\n\t- __-fa__ or __--file_articles__ <br>\r\n\t\t(For mode 2) File with the articles (First column indicating if it is from pmc or pubmed and the second one is the article id) in tsv format)<br>\r\n\t\t\r\n\t\tExample of this file: ppipubminer/running_example/mode_2/articles_info.tsv\r\n\r\n* Running modes examples:\r\n\t- Go to the second filtering part folder: <br>\r\n\t````cd ppipubminer/````\r\n\r\n\t- Mode 1 - From protein pairs (PPIs) to sentences in articles\r\n\t\t1. Running all three steps of mode 1: <br>\r\n\t\t````python3 pubmed_pmc_literature_pipeline.py -em 1 -rtm1 0 -fo running_example/mode_1/ -fp all_pairs.tsv````\r\n\r\n\t\t2. Running only step 1 of mode 1: <br>\r\n\t\t````python3 pubmed_pmc_literature_pipeline.py -em 1 -rtm1 1 -fo running_example/mode_1/ -fp all_pairs.tsv````\r\n\r\n\t\t3. Running only step 2 of mode 1: <br>\r\n\t\t````python3 pubmed_pmc_literature_pipeline.py -em 1 -rtm1 2 -fo running_example/mode_1/ -fp all_pairs.tsv -fe literature_evaluation_pairs.tsv````\r\n\r\n\t\t4. Running only step 3 of mode 1: <br>\r\n\t\t````python3 pubmed_pmc_literature_pipeline.py -em 1 -rtm1 3 -fo running_example/mode_1/ -fp all_pairs.tsv -fe literature_evaluation_pairs.tsv````\r\n\r\n\t- Mode 2 - From articles to report of sentences with any protein pairs (PPIs)\r\n\t\t1. Running all three steps of mode 2: <br>\r\n\t\t````python3 pubmed_pmc_literature_pipeline.py -em 2 -rtm1 0 -fo running_example/mode_2/ -fa articles_info.tsv````\r\n\r\n\t\t2. Running only step 1 of mode 2: <br>\r\n\t\t````python3 pubmed_pmc_literature_pipeline.py -em 2 -rtm1 1 -fo running_example/mode_2/ -fa articles_info.tsv````\r\n\r\n\t\t3. Running only step 2 of mode 2: <br>\r\n\t\t````python3 pubmed_pmc_literature_pipeline.py -em 2 -rtm1 2 -fo running_example/mode_2/ -fa articles_info.tsv ````\r\n\r\n## Reference\r\nMartins YC, Ziviani A, Nicol\u00e1s MF, de Vasconcelos AT. Large-Scale Protein Interactions Prediction by Multiple Evidence Analysis Associated With an In-Silico Curation Strategy. Frontiers in Bioinformatics. 2021:38.\r\nhttps://www.frontiersin.org/articles/10.3389/fbinf.2021.731345/full\r\n\r\n## Bug Report\r\nPlease, use the [Issues](https://github.com/YasCoMa/ppi_validation_process/issues) tab to report any bug.",
        "doi": null,
        "edam_operation": [
            "Data retrieval",
            "Text mining"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Protein interaction experiment",
            "Protein interactions"
        ],
        "filtered_on": "binn* in description",
        "id": "617",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/617?version=1",
        "name": "PPIVPro - PPI Validation Process",
        "number_of_steps": 0,
        "projects": [
            "yPublish - Bioinfo tools"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "scientific publication text mining",
            "validaiton o protein interaction predictions"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2023-10-21",
        "versions": 1
    },
    {
        "create_time": "2023-10-21",
        "creators": [
            "Yasmmin Martins"
        ],
        "description": "## Summary\r\nPredPrIn is a scientific workflow to predict Protein-Protein Interactions (PPIs) using machine learning to combine multiple PPI detection methods of proteins according to three categories: structural,  based on primary aminoacid sequence and functional annotations.<br>\r\n\r\nPredPrIn contains three main steps: (i) acquirement and treatment of protein information, (ii) feature generation, and (iii) classification and analysis.\r\n\r\n(i) The first step builds a knowledge base with the available annotations of proteins and reuses this base for other prediction experiments, saving time and becoming more efficient. \r\n\r\n(ii) The feature generation step involves several evidence from different classes, such as: Gene Ontology (GO) information, domain interaction, metabolic pathway participation and sequence-based interaction. For the GO branches, we made a study to evaluate the best method to calculate semantic similarity to enhance the workflow performance. This step can be easily modified by adding new metrics, making PredPrIn flexible for future improvements. \r\n\r\nFinally, (iii) in the third step, the adaboost classifier is responsible for predicting the final scores from the numerical features dataset, exporting results of performance evaluation metrics.\r\n\r\n## Requirements:\r\n* Python packages needed:\r\n    - pip3 install luigi\r\n\t- pip3 install sqlalchemy\r\n\t- pip3 install rdflib\r\n\t- pip3 install sklearn\r\n\t- pip3 install matplotlib\r\n\t- pip3 install numpy\r\n\r\n* Other instalation:\r\n\t- sqlite (to be able to see the documentation generated by luigi about the tasks after execution)\r\n\r\n## Usage Instructions\r\nThe steps below consider the creation of a sqlite database file with all he tasks events which can be used after to retrieve the execution time taken by the tasks. It is possible run locally too (see luigi's documentation to change the running command). <br ><br>\r\n* Preparation:\r\n\t1. ````git clone https://github.com/YasCoMa/predprin.git````\r\n\t2. ````cd PredPrIn````\r\n\t3. `pip3 install -r requirements.txt`\r\n\t4. Download annotation_data.zip (https://drive.google.com/file/d/1bWPSyULaooj7GTrDf6QBY3ZyeyH5MRpm/view?usp=share_link)\r\n\t5. Download rdf_data.zip (https://drive.google.com/file/d/1Cp511ioXiw2PiOHdkxa4XsZnxOeM3Pan/view?usp=share_link)\r\n\t6. Download sequence_data.zip (https://drive.google.com/file/d/1uEKh5EF9X_6fgZ9cTTp0jW3XaL48stxA/view?usp=share_link)\r\n\t7. Unzip annotation_data.zip\r\n\t8. Unzip rdf_data.zip\r\n\t9. Unzip sequence_data.zip\r\n\t10. Download SPRINT pre-computed similarities in https://www.csd.uwo.ca/~ilie/SPRINT/precomputed_similarities.zip and unzip it inside core/sprint/HSP/\r\n\t11. Certify that there is a file named client.cfg (to configure the history log and feed the sqlite database). It must have the following data:\r\n\t````\r\n\t[core]\r\n\tdefault-scheduler-host=localhost\r\n\tdefault-scheduler-port=8082\r\n\trpc-connect-timeout=60.0 \r\n\trpc-retry-attempts=10    \r\n\trpc-retry-wait=60        \r\n\r\n\t[scheduler]\r\n\trecord_task_history = True\r\n\r\n\t[task_history]\r\n\tdb_connection = sqlite:///luigi-task-hist.db\r\n\t````\r\n* Parameters:\r\n\t1. parameters-file -> json file with all the information to process the prediction experiment (example: params.json)\r\n\t2. mode -> it can have two values: train (executes cross validation and save the model as a .joblib file) or test (uses a model obtained in train mode to test in some dataset listed in the parameters file)\r\n\t3. model -> it is the model file full path saved in train mode as .joblib\r\n\t\r\n* Running:\r\n\t1. ````mkdir luigi_log```` (or other name for the log folder of your choice)\r\n\t2. ````luigid --background --logdir luigi_log```` (start luigi server)\r\n\t3. ````nohup python3.5 -m luigi --module main RunPPIExperiment --parameters-file params.json --mode 'train' --model none.joblib --workers 3 &```` <br >\r\n\t   ````nohup python3.5 -m luigi --module main RunPPIExperiment --parameters-file params.json --mode 'test' --model model.jolib --workers 3 &```` <br >\r\n\t\t- Replace python3.5 by the command python of your environment <br>\r\n\t\t- Replace the data given as example in params.json using your own data <br > \r\n\t\t- Adapt the number of workers to use as you need and the capacity of your computational resource available\r\n\r\n\tYou can monitor the prediction experiment execution in localhost:8082\r\n\r\n## Reference\r\nMartins YC, Ziviani A, Nicol\u00e1s MF, de Vasconcelos AT. Large-Scale Protein Interactions Prediction by Multiple Evidence Analysis Associated With an In-Silico Curation Strategy. Frontiers in Bioinformatics. 2021:38.\r\nhttps://www.frontiersin.org/articles/10.3389/fbinf.2021.731345/full\r\n\r\n## Bug Report\r\nPlease, use the [Issues](https://github.com/YasCoMa/PredPrIn/issues) tab to report any bug.",
        "doi": null,
        "edam_operation": [
            "Protein interaction network prediction",
            "Protein-protein interaction analysis"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Protein interactions",
            "Workflows"
        ],
        "filtered_on": "binn* in name",
        "id": "616",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/616?version=1",
        "name": "PredPrIn - Scientific workflow to predict protein-protein interactions based in a combined analysis of multiple protein characteristics.",
        "number_of_steps": 0,
        "projects": [
            "yPublish - Bioinfo tools"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "domain-domain interaction",
            "gene ontology term sets similarity",
            "luigi & rufus workflow",
            "pathway co-occurrence"
        ],
        "tools": [
            "PredPrIn"
        ],
        "type": "Python",
        "update_time": "2023-10-21",
        "versions": 1
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Lucille Delisle"
        ],
        "description": "Run baredSC in 1 dimension in logNorm for 1 to N gaussians and combine models.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "615",
        "keep": true,
        "latest_version": 6,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/615?version=6",
        "name": "baredsc/baredSC-1d-logNorm",
        "number_of_steps": 3,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "",
            "baredsc_1d",
            "baredsc_combine_1d"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 6
    },
    {
        "create_time": "2024-06-22",
        "creators": [
            "Matthias Bernt"
        ],
        "description": "Automated inference of stable isotope incorporation rates in proteins for functional metaproteomics ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in name",
        "id": "613",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/613?version=2",
        "name": "openms-metaprosip/main",
        "number_of_steps": 8,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "PeptideIndexer",
            "DecoyDatabase",
            "FeatureFinderMultiplex",
            "IDMapper",
            "FalseDiscoveryRate",
            "MetaProSIP",
            "__SORTLIST__",
            "MSGFPlusAdapter"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2023-10-19",
        "creators": [
            "Yasmmin Martins"
        ],
        "description": "## Summary\r\nHPPIDiscovery is a scientific workflow to augment, predict and perform an insilico curation of host-pathogen Protein-Protein Interactions (PPIs) using graph theory to build new candidate ppis and machine learning to predict and evaluate them by combining multiple PPI detection methods of proteins according to three categories: structural,  based on primary aminoacid sequence and functional annotations.<br>\r\n\r\nHPPIDiscovery contains three main steps: (i) acquirement of pathogen and host proteins information from seed ppis provided by HPIDB search methods, (ii) Model training and generation of new candidate ppis from HPIDB seed proteins' partners, and (iii) Evaluation of new candidate ppis and results exportation.\r\n\r\n(i) The first step acquires the identification of the taxonomy ids of the host and pathogen organisms in the result files. Then it proceeds parsing and cleaning the HPIDB results and downloading the protein interactions of the found organisms from the STRING database. The string protein identifiers are also mapped using the id mapping tool of uniprot API and we retrieve the uniprot entry ids along with the functional annotations, sequence, domain and kegg enzymes.\r\n\r\n(ii) The second step builds the training dataset using the non redundant hpidb validated interactions of each genome as positive set and random string low confidence ppis from each genome as negative set. Then, PredPrin tool is executed in the training mode to obtain the model that will evaluate the new candidate PPIs. The new ppis are then generated by performing a pairwise combination of string partners of host and pathogen hpidb proteins. \r\n\r\nFinally, (iii) in the third step, the predprin tool is used in the test mode to evaluate the new ppis and generate the reports and list of positively predicted ppis.\r\n\r\nThe figure below illustrates the steps of this workflow.\r\n\r\n## Requirements:\r\n* Edit the configuration file (config.yaml) according to your own data, filling out the following fields:\r\n\t- base_data: location of the organism folders directory, example: /home/user/data/genomes \r\n\t- parameters_file: Since this workflow may perform parallel processing of multiple organisms at the same time, you must prepate a tabulated file containng the genome folder names located in base data, where the hpidb files are located. Example: /home/user/data/params.tsv. It must have the following columns: genome (folder name), hpidb_seed_network (the result exported by one of the search methods available in hpidb database), hpidb_search_method (the type of search used to generate the results) and target_taxon (the target taxon id). The column hpidb_source may have two values: keyword or homology. In the keyword mode, you provide a taxonomy, protein name, publication id or detection method and you save all results (mitab.zip) in the genome folder. Finally, in the homology mode allows the user to search for host pathogen ppis giving as input fasta sequences of a set of proteins of the target pathgen for enrichment (so you have to select the search for a pathogen set) and you save the zip folder results (interaction data) in the genome folder. This option is extremely useful when you are not sure that your organism has validated protein interactions, then it finds validated interactions from the closest proteins in the database. In case of using the homology mode, the identifiers of the pathogens' query fasta sequences must be a Uniprot ID. All the query protein IDs must belong to the same target organism (taxon id).\r\n\t- model_file: path of a previously trained model in joblib format (if you want to train from the known validated PPIs given as seeds, just put a 'None' value)\r\n\r\n## Usage Instructions\r\nThe steps below consider the creation of a sqlite database file with all he tasks events which can be used after to retrieve the execution time taken by the tasks. It is possible run locally too (see luigi's documentation to change the running command). <br ><br>\r\n* Preparation:\r\n\t1. ````git clone https://github.com/YasCoMa/hppidiscovery.git````\r\n\t2. ````cd hppidiscovery````\r\n\t3. ````mkdir luigi_log```` \r\n\t4. ````luigid --background --logdir luigi_log```` (start luigi server)\r\n\t5. conda env create -f hp_ppi_augmentation.yml\r\n\t6. conda activate hp_ppi_augmentation\r\n\t6.1. (execute ````pip3 install wget```` (it is not installed in the environment))\r\n\t7. run ````pwd```` command and get the full path\r\n\t8. Substitute <path> in config_example.yaml with the full path obtained in the previous step\r\n\t9. Download SPRINT pre-computed similarities in https://www.csd.uwo.ca/~ilie/SPRINT/precomputed_similarities.zip and unzip it inside workflow_hpAugmentation/predprin/core/sprint/HSP/\r\n\t10. ````cd workflow_hpAugmentation/predprin/````\r\n\t11. Uncompress annotation_data.zip\r\n\t12. Uncompress sequence_data.zip\r\n\t13. ````cd ../../````\r\n\t14. ````cd workflow_hpAugmentation````\r\n\t15. snake -n (check the plan of jobs, it should return no errors and exceptions)\r\n\t16. snakemake -j 4 (change this number according the number of genomes to analyse and the amount of cores available in your machine)",
        "doi": null,
        "edam_operation": [
            "Protein interaction network analysis",
            "Protein interaction network prediction"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Protein interaction experiment"
        ],
        "filtered_on": "binn* in description",
        "id": "611",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/611?version=1",
        "name": "HPPIDiscovery - Scientific workflow to augment, predict and evaluate host-pathogen protein-protein interactions",
        "number_of_steps": 0,
        "projects": [
            "yPublish - Bioinfo tools"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "protein-protein interaction prediction",
            "host-pathogen ppis",
            "proteins network augmentation"
        ],
        "tools": [
            "PredPrIn"
        ],
        "type": "Snakemake",
        "update_time": "2023-10-19",
        "versions": 1
    },
    {
        "create_time": "2023-10-10",
        "creators": [
            "Wolfgang Maier"
        ],
        "description": "This Galaxy workflow takes a list of tumor/normal sample pair variants in VCF format and\r\n1. annotates them using the ENSEMBL Variant Effect Predictor and custom annotation data\r\n2. turns the annotated VCF into a MAF file for import into cBioPortal\r\n3. generates human-readable variant- and gene-centric reports\r\n\r\nThe input VCF is expected to encode somatic status, somatic p-value and germline p-value of each variant in varscan somatic format, i.e., via SS, SPV and GPV INFO keys, respectively.",
        "doi": "10.48546/workflowhub.workflow.607.1",
        "edam_operation": [
            "Annotation",
            "SNP annotation"
        ],
        "edam_topic": [
            "Biomedical science",
            "Genetic variation",
            "Oncology"
        ],
        "filtered_on": "annot* in name",
        "id": "607",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/607?version=1",
        "name": "Cancer variant annotation (hg38 VEP-based)",
        "number_of_steps": 78,
        "projects": [
            "usegalaxy-eu",
            "EOSC4Cancer"
        ],
        "source": "WorkflowHub",
        "tags": [
            "eosc4cancer"
        ],
        "tools": [
            "snpSift_filter",
            "Add_a_column1",
            "tp_easyjoin_tool",
            "Cut1",
            "bg_column_arrange_by_header",
            "ensembl_vep",
            "vcfanno",
            "__BUILD_LIST__",
            "split_file_to_collection",
            "tp_text_file_with_recurring_lines",
            "datamash_ops",
            "vcf2maf",
            "__SORTLIST__",
            "add_line_to_file",
            "Filter1",
            "tp_find_and_replace",
            "__FILTER_FROM_FILE__",
            "__MERGE_COLLECTION__",
            "__EXTRACT_DATASET__",
            "__RELABEL_FROM_FILE__",
            "tp_replace_in_column",
            "collapse_dataset",
            "bcftools_plugin_split_vep",
            "datamash_transpose",
            "snpSift_extractFields"
        ],
        "type": "Galaxy",
        "update_time": "2025-05-08",
        "versions": 1
    },
    {
        "create_time": "2023-10-04",
        "creators": [],
        "description": "The ultimate-level complexity workflow is one among a collection of workflows designed to address tasks up to CTF estimation. In addition to the functionalities provided by layer 0 and 1 workflows, this workflow aims to enhance the quality of both **acquisition images** and **processing**.\r\n\r\n**Quality control protocols**\r\n\r\n\u2026\r\n\r\n**Combination of methods**\r\n* **CTF consensus**\r\n\t* New methods to compare ctf estimations\r\n\t* CTF xmipp criteria (richer parameters i.e. ice detection)\r\n\r\n**Advantages**:\u00a0\r\n* Control of the acquisition quality\r\n* Robust estimations to continue with the processing",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Sample collections"
        ],
        "filtered_on": "binn* in description",
        "id": "600",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/600?version=1",
        "name": "CEITEC layer 2 workflow",
        "number_of_steps": 0,
        "projects": [
            "Scipion CNB"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cryoem",
            "image processing",
            "scipion",
            "spa"
        ],
        "tools": [
            "Scipion",
            "XMIPP",
            "MotionCor2",
            "cisTEM"
        ],
        "type": "Scipion",
        "update_time": "2024-07-10",
        "versions": 1
    },
    {
        "create_time": "2023-09-25",
        "creators": [
            "Davide Gurnari"
        ],
        "description": "This repository contains the python code to reproduce the experiments in D\u0142otko, Gurnari \"Euler Characteristic Curves and Profiles: a stable shape invariant for big data problems\"",
        "doi": "10.48546/workflowhub.workflow.576.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "576",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/576?version=1",
        "name": "ECP experiments",
        "number_of_steps": 0,
        "projects": [
            "Dioscuri TDA"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2023-09-25",
        "versions": 1
    },
    {
        "create_time": "2023-09-12",
        "creators": [
            "Sagane Joye-Dind"
        ],
        "description": "# ERGA Protein-coding gene annotation workflow.\r\nAdapted from the work of Sagane Joye:\r\n\r\nhttps://github.com/sdind/genome_annotation_workflow\r\n\r\n## Prerequisites\r\n\r\nThe following programs are required to run the workflow and the listed version were tested. It should be noted that older versions of snakemake are not compatible with newer versions of singularity as is noted here: [https://github.com/nextflow-io/nextflow/issues/1659](https://github.com/nextflow-io/nextflow/issues/1659).\r\n\r\n`conda v 23.7.3`\r\n\r\n`singularity v 3.7.3`\r\n\r\n`snakemake v 7.32.3` \r\n\r\nYou will also need to acquire a licence key for Genemark and place this in your home directory with name `~/.gm_key` The key file can be obtained from the following location, where the licence should be read and agreed to: http://topaz.gatech.edu/GeneMark/license_download.cgi\r\n\r\n## Workflow\r\n\r\nThe pipeline is based on braker3 and was tested on the following dataset from Drosophila melanogaster: [https://doi.org/10.5281/zenodo.8013373](https://doi.org/10.5281/zenodo.8013373)\r\n\r\n### Input data\r\n\r\n- Reference genome in fasta format\r\n\r\n- RNAseq data in paired-end zipped fastq format\r\n\r\n- uniprot fasta sequences in zipped fasta format\r\n\r\n### Pipeline steps\r\n\r\n- **Repeat Model and Mask** Run RepeatModeler using the genome as input, filter any repeats also annotated as protein sequences in the uniprot database and use this filtered libray to mask the genome with RepeatMasker\r\n\r\n- **Map RNAseq data** Trim any remaining adapter sequences and map the trimmed reads to the input genome\r\n\r\n- **Run gene prediction software** Use the mapped RNAseq reads and the uniprot sequences to create hints for gene prediction using Braker3 on the masked genome\r\n\r\n- **Evaluate annotation** Run BUSCO to evaluate the completeness of the annotation produced\r\n\r\n### Output data\r\n\r\n- FastQC reports for input RNAseq data before and after adapter trimming\r\n\r\n- RepeatMasker report containing quantity of masked sequence and distribution among TE families\r\n\r\n- Protein-coding gene annotation file in gff3 format\r\n\r\n- BUSCO summary of annotated sequences\r\n\r\n## Setup\r\n\r\nYour data should be placed in the `data` folder, with the reference genome in the folder `data/ref` and the transcript data in the foler `data/rnaseq`.\r\n\r\nThe config file requires the following to be given:\r\n\r\n```\r\nasm: 'absolute path to reference fasta'\r\nsnakemake_dir_path: 'path to snakemake working directory'\r\nname: 'name for project, e.g. mHomSap1'\r\nRNA_dir: 'absolute path to rnaseq directory'\r\nbusco_phylum: 'busco database to use for evaluation e.g. mammalia_odb10'\r\n```\r\n",
        "doi": "10.48546/workflowhub.workflow.569.1",
        "edam_operation": [],
        "edam_topic": [
            "Gene structure",
            "Gene transcripts",
            "Genomics"
        ],
        "filtered_on": "annot* in tags",
        "id": "569",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/569?version=1",
        "name": "ERGA Protein-coding gene annotation workflow",
        "number_of_steps": 0,
        "projects": [
            "ERGA Annotation"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "biodiversity",
            "genomics",
            "transcriptomics"
        ],
        "tools": [
            "HISAT2",
            "RepeatMasker",
            "RepeatModeler",
            "BRAKER1"
        ],
        "type": "Snakemake",
        "update_time": "2023-09-13",
        "versions": 1
    },
    {
        "create_time": "2025-02-07",
        "creators": [
            "Lucille Delisle"
        ],
        "description": "This workflow takes as input SR BAM from ChIP-seq. It calls peaks on each replicate and intersect them. In parallel, each BAM is subsetted to smallest number of reads. Peaks are called using both subsets combined. Only peaks called using a combination of both subsets which have summits intersecting the intersection of both replicates will be kept.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "561",
        "keep": true,
        "latest_version": 11,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/561?version=11",
        "name": "consensus-peaks/consensus-peaks-chip-sr",
        "number_of_steps": 17,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "chip"
        ],
        "tools": [
            "__EXTRACT_DATASET__",
            "deeptools_bigwig_average",
            "samtools_view",
            "bedtools_intersectbed",
            "wig_to_bigWig",
            "macs2_callpeak",
            "Cut1",
            "Filter1",
            "table_compute",
            "collapse_dataset",
            "param_value_from_file",
            "multiqc",
            "tp_sorted_uniq"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 11
    },
    {
        "create_time": "2023-08-11",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# Macromolecular Coarse-Grained Flexibility (FlexServ) tutorial using BioExcel Building Blocks (biobb)\r\n\r\nThis tutorial aims to illustrate the process of generating protein conformational ensembles from 3D structures and analysing its molecular flexibility, step by step, using the BioExcel Building Blocks library (biobb).\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.557.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "557",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/557?version=1",
        "name": "Galaxy Macromolecular Coarse-Grained Flexibility tutorial",
        "number_of_steps": 27,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "biobb_flexserv_bd_run_ext",
            "biobb_io_pdb_ext",
            "biobb_flexserv_pcz_animate_ext",
            "biobb_flexserv_nma_run_ext",
            "biobb_structure_utils_extract_atoms_ext",
            "biobb_analysis_cpptraj_convert_ext",
            "biobb_flexserv_pcz_unzip_ext",
            "biobb_flexserv_pcz_hinges_ext",
            "biobb_flexserv_pcz_collectivity_ext",
            "biobb_flexserv_dmd_run_ext",
            "biobb_flexserv_pcz_zip_ext",
            "biobb_flexserv_pcz_info_ext",
            "biobb_flexserv_pcz_stiffness_ext",
            "biobb_flexserv_pcz_evecs_ext",
            "biobb_flexserv_pcz_bfactor_ext",
            "biobb_analysis_cpptraj_rms_ext"
        ],
        "type": "Galaxy",
        "update_time": "2023-08-11",
        "versions": 1
    },
    {
        "create_time": "2023-11-24",
        "creators": [
            "Casper de Visser",
            "Anna Niehues"
        ],
        "description": "This workflow is designed to analyze to a multi-omics data set that comprises genome-wide DNA methylation profiles, targeted metabolomics, and behavioral data of two cohorts that participated in the ACTION Biomarker Study (ACTION, Aggression in Children: Unraveling gene-environment interplay to inform Treatment and InterventiON strategies. (Boomsma 2015, Bartels 2018, Hagenbeek 2020, van Dongen 2021, Hagenbeek 2022). The ACTION-NTR cohort consists of twins that are either longitudinally concordant or discordant for childhood aggression. The ACTION-Curium-LUMC cohort consists of children referred to the Dutch LUMC Curium academic center for child and youth psychiatry. With the joint analysis of multi-omics data and behavioral data, we aim to identify substructures in the ACTION-NTR cohort and link them to aggressive behavior. First, the individuals are clustered using Similarity Network Fusion (SNF, Wang 2014), and latent feature dimensions are uncovered using different unsupervised methods including Multi-Omics Factor Analysis (MOFA) (Argelaguet 2018) and Multiple Correspondence Analysis (MCA, L\u00ea 2008, Husson 2017). In a second step, we determine correlations between -omics and phenotype dimensions, and use them to explain the subgroups of individuals from the ACTION-NTR cohort. In order to validate the results, we project data of the ACTION-Curium-LUMC cohort onto the latent dimensions and determine if correlations between omics and phenotype data can be reproduced.",
        "doi": "10.48546/workflowhub.workflow.402.8",
        "edam_operation": [
            "Clustering",
            "Dimensionality reduction"
        ],
        "edam_topic": [
            "Epigenomics",
            "Metabolomics"
        ],
        "filtered_on": "metap* in tags",
        "id": "402",
        "keep": true,
        "latest_version": 8,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/402?version=8",
        "name": "X-omics ACTIONdemonstrator analysis workflow",
        "number_of_steps": 0,
        "projects": [
            "X-omics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "behavioral data",
            "epigenomics",
            "fair",
            "metabolomics",
            "multi-omics",
            "nextflow"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-24",
        "versions": 8
    },
    {
        "create_time": "2023-08-01",
        "creators": [
            "Agata Kilar"
        ],
        "description": "# GERONIMO\r\n\r\n## Introduction\r\nGERONIMO is a bioinformatics pipeline designed to conduct high-throughput homology searches of structural genes using covariance models. These models are based on the alignment of sequences and the consensus of secondary structures. The pipeline is built using Snakemake, a workflow management tool that allows for the reproducible execution of analyses on various computational platforms.  \r\n\r\nThe idea for developing GERONIMO emerged from a comprehensive search for [telomerase RNA in lower plants] and was subsequently refined through an [expanded search of telomerase RNA across Insecta]. GERONIMO can test hundreds of genomes and ensures the stability and reproducibility of the analyses performed.\r\n\r\n\r\n[telomerase RNA in lower plants]: https://doi.org/10.1093/nar/gkab545\r\n[expanded search of telomerase RNA across Insecta]: https://doi.org/10.1093/nar/gkac1202\r\n\r\n## Scope\r\nThe GERONIMO tool utilises covariance models (CMs) to conduct homology searches of RNA sequences across a wide range of gene families in a broad evolutionary context. Specifically, it can be utilised to:\r\n\r\n* Detect RNA sequences that share a common evolutionary ancestor\r\n* Identify and align orthologous RNA sequences among closely related species, as well as paralogous sequences within a single species\r\n* Identify conserved non-coding RNAs in a genome, and extract upstream genomic regions to characterise potential promoter regions.  \r\nIt is important to note that GERONIMO is a computational tool, and as such, it is intended to be run on a computer with a small amount of data. Appropriate computational infrastructure is necessary for analysing hundreds of genomes.\r\n\r\nAlthough GERONIMO was primarily designed for Telomerase RNA identification, its functionality extends to include the detection and alignment of other RNA gene families, including **rRNA**, **tRNA**, **snRNA**, **miRNA**, and **lncRNA**. This can aid in identifying paralogs and orthologs across different species that may carry specific functions, making it useful for phylogenetic analyses.  \r\n\r\nIt is crucial to remember that some gene families may exhibit similar characteristics but different functions. Therefore, analysing the data and functional annotation after conducting the search is essential to characterise the sequences properly.\r\n\r\n## Pipeline overview\r\n\r\n\r\nBy default, the GERONIMO pipeline conducts high-throughput searches of homology sequences in downloaded genomes utilizing covariance models. If a significant similarity is detected between the model and genome sequence, the pipeline extracts the upstream region, making it convenient to identify the promoter of the discovered gene. In brief, the pipeline:\r\n- Compiles a list of genomes using the NCBI's [Entrez] database based on a specified query, *e.g. \"Rhodophyta\"[Organism]*\r\n- Downloads and decompresses the requested genomes using *rsync* and *gunzip*, respectively\r\n- *Optionally*, generates a covariance model based on a provided alignment using [Infernal]\r\n- Conducts searches among the genomes using the covariance model [Infernal]\r\n- Supplements genome information with taxonomy data using [rentrez]\r\n- Expands the significant hits sequence by extracting upstream genomic regions using [*blastcmd*]\r\n- Compiles the results, organizes them into a tabular format, and generates a visual summary of the performed analysis.\r\n\r\n[Entrez]: https://www.ncbi.nlm.nih.gov/books/NBK179288/\r\n[Infernal]: http://eddylab.org/infernal/\r\n[rentrez]: https://github.com/ropensci/rentrez\r\n[*blastcmd*]: https://www.ncbi.nlm.nih.gov/books/NBK569853/\r\n\r\n## Quick start\r\nThe GERONIMO is available as a `snakemake pipeline` running on Linux and Windows operating systems.\r\n\r\n### Windows 10\r\nInstal Linux on Windows 10 (WSL) according to [instructions], which bottling down to opening PowerShell or Windows Command Prompt in *administrator mode* and pasting the following:\r\n```shell\r\nwsl --install\r\nwsl.exe --install UBUNTU\r\n```\r\nThen restart the machine and follow the instructions for setting up the Linux environment.\r\n\r\n[instructions]: https://learn.microsoft.com/en-us/windows/wsl/install\r\n\r\n### Linux:\r\n#### Check whether the conda is installed:\r\n```shell\r\nconda -V\r\n```\r\n> GERONIMO was tested on conda 23.3.1\r\n#### 1) If you do not have installed `conda`, please install `miniconda`\r\nPlease follow the instructions for installing [miniconda]\r\n\r\n[miniconda]: https://conda.io/projects/conda/en/stable/user-guide/install/linux.html\r\n\r\n#### 2) Continue with installing `mamba` (recommended but optional)\r\n```shell\r\nconda install -n base -c conda-forge mamba\r\n```\r\n#### 3) Install `snakemake`\r\n```shell\r\nconda activate base\r\nmamba create -p env_snakemake -c conda-forge -c bioconda snakemake\r\nmamba activate env_snakemake\r\nsnakemake --help\r\n```\r\nIn case of complications, please check the section `Questions & Answers` below or follow the [official documentation] for troubleshooting.\r\n\r\n[official documentation]: https://snakemake.readthedocs.io/en/stable/getting_started/installation.html\r\n\r\n### Clone the GERONIMO repository\r\nGo to the path in which you want to run the analysis and clone the repository:\r\n```shell\r\ncd <PATH>\r\ngit clone https://github.com/amkilar/GERONIMO.git\r\n```\r\n\r\n### Run sample analysis to ensure GERONIMO installation was successful\r\nAll files are prepared for the sample analysis as a default. Please execute the line below:\r\n```shell\r\nsnakemake -s GERONIMO.sm --cores 1 --use-conda results/summary_table.xlsx\r\n```\r\n\r\nThis will prompt GERONIMO to quickly scan all modules, verifying the correct setup of the pipeline without executing any analysis.\r\nYou should see the message `Building DAG of jobs...`, followed by `Nothing to be done (all requested files are present and up to date).`, when successfully completed.\r\n\r\nIf you want to run the sample analysis fully, please remove the folder `results` from the GERONIMO directory and execute GERONIMO again with:\r\n\r\n`snakemake -s GERONIMO.sm --cores 1 --use-conda results/summary_table.xlsx`\r\n\r\n> You might consider allowing more cores to speed up the analysis, which might take up to several hours.\r\n\r\n#### You might want to clean `GERONIMO/` directory from the files produced by the example analysis. You can safely remove the following:\r\n- `GERONIMO/results`\r\n- `GERONIMO/database`\r\n- `GERONIMO/taxonomy`\r\n- `GERONIMO/temp`\r\n- `.create_genome_list.touch`\r\n- `list_of_genomes.txt`\r\n\r\n## Setup the inputs\r\n\r\n### 1) Prepare the `covariance models`:\r\n\r\n#### Browse the collection of available `covariance models` at [Rfam] (*You can find the covariance model in the tab `Curation`.*)  \r\nPaste the covariance model to the folder `GERONIMO/models` and ensure its name follows the convention: `cov_model_<NAME>`\r\n\r\n[Rfam]: https://rfam.org/\r\n\r\n#### **OR**\r\n\r\n#### Prepare your own `covariance model` using [LocARNA]\r\n1. Paste or upload your sequences to the web server and download the `.stk` file with the alignment result.  \r\n  \r\n    > *Please note that the `.stk` file format is crucial for the analysis, containing sequence alignment and secondary structure consensus.*\r\n    \r\n    > The LocARNA web service allows you to align 30 sequences at once - if you need to align more sequences, please use the standalone version available [here]  \r\n    > After installation run: \r\n    ```shell\r\n    mlocarna my_fasta_sequences.fasta\r\n    ```\r\n  \r\n2. Paste the `.stk` alignment file to the folder `GERONIMO/model_to_build` and ensure its name follows the convention: `<NAME>.stk`\r\n\r\n   > Please check the example `heterotrichea.stk` format in `GERONIMO/models_to_built` for reference\r\n   \r\n\r\n[LocARNA]: http://rna.informatik.uni-freiburg.de/LocARNA/Input.jsp\r\n[here]: http://www.bioinf.uni-freiburg.de/Software/LocARNA/\r\n\r\n\r\n### 2) Adjust the `config.yaml` file\r\nPlease adjust the analysis specifications, as in the following example:\r\n\r\n> - database: '<DATABASE_QUERY> [Organism]' (in case of difficulties with defining the database query, please follow the instructions below)\r\n> - extract_genomic_region-length:  <number> (here you can determine how long the upstream genomic region should be extracted; tested for 200)\r\n> - models: [\"<NAME>\", \"<NAME>\"] (here specify the names of models that should be used to perform analysis)\r\n>   \r\n>   *Here you can also insert the name of the covariance model you want to build with GERONIMO - just be sure you placed `<NAME>.stk` file in `GERONIMO/models_to_build` before starting analysis*\r\n> - CPU_for_model_building: <number> (specify the number of available CPUs devoted to the process of building model (cannot exceed the CPU number allowed to snakemake with `--cores`)\r\n>\r\n>   *You might ignore this parameter when you do not need to create a new covariance model*\r\n\r\n\r\nKeep in mind that the covariance models and alignments must be present in the respective GERONIMO folders.\r\n \r\n### 3) Remove folder `results`, which contains example analysis output\r\n### 4) **Please ensure you have enough storage capacity to download all the requested genomes (in the `GERONIMO/` directory)**\r\n\r\n## Run GERONIMO\r\n```shell\r\nmamba activate env_snakemake\r\ncd ~/GERONIMO\r\nsnakemake -s GERONIMO.sm --cores <declare number of CPUs> --use-conda results/summary_table.xlsx\r\n```\r\n  \r\n## Example results\r\n\r\n### Outputs characterisation\r\n\r\n#### A) Summary table\r\nThe Excel table contains the results arranged by taxonomy information and hit significance. The specific columns include:\r\n* family, organism_name, class, order, phylum (taxonomy context)\r\n* GCA_id - corresponds to the genome assembly in the *NCBI database*\r\n* model - describes which covariance model identified the result\r\n* label - follows the *Infernal* convention of categorizing hits\r\n* number - the counter of the result\r\n* e_value - indicates the significance level of the hit\r\n* HIT_sequence - the exact HIT sequence found by *Infernal*, which corresponds to the covariance model\r\n* HIT_ID - describes in which part of the genome assembly the hit was found, which may help publish novel sequences\r\n* extended_genomic_region - upstream sequence, which may contain a possible promoter sequence\r\n* secondary_structure - the secondary structure consensus of the covariance model\r\n\r\n\r\n#### B) Significant Hits Distribution Across Taxonomy Families\r\nThe plot provides an overview of the number of genomes in which at least one significant hit was identified, grouped by family. The bold black line corresponds to the number of genomes present in each family, helping to minimize bias regarding unequal data representation across the taxonomy.\r\n\r\n\r\n#### C) Hits Distribution in Genomes Across Families\r\nThe heatmap provides information about the most significant hits from the genome, identified by a specific covariance model. Genomes are grouped by families (on the right). Hits are classified into three categories based on their e-values. Generally, these categories correspond to hit classifications (\"HIT,\" \"MAYBE,\" \"NO HIT\"). The \"HIT\" category is further divided to distinguish between highly significant hits and moderately significant ones.\r\n\r\n\r\n\r\n### GERONIMO directory structure\r\n\r\nThe GERONIMO directory structure is designed to produce files in a highly structured manner, ensuring clear insight and facilitating the analysis of results. During a successful run, GERONIMO produces the following folders:\r\n* `/database` - which contains genome assemblies that were downloaded from the *NCBI database* and grouped in subfolders\r\n* `/taxonomy` - where taxonomy information is gathered and stored in the form of tables\r\n* `/results` - the main folder containing all produced results:\r\n  * `/infernal_raw` - contains the raw results produced by *Infernal*\r\n  * `/infernal` - contains restructured results of *Infernal* in table format\r\n  * `/cmdBLAST` - contains results of *cmdblast*, which extracts the extended genomic region\r\n  * `/summary` - contains summary files that join results from *Infernal*, *cmdblast*, and attach taxonomy context\r\n  * `/plots` - contains two types of summary plots\r\n* `/temp` - folder contains the information necessary to download genome assemblies from *NCBI database*\r\n\r\n* `/env` - stores instructions for dependency installation\r\n* `/models` - where calibrated covariance models can be pasted, *for example, from the Rfam database*\r\n* `/modes_to_built` - where multiple alignments in *.stk* format can be pasted\r\n* `/scripts` - contains developed scripts that perform results structurization\r\n\r\n#### The example GERONIMO directory structure:\r\n\r\n```shell\r\nGERONIMO\r\n\u251c\u2500\u2500 database\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000091205.1_ASM9120v1_genomic\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000341285.1_ASM34128v1_genomic\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000350225.2_ASM35022v2_genomic\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\r\n\u251c\u2500\u2500 env\r\n\u251c\u2500\u2500 models\r\n\u251c\u2500\u2500 model_to_build\r\n\u251c\u2500\u2500 results\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cmdBLAST\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 MRP\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000091205.1_ASM9120v1_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 extended\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 filtered\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000341285.1_ASM34128v1_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 extended\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 filtered\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000350225.2_ASM35022v2_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 extended\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 filtered\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 SRP\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000091205.1_ASM9120v1_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 extended\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 filtered\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000341285.1_ASM34128v1_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 extended\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 filtered\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000350225.2_ASM35022v2_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 extended\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 filtered\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 infernal\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 MRP\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000091205.1_ASM9120v1_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000341285.1_ASM34128v1_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000350225.2_ASM35022v2_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 SRP\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000091205.1_ASM9120v1_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000341285.1_ASM34128v1_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000350225.2_ASM35022v2_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 plots\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 raw_infernal\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 MRP\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000091205.1_ASM9120v1_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000341285.1_ASM34128v1_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000350225.2_ASM35022v2_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 SRP\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000091205.1_ASM9120v1_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000341285.1_ASM34128v1_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 GCA_000350225.2_ASM35022v2_genomic\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 summary\r\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 GCA_000091205.1_ASM9120v1_genomic\r\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 GCA_000341285.1_ASM34128v1_genomic\r\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 GCA_000350225.2_ASM35022v2_genomic\r\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ...\r\n\u251c\u2500\u2500 scripts\r\n\u251c\u2500\u2500 taxonomy\r\n\u2514\u2500\u2500 temp\r\n```\r\n\r\n## GERONIMO applicability\r\n\r\n### Expanding the evolutionary context\r\nTo add new genomes or database queries to an existing analysis, please follow the instructions:\r\n1) Rename the `list_of_genomes.txt` file to `previous_list_of_genomes.txt` or any other preferred name.\r\n2) Modify the `config.yaml` file by replacing the previous database query with the new one.\r\n3) Delete:\r\n   - `summary_table.xlsx`, `part_summary_table.csv`, `summary_table_models.xlsx` files located in the `GERONIMO\\results` directory\r\n   - `.create_genome_list.touch` file\r\n5) Run GERONIMO to calculate new results using the command:\r\n     ```shell\r\n     snakemake -s GERONIMO.sm --cores <declare number of CPUs> --use-conda results/summary_table.xlsx\r\n     ```\r\n7) Once the new results are generated, reviewing them before merging them with the original results is recommended.\r\n8) Copy the contents of the `previous_list_of_genomes.txt` file and paste them into the current `list_of_genomes.txt`.\r\n9) Delete:\r\n   - `summary_table.xlsx` located in the `GERONIMO\\results` directory\r\n   - `.create_genome_list.touch` file\r\n10) Run GERONIMO to merge the results from both analyses using the command:\r\n    ```shell\r\n      snakemake -s GERONIMO.sm --cores 1 --use-conda results/summary_table.xlsx\r\n    ```\r\n\r\n### Incorporating new covariance models into existing analysis\r\n1) Copy the new covariance model to `GERONIMO/models`\r\n2) Modify the `config.yaml` file by adding the name of the new model to the line `models: [...]`\r\n3) Run GERONIMO to see the updated analysis outcome\r\n\r\n### Building a new covariance model\r\nWith GERONIMO, building a new covariance model from multiple sequence alignment in the `.stk` format is possible. \r\n\r\nTo do so, simply paste `<NAME>.stk` file to `GERONIMO/models_to_build` and paste the name of the new covariance  model to `config.yaml` file to the line `models: [\"<NAME>\"]`\r\n\r\nand run GERONIMO.\r\n\r\n\r\n## Questions & Answers\r\n\r\n### How to specify the database query?\r\n- Visit the [NCBI Assemblies] website.  \r\n- Follow the instruction on the graphic below:\r\n\r\n[NCBI Assemblies]: https://www.ncbi.nlm.nih.gov/assembly/?term=\r\n\r\n### WSL: problem with creating `snakemake_env`\r\nIn the case of an error similar to the one below:\r\n> CondaError: Unable to create prefix directory '/mnt/c/Windows/system32/env_snakemake'.\r\n> Check that you have sufficient permissions.  \r\n  \r\nYou might try to delete the cache with: `rm -r ~/.cache/` and try again.\r\n\r\n### When `snakemake` does not seem to be installed properly\r\nIn the case of the following error:\r\n> Command 'snakemake' not found ...\r\n\r\nCheck whether the `env_snakemake` is activated.\r\n> It should result in a change from (base) to (env_snakemake) before your login name in the command line window.\r\n\r\nIf you still see `(base)` before your login name, please try to activate the environment with conda:\r\n`conda activate env_snakemake`\r\n\r\n\r\nPlease note that you might need to specify the full path to the `env_snakemake`, like /home/your user name/env_snakemake\r\n\r\n### How to browse GERONIMO results obtained in WSL?\r\nYou can easily access the results obtained on WSL from your Windows environment by opening `File Explorer` and pasting the following line into the search bar: `\\\\wsl.localhost\\Ubuntu\\home\\`. This will reveal a folder with your username, as specified during the configuration of your Ubuntu system. To locate the GERONIMO results, simply navigate to the folder with your username and then to the `home` folder. (`\\\\wsl.localhost\\Ubuntu\\home\\<user>\\home\\GERONIMO`)\r\n\r\n### GERONIMO occupies a lot of storage space\r\nThrough genome downloads, GERONIMO can potentially consume storage space, rapidly leading to a shortage. Currently, downloading genomes is an essential step for optimal GERONIMO performance.\r\n\r\nRegrettably, if the analysis is rerun without the `/database` folder, it will result in the need to redownload genomes, which is a highly time-consuming process.\r\n\r\nNevertheless, if you do not intend to repeat the analysis and have no requirement for additional genomes or models, you are welcome to retain your results tables and plots while removing the remaining files.\r\n\r\nIt is strongly advised against using local machines for extensive analyses. If you lack access to external storage space, it is recommended to divide the analysis into smaller segments, which can be later merged, as explained in the section titled `Expanding the evolutionary context`.\r\n\r\nConsidering this limitation, I am currently working on implementing a solution that will help circumvent the need for redundant genome downloads without compromising GERONIMO performance in the future.\r\n\r\nYou might consider deleting the `.snakemake` folder to free up storage space. However, please note that deleting this folder will require the reinstallation of GERONIMO dependencies when the analysis is rerun.\r\n\r\n## License\r\nCopyright (c) 2023 Agata M. Kilar\r\n\r\nPermission is hereby granted, free of charge, to any person obtaining a copy\r\nof this software and associated documentation files (the \"Software\"), to deal\r\nin the Software without restriction, including without limitation the rights\r\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\ncopies of the Software, and to permit persons to whom the Software is\r\nfurnished to do so, subject to the following conditions:\r\n\r\nThe above copyright notice and this permission notice shall be included in all\r\ncopies or substantial portions of the Software.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\nSOFTWARE.\r\n\r\n## Contact\r\nmgr in\u017c. Agata Magdalena Kilar, PhD (agata.kilar@ceitec.muni.cz)\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.547.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "547",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/547?version=1",
        "name": "GERONIMO",
        "number_of_steps": 0,
        "projects": [
            "Mendel Centre for Plant Genomics and Proteomics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "snakemake",
            "rna"
        ],
        "tools": [
            "Infernal",
            "BLAST"
        ],
        "type": "Snakemake",
        "update_time": "2023-08-03",
        "versions": 1
    },
    {
        "create_time": "2023-08-02",
        "creators": [
            "Nachida Tadrent",
            "Franck Dedeine",
            "Vincent Herv\u00e9"
        ],
        "description": "[![Snakemake](https://img.shields.io/badge/snakemake-\u22657.0.0-brightgreen.svg?style=flat)](https://snakemake.readthedocs.io)\r\n\r\n\r\n# About SnakeMAGs\r\nSnakeMAGs is a workflow to reconstruct prokaryotic genomes from metagenomes. The main purpose of SnakeMAGs is to process Illumina data from raw reads to metagenome-assembled genomes (MAGs).\r\nSnakeMAGs is efficient, easy to handle and flexible to different projects. The workflow is CeCILL licensed, implemented in Snakemake (run on multiple cores) and available for Linux.\r\nSnakeMAGs performed eight main steps:\r\n- Quality filtering of the reads\r\n- Adapter trimming\r\n- Filtering of the host sequences (optional)\r\n- Assembly\r\n- Binning\r\n- Evaluation of the quality of the bins\r\n- Classification of the MAGs\r\n- Estimation of the relative abundance of the MAGs\r\n\r\n\r\n![scheme of workflow](SnakeMAGs_schema.jpg?raw=true)\r\n\r\n# How to use SnakeMAGs\r\n## Install conda\r\nThe easiest way to install and run SnakeMAGs is to use [conda](https://www.anaconda.com/products/distribution). These package managers will help you to easily install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html).\r\n\r\n## Install and activate Snakemake environment\r\nNote: The workflow was developed with Snakemake 7.0.0\r\n```\r\nconda activate\r\n\r\n# First, set up your channel priorities\r\nconda config --add channels defaults\r\nconda config --add channels bioconda\r\nconda config --add channels conda-forge\r\n\r\n# Then, create a new environment for the Snakemake version you require\r\nconda create -n snakemake_7.0.0 snakemake=7.0.0\r\n\r\n# And activate it\r\nconda activate snakemake_7.0.0\r\n```\r\n\r\nAlternatively, you can also install Snakemake via mamba:\r\n```\r\n# If you do not have mamba yet on your machine, you can install it with:\r\nconda install -n base -c conda-forge mamba\r\n\r\n# Then you can install Snakemake\r\nconda activate base\r\nmamba create -c conda-forge -c bioconda -n snakemake snakemake\r\n\r\n# And activate it\r\nconda activate snakemake\r\n\r\n```\r\n\r\n## SnakeMAGs executable\r\nThe easiest way to procure SnakeMAGs and its related files is to clone the repository using git:\r\n```\r\ngit clone https://github.com/Nachida08/SnakeMAGs.git\r\n```\r\nAlternatively, you can download the relevant files:\r\n```\r\nwget https://github.com/Nachida08/SnakeMAGs/blob/main/SnakeMAGs.smk https://github.com/Nachida08/SnakeMAGs/blob/main/config.yaml\r\n```\r\n\r\n## SnakeMAGs input files\r\n- Illumina paired-end reads in FASTQ.\r\n- Adapter sequence file ([adapter.fa](https://github.com/Nachida08/SnakeMAGs/blob/main/adapters.fa)).\r\n- Host genome sequences in FASTA (if host_genome: \"yes\"), in case you work with host-associated metagenomes (e.g. human gut metagenome).\r\n\r\n## Download Genome Taxonomy Database (GTDB)\r\nGTDB-Tk requires ~66G+ of external data (GTDB) that need to be downloaded and unarchived. Because this database is voluminous, we let you decide where you want to store it.\r\nSnakeMAGs do not download automatically GTDB, you have to do it:\r\n\r\n```\r\n#Download the latest release (tested with release207)\r\n#Note: SnakeMAGs uses GTDBtk v2.1.0 and therefore require release 207 as minimum version. See https://ecogenomics.github.io/GTDBTk/installing/index.html#installing for details.\r\nwget https://data.gtdb.ecogenomic.org/releases/latest/auxillary_files/gtdbtk_v2_data.tar.gz\r\n#Decompress\r\ntar -xzvf *tar.gz\r\n#This will create a folder called release207_v2\r\n```\r\nAll you have to do now is to indicate the path to the database folder (in our example, the folder is called release207_v2) in the config file, Classification section.\r\n\r\n## Download the GUNC database (required if gunc: \"yes\")\r\nGUNC accepts either a progenomes or GTDB based reference database. Both can be downloaded using the ```gunc download_db``` command. For our study we used the default proGenome-derived GUNC database. It requires less resources with similar performance.\r\n\r\n```\r\nconda activate\r\n# Install and activate GUNC environment\r\nconda create --prefix /path/to/gunc_env\r\nconda install -c bioconda metabat2 --prefix /path/to/gunc_env\r\nsource activate /path/to/gunc_env\r\n\r\n#Download the proGenome-derived GUNC database (tested with gunc_db_progenomes2.1)\r\n#Note: SnakeMAGs uses GUNC v1.0.5\r\ngunc download_db -db progenomes /path/to/GUNC_DB\r\n```\r\nAll you have to do now is to indicate the path to the GUNC database file in the config file,  Bins quality section.\r\n\r\n## Edit config file\r\nYou need to edit the config.yaml file. In particular, you need to set the correct paths: for the working directory, to specify where are your fastq files, where you want to place the conda environments (that will be created using the provided .yaml files available in [SnakeMAGs_conda_env directory](https://github.com/Nachida08/SnakeMAGs/tree/main/SnakeMAGs_conda_env)), where are the adapters, where is GTDB and optionally where is the GUNC database and where is your host genome reference.\r\n\r\nLastly, you need to allocate the proper computational resources (threads, memory) for each of the main steps. These can be optimized according to your hardware.\r\n\r\n\r\n\r\nHere is an example of a config file:\r\n\r\n```\r\n#####################################################################################################\r\n#####  _____    ___    _              _   _    ______   __    __              _______   _____   #####\r\n##### /  ___|  |   \\  | |     /\\     | | / /  |  ____| |  \\  /  |     /\\     /  _____| /  ___|  #####\r\n##### | (___   | |\\ \\ | |    /  \\    | |/ /   | |____  |   \\/   |    /  \\    | |   __  | (___   #####\r\n#####  \\___ \\  | | \\ \\| |   / /\\ \\   | |\\ \\   |  ____| | |\\  /| |   / /\\ \\   | |  |_ |  \\___ \\  #####\r\n#####  ____) | | |  \\   |  / /__\\ \\  | | \\ \\  | |____  | | \\/ | |  / /__\\ \\  | |____||  ____) | #####\r\n##### |_____/  |_|   \\__| /_/    \\_\\ |_|  \\_\\ |______| |_|    |_| /_/    \\_\\  \\______/ |_____/  #####\r\n#####                                                                                           #####\r\n#####################################################################################################\r\n\r\n############################\r\n### Execution parameters ###\r\n############################\r\n\r\nworking_dir: /path/to/working/directory/                                 #The main directory for the project\r\nraw_fastq: /path/to/raw_fastq/                                           #The directory that contains all the fastq files of all the samples (eg. sample1_R1.fastq & sample1_R2.fastq, sample2_R1.fastq & sample2_R2.fastq...)\r\nsuffix_1: \"_R1.fastq\"                                                    #Main type of suffix for forward reads file (eg. _1.fastq or _R1.fastq or _r1.fastq or _1.fq or _R1.fq or _r1.fq )\r\nsuffix_2: \"_R2.fastq\"                                                    #Main type of suffix for reverse reads file (eg. _2.fastq or _R2.fastq or _r2.fastq or _2.fq or _R2.fq or _r2.fq )\r\n\r\n###########################\r\n### Conda environnemnts ###\r\n###########################\r\n\r\nconda_env: \"/path/to/SnakeMAGs_conda_env/\"                               #Path to the provided SnakeMAGs_conda_env directory which contains the yaml file for each conda environment\r\n\r\n#########################\r\n### Quality filtering ###\r\n#########################\r\nemail: name.surname@your-univ.com                                        #Your e-mail address\r\nthreads_filter: 10                                                       #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_filter: 150                                                    #Memory according to tools need (in GB)\r\n\r\n########################\r\n### Adapter trimming ###\r\n########################\r\nadapters: /path/to/working/directory/adapters.fa                         #A fasta file contanning a set of various Illumina adaptors (this file is provided and is also available on github)\r\ntrim_params: \"2:40:15\"                                                   #For further details, see the Trimmomatic documentation\r\nthreads_trim: 10                                                         #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_trim: 150                                                      #Memory according to tools need (in GB)\r\n\r\n######################\r\n### Host filtering ###\r\n######################\r\nhost_genome: \"yes\"                                                      #yes or no. An optional step for host-associated samples (eg. termite, human, plant...)\r\nthreads_bowtie2: 50                                                     #The number of threads to run this process. To be adjusted according to your hardware\r\nhost_genomes_directory: /path/to/working/host_genomes/                  #the directory where the host genome is stored\r\nhost_genomes: /path/to/working/host_genomes/host_genomes.fa             #A fasta file containing the DNA sequences of the host genome(s)\r\nthreads_samtools: 50                                                    #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_host_filtering: 150                                           #Memory according to tools need (in GB)\r\n\r\n################\r\n### Assembly ###\r\n################\r\nthreads_megahit: 50                                                    #The number of threads to run this process. To be adjusted according to your hardware\r\nmin_contig_len: 1000                                                   #Minimum length (in bp) of the assembled contigs\r\nk_list: \"21,31,41,51,61,71,81,91,99,109,119\"                           #Kmer size (for further details, see the megahit documentation)\r\nresources_megahit: 250                                                 #Memory according to tools need (in GB)\r\n\r\n###############\r\n### Binning ###\r\n###############\r\nthreads_bwa: 50                                                        #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_bwa: 150                                                     #Memory according to tools need (in GB)\r\nthreads_samtools: 50                                                   #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_samtools: 150                                                #Memory according to tools need (in GB)\r\nseed: 19860615                                                         #Seed number for reproducible results\r\nthreads_metabat: 50                                                    #The number of threads to run this process. To be adjusted according to your hardware\r\nminContig: 2500                                                        #Minimum length (in bp) of the contigs\r\nresources_binning: 250                                                 #Memory according to tools need (in GB)\r\n\r\n####################\r\n### Bins quality ###\r\n####################\r\n#checkM\r\nthreads_checkm: 50                                                    #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_checkm: 250                                                 #Memory according to tools need (in GB)\r\n#bins_quality_filtering\r\ncompletion: 50                                                        #The minimum completion rate of bins\r\ncontamination: 10                                                     #The maximum contamination rate of bins\r\nparks_quality_score: \"yes\"                                            #yes or no. If yes bins are filtered according to the Parks quality score (completion-5*contamination >= 50)\r\n#GUNC\r\ngunc: \"yes\"                                                           #yes or no. An optional step to detect and discard chimeric and contaminated genomes using the GUNC tool\r\nthreads_gunc: 50                                                      #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_gunc: 250                                                   #Memory according to tools need (in GB)\r\nGUNC_db: /path/to/GUNC_DB/gunc_db_progenomes2.1.dmnd                  #Path to the downloaded GUNC database (see the readme file)\r\n\r\n######################\r\n### Classification ###\r\n######################\r\nGTDB_data_ref: /path/to/downloaded/GTDB                                #Path to uncompressed GTDB-Tk reference data (GTDB)\r\nthreads_gtdb: 10                                                       #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_gtdb: 250                                                    #Memory according to tools need (in GB)\r\n\r\n##################\r\n### Abundances ###\r\n##################\r\nthreads_coverM: 10                                                     #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_coverM: 150                                                  #Memory according to tools need (in GB)\r\n```\r\n# Run SnakeMAGs\r\nIf you are using a workstation with Ubuntu (tested on Ubuntu 22.04):\r\n```{bash}\r\nsnakemake --cores 30 --snakefile SnakeMAGs.smk --use-conda --conda-prefix /path/to/SnakeMAGs_conda_env/ --configfile /path/to/config.yaml --keep-going --latency-wait 180\r\n```\r\n\r\nIf you are working on a cluster with Slurm (tested with version 18.08.7):\r\n```{bash}\r\nsnakemake --snakefile SnakeMAGs.smk --cluster 'sbatch -p <cluster_partition> --mem <memory> -c <cores> -o \"cluster_logs/{wildcards}.{rule}.{jobid}.out\" -e \"cluster_logs/{wildcards}.{rule}.{jobid}.err\" ' --jobs <nbr_of_parallel_jobs> --use-conda --conda-frontend conda --conda-prefix /path/to/SnakeMAGs_conda_env/ --jobname \"{rule}.{wildcards}.{jobid}\" --latency-wait 180 --configfile /path/to/config.yaml --keep-going\r\n```\r\n\r\nIf you are working on a cluster with SGE (tested with version 8.1.9):\r\n```{bash}\r\nsnakemake --snakefile SnakeMAGs.smk --cluster \"qsub -cwd -V -q <short.q/long.q> -pe thread {threads} -e cluster_logs/{rule}.e{jobid} -o cluster_logs/{rule}.o{jobid}\" --jobs <nbr_of_parallel_jobs> --use-conda --conda-frontend conda --conda-prefix /path/to/SnakeMAGs_conda_env/ --jobname \"{rule}.{wildcards}.{jobid}\" --latency-wait 180 --configfile /path/to/config.yaml --keep-going\r\n```\r\n\r\n\r\n# Test\r\nWe provide you a small data set in the [test](https://github.com/Nachida08/SnakeMAGs/tree/main/test) directory which will allow you to validate your instalation and take your first steps with SnakeMAGs. This data set is a subset from [ZymoBiomics Mock Community](https://www.zymoresearch.com/blogs/blog/zymobiomics-microbial-standards-optimize-your-microbiomics-workflow) (250K reads) used in this tutoriel [metagenomics_tutorial](https://github.com/pjtorres/metagenomics_tutorial).\r\n\r\n1. Before getting started make sure you have cloned the SnakeMAGs repository or you have downloaded all the necessary files (SnakeMAGs.smk, config.yaml, chr19.fa.gz, insub732_2_R1.fastq.gz, insub732_2_R2.fastq.gz). See the [SnakeMAGs executable](#snakemags-executable) section.\r\n2. Unzip the fastq files and the host sequences file.\r\n```\r\ngunzip fastqs/insub732_2_R1.fastq.gz fastqs/insub732_2_R2.fastq.gz host_genomes/chr19.fa.gz\r\n```\r\n3. For better organisation put all the read files in the same directory (eg. fastqs) and the host sequences file in a separate directory (eg. host_genomes)\r\n4. Edit the config file (see [Edit config file](#edit-config-file) section)\r\n5. Run the test (see [Run SnakeMAGs](#run-snakemags) section)\r\n\r\nNote: the analysis of these files took 1159.32 secondes to complete on a Ubuntu 22.04 LTS with an Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz x 40 processor, 96GB of RAM.\r\n\r\n# Genome reference for host reads filtering\r\nFor host-associated samples, one can remove host sequences from the metagenomic reads by mapping these reads against a reference genome. In the case of termite gut metagenomes, we are providing [here](https://zenodo.org/record/6908287#.YuAdFXZBx8M) the relevant files (fasta and index files) from termite genomes.\r\n\r\nUpon request, we can help you to generate these files for your own reference genome and make them available to the community.\r\n\r\nNB. These steps of mapping generate voluminous files such as .bam and .sam. Depending on your disk space, you might want to delete these files after use.\r\n\r\n\r\n# Use case\r\nDuring the test phase of the development of SnakeMAGs, we used this workflow to process 10 publicly available termite gut metagenomes generated by Illumina sequencing, to ultimately reconstruct prokaryotic MAGs. These metagenomes were retrieved from the NCBI database using the following accession numbers: SRR10402454; SRR14739927; SRR8296321; SRR8296327; SRR8296329; SRR8296337; SRR8296343; DRR097505; SRR7466794; SRR7466795. They come from five different studies: Waidele et al, 2019; Tokuda et al, 2018; Romero Victorica et al, 2020; Moreira et al, 2021; and Calusinska et al, 2020.\r\n\r\n## Download the Illumina pair-end reads\r\nWe use fasterq-dump tool to extract data in FASTQ-format from SRA-accessions. It is a commandline-tool which offers a faster solution for downloading those large files.\r\n\r\n```\r\n# Install and activate sra-tools environment\r\n## Note: For this study we used sra-tools 2.11.0\r\n\r\nconda activate\r\nconda install -c bioconda sra-tools\r\nconda activate sra-tools\r\n\r\n# Download fastqs in a single directory\r\nmkdir raw_fastq\r\ncd raw_fastq\r\nfasterq-dump <SRA-accession> --threads <threads_nbr> --skip-technical --split-3\r\n```\r\n\r\n## Download Genome reference for host reads filtering\r\n```\r\nmkdir host_genomes\r\ncd host_genomes\r\nwget https://zenodo.org/record/6908287/files/termite_genomes.fasta.gz\r\ngunzip termite_genomes.fasta.gz\r\n```\r\n\r\n## Edit the config file\r\nSee [Edit config file](#edit-config-file) section.\r\n\r\n## Run SnakeMAGs\r\n```\r\nconda activate snakemake_7.0.0\r\nmkdir cluster_logs\r\nsnakemake --snakefile SnakeMAGs.smk --cluster 'sbatch -p <cluster_partition> --mem <memory> -c <cores> -o \"cluster_logs/{wildcards}.{rule}.{jobid}.out\" -e \"cluster_logs/{wildcards}.{rule}.{jobid}.err\" ' --jobs <nbr_of_parallel_jobs> --use-conda --conda-frontend conda --conda-prefix /path/to/SnakeMAGs_conda_env/ --jobname \"{rule}.{wildcards}.{jobid}\" --latency-wait 180 --configfile /path/to/config.yaml --keep-going\r\n```\r\n\r\n## Study results\r\nThe MAGs reconstructed from each metagenome and their taxonomic classification are available in this [repository](https://doi.org/10.5281/zenodo.7661004).\r\n\r\n# Citations\r\n\r\nIf you use SnakeMAGs, please cite:\r\n> Tadrent N, Dedeine F and Herv\u00e9 V. SnakeMAGs: a simple, efficient, flexible and scalable workflow to reconstruct prokaryotic genomes from metagenomes [version 2; peer review: 2 approved]. F1000Research 2023, 11:1522 (https://doi.org/10.12688/f1000research.128091.2)\r\n\r\n\r\nPlease also cite the dependencies:\r\n- [Snakemake](https://doi.org/10.12688/f1000research.29032.2) : M\u00f6lder, F., Jablonski, K. P., Letcher, B., Hall, M. B., Tomkins-tinch, C. H., Sochat, V., Forster, J., Lee, S., Twardziok, S. O., Kanitz, A., Wilm, A., Holtgrewe, M., Rahmann, S., Nahnsen, S., & K\u00f6ster, J. (2021) Sustainable data analysis with Snakemake [version 2; peer review: 2 approved]. *F1000Research* 2021, 10:33.\r\n- [illumina-utils](https://doi.org/10.1371/journal.pone.0066643) : Murat Eren, A., Vineis, J. H., Morrison, H. G., & Sogin, M. L. (2013). A Filtering Method to Generate High Quality Short Reads Using Illumina Paired-End Technology. *PloS ONE*, 8(6), e66643.\r\n- [Trimmomatic](https://doi.org/10.1093/bioinformatics/btu170) : Bolger, A. M., Lohse, M., & Usadel, B. (2014). Genome analysis Trimmomatic: a flexible trimmer for Illumina sequence data. *Bioinformatics*, 30(15), 2114-2120.\r\n- [Bowtie2](https://doi.org/10.1038/nmeth.1923) : Langmead, B., & Salzberg, S. L. (2012). Fast gapped-read alignment with Bowtie 2. *Nature Methods*, 9(4), 357\u2013359.\r\n- [SAMtools](https://doi.org/10.1093/bioinformatics/btp352) : Li, H., Handsaker, B., Wysoker, A., Fennell, T., Ruan, J., Homer, N., Marth, G., Abecasis, G., & Durbin, R. (2009). The Sequence Alignment/Map format and SAMtools. *Bioinformatics*, 25(16), 2078\u20132079.\r\n- [BEDtools](https://doi.org/10.1093/bioinformatics/btq033) : Quinlan, A. R., & Hall, I. M. (2010). BEDTools: A flexible suite of utilities for comparing genomic features. *Bioinformatics*, 26(6), 841\u2013842.\r\n- [MEGAHIT](https://doi.org/10.1093/bioinformatics/btv033) : Li, D., Liu, C. M., Luo, R., Sadakane, K., & Lam, T. W. (2015). MEGAHIT: An ultra-fast single-node solution for large and complex metagenomics assembly via succinct de Bruijn graph. *Bioinformatics*, 31(10), 1674\u20131676.\r\n- [bwa](https://doi.org/10.1093/bioinformatics/btp324) : Li, H., & Durbin, R. (2009). Fast and accurate short read alignment with Burrows-Wheeler transform. *Bioinformatics*, 25(14), 1754\u20131760.\r\n- [MetaBAT2](https://doi.org/10.7717/peerj.7359) : Kang, D. D., Li, F., Kirton, E., Thomas, A., Egan, R., An, H., & Wang, Z. (2019). MetaBAT 2: An adaptive binning algorithm for robust and efficient genome reconstruction from metagenome assemblies. *PeerJ*, 2019(7), 1\u201313.\r\n- [CheckM](https://doi.org/10.1101/gr.186072.114) : Parks, D. H., Imelfort, M., Skennerton, C. T., Hugenholtz, P., & Tyson, G. W. (2015). CheckM: Assessing the quality of microbial genomes recovered from isolates, single cells, and metagenomes. *Genome Research*, 25(7), 1043\u20131055.\r\n- [GTDB-Tk](https://doi.org/10.1093/BIOINFORMATICS/BTAC672) : Chaumeil, P.-A., Mussig, A. J., Hugenholtz, P., Parks, D. H. (2022). GTDB-Tk v2: memory friendly classification with the genome taxonomy database. *Bioinformatics*.\r\n- [CoverM](https://github.com/wwood/CoverM)\r\n- [Waidele et al, 2019](https://doi.org/10.1101/526038) : Waidele, L., Korb, J., Voolstra, C. R., Dedeine, F., & Staubach, F. (2019). Ecological specificity of the metagenome in a set of lower termite species supports contribution of the microbiome to adaptation of the host. *Animal Microbiome*, 1(1), 1\u201313.\r\n- [Tokuda et al, 2018](https://doi.org/10.1073/pnas.1810550115) : Tokuda, G., Mikaelyan, A., Fukui, C., Matsuura, Y., Watanabe, H., Fujishima, M., & Brune, A. (2018). Fiber-associated spirochetes are major agents of hemicellulose degradation in the hindgut of wood-feeding higher termites. *Proceedings of the National Academy of Sciences of the United States of America*, 115(51), E11996\u2013E12004.\r\n- [Romero Victorica et al, 2020](https://doi.org/10.1038/s41598-020-60850-5) : Romero Victorica, M., Soria, M. A., Batista-Garc\u00eda, R. A., Ceja-Navarro, J. A., Vikram, S., Ortiz, M., Onta\u00f1on, O., Ghio, S., Mart\u00ednez-\u00c1vila, L., Quintero Garc\u00eda, O. J., Etcheverry, C., Campos, E., Cowan, D., Arneodo, J., & Talia, P. M. (2020). Neotropical termite microbiomes as sources of novel plant cell wall degrading enzymes. *Scientific Reports*, 10(1), 1\u201314.\r\n- [Moreira et al, 2021](https://doi.org/10.3389/fevo.2021.632590) : Moreira, E. A., Persinoti, G. F., Menezes, L. R., Paix\u00e3o, D. A. A., Alvarez, T. M., Cairo, J. P. L. F., Squina, F. M., Costa-Leonardo, A. M., Rodrigues, A., Sillam-Duss\u00e8s, D., & Arab, A. (2021). Complementary contribution of Fungi and Bacteria to lignocellulose digestion in the food stored by a neotropical higher termite. *Frontiers in Ecology and Evolution*, 9(April), 1\u201312.\r\n- [Calusinska et al, 2020](https://doi.org/10.1038/s42003-020-1004-3) : Calusinska, M., Marynowska, M., Bertucci, M., Untereiner, B., Klimek, D., Goux, X., Sillam-Duss\u00e8s, D., Gawron, P., Halder, R., Wilmes, P., Ferrer, P., Gerin, P., Roisin, Y., & Delfosse, P. (2020). Integrative omics analysis of the termite gut system adaptation to Miscanthus diet identifies lignocellulose degradation enzymes. *Communications Biology*, 3(1), 1\u201312.\r\n- [Orakov et al, 2021](https://doi.org/10.1186/s13059-021-02393-0) : Orakov, A., Fullam, A., Coelho, L. P., Khedkar, S., Szklarczyk, D., Mende, D. R., Schmidt, T. S. B., & Bork, P. (2021). GUNC: detection of chimerism and contamination in prokaryotic genomes. *Genome Biology*, 22(1).\r\n- [Parks et al, 2015](https://doi.org/10.1101/gr.186072.114) : Parks, D. H., Imelfort, M., Skennerton, C. T., Hugenholtz, P., & Tyson, G. W. (2015). CheckM: Assessing the quality of microbial genomes recovered from isolates, single cells, and metagenomes. *Genome Research*, 25(7), 1043\u20131055.\r\n# License\r\nThis project is licensed under the CeCILL License - see the [LICENSE](https://github.com/Nachida08/SnakeMAGs/blob/main/LICENCE) file for details.\r\n\r\nDeveloped by Nachida Tadrent at the Insect Biology Research Institute ([IRBI](https://irbi.univ-tours.fr/)), under the supervision of Franck Dedeine and Vincent Herv\u00e9.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "554",
        "keep": true,
        "latest_version": 1,
        "license": "CECILL-2.1",
        "link": "https:/workflowhub.eu/workflows/554?version=1",
        "name": "SnakeMAGs: a simple, efficient, flexible and scalable workflow to reconstruct prokaryotic genomes from metagenomes",
        "number_of_steps": 0,
        "projects": [
            "Metagenomic tools"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "mag",
            "metagenomics",
            "binning"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2023-08-02",
        "versions": 1
    },
    {
        "create_time": "2023-08-02",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# Macromolecular Coarse-Grained Flexibility (FlexServ) tutorial using BioExcel Building Blocks (biobb)\r\n\r\nThis tutorial aims to illustrate the process of generating protein conformational ensembles from 3D structures and analysing its molecular flexibility, step by step, using the BioExcel Building Blocks library (biobb).\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.552.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "552",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/552?version=1",
        "name": "CWL Macromolecular Coarse-Grained Flexibility tutorial",
        "number_of_steps": 29,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Compress MD simulation trajectories with PCA suite",
            "Run Brownian Dynamics from FlexServ",
            "Extract residue bfactors x PCA mode from a compressed PCZ file",
            "Extract PCA collectivity (numerical measure of how many atoms are affected by a given mode) from a compressed PCZ file",
            "Wrapper of the Ambertools Cpptraj module for converting between cpptraj compatible trajectory file formats and/or extracting a selection of atoms or frames.",
            "Class to extract atoms from a 3D structure.",
            "Extract PCA stiffness from a compressed PCZ file",
            "Run Normal Mode Analysis from FlexServ",
            "Uncompress MD simulation trajectories with PCA suite",
            "Extract PCA info (variance, Dimensionality) from a compressed PCZ file",
            "Compute PCA similarity between two given compressed PCZ files",
            "Compute possible hinge regions (residues around which large protein movements are organized) of a molecule from a compressed PCZ file",
            "Wrapper of the Ambertools Cpptraj module for calculating the Root Mean Square deviation (RMSd) of a given cpptraj compatible trajectory.",
            "Run Discrete Molecular Dynamics from FlexServ",
            "Extract PCA Eigen Vectors from a compressed PCZ file",
            "Extract PCA animations from a compressed PCZ file"
        ],
        "type": "Common Workflow Language",
        "update_time": "2023-08-02",
        "versions": 1
    },
    {
        "create_time": "2023-08-01",
        "creators": [
            "Mahnoor Zulfiqar",
            "Michael R. Crusoe",
            "Luiz Gadelha",
            "Christoph  Steinbeck",
            "Maria Sorokina",
            "Kristian Peters"
        ],
        "description": "\r\nThis repository hosts Metabolome Annotation Workflow (MAW). The workflow takes MS2 .mzML format data files as an input in R. It performs spectral database dereplication using R Package Spectra and compound database dereplication using SIRIUS OR MetFrag . Final candidate selection is done in Python using RDKit and PubChemPy.",
        "doi": "10.48546/workflowhub.workflow.510.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in tags",
        "id": "510",
        "keep": true,
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/510?version=2",
        "name": "Metabolome Annotation Workflow (MAW)",
        "number_of_steps": 1,
        "projects": [
            "Metabolomics-Reproducibility"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "bioinformatics",
            "cheminformatics",
            "fair workflows",
            "metabolomics",
            "gnps",
            "hmdb",
            "identification",
            "mass-spectrometry",
            "massbank",
            "rdkit",
            "spectra",
            "workflow"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-08-01",
        "versions": 2
    },
    {
        "create_time": "2023-07-31",
        "creators": [
            "Anand Maurya",
            "Maciej Szymanski",
            "Wojciech Karlowski"
        ],
        "description": "## ARA (Automated Record Analysis) : An automatic pipeline for exploration of SRA datasets with sequences as a query\r\n\r\n### Requirements\r\n\r\n- **Docker**\r\n\r\n  - Please checkout the [Docker installation](https://docs.docker.com/get-docker/) guide.\r\n\r\n    _or_\r\n\r\n- **Mamba package manager**\r\n\r\n  - Please checkout the [mamba or micromamba](https://mamba.readthedocs.io/en/latest/installation.html) official installation guide.\r\n\r\n  - We prefer `mamba` over [`conda`](https://docs.conda.io/en/latest/) since it is faster and uses `libsolv` to effectively resolve the dependencies.\r\n\r\n  - `conda` can still be used to install the pipeline using the same commands as described in the installation section.\r\n\r\n    > Note: **It is important to include the 'bioconda' channel in addition to the other channels as indicated in the [official manual](https://bioconda.github.io/#usage \"Bioconda - Usage\")**. Use the following commands in the given order to configure the channels (one-time setup).\r\n    >\r\n    > ```bash\r\n    > conda config --add channels defaults\r\n    > conda config --add channels bioconda\r\n    > conda config --add channels conda-forge\r\n    > conda config --set channel_priority strict\r\n    > ```\r\n\r\n---\r\n\r\n### Installation\r\n\r\nThe user can install the pipeline by using either Docker or Mamba using the steps mentioned below.\r\n\r\nFirst, click the green \"Code\" button, then select \"Download Zip\" to begin downloading the contents of this repository. Once the download is complete, extract the zip file by into the desired location before starting the setup. Please use the commands shown below to begin installing the pipeline.\r\n\r\nAlternatively, the github repo can also be cloned through the options shown after clicking the \"Code\" button. Navigate inside the folder after by using the `cd ARA/` command before starting the setup.\r\n\r\n> _Warning: Before starting any analysis with the pipeline, please make sure that the system has enough disk space available for the data you wish to retrieve and process from the SRA repository._\r\n\r\n- **Using Docker**\r\n\r\n  ```bash\r\n  cd ARA-main/\r\n  docker build -t ara_img .\r\n  ```\r\n\r\n_or_\r\n\r\n- **Using Mamba**\r\n\r\n  ```bash\r\n  cd ARA-main/\r\n  mamba env create --file requirements.yaml\r\n  mamba activate ara_env\r\n  perl setup.pl\r\n  ```\r\n\r\n  > _Note: After installation, the virtual environment consumes approximately 1.5 GB of disk space. The installation was tested on \"Ubuntu 20.04.4 LTS\", \"Ubuntu 22.04.1 LTS\" and \"Fedora 37\" using the procedure mentioned above._\r\n\r\nPlease be patient because downloading and configuring the tools/modules may take several minutes. The warning messages that appear during the installation of certain Perl modules can be ignored by users.\r\n\r\nOptional: The user can also add the current directory to PATH for ease of use. Use the `chmod +x ara.pl` followed by `export PATH=\"$(pwd):$PATH\"` command. Alternatively, the user is free to create symbolic, copy the executable to `/bin/`, or use any other method depending on their operating system.\r\n\r\nRefer the 'Troubleshooting' section in case of any installation related issues.\r\n\r\n---\r\n\r\n### Example usage\r\n\r\n- **Docker**\r\n\r\n  `docker run -it ara_img /home/ARA-main/ara.pl --input /home/ARA-main/example/SraRunInfo.csv --sequences /home/ARA-main/example/Arabidopsis_thaliana.TAIR10.ncrna.fa`\r\n\r\n- **Mamba environment**\r\n\r\n  `perl ara.pl --input example/SraRunInfo.csv --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa`\r\n\r\nTo get full usage info: `perl ara.pl --help`\r\n\r\n> _Note_: The user can delete the contents of `results/` directory after testing the tool using the example mentioned above.\r\n\r\n### Configuration file\r\n\r\nThe configuration file `conf.txt` is automatically generated during the installation by setup script. It contains certain default parameters as well as the location to the executable binaries of the tools incorporated in the pipeline.\r\n\r\nThe user can modify the default parameters in `conf.txt` and pass it to the pipeline as an input. For example, the `data_perc` option in the configuration refers to the default value of 5% of the dataset selected for analysis. However, the user has the flexibility to provide any integer value between 1 and 100 to specify the desired percentage of the dataset to be used.\r\n\r\nSimilarly, the user can choose between _blastn_ or _bowtie2_ by changing the 'execute flag' to either 0 or 1 in the configuration file while leaving the rest of the parameters to default values. By default, both the tools are enabled _ie_. `execute = 1`.\r\n\r\nThe `read_drop_perc_cutoff` in `conf.txt` config file denotes the cutoff to discard a sample if the total reads left after executing the trimmomatic are higher than the threshold (by default, if the more than 70% of reads are dropped as per the trimmomatic log, then the sample will fail the quality criteria and will not be processed downstream). Please refer the documentation of [Trimmomatic ](https://github.com/usadellab/Trimmomatic) for more details about the parameters present in the config file.\r\n\r\nSimilarly, the criteria to check the minimal alignment rate are indicated by the `alignment perc cutoff` parameter under blastn and bowtie2 in the `conf.txt` configuration file (if the total alignment percentage is less than the threshold then the pipeline will report that the sample failed the quality criteria). More details about the parameters used in the `conf.txt` file can be found in the respective documentations of [Blastn](https://www.ncbi.nlm.nih.gov/books/NBK279690/) and [Bowtie2](https://bowtie-bio.sourceforge.net/bowtie2/manual.shtml).\r\n\r\nBy default, the pipeline uses a pre-built Kraken2 viral genomic database ([release: 9/8/2022](https://genome-idx.s3.amazonaws.com/kraken/k2_viral_20220908.tar.gz)) from <https://benlangmead.github.io/aws-indexes/k2>. Users can provide their own database by changing the `kraken2_db_path` parameter in the `conf.txt` file.\r\n\r\n> _Note:_ If the user wishes to use a different installation than Bioconda, the user can manually install the required tools and specify the absolute path of the executable binaries in the configuration.\r\n\r\n---\r\n\r\n### Pipeline parameters\r\n\r\n- **`--input`** (mandatory) The user can provide input in either of the following ways:\r\n\r\n  - A single SRA run accession. eg: **`perl ara.pl --input SRR12548227 --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa`**\r\n\r\n  - A list of run accessions in a text file (1 run accession per line). eg: **`perl ara.pl --input example/list.txt --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa`**\r\n\r\n  - The SRA runInfo exported directly from the NCBI-SRA web portal. Goto the [SRA homepage](https://www.ncbi.nlm.nih.gov/sra \"Home - NCBI - SRA\") and search for the desired keyword. Export the `SraRunInfo.csv` by clicking 'Send to' =\\> File =\\> RunInfo). eg: **`perl ara.pl --input example/SraRunInfo.csv --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa`**\r\n\r\n- **`--sequences`** (mandatory) The user should provide a fasta file containing the query sequences.\r\n\r\n- **`--output`** (optional) The output directory to store the results. By default, the output will be stored into the **`results/`** directory of the package. eg: **`perl ara.pl --input example/SraRunInfo.csv --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa --output /src/main/test/`**\r\n\r\n- **`--mode`** (optional) Choose one of the three modes to run the pipeline.\r\n\r\n  - The **`screen`** is the default mode which will only download a fraction of the data-set per SRA-run accession and analyse the file as per the given configuration.\r\n\r\n  - The **`full`** mode will execute the pipeline by downloading the complete fastq file per SRA-run accession.\r\n\r\n  - The **`both`** option searches for samples using a fraction of the data that meet the minimum alignment cutoff from either 'bowtie2' or 'blastn', and then automatically performs alignment by downloading the entire fastq file. eg: **`perl ara.pl --input example/SraRunInfo.csv --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa --output /src/main/test/ --mode screen`**\r\n\r\n    > _Note:_ There is a supporting **`summary`** mode, that will generate a unified alignment summary by examining the output files created by either screen-mode or full-mode. The summary mode should only be used when the user needs to recreate the summary stats from the pre-existing results. The user must enter **`\u2013mode summary`** along with the previously used command parameters to re-generate the summary.\r\n\r\n  - **`--config`** (optional) Pipeline configuration. By default it will use the **`conf.txt`** generated by the setup script. eg: **`perl ara.pl --input example/SraRunInfo.csv --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa --output /src/main/test/ --mode screen --config conf.txt`**\r\n\r\n---\r\n\r\n### Output structure\r\n\r\nThe pipeline will create folders per SRA run accession and generate results using the run accession as the prefix. The analysis related to the screening a fraction of data will be stored in `screening_results` directory whereas the analysis conducted on the whole dataset will be stored in `full_analyis_results` directory.\r\n\r\nAn outline of directory structure containing the results is shown below-\r\n\r\n    results/\r\n    `-- test/ (name derived from the input fasta sequence file)\r\n        |-- test.screening.analysis.stats.sorted.by.alignment.txt (combined metadata and analysis report generated after processing all the SRA run accessions, sorted in decreasing order of total alignment percentage)\r\n        |-- metadata/\r\n        |   |-- test.metadata.txt (Combined metadata downloaded from SRA)\r\n        |   |-- test.metadata.screened.txt (List of SRA accessions which qualify the filter criteria specified in the config.)\r\n        |   |-- SRA_RUN.run.metadata.txt (unprocessed metadata on a single SRA accession as retrieved from NCBI)\r\n        |-- reference/\r\n        |   |-- blastn_db/ (folder containing the blast database created from the input fasta sequence)\r\n        |   |-- bowtie2_index/ (folder containing the bowtie index created from the input fasta sequence)\r\n        |   |-- bowtie2_index.stdout.txt (stdout captured from bowtie2 index creation)\r\n        |   `-- makeblastdb.stdout.txt (stdout captured from blastn database creation)\r\n        `-- screening_results/ (similar structure for screeing or full mode)\r\n            |-- SRA_RUN/ (each SRA run accession will be processed into a seperate folder)\r\n            |   |-- blastn/\r\n            |   |   |-- SRA_RUN.blast.results.txt (output from NCBI Blastn)\r\n            |   |   `-- blast.stats.txt (blastn overall alignment stats)\r\n            |   |-- bowtie2/\r\n            |   |   |-- SRA_RUN.bam (output from bowtie2)\r\n            |   |   |-- alignment.stats.txt (bowtie2 stdout)\r\n            |   |   `-- alignment.txt (bowtie2 overall alignment summary)\r\n            |   |-- fastQC/\r\n            |   |   |-- <Raw data FastQC report>\r\n            |   |   |-- <Adapter trimmed FastQC report>\r\n            |   |-- kraken2/\r\n            |   |   |-- SRA_RUN.kraken (kraken2 standard classification table)\r\n            |   |   |-- SRA_RUN.report (kraken2 classification report)\r\n            |   |   `-- SRA_RUN.stdout.txt (kraken2 stdout)\r\n            |   |-- raw_fastq/\r\n            |   |   |-- <Downloaded single end or paired end fastq file(s)>\r\n            |   |   |-- fastq_dump.stdout.txt\r\n            |   |   |-- sra/\r\n            |   |   `-- wget.full.sra.stdout.txt\r\n            |   `-- trimmed_data/\r\n            |       |-- <Adapter trimmed single end or paired end fastq file(s)>\r\n            |       `-- SRA_RUN_trim_stdout_log.txt (trimmomatic stdout)\r\n            `-- runlog.SRA_RUN.txt (Complete run log of the pipeline per SRA run accession)\r\n\r\nFor a thorough understanding of the results of the third-party tools, take a look at the following documentations:\r\n\r\n- [Blastn](https://www.ncbi.nlm.nih.gov/books/NBK279690/)\r\n- [Bowtie2](https://bowtie-bio.sourceforge.net/bowtie2/manual.shtml)\r\n- [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)\r\n- [Kraken2](https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown)\r\n- [Trimmomatic](https://github.com/usadellab/Trimmomatic)\r\n\r\n---\r\n\r\n### Disk usage using the input from the example\r\n\r\nThe table below provides a summary of the disk usage for different analyses conducted on varying dataset sizes. It demonstrates how disk usage can increase depending on the choice of the fraction of the dataset the user wishes to analyze.\r\n\r\n| RUN ACCESSION | 100% of dataset | 5% of dataset | 10% of dataset |\r\n| ------------- | --------------- | ------------- | -------------- |\r\n| SRR8392720    | 1.3G            | 85M           | 156M           |\r\n| SRR7289585    | 1.4G            | 150M          | 288M           |\r\n| SRR12548227   | 15M             | 9.0M          | 9.1M           |\r\n\r\nThis summary highlights how the disk usage (in megabytes or gigabytes) can vary depending on the chosen fraction of the dataset for analysis.\r\n\r\n---\r\n\r\n### Troubleshooting\r\n\r\n- Errors related to mamba/conda environment:\r\n\r\n  Since `mamba` is a drop-in replacement and uses the same commands and configuration options as **conda**, it's possible to swap almost all commands between **conda** & **mamba**.\r\n\r\n  Use **`conda list`** command to verify whether the packages mentioned in the `requirements.yaml` are successfully installed into your environment.\r\n\r\n  > _Note:_ The `requirements.yaml` provided in this package was exported from `mamba 0.25.0` installation running on `Ubuntu 20.04.4 LTS`.\r\n\r\n  In case of any missing tool/ conflicting dependencies in the environment, the user can try using **`conda search <tool name>`** or `mamba repoquery search <tool name>` command to find the supported version of the tool and then manually install it by typing **`conda install <tool name>`** or `mamba install <tool name>` inside the environment. Please refer the official [troubleshooting guide](https://conda.io/projects/conda/en/latest/user-guide/troubleshooting.html \"User guide \u00bb Troubleshooting\") for further help.\r\n\r\n  > _Note:_ On macOS and Linux, the supported tools and their dependencies aren't always the same. Even when all of the requirements are completely aligned, the set of available versions isn't necessarily the same. User may try setting up the environment using any of the supplementary `requirements-*.txt` provided in the `src/main/resources/` directory.\r\n\r\n- Error installing Perl modules:\r\n\r\n  Users must ensure that they have write permission to the `/Users/\\*/.cpan/` or similar directory, and the CPAN is properly configured.\r\n\r\n  You might need to define the PERLLIB/PERL5LIB environment variable if you see an error similar to the following:\r\n\r\n  ```bash\r\n      Cant locate My/Module.pm in @INC (@INC contains:\r\n      ...\r\n      ...\r\n      .).\r\n      BEGIN failed--compilation aborted.\r\n  ```\r\n\r\n  > _Note about MAKE_: 'make' is an essential tool for building Perl modules. Please make sure that you have 'make' installed in your system. The setup script provided in this package utilizes 'cpan' to build the required Perl modules automatically.\r\n\r\n  If the automatic setup provided in the package fails to install the required dependencies, you may need to install them manually by using the command `cpan install <module name>` or searching the package on [Metacpan](https://metacpan.org/).\r\n\r\n  Additionally, some Perl modules can also be installed through `mamba` (eg. the compatible version of Perl module `Config::Simple` can be searched on mamba by `mamba repoquery search perl-config-simple`)\r\n\r\n---\r\n\r\n### List of Perl modules and tools incorporated in the pipeline\r\n\r\n- Perl modules:\r\n\r\n  - Config::Simple\r\n  - Parallel::ForkManager\r\n  - Log::Log4perl\r\n  - Getopt::Long\r\n  - Text::CSV\r\n  - Text::Unidecode\r\n\r\n- Tools:\r\n\r\n  - [NCBI EDirect utilities \\>=16.2](https://www.ncbi.nlm.nih.gov/books/NBK179288/)\r\n  - [NCBI SRA Toolkit \\>=2.10.7](https://www.ncbi.nlm.nih.gov/home/tools/)\r\n  - [FastQC \\>=0.11.9](https://www.bioinformatics.babraham.ac.uk/projects/download.html#fastqc)\r\n  - [Trimmomatic \\>=0.39](http://www.usadellab.org/cms/?page=trimmomatic)\r\n  - [FASTX-Toolkit \\>=0.0.14](http://hannonlab.cshl.edu/fastx_toolkit/)\r\n  - [NCBI Blast \\>=2.10.1](https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE_TYPE=BlastDocs&DOC_TYPE=Download)\r\n  - [Bowtie2 \\>=2.4.5](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml)\r\n  - [Samtools \\>=1.15.1](http://www.htslib.org/download/)\r\n  - [Kraken2 \\>=2.1.2](https://ccb.jhu.edu/software/kraken2/)\r\n\r\n---\r\n",
        "doi": "10.48546/workflowhub.workflow.546.1",
        "edam_operation": [],
        "edam_topic": [
            "Sequence analysis"
        ],
        "filtered_on": "annot* in tags",
        "id": "546",
        "keep": true,
        "latest_version": 1,
        "license": "LGPL-3.0",
        "link": "https:/workflowhub.eu/workflows/546?version=1",
        "name": "ARA (Automated Record Analysis)",
        "number_of_steps": 0,
        "projects": [
            "ARA-dev"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genomics",
            "perl",
            "pipeline",
            "ncbi sra",
            "sequence annotation",
            "sequence search"
        ],
        "tools": [],
        "type": "Perl",
        "update_time": "2023-07-31",
        "versions": 1
    },
    {
        "create_time": "2023-07-20",
        "creators": [
            "Rosa M Badia",
            "Javier Conejero"
        ],
        "description": "Sample workflow template that combines simulations with data analytics. It is not a real workflow, but it mimics this type of workflows. It illustrates how COMPSs invokes binaries. It can be extended to invoke MPI applications. ",
        "doi": "10.48546/workflowhub.workflow.541.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in name",
        "id": "541",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/541?version=1",
        "name": "Sample workflow that combines simulations with data analytics.",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [
            "compss",
            "hybrid workflow",
            "marenostrum iv",
            "pycompss",
            "supercomputer",
            "eflows4hpc",
            "non_data_persistence"
        ],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2023-07-21",
        "versions": 1
    },
    {
        "create_time": "2023-07-09",
        "creators": [
            "Stevie Pederson"
        ],
        "description": "# prepareChIPs\r\n\r\nThis is a simple `snakemake` workflow template for preparing **single-end** ChIP-Seq data.\r\nThe steps implemented are:\r\n\r\n1. Download raw fastq files from SRA\r\n2. Trim and Filter raw fastq files using `AdapterRemoval`\r\n3. Align to the supplied genome using `bowtie2`\r\n4. Deduplicate Alignments using `Picard MarkDuplicates`\r\n5. Call Macs2 Peaks using `macs2`\r\n\r\nA pdf of the rulegraph is available [here](workflow/rules/rulegraph.pdf)\r\n\r\nFull details for each step are given below.\r\nAny additional parameters for tools can be specified using `config/config.yml`, along with many of the requisite paths\r\n\r\nTo run the workflow with default settings, simply run as follows (after editing `config/samples.tsv`)\r\n\r\n```bash\r\nsnakemake --use-conda --cores 16\r\n```\r\n\r\nIf running on an HPC cluster, a snakemake profile will required for submission to the queueing system and appropriate resource allocation.\r\nPlease discuss this will your HPC support team.\r\nNodes may also have restricted internet access and rules which download files may not work on many HPCs.\r\nPlease see below or discuss this with your support team\r\n\r\nWhilst no snakemake wrappers are explicitly used in this workflow, the underlying scripts are utilised where possible to minimise any issues with HPC clusters with restrictions on internet access.\r\nThese scripts are based on `v1.31.1` of the snakemake wrappers\r\n\r\n### Important Note Regarding OSX Systems\r\n\r\nIt should be noted that this workflow is **currently incompatible with OSX-based systems**. \r\nThere are two unsolved issues\r\n\r\n1. `fasterq-dump` has a bug which is specific to conda environments. This has been updated in v3.0.3 but this patch has not yet been made available to conda environments for OSX. Please check [here](https://anaconda.org/bioconda/sra-tools) to see if this has been updated.\r\n2. The following  error appears in some OSX-based R sessions, in a system-dependent manner:\r\n```\r\nError in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y,  : \r\n  polygon edge not found\r\n```\r\n\r\nThe fix for this bug is currently unknown\r\n\r\n## Download Raw Data\r\n\r\n### Outline\r\n\r\nThe file `samples.tsv` is used to specify all steps for this workflow.\r\nThis file must contain the columns: `accession`, `target`, `treatment` and `input`\r\n\r\n1. `accession` must be an SRA accession. Only single-end data is currently supported by this workflow\r\n2. `target` defines the ChIP target. All files common to a target and treatment will be used to generate summarised coverage in bigWig Files\r\n3. `treatment` defines the treatment group each file belongs to. If only one treatment exists, simply use the value 'control' or similar for every file\r\n4. `input` should contain the accession for the relevant input sample. These will only be downloaded once. Valid input samples are *required* for this workflow\r\n\r\nAs some HPCs restrict internet access for submitted jobs, *it may be prudent to run the initial rules in an interactive session* if at all possible.\r\nThis can be performed using the following (with 2 cores provided as an example)\r\n\r\n```bash\r\nsnakemake --use-conda --until get_fastq --cores 2\r\n```\r\n\r\n### Outputs\r\n\r\n- Downloaded files will be gzipped and written to `data/fastq/raw`.\r\n- `FastQC` and `MultiQC` will also be run, with output in `docs/qc/raw`\r\n\r\nBoth of these directories are able to be specified as relative paths in `config.yml`\r\n\r\n## Read Filtering\r\n\r\n### Outline\r\n\r\nRead trimming is performed using [AdapterRemoval](https://adapterremoval.readthedocs.io/en/stable/).\r\nDefault settings are customisable using config.yml, with the defaults set to discard reads shorter than 50nt, and to trim using quality scores with a threshold of Q30.\r\n\r\n### Outputs\r\n\r\n- Trimmed fastq.gz files will be written to `data/fastq/trimmed`\r\n- `FastQC` and `MultiQC` will also be run, with output in `docs/qc/trimmed`\r\n- AdapterRemoval 'settings' files will be written to `output/adapterremoval`\r\n\r\n## Alignments\r\n\r\n### Outline\r\n\r\nAlignment is performed using [`bowtie2`](https://bowtie-bio.sourceforge.net/bowtie2/manual.shtml) and it is assumed that this index is available before running this workflow.\r\nThe path and prefix must be provided using config.yml\r\n\r\nThis index will also be used to produce the file `chrom.sizes` which is essential for conversion of bedGraph files to the more efficient bigWig files.\r\n\r\n### Outputs\r\n\r\n- Alignments will be written to `data/aligned`\r\n- `bowtie2` log files will be written to `output/bowtie2` (not the conenvtional log directory)\r\n- The file `chrom.sizes` will be written to `output/annotations`\r\n\r\nBoth sorted and the original unsorted alignments will be returned.\r\nHowever, the unsorted alignments are marked with `temp()` and can be deleted using \r\n\r\n```bash\r\nsnakemake --delete-temp-output --cores 1\r\n```\r\n\r\n## Deduplication\r\n\r\n### Outline\r\n\r\nDeduplication is performed using [MarkDuplicates](https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard-) from the Picard set of tools.\r\nBy default, deduplication will remove the duplicates from the set of alignments.\r\nAll resultant bam files will be sorted and indexed.\r\n\r\n### Outputs\r\n\r\n- Deduplicated alignments are written to `data/deduplicated` and are indexed\r\n- DuplicationMetrics files are written to `output/markDuplicates`\r\n\r\n## Peak Calling\r\n\r\n### Outline\r\n\r\nThis is performed using [`macs2 callpeak`](https://pypi.org/project/MACS2/).\r\n\r\n- Peak calling will be performed on:\r\n    a. each sample individually, and \r\n    b. merged samples for those sharing a common ChIP target and treatment group.\r\n- Coverage bigWig files for each individual sample are produced using CPM values (i.e. Signal Per Million Reads, SPMR)\r\n- For all combinations of target and treatment coverage bigWig files are also produced, along with fold-enrichment bigWig files\r\n\r\n### Outputs\r\n\r\n- Individual outputs are written to `output/macs2/{accession}`\r\n\t+ Peaks are written in `narrowPeak` format along with `summits.bed`\r\n\t+ bedGraph files are automatically converted to bigWig files, and the originals are marked with `temp()` for subsequent deletion\r\n\t+ callpeak log files are also added to this directory\r\n- Merged outputs are written to `output/macs2/{target}/`\r\n\t+ bedGraph Files are also converted to bigWig and marked with `temp()`\r\n\t+ Fold-Enrichment bigWig files are also created with the original bedGraph files marked with `temp()`\r\n",
        "doi": "10.48546/workflowhub.workflow.528.1",
        "edam_operation": [
            "Transcription factor binding site prediction"
        ],
        "edam_topic": [
            "ChIP-seq"
        ],
        "filtered_on": "binn* in description",
        "id": "528",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/528?version=1",
        "name": "prepareChIPs:",
        "number_of_steps": 0,
        "projects": [
            "Black Ochre Data Labs"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "transcriptomics"
        ],
        "tools": [
            "MACS",
            "Bowtie 2",
            "AdapterRemoval",
            "R",
            "FastQC",
            "MultiQC",
            "Bioconductor",
            "ngsReports"
        ],
        "type": "Snakemake",
        "update_time": "2023-07-09",
        "versions": 1
    },
    {
        "create_time": "2023-07-05",
        "creators": [
            "Bugra Oezdemir"
        ],
        "description": "# BatchConvert  ![DOI:10.5281](https://zenodo.org/badge/doi/10.5281/zenodo.7955974.svg)\r\n\r\nA command line tool for converting image data into either of the standard file formats OME-TIFF or OME-Zarr. \r\n\r\nThe tool wraps the dedicated file converters bfconvert and bioformats2raw to convert into OME-TIFF or OME-Zarr,\r\nrespectively. The workflow management system NextFlow is used to perform conversion in parallel for batches of images. \r\n\r\nThe tool also wraps s3 and Aspera clients (go-mc and aspera-cli, respectively). Therefore, input and output locations can \r\nbe specified as local or remote storage and file transfer will be performed automatically. The conversion can be run on \r\nHPC with Slurm.  \r\n\r\n![](figures/diagram.png)\r\n\r\n## Installation & Dependencies\r\n\r\n**Important** note: The package has been so far only tested on Ubuntu 20.04.\r\n\r\nThe minimal dependency to run the tool is NextFlow, which should be installed and made accessible from the command line.\r\n\r\nIf conda exists on your system, you can install BatchConvert together with NextFlow using the following script:\r\n```\r\ngit clone https://github.com/Euro-BioImaging/BatchConvert.git && \\ \r\nsource BatchConvert/installation/install_with_nextflow.sh\r\n```\r\n\r\n\r\nIf you already have NextFlow installed and accessible from the command line (or if you prefer to install it manually \r\ne.g., as shown [here](https://www.nextflow.io/docs/latest/getstarted.html)), you can also install BatchConvert alone, using the following script:\r\n```\r\ngit clone https://github.com/Euro-BioImaging/BatchConvert.git && \\ \r\nsource BatchConvert/installation/install.sh\r\n```\r\n\r\n\r\nOther dependencies (which will be **automatically** installed):\r\n- bioformats2raw (entrypoint bioformats2raw)\r\n- bftools (entrypoint bfconvert)\r\n- go-mc (entrypoint mc)\r\n- aspera-cli (entrypoint ascp)\r\n\r\nThese dependencies will be pulled and cached automatically at the first execution of the conversion command. \r\nThe mode of dependency management can  be specified by using the command line option ``--profile`` or `-pf`. Depending \r\non how this option is specified, the dependencies will be acquired / run either via conda or via docker/singularity containers. \r\n\r\nSpecifying ``--profile conda`` (default) will install the dependencies to an \r\nenvironment at ``./.condaCache`` and use this environment to run the workflow. This option \r\nrequires that miniconda/anaconda is installed on your system.    \r\n\r\nAlternatively, specifying ``--profile docker`` or ``--profile singularity`` will pull a docker or \r\nsingularity image with the dependencies, respectively, and use this image to run the workflow.\r\nThese options assume that the respective container runtime (docker or singularity) is available on \r\nyour system. If singularity is being used, a cache directory will be created at the path \r\n``./.singularityCache`` where the singularity image is stored. \r\n\r\nFinally, you can still choose to install the dependencies manually and use your own installations to run\r\nthe workflow. In this case, you should specify ``--profile standard`` and make sure the entrypoints\r\nspecified above are recognised by your shell.  \r\n\r\n\r\n## Configuration\r\n\r\nBatchConvert can be configured to have default options for file conversion and transfer. Probably, the most important sets of parameters\r\nto be configured include credentials for the remote ends. The easiest way to configure remote stores is by running the interactive \r\nconfiguration command as indicated below.\r\n\r\n### Configuration of the s3 object store\r\n\r\nRun the interactive configuration command: \r\n\r\n`batchconvert configure_s3_remote`\r\n\r\nThis will start a sequence of requests for s3 credentials such as name, url, access, etc. Provide each requested credential and click\r\nenter. Continue this cycle until the process is finished. Upon completing the configuration, the sequence of commands should roughly look like this:\r\n\r\n```\r\noezdemir@pc-ellenberg108:~$ batchconvert configure_s3_remote\r\nenter remote name (for example s3)\r\ns3\r\nenter url:\r\nhttps://s3.embl.de\r\nenter access key:\r\n\"your-access-key\"\r\nenter secret key:\r\n\"your-secret-key\"\r\nenter bucket name:\r\n\"your-bucket\"\r\nConfiguration of the default s3 credentials is complete\r\n```\r\n\r\n\r\n### Configuration of the BioStudies user space\r\n\r\nRun the interactive configuration command: \r\n\r\n`batchconvert configure_bia_remote`\r\n\r\nThis will prompt a request for the secret directory to connect to. Enter the secret directory for your user space and click enter. \r\nUpon completing the configuration, the sequence of commands should roughly look like this:\r\n\r\n```\r\noezdemir@pc-ellenberg108:~$ batchconvert configure_bia_remote\r\nenter the secret directory for BioImage Archive user space:\r\n\"your-secret-directory\"\r\nconfiguration of the default bia credentials is complete\r\n```\r\n\r\n### Configuration of the slurm options\r\n\r\nBatchConvert can also run on slurm clusters. In order to configure the slurm parameters, run the interactive configuration command: \r\n\r\n`batchconvert configure_slurm`\r\n\r\nThis will start a sequence of requests for slurm options. Provide each requested option and click enter. \r\nContinue this cycle until the process is finished. Upon completing the configuration, the sequence of commands should \r\nroughly look like this:\r\n\r\n```\r\noezdemir@pc-ellenberg108:~$ batchconvert configure_slurm\r\nPlease enter value for queue_size\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the current value \u00b450\u00b4\r\ns\r\nPlease enter value for submit_rate_limit\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the current value \u00b410/2min\u00b4\r\ns\r\nPlease enter value for cluster_options\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the current value \u00b4--mem-per-cpu=3140 --cpus-per-task=16\u00b4\r\ns\r\nPlease enter value for time\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the current value \u00b46h\u00b4\r\ns\r\nconfiguration of the default slurm parameters is complete\r\n```\r\n\r\n### Configuration of the default conversion parameters\r\n\r\nWhile all conversion parameters can be specified as command line arguments, it can\r\nbe useful for the users to set their own default parameters to avoid re-entering those\r\nparameters for subsequent executions. BatchConvert allows for interactive configuration of \r\nconversion in the same way as configuration of the remote stores described above.\r\n\r\nTo configure the conversion into OME-TIFF, run the following command:\r\n\r\n`batchconvert configure_ometiff`\r\n\r\nThis will prompt the user to enter a series of parameters, which will then be saved as the \r\ndefault parameters to be passed to the `batchconvert ometiff` command. Upon completing the \r\nconfiguration, the sequence of commands should look similar to:\r\n\r\n```\r\noezdemir@pc-ellenberg108:~$ batchconvert configure_ometiff\r\nPlease enter value for noflat\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the parameter\u00b4s current value, which is \"bfconvert defaults\"\r\ns\r\nPlease enter value for series\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the parameter\u00b4s current value, which is \"bfconvert defaults\"\r\ns\r\nPlease enter value for timepoint\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the parameter\u00b4s current value, which is \"bfconvert defaults\"\r\ns\r\n...\r\n...\r\n...\r\n...\r\n...\r\n...\r\nConfiguration of the default parameters for 'bfconvert' is complete\r\n```\r\n\r\n\r\nTo configure the conversion into OME-Zarr, run the following command:\r\n\r\n`batchconvert configure_omezarr`\r\n\r\nSimilarly, this will prompt the user to enter a series of parameters, which will then be saved as the \r\ndefault parameters to be passed to the `batchconvert omezarr` command. Upon completing the configuration, \r\nthe sequence of commands should look similar to:\r\n\r\n```\r\noezdemir@pc-ellenberg108:~$ batchconvert configure_omezarr\r\nPlease enter value for resolutions_zarr\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the parameter\u00b4s current value, which is \"bioformats2raw defaults\"\r\ns\r\nPlease enter value for chunk_h\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the parameter\u00b4s current value, which is \"bioformats2raw defaults\"\r\ns\r\nPlease enter value for chunk_w\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the parameter\u00b4s current value, which is \"bioformats2raw defaults\"\r\n...\r\n...\r\n...\r\n...\r\n...\r\n...\r\nConfiguration of the default parameters for 'bioformats2raw' is complete\r\n```\r\n\r\nIt is important to note that the initial defaults for the conversion parameters are the same as the defaults\r\nof the backend tools bfconvert and bioformats2raw, as noted in the prompt excerpt above. Through interactive configuration, \r\nthe user is overriding these initial defaults and setting their own defaults. It is possible to reset the initial\r\ndefaults by running the following command.\r\n\r\n`batchconvert reset_defaults`\r\n\r\nAnother important point is that any of these configured parameters can be overridden by passing a value to that\r\nparameter in the commandline. For instance, in the following command, the value of 20 will be assigned to `chunk_h` parameter \r\neven if the value for the same parameter might be different in the configuration file. \r\n\r\n`batchconvert omezarr --chunk_h 20 \"path/to/input\" \"path/to/output\"`\r\n\r\n\r\n## Examples\r\n\r\n### Local conversion\r\n\r\n#### Parallel conversion of files to separate OME-TIFFs / OME-Zarrs:\r\nConvert a batch of images on your local storage into OME-TIFF format. \r\nNote that the `input_path` in the command given below is typically a \r\ndirectory with multiple image files but a single image file can also be passed:\\\r\n`batchconvert ometiff -pf conda \"input_path\" \"output_path\"` \r\n\r\nNote that if this is your first conversion with the profile `conda`, \r\nit will take a while for a conda environment with the dependencies to be\r\ncreated. All the subsequent conversion commands with the profile `conda`,\r\nhowever, will use this environment, and thus show no such delay.\r\n\r\nSince conda is the default profile, it does not have to be \r\nexplicitly included in the command line. Thus, the command can be shortened to:\\\r\n`batchconvert ometiff \"input_path\" \"output_path\"`\r\n\r\nConvert only the first channel of the images:\\\r\n`batchconvert ometiff -chn 0 \"input_path\" \"output_path\"`\r\n\r\nCrop the images being converted along x and y axis by 150 pixels:\\\r\n`batchconvert ometiff -cr 0,0,150,150 \"input_path\" \"output_path\"`\r\n\r\nConvert into OME-Zarr instead:\\\r\n`batchconvert omezarr \"input_path\" \"output_path\"`\r\n\r\nConvert into OME-Zarr with 3 resolution levels:\\\r\n`batchconvert omezarr -rz 3 \"input_path\" \"output_path\"`\r\n\r\nSelect a subset of images with a matching string such as \"mutation\":\\\r\n`batchconvert omezarr -p mutation \"input_path\" \"output_path\"`\r\n\r\nSelect a subset of images using wildcards. Note that the use of \"\" around \r\nthe input path is necessary when using wildcards:\\\r\n`batchconvert omezarr \"input_path/*D3*.oir\" \"output_path\"`\r\n\r\nConvert by using a singularity container instead of conda environment (requires\r\nsingularity to be installed on your system):\\\r\n`batchconvert omezarr -pf singularity \"input_path/*D3*.oir\" \"output_path\"`\r\n\r\nConvert by using a docker container instead of conda environment (requires docker\r\nto be installed on your system):\\\r\n`batchconvert omezarr -pf docker \"input_path/*D3*.oir\" \"output_path\"`\r\n\r\nNote that similarly to the case with the profile `conda`, the first execution of\r\na conversion with the profile `singularity` or `docker` will take a while for the\r\ncontainer image to be pulled. All the subsequent conversion commands using a \r\ncontainer option will use this image, and thus show no such delay. \r\n\r\nConvert local data and upload the output to an s3 bucket. Note that the output \r\npath is created relative to the bucket specified in your s3 configuration:\\\r\n`batchconvert omezarr -dt s3 \"input_path\" \"output_path\"`\r\n\r\nReceive input files from an s3 bucket, convert locally and upload the output to \r\nthe same bucket. Note that wildcards cannot be used when the input is from s3. \r\nUse pattern matching option `-p` for selecting a subset of input files:\\\r\n`batchconvert omezarr -p mutation -st s3 -dt s3 \"input_path\" \"output_path\"`\r\n\r\nReceive input files from your private BioStudies user space and convert them locally.\r\nUse pattern matching option `-p` for selecting a subset of input files:\\\r\n`batchconvert omezarr -p mutation -st bia \"input_path\" \"output_path\"`\r\n\r\nReceive an input from an s3 bucket, convert locally and upload the output to your \r\nprivate BioStudies user space. Use pattern matching option `-p` for selecting a subset \r\nof input files:\\\r\n`batchconvert omezarr -p mutation -st s3 -dt bia \"input_path\" \"output_path\"`\r\n\r\nNote that in all the examples shown above, BatchConvert treats each input file as separate,\r\nstandalone data point, disregarding the possibility that some of the input files might belong to \r\nthe same multidimensional array. Thus, each input file is converted to an independent \r\nOME-TIFF / OME-Zarr and the number of outputs will thus equal the number of selected input files.\r\nAn alternative scenario is discussed below.\r\n\r\n#### Parallel conversion of file groups by stacking multiple files into single OME-TIFFs / OME-Zarrs:\r\n\r\nWhen the flag `--merge_files` is specified, BatchConvert tries to detect which input files might \r\nbelong to the same multidimensional array based on the patterns in the filenames. Then a \"grouped conversion\" \r\nis performed, meaning that the files belonging to the same dataset will be incorporated into \r\na single OME-TIFF / OME-Zarr series, in that files will be concatenated along specific dimension(s) \r\nduring the conversion. Multiple file groups in the input directory can be detected and converted \r\nin parallel. \r\n\r\nThis feature uses Bio-Formats's pattern files as described [here](https://docs.openmicroscopy.org/bio-formats/6.6.0/formats/pattern-file.html).\r\nHowever, BatchConvert generates pattern files automatically, allowing the user to directly use the \r\ninput directory in the conversion command. BatchConvert also has the option of specifying the \r\nconcatenation axes in the command line, which is especially useful in cases where the filenames \r\nmay not contain dimension information.  \r\n\r\nTo be able to use the `--merge files` flag, the input file names must obey certain rules:\r\n1. File names in the same group must be uniform, except for one or more **numeric field(s)**, which\r\nshould show incremental change across the files. These so-called **variable fields** \r\nwill be detected and used as the dimension(s) of concatenation.\r\n2. The length of variable fields must be uniform within the group. For instance, if the\r\nvariable field has values reaching multi-digit numbers, leading \"0\"s should be included where needed \r\nin the file names to make the variable field length uniform within the group.\r\n3. Typically, each variable field should follow a dimension specifier. What patterns can be used as \r\ndimension specifiers are explained [here](https://docs.openmicroscopy.org/bio-formats/6.6.0/formats/pattern-file.html).\r\nHowever, BatchConvert also has the option `--concatenation_order`, which allows the user to\r\nspecify from the command line, the dimension(s), along which the files must be concatenated.\r\n4. File names that are unique and cannot be associated with any group will be assumed as\r\nstandalone images and converted accordingly. \r\n\r\nBelow are some examples of grouped conversion commands in the context of different possible use-case scenarios:\r\n\r\n**Example 1:**\r\n\r\nThis is an example of a folder with non-uniform filename lengths:\r\n```\r\ntime-series/test_img_T2\r\ntime-series/test_img_T4\r\ntime-series/test_img_T6\r\ntime-series/test_img_T8\r\ntime-series/test_img_T10\r\ntime-series/test_img_T12\r\n```\r\nIn this example, leading zeroes are missing in the variable fields of some filenames. \r\nA typical command to convert this folder to a single OME-TIFF would look like: \\\r\n`batchconvert --ometiff --merge_files \"input_dir/time-series\" \"output_path\"`\r\n\r\nHowever, this command would fail to create a single OME-Zarr folder due to the non-uniform \r\nlengths of the filenames. Instead, the files would be split into two groups based on the\r\nfilename length, leading to two separate OME-Zarrs with names:\r\n\r\n`test_img_TRange{2-8-2}.ome.zarr` and `test_img_TRange{10-12-2}.ome.zarr`\r\n\r\nHere is the corrected version of the folder for the above example-\r\n```\r\ntime-series/test_img_T02\r\ntime-series/test_img_T04\r\ntime-series/test_img_T06\r\ntime-series/test_img_T08\r\ntime-series/test_img_T10\r\ntime-series/test_img_T12\r\n```\r\n\r\nExecuting the same command on this folder would result in a single OME-Zarr with the name:\r\n`test_img_TRange{02-12-2}.ome.zarr`\r\n\r\n**Example 2**- \r\n\r\nIn this example, the filename lengths are uniform but the incrementation within the variable field is not.\r\n```\r\ntime-series/test_img_T2\r\ntime-series/test_img_T4\r\ntime-series/test_img_T5\r\ntime-series/test_img_T7\r\n```\r\n\r\nA typical command to convert this folder to a single OME-Zarr would look like: \\\r\n`batchconvert --omezarr --merge_files \"input_dir/time-series\" \"output_path\"`\r\n\r\nHowever, the command would fail to assume these files as a single group due to the\r\nnon-uniform incrementation in the variable field of the filenames. Instead, the dataset \r\nwould be split into two groups, leading to two separate OME-Zarrs with the following names:\r\n`test_img_TRange{2-4-2}.ome.zarr` and `test_img_TRange{5-7-2}.ome.zarr`  \r\n\r\n\r\n**Example 3**\r\n\r\nThis is an example of a case where the conversion attempts to concatenate files along two\r\ndimensions, channel and time.\r\n```\r\nmultichannel_time-series/test_img_C1-T1\r\nmultichannel_time-series/test_img_C1-T2\r\nmultichannel_time-series/test_img_C1-T3\r\nmultichannel_time-series/test_img_C2-T1\r\nmultichannel_time-series/test_img_C2-T2\r\n```\r\nTo convert this folder to a single OME-Zarr, one could try the following command: \\\r\n`batchconvert --omezarr --merge_files \"input_dir/multichannel_time-series\" \"output_path\"`\r\n\r\nHowever, since the channel-2 does not have the same number of timeframes as the channel-1, \r\nBatchConvert will fail to assume these two channels as part of the same series and\r\nwill instead split the two channels into two separate OME-Zarrs. \r\n\r\nThe output would look like: \\\r\n`test_img_C1-TRange{1-3-1}.ome.zarr` \\\r\n`test_img_C2-TRange{1-2-1}.ome.zarr`\r\n\r\nTo be able to really incorporate all files into a single OME-Zarr, the folder should have equal\r\nnumber of images corresponding to both channels, as shown below:\r\n```\r\nmultichannel_time-series/test_img_C1-T1\r\nmultichannel_time-series/test_img_C1-T2\r\nmultichannel_time-series/test_img_C1-T3\r\nmultichannel_time-series/test_img_C2-T1\r\nmultichannel_time-series/test_img_C2-T2\r\nmultichannel_time-series/test_img_C2-T3\r\n```\r\nThe same conversion command on this version of the input folder would result in a single \r\nOME-Zarr with the name: \\\r\n`test_img_CRange{1-2-1}-TRange{1-3-1}.ome.zarr`\r\n\r\n\r\n**Example 4**\r\n\r\nThis is another example of a case, where there are multiple filename patterns in the input folder.\r\n\r\n```\r\nfolder_with_multiple_groups/test_img_C1-T1\r\nfolder_with_multiple_groups/test_img_C1-T2\r\nfolder_with_multiple_groups/test_img_C2-T1\r\nfolder_with_multiple_groups/test_img_C2-T2\r\nfolder_with_multiple_groups/test_img_T1-Z1\r\nfolder_with_multiple_groups/test_img_T1-Z2\r\nfolder_with_multiple_groups/test_img_T1-Z3\r\nfolder_with_multiple_groups/test_img_T2-Z1\r\nfolder_with_multiple_groups/test_img_T2-Z2\r\nfolder_with_multiple_groups/test_img_T2-Z3\r\n```\r\n\r\nOne can convert this folder with- \\\r\n`batchconvert --omezarr --merge_files \"input_dir/folder_with_multiple_groups\" \"output_path\"`\r\n \r\nBatchConvert will detect the two patterns in this folder and perform two grouped conversions. \r\nThe output folders will be named as `test_img_CRange{1-2-1}-TRange{1-2-1}.ome.zarr` and \r\n`test_img_TRange{1-2-1}-ZRange{1-3-1}.ome.zarr`. \r\n\r\n\r\n**Example 5**\r\n\r\nNow imagine that we have the same files as in the example 4 but the filenames of the\r\nfirst group lack any dimension specifier, so we have the following folder:\r\n\r\n```\r\nfolder_with_multiple_groups/test_img_1-1\r\nfolder_with_multiple_groups/test_img_1-2\r\nfolder_with_multiple_groups/test_img_2-1\r\nfolder_with_multiple_groups/test_img_2-2\r\nfolder_with_multiple_groups/test_img_T1-Z1\r\nfolder_with_multiple_groups/test_img_T1-Z2\r\nfolder_with_multiple_groups/test_img_T1-Z3\r\nfolder_with_multiple_groups/test_img_T2-Z1\r\nfolder_with_multiple_groups/test_img_T2-Z2\r\nfolder_with_multiple_groups/test_img_T2-Z3\r\n```\r\n\r\nIn such a scenario, BatchConvert allows the user to specify the concatenation axes \r\nvia `--concatenation_order` option. This option expects comma-separated strings of dimensions \r\nfor each group. In this example, the user must provide a string of 2 characters, such as `ct` for \r\nchannel and time, for group 1, since there are two variable fields for this group. Since group 2 \r\nalready has dimension specifiers (T and Z as specified in the filenames preceding the variable fields),\r\nthe user does not need to specify anything for this group, and can enter `auto` or `aa` for automatic\r\ndetection of the specifiers. \r\n\r\nSo the following line can be used to convert this folder: \\\r\n`batchconvert --omezarr --merge_files --concatenation_order ct,aa \"input_dir/folder_with_multiple_groups\" \"output_path\"`\r\n\r\nThe resulting OME-Zarrs will have the names:\r\n`test_img_CRange{1-2-1}-TRange{1-2-1}.ome.zarr` and\r\n`test_img_TRange{1-2-1}-ZRange{1-3-1}.ome.zarr`\r\n\r\nNote that `--concatenation_order` will override any dimension specifiers already\r\nexisting in the filenames.\r\n\r\n\r\n**Example 6**\r\n\r\nThere can be scenarios where the user may want to have further control over the axes along \r\nwhich to concatenate the images. For example, the filenames might contain the data acquisition\r\ndate, which can be recognised by BatchConvert as a concatenation axis in the automatic \r\ndetection mode. An example of such a fileset might look like:\r\n\r\n```\r\nfilenames_with_dates/test_data_date03.03.2023_imageZ1-T1\r\nfilenames_with_dates/test_data_date03.03.2023_imageZ1-T2\r\nfilenames_with_dates/test_data_date03.03.2023_imageZ1-T3\r\nfilenames_with_dates/test_data_date03.03.2023_imageZ2-T1\r\nfilenames_with_dates/test_data_date03.03.2023_imageZ2-T2\r\nfilenames_with_dates/test_data_date03.03.2023_imageZ2-T3\r\nfilenames_with_dates/test_data_date04.03.2023_imageZ1-T1\r\nfilenames_with_dates/test_data_date04.03.2023_imageZ1-T2\r\nfilenames_with_dates/test_data_date04.03.2023_imageZ1-T3\r\nfilenames_with_dates/test_data_date04.03.2023_imageZ2-T1\r\nfilenames_with_dates/test_data_date04.03.2023_imageZ2-T2\r\nfilenames_with_dates/test_data_date04.03.2023_imageZ2-T3\r\n```\r\n\r\nOne may try the following command to convert this folder:\r\n\r\n`batchconvert --omezarr --merge_files \"input_dir/filenames_with_dates\" \"output_path\"`\r\n\r\nSince the concatenation axes are not specified, this command would try to create\r\na single OME-Zarr with name: `test_data_dateRange{03-04-1}.03.2023_imageZRange{1-2-1}-TRange{1-3-1}`.\r\n\r\nIn order to force BatchConvert to ignore the date field, the user can restrict the concatenation \r\naxes to the last two numeric fields. This can be done by using a command such as: \\\r\n`batchconvert --omezarr --merge_files --concatenation_order aa \"input_dir/filenames_with_dates\" \"output_path\"` \\\r\nThis command will avoid concatenation along the date field, and therefore, there will be two\r\nOME-Zarrs corresponding to the two dates. The number of characters being passed to the \r\n`--concatenation_order` option specifies the number of numeric fields (starting from the right \r\nend of the filename) that are recognised by the BatchConvert as valid concatenation axes. \r\nPassing `aa`, therefore, means that the last two numeric fields must be recognised as \r\nconcatenation axes and the dimension type should be automatically detected (`a` for automatic). \r\nIn the same logic, one could, for example, convert each Z section into a separate OME-Zarr by \r\nspecifying `--concatenation_order a`.\r\n\r\n\r\n\r\n### Conversion on slurm\r\n\r\nAll the examples given above can also be run on slurm by specifying `-pf cluster` option. \r\nNote that this option automatically uses the singularity profile:\\\r\n`batchconvert omezarr -pf cluster -p .oir \"input_path\" \"output_path\"`\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.453.3",
        "edam_operation": [
            "Conversion",
            "Data handling",
            "Format detection"
        ],
        "edam_topic": [
            "Bioimaging",
            "Data management",
            "Imaging",
            "Medical imaging"
        ],
        "filtered_on": "annot* in description",
        "id": "453",
        "keep": true,
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/453?version=3",
        "name": "BatchConvert",
        "number_of_steps": 0,
        "projects": [
            "NGFF Tools",
            "Euro-BioImaging"
        ],
        "source": "WorkflowHub",
        "tags": [
            "biostudies",
            "conversion",
            "ngff",
            "nextflow",
            "ome-tiff",
            "ome-zarr",
            "python",
            "s3",
            "bash",
            "bioformats",
            "bioformats2raw",
            "bioimaging",
            "file conversion",
            "image file format",
            "imaging"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-07-11",
        "versions": 3
    },
    {
        "create_time": "2023-07-05",
        "creators": [
            "Konstantinos Kyritsis",
            "Nikolaos Pechlivanis",
            "Fotis Psomopoulos"
        ],
        "description": "A CWL-based pipeline for calling small germline variants, namely SNPs and small INDELs, by processing data from Whole-genome Sequencing (WGS) or Targeted Sequencing (e.g., Whole-exome sequencing; WES) experiments.\r\n\r\nOn the respective GitHub folder are available:\r\n\r\n- The CWL wrappers and subworkflows for the workflow\r\n- A pre-configured YAML template, based on validation analysis of publicly available HTS data\r\n\r\nBriefly, the workflow performs the following steps:\r\n\r\n1. Quality control of Illumina reads (FastQC)\r\n2. Trimming of the reads (e.g., removal of adapter and/or low quality sequences) (Trim galore)\r\n3. Mapping to reference genome (BWA-MEM)\r\n4. Convertion of mapped reads from SAM (Sequence Alignment Map) to BAM (Binary Alignment Map) format (samtools)\r\n5. Sorting mapped reads based on read names (samtools)\r\n6. Adding information regarding paired end reads (e.g., CIGAR field information) (samtools)\r\n7. Re-sorting mapped reads based on chromosomal coordinates (samtools)\r\n8. Adding basic Read-Group information regarding sample name, platform unit, platform (e.g., ILLUMINA), library and identifier (picard AddOrReplaceReadGroups)\r\n9. Marking PCR and/or optical duplicate reads (picard MarkDuplicates)\r\n10. Collection of summary statistics (samtools) \r\n11. Creation of indexes for coordinate-sorted BAM files to enable fast random access (samtools)\r\n12. Splitting the reference genome into a predefined number of intervals for parallel processing (GATK SplitIntervals)\r\n\r\nAt this point the application of single-sample workflow follows, during which multiple samples are accepted as input and they are not merged into a unified VCF file but are rather processed separately in each step of the workflow, leading to the production of a VCF file for each sample:\r\n\r\n13. Application of Base Quality Score Recalibration (BQSR) (GATK BaseRecalibrator, GatherBQSRReports and ApplyBQSR tools)\r\n14. Variant calling (GATK HaplotypeCaller)  \r\n15. Merging of all genomic interval-split gVCF files for each sample (GATK MergeVCFs)\r\n16. Separate annotation of SNPs and INDELs based on pretrained Convolutional Neural Network (CNN) models (GATK SelectVariants, CNNScoreVariants and FilterVariantTranches tools)\r\n17. (Optional) Independent step of hard-filtering (GATK VariantFiltration)\r\n18. Variant filtering based on the information added during VQSR and/or custom filters (bcftools)\r\n19. Normalization of INDELs (split multiallelic sites) (bcftools)\r\n20. Annotation of the final dataset of filtered variants with genomic, population-related and/or clinical information (ANNOVAR)\r\n",
        "doi": "10.48546/workflowhub.workflow.527.1",
        "edam_operation": [
            "Variant calling"
        ],
        "edam_topic": [
            "Bioinformatics"
        ],
        "filtered_on": "binn* in description",
        "id": "527",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/527?version=1",
        "name": "CWL-based (single-sample) workflow for germline variant calling",
        "number_of_steps": 51,
        "projects": [
            "Biodata Analysis Group"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cwl",
            "genomics",
            "germline",
            "variant calling",
            "workflow"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-07-05",
        "versions": 1
    },
    {
        "create_time": "2023-07-05",
        "creators": [
            "Konstantinos Kyritsis",
            "Nikolaos Pechlivanis",
            "Fotis Psomopoulos"
        ],
        "description": "A CWL-based pipeline for calling small germline variants, namely SNPs and small INDELs, by processing data from Whole-genome Sequencing (WGS) or Targeted Sequencing (e.g., Whole-exome sequencing; WES) experiments. \r\n\r\nOn the respective GitHub folder are available:\r\n\r\n- The CWL wrappers and subworkflows for the workflow\r\n- A pre-configured YAML template, based on validation analysis of publicly available HTS data\r\n\r\nBriefly, the workflow performs the following steps:\r\n\r\n1. Quality control of Illumina reads (FastQC)\r\n2. Trimming of the reads (e.g., removal of adapter and/or low quality sequences) (Trim galore)\r\n3. Mapping to reference genome (BWA-MEM)\r\n4. Convertion of mapped reads from SAM (Sequence Alignment Map) to BAM (Binary Alignment Map) format (samtools)\r\n5. Sorting mapped reads based on read names (samtools)\r\n6. Adding information regarding paired end reads (e.g., CIGAR field information) (samtools)\r\n7. Re-sorting mapped reads based on chromosomal coordinates (samtools)\r\n8. Adding basic Read-Group information regarding sample name, platform unit, platform (e.g., ILLUMINA), library and identifier (picard AddOrReplaceReadGroups)\r\n9. Marking PCR and/or optical duplicate reads (picard MarkDuplicates)\r\n10. Collection of summary statistics (samtools) \r\n11. Creation of indexes for coordinate-sorted BAM files to enable fast random access (samtools)\r\n12. Splitting the reference genome into a predefined number of intervals for parallel processing (GATK SplitIntervals)\r\n\r\nAt this point the application of multi-sample workflow follows, during which multiple samples are concatenated into a single, unified VCF (Variant Calling Format) file, which contains the variant information for all samples:\r\n\r\n13. Application of Base Quality Score Recalibration (BQSR) (GATK BaseRecalibrator and ApplyBQSR tools)\r\n14. Variant calling in gVCF (genomic VCF) mode (-ERC GVCF) (GATK HaplotypeCaller)  \r\n15. Merging of all genomic interval-split gVCF files for each sample (GATK MergeVCFs)\r\n16. Generation of the unified VCF file (GATK CombineGVCFs and GenotypeGVCFs tools)\r\n17. Separate annotation for SNP and INDEL variants, using the Variant Quality Score Recalibration (VQSR) method (GATK VariantRecalibrator and ApplyVQSR tools)\r\n18. Variant filtering based on the information added during VQSR and/or custom filters (bcftools)\r\n19. Normalization of INDELs (split multiallelic sites) (bcftools)\r\n20. Annotation of the final dataset of filtered variants with genomic, population-related and/or clinical information (ANNOVAR)\r\n",
        "doi": "10.48546/workflowhub.workflow.526.1",
        "edam_operation": [
            "Variant calling"
        ],
        "edam_topic": [
            "Bioinformatics"
        ],
        "filtered_on": "binn* in description",
        "id": "526",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/526?version=1",
        "name": "CWL-based (multi-sample) workflow for germline variant calling",
        "number_of_steps": 42,
        "projects": [
            "Biodata Analysis Group"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cwl",
            "genomics",
            "germline",
            "variant calling",
            "workflow"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-07-05",
        "versions": 1
    },
    {
        "create_time": "2023-07-05",
        "creators": [
            "Konstantinos Kyritsis",
            "Nikolaos Pechlivanis",
            "Fotis Psomopoulos"
        ],
        "description": "A CWL-based pipeline for processing ChIP-Seq data (FASTQ format) and performing: \r\n\r\n- Peak calling\r\n- Consensus peak count table generation\r\n- Detection of super-enhancer regions\r\n- Differential binding analysis\r\n\r\nOn the respective GitHub folder are available:\r\n\r\n- The CWL wrappers for the workflow\r\n- A pre-configured YAML template, based on validation analysis of publicly available HTS data\r\n- Tables of metadata (``EZH2_metadata_CLL.csv`` and ``H3K27me3_metadata_CLL.csv``), based on the same validation analysis, to serve as input examples for the design of comparisons during differential binding analysis\r\n- A list of ChIP-Seq blacklisted regions (human genome version 38; hg38) from the ENCODE project, which is can be used as input for the workflow, is provided in BED format (``hg38-blacklist.v2.bed``)\r\n\r\nBriefly, the workflow performs the following steps:\r\n\r\n1. Quality control of short reads (FastQC)\r\n2. Trimming of the reads (e.g., removal of adapter and/or low quality sequences) (Trimmomatic)\r\n3. Mapping to reference genome (HISAT2)\r\n5. Convertion of mapped reads from SAM (Sequence Alignment Map) to BAM (Binary Alignment Map) format (samtools)\r\n6. Sorting mapped reads based on chromosomal coordinates (samtools)\r\n7. Adding information regarding paired end reads (e.g., CIGAR field information) (samtools)\r\n8. Re-sorting based on chromosomal coordinates (samtools)\r\n9. Removal of duplicate reads (samtools)\r\n10. Index creation for coordinate-sorted BAM files to enable fast random access (samtools)\r\n11. Production of quality metrics and files for the inspection of the mapped ChIP-Seq reads, taking into consideration the experimental design (deeptools2):\r\n - Read coverages for genomic regions of two or more BAM files are computed (multiBamSummary). The results are produced in compressed numpy array (NPZ) format and are used to calculate and visualize pairwise correlation values between the read coverages (plotCorrelation). \r\n - Estimation of sequencing depth, through genomic position (base pair) sampling, and visualization is performed for multiple BAM files (plotCoverage).\r\n - Cumulative read coverages for each indexed BAM file are plotted by counting and sorting all reads overlapping a \u201cwindow\u201d of specified length (plotFingerprint).\r\n - Production of coverage track files (bigWig), with the coverage calculated as the number of reads per consecutive windows of predefined size (bamCoverage), and normalized through various available methods (e.g., Reads Per Kilobase per Million mapped reads; RPKM). The coverage track files are used to calculate scores per selected genomic regions (computeMatrix), typically genes, and a heatmap, based on the scores associated with these genomic regions, is produced (plotHeatmap).\r\n12. Calling potential binding positions (peaks) to the genome (peak calling) (MACS2)\r\n13. Generation of consensus peak count table for the application of custom analyses on MACS2 peak calling results (bedtools)\r\n14. Detection of super-enhancer regions (Rank Ordering of Super-Enhancers; ROSE)\r\n15. Differential binding analyses (DiffBind) for:\r\n - MACS2 peak calling results\r\n - ROSE-detected super-enhancer regions \r\n ",
        "doi": "10.48546/workflowhub.workflow.525.1",
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics"
        ],
        "filtered_on": "metap* in description",
        "id": "525",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/525?version=1",
        "name": "CWL-based ChIP-Seq workflow",
        "number_of_steps": 53,
        "projects": [
            "Biodata Analysis Group"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cwl",
            "chip-seq",
            "epigenomics",
            "workflow"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-07-05",
        "versions": 1
    },
    {
        "create_time": "2023-07-05",
        "creators": [
            "Konstantinos Kyritsis",
            "Nikolaos Pechlivanis",
            "Fotis Psomopoulos"
        ],
        "description": "A CWL-based pipeline for processing RNA-Seq data (FASTQ format) and performing differential gene/transcript expression analysis. \r\n\r\nOn the respective GitHub folder are available:\r\n\r\n- The CWL wrappers for the workflow\r\n- A pre-configured YAML template, based on validation analysis of publicly available HTS data\r\n- A table of metadata (``mrna_cll_subsets_phenotypes.csv``), based on the same validation analysis, to serve as an input example for the design of comparisons during differential expression analysis\r\n\r\nBriefly, the workflow performs the following steps:\r\n\r\n1. Quality control of Illumina reads (FastQC)\r\n2. Trimming of the reads (e.g., removal of adapter and/or low quality sequences) (Trim galore)\r\n3. (Optional)  custom processing of the reads using FASTA/Q Trimmer (part of the FASTX-toolkit) \r\n4. Mapping to reference genome (HISAT2)\r\n5. Convertion of mapped reads from SAM (Sequence Alignment Map) to BAM (Binary Alignment Map) format (samtools)\r\n6. Sorting mapped reads based on chromosomal coordinates (samtools)\r\n\r\nSubsequently, two independent workflows are implemented for differential expression analysis at the transcript and gene level. \r\n\r\n**First**, following the [reference protocol](https://doi.org/10.1038/nprot.2016.095) for HISAT, StringTie and Ballgown transcript expression analysis, StringTie along with a reference transcript annotation GTF (Gene Transfer Format) file (if one is available) is used to:\r\n\r\n- Assemble transcripts for each RNA-Seq sample using the previous read alignments (BAM files)\r\n- Generate a global, non-redundant set of transcripts observed in any of the RNA-Seq samples\r\n- Estimate transcript abundances and generate read coverage tables for each RNA-Seq sample, based on the global, merged set of transcripts (rather than the reference) which is observed across all samples\r\n\r\nBallgown program is then used to load the coverage tables generated in the previous step and perform statistical analyses for differential expression at the transcript level. Notably, the StringTie - Ballgown protocol applied here was selected to include potentially novel transcripts in the analysis. \r\n\r\n**Second**, featureCounts is used to count reads that are mapped to selected genomic features, in this case genes by default, and generate a table of read counts per gene and sample. This table is passed as input to DESeq2 to perform differential expression analysis at the gene level. Both Ballgown and DESeq2 R scripts, along with their respective CWL wrappers, were designed to receive as input various parameters, such as experimental design, contrasts of interest, numeric thresholds, and hidden batch effects.\r\n",
        "doi": "10.48546/workflowhub.workflow.524.1",
        "edam_operation": [
            "RNA-Seq analysis"
        ],
        "edam_topic": [
            "Bioinformatics"
        ],
        "filtered_on": "metap* in description",
        "id": "524",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/524?version=1",
        "name": "CWL-based RNA-Seq workflow",
        "number_of_steps": 27,
        "projects": [
            "Biodata Analysis Group"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cwl",
            "rnaseq",
            "transcriptomics",
            "workflow"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-07-05",
        "versions": 1
    },
    {
        "create_time": "2023-06-28",
        "creators": [
            "Peter van Heusden",
            "Bradley W. Langhorst"
        ],
        "description": "SARS-CoV-2 variant prediction using Read It And Keep, fastp, bbmap and iVar",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in name",
        "id": "519",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/519?version=1",
        "name": "SARS-CoV-2 Illumina Amplicon pipeline - SANBI - v1.2",
        "number_of_steps": 20,
        "projects": [
            "SANBI Pathogen Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "artic",
            "sanbi",
            "sars-cov-2",
            "covid-19"
        ],
        "tools": [
            "fastp",
            "compose_text_param",
            "samtools_stats",
            "samtools_view",
            "ivar_variants",
            "snpeff_sars_cov_2",
            "read_it_and_keep",
            "tp_awk_tool",
            "__FLATTEN__",
            "qualimap_bamqc",
            "tp_sed_tool",
            "collapse_dataset",
            "samtools_ampliconclip",
            "ivar_consensus",
            "bbtools_bbmap",
            "tp_cat",
            "multiqc",
            "deeptools_bam_coverage"
        ],
        "type": "Galaxy",
        "update_time": "2023-06-30",
        "versions": 1
    },
    {
        "create_time": "2023-06-29",
        "creators": [],
        "description": "",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in name",
        "id": "521",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/521?version=1",
        "name": "SARS-CoV-2 ONT Amplicon Sequencing SANBI 1.0",
        "number_of_steps": 5,
        "projects": [
            "SANBI Pathogen Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "sanbi",
            "sars-cov-2",
            "nanopore"
        ],
        "tools": [
            "nanoplot",
            "artic_guppyplex",
            "artic_minion",
            "read_it_and_keep"
        ],
        "type": "Galaxy",
        "update_time": "2023-06-29",
        "versions": 1
    },
    {
        "create_time": "2023-06-27",
        "creators": [
            "Elisabetta Spinazzola"
        ],
        "description": "The project allowed us to manage and build structured code scripts on the Jupyter Notebook, a simple web application which is user-friendly, flexible to use in the research community. The script is developed to address the specific needs of research between different platforms of dataset.\r\nThese stakeholders have developed their own platforms for the annotation and standardisation of both data and metadata produced within their respective field.\r\n-The INFRAFRONTIER - European Mutant Mouse Archive (EMMA) comprises over 7200 mutant mouse lines that are extensively integrated and enriched with other public dataset.\r\n-The EU-OpenScreen offers compound screening protocols containing several metadata and will contribute to the development of tools for linking to the chemical entity database.\r\n-The IDR Image Data Resource is a public repository of reference image datasets from published scientific studies, where the community can submit, search and access high-quality bio-image data. \r\n-The CIM-XNAT is an XNAT deployment of the Molecular Imaging Center at UniTo that offers a suite of tools for uploading preclinical images.\r\nTo address the challenges of integrating several EU-RI datasets with focus on preclinical and discovery research bioimaging, our aim is to develop cross researching queries through a web based interface  to combine the resources of the RIs for integrating the information associated with data belonging to the involved RIs. Furthermore, the open-source tool provides users with free, open access to collections of datasets distributed over multiple sources that result from searches by specific keywords. \r\nThe script allows the cross research in different fields of research as: Species, Strain, Gene, Cell line, Disease model, Chemical Compound.\r\nThe novel aspects of this tool are mainly:\r\na) user friendly, e.g. the user has the flexibility to research among the dataset easily with a simple API, intuitive for researchers and biomedical users.  \r\nb) the possibility of making a research between different platforms and repositories, from a unique simple way. \r\nc) the workflow project follows the FAIR principles in the treatment of data and datasets. \r\nThe access to Notebook Jupyter needs the installation of Anaconda, which consents to open the web application. \r\nInside the Jupyter, the script was built using Python. The query code is also easy to download and share in a .ipynb file.\r\nA visual representation of the detailed results (dataset, metadata, information, query results) of the workflow can be printed immediately after the query run. \r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "516",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/516?version=1",
        "name": "Life Science cross-RI (Research Infrastructure) project",
        "number_of_steps": 0,
        "projects": [
            "EOSC-Life WP3 OC Team, cross RI project"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Jupyter",
        "update_time": "2023-06-27",
        "versions": 1
    },
    {
        "create_time": "2023-06-21",
        "creators": [],
        "description": "# CWL-assembly\r\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/684724bbc0134960ab41748f4a4b732f)](https://www.codacy.com/app/mb1069/CWL-assembly?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=EBI-Metagenomics/CWL-assembly&amp;utm_campaign=Badge_Grade)\r\n[![Build Status](https://travis-ci.org/EBI-Metagenomics/CWL-assembly.svg?branch=develop)](https://travis-ci.org/EBI-Metagenomics/CWL-assembly)\r\n\r\n## Description\r\n\r\nThis repository contains two workflows for metagenome and metatranscriptome assembly of short read data. MetaSPAdes is used as default for paired-end data, and MEGAHIT for single-end data and co-assemblies. MEGAHIT can be specified as the default assembler in the yaml file if preferred. Steps include:\r\n\r\n  * _QC_: removal of short reads, low quality regions, adapters and host decontamination\r\n  * _Assembly_: with metaSPADES or MEGAHIT\r\n  * _Post-assembly_: Host and PhiX decontamination, contig length filter (500bp), stats generation\r\n\r\n## Requirements - How to install\r\n\r\nThis pipeline requires a conda environment with cwltool, blastn, and metaspades. If created with `requirements.yml`, the environment will be called `cwl_assembly`. \r\n\r\n```\r\nconda env create -f requirements.yml\r\nconda activate cwl_assembly\r\npip install cwltool==3.1.20230601100705\r\n```\r\n\r\n## Databases\r\n\r\nYou will need to pre-download fasta files for host decontamination and generate the following databases accordingly:\r\n  * bwa index\r\n  * blast index\r\n    \r\nSpecify the locations in the yaml file when running the pipeline.\r\n\r\n## Main pipeline executables\r\n\r\n  * `src/workflows/metagenome_pipeline.cwl`\r\n  * `src/workflows/metatranscriptome_pipeline.cwl`\r\n\r\n## Example command\r\n\r\n```cwltool --singularity --outdir ${OUTDIR} ${CWL} ${YML}```\r\n\r\n`$CWL` is going to be one of the executables mentioned above\r\n`$YML` should be a config yaml file including entries among what follows. \r\nYou can find a yml template in the `examples` folder.\r\n\r\n## Example output directory structure\r\n```\r\nRoot directory\r\n    \u251c\u2500\u2500 megahit\r\n    \u2502   \u2514\u2500\u2500 001 -------------------------------- Assembly root directory\r\n    \u2502       \u251c\u2500\u2500 assembly_stats.json ------------ Human-readable assembly stats file\r\n    \u2502       \u251c\u2500\u2500 coverage.tab ------------------- Coverage file\r\n    \u2502       \u251c\u2500\u2500 log ---------------------------- CwlToil+megahit output log\r\n    |       \u251c\u2500\u2500 options.json ------------------- Megahit input options\r\n    \u2502       \u251c\u2500\u2500 SRR6257420.fasta.gz ------------ Archived and trimmed assembly\r\n    \u2502       \u2514\u2500\u2500 SRR6257420.fasta.gz.md5 -------- MD5 hash of above archive\r\n    \u251c\u2500\u2500 metaspades\r\n    \u2502   \u2514\u2500\u2500 001 -------------------------------- Assembly root directory\r\n    \u2502       \u251c\u2500\u2500 assembly_graph.fastg ----------- Assembly graph\r\n    \u2502       \u251c\u2500\u2500 assembly_stats.json ------------ Human-readable assembly stats file\r\n    \u2502       \u251c\u2500\u2500 coverage.tab ------------------- Coverage file\r\n    |       \u251c\u2500\u2500 params.txt --------------------- Metaspades input options\r\n    \u2502       \u251c\u2500\u2500 spades.log --------------------- Metaspades output log\r\n    \u2502       \u251c\u2500\u2500 SRR6257420.fasta.gz ------------ Archived and trimmed assembly\r\n    \u2502       \u2514\u2500\u2500 SRR6257420.fasta.gz.md5 -------- MD5 hash of above archive\r\n    \u2502\u00a0\r\n    \u2514\u2500\u2500 raw ------------------------------------ Raw data directory\r\n        \u251c\u2500\u2500 SRR6257420.fastq.qc_stats.tsv ------ Stats for cleaned fastq\r\n        \u251c\u2500\u2500 SRR6257420_fastp_clean_1.fastq.gz -- Cleaned paired-end file_1\r\n        \u2514\u2500\u2500 SRR6257420_fastp_clean_2.fastq.gz -- Cleaned paired-end file_2\r\n```\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in name",
        "id": "474",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/474?version=2",
        "name": "Metagenome and metatranscriptome assembly in CWL",
        "number_of_steps": 0,
        "projects": [
            "HoloFood at MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-06-21",
        "versions": 2
    },
    {
        "create_time": "2020-11-03",
        "creators": [],
        "description": "This workflow has been created as part of Demonstrator 6 of the project EOSC-Life (within WP3) and is focused on reusing publicly available RNAi screens to gain insights into the nucleolus biology. The workflow downloads images from the Image Data Resource (IDR), performs object segmentation (of nuclei and nucleoli) and feature extraction of the images and objects identified.\r\n\r\nTutorial: https://training.galaxyproject.org/training-material/topics/imaging/tutorials/tutorial-CP/tutorial.html",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioimaging",
            "Imaging"
        ],
        "filtered_on": "profil* in tags",
        "id": "41",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/41?version=2",
        "name": "Nucleoli segmentation using CellProfiler (EOSC-Life D6)",
        "number_of_steps": 25,
        "projects": [
            "IBISBA Workflows",
            "Euro-BioImaging",
            "EOSC-Life WP3"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cellprofiler",
            "galaxy",
            "image processing",
            "imaging"
        ],
        "tools": [
            "cp_measure_granularity",
            "cp_measure_object_size_shape",
            "cp_save_images",
            "cp_image_math",
            "cp_mask_image",
            "cp_measure_image_quality",
            "cp_enhance_or_suppress_features",
            "cp_export_to_spreadsheet",
            "cp_relate_objects",
            "cp_measure_texture",
            "cp_convert_objects_to_image",
            "cp_measure_object_intensity",
            "cp_cellprofiler",
            "cp_gray_to_color",
            "cp_measure_image_area_occupied",
            "cp_identify_primary_objects",
            "cp_display_data_on_image",
            "cp_common",
            "idr_download_by_ids",
            "cp_measure_image_intensity"
        ],
        "type": "Galaxy",
        "update_time": "2023-07-03",
        "versions": 1
    },
    {
        "create_time": "2023-04-27",
        "creators": [
            "Andrea Furlani",
            "Philipp Gormanns"
        ],
        "description": "# Gene similariy anaylsis across physiological systems in IMPC phenotype data\r\n\r\nA Jupyter Notebook tool for analysing user specified genes across the different physiological systems in IMPC data.\r\n\r\n**_Input_**\r\n\r\nThe tool takes as input a list of gene ids (MGI ids or Gene Symbol ids). The elemnts in the list could be separated by a comma, semicolumn, tab or newline.\r\n\r\n**_Operation_**\r\n\r\nThe program will create an heatmap representing the number of phenotypes and the mp term list for each gene contained in an [IMPC physiological system](https://www.mousephenotype.org/help/data-visualization/gene-pages/phenogrid/).\r\nUsing the slider, adjust the treshold to set the minimum count to be displayed in the heatmap. \r\n\r\nNB: Genes without phenotypes in any physiological system will not be displayed. Also, the labels of the heatmap will use Gene Symbols independently from the type of id used in the input.\r\n\r\n**_Tool access:_** [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AndreaFurlani/Jupyter_interactive_plots/main?urlpath=voila%2Frender%2FInteractive_plots.ipynb)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "460",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/460?version=1",
        "name": "Gene similariy anaylsis across physiological systems in IMPC phenotype data",
        "number_of_steps": 0,
        "projects": [
            "INFRAFRONTIER workflows"
        ],
        "source": "WorkflowHub",
        "tags": [
            "jupyter"
        ],
        "tools": [],
        "type": "Jupyter",
        "update_time": "2023-06-16",
        "versions": 1
    },
    {
        "create_time": "2023-06-01",
        "creators": [
            "Jean-Marie Burel",
            "Petr Walczysko"
        ],
        "description": "## Learning Objectives\r\n- How to access genomic resource via its Python API\r\n- How to access image resource via its Python API\r\n- Relate image data to genomic data\r\n\r\n## Diabetes related genes expressed in pancreas\r\n\r\nThis notebook looks at the question **Which diabetes related genes are expressed in the pancreas?** Tissue and disease can be modified.\r\n\r\nSteps:\r\n\r\n- Query [humanmine.org](https://www.humanmine.org/humanmine), an integrated database of Homo sapiens genomic data using the intermine API to find the genes.\r\n- Using the list of found genes, search in the [Image Data Resource (IDR)](https://idr.openmicroscopy.org/) for images linked to the genes, tissue and disease.\r\n- Analyse the images found.\r\n\r\n## Launch\r\nThis notebook uses the [environment.yml](https://github.com/ome/EMBL-EBI-imaging-course-05-2023/blob/main/Day_4/environment.yml) file.\r\n\r\nSee [Setup](https://github.com/ome/EMBL-EBI-imaging-course-05-2023/blob/main/Day_4/setup.md).\r\n",
        "doi": "10.48546/workflowhub.workflow.494.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "494",
        "keep": true,
        "latest_version": 1,
        "license": "BSD-2-Clause",
        "link": "https:/workflowhub.eu/workflows/494?version=1",
        "name": "Use Public Resources to answer a biological question",
        "number_of_steps": 0,
        "projects": [
            "OME"
        ],
        "source": "WorkflowHub",
        "tags": [
            "python",
            "imaging"
        ],
        "tools": [],
        "type": "Jupyter",
        "update_time": "2023-06-01",
        "versions": 1
    },
    {
        "create_time": "2023-06-01",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# Protein Conformational ensembles generation\r\n\r\n## Workflow included in the [ELIXIR 3D-Bioinfo](https://elixir-europe.org/communities/3d-bioinfo) Implementation Study:\r\n\r\n### Building on PDBe-KB to chart and characterize the conformation landscape of native proteins\r\n\r\nThis tutorial aims to illustrate the process of generating **protein conformational ensembles** from** 3D structures **and analysing its **molecular flexibility**, step by step, using the **BioExcel Building Blocks library (biobb)**.\r\n\r\n## Conformational landscape of native proteins\r\n**Proteins** are **dynamic** systems that adopt multiple **conformational states**, a property essential for many **biological processes** (e.g. binding other proteins, nucleic acids, small molecule ligands, or switching between functionaly active and inactive states). Characterizing the different **conformational states** of proteins and the transitions between them is therefore critical for gaining insight into their **biological function** and can help explain the effects of genetic variants in **health** and **disease** and the action of drugs.\r\n\r\n**Structural biology** has become increasingly efficient in sampling the different **conformational states** of proteins. The **PDB** has currently archived more than **170,000 individual structures**, but over two thirds of these structures represent **multiple conformations** of the same or related protein, observed in different crystal forms, when interacting with other proteins or other macromolecules, or upon binding small molecule ligands. Charting this conformational diversity across the PDB can therefore be employed to build a useful approximation of the **conformational landscape** of native proteins.\r\n\r\nA number of resources and **tools** describing and characterizing various often complementary aspects of protein **conformational diversity** in known structures have been developed, notably by groups in Europe. These tools include algorithms with varying degree of sophistication, for aligning the 3D structures of individual protein chains or domains, of protein assemblies, and evaluating their degree of **structural similarity**. Using such tools one can **align structures pairwise**, compute the corresponding **similarity matrix**, and identify ensembles of **structures/conformations** with a defined **similarity level** that tend to recur in different PDB entries, an operation typically performed using **clustering** methods. Such workflows are at the basis of resources such as **CATH, Contemplate, or PDBflex** that offer access to **conformational ensembles** comprised of similar **conformations** clustered according to various criteria. Other types of tools focus on differences between **protein conformations**, identifying regions of proteins that undergo large **collective displacements** in different PDB entries, those that act as **hinges or linkers**, or regions that are inherently **flexible**.\r\n\r\nTo build a meaningful approximation of the **conformational landscape** of native proteins, the **conformational ensembles** (and the differences between them), identified on the basis of **structural similarity/dissimilarity** measures alone, need to be **biophysically characterized**. This may be approached at **two different levels**. \r\n- At the **biological level**, it is important to link observed **conformational ensembles**, to their **functional roles** by evaluating the correspondence with **protein family classifications** based on sequence information and **functional annotations** in public databases e.g. Uniprot, PDKe-Knowledge Base (KB). These links should provide valuable mechanistic insights into how the **conformational and dynamic properties** of proteins are exploited by evolution to regulate their **biological function**. <br><br>\r\n\r\n- At the **physical level** one needs to introduce **energetic consideration** to evaluate the likelihood that the identified **conformational ensembles** represent **conformational states** that the protein (or domain under study) samples in isolation. Such evaluation is notoriously **challenging** and can only be roughly approximated by using **computational methods** to evaluate the extent to which the observed **conformational ensembles** can be reproduced by algorithms that simulate the **dynamic behavior** of protein systems. These algorithms include the computationally expensive **classical molecular dynamics (MD) simulations** to sample local thermal fluctuations but also faster more approximate methods such as **Elastic Network Models** and **Normal Node Analysis** (NMA) to model low energy **collective motions**. Alternatively, **enhanced sampling molecular dynamics** can be used to model complex types of **conformational changes** but at a very high computational cost. \r\n\r\nThe **ELIXIR 3D-Bioinfo Implementation Study** *Building on PDBe-KB to chart and characterize the conformation landscape of native proteins* focuses on:\r\n\r\n1. Mapping the **conformational diversity** of proteins and their homologs across the PDB. \r\n2. Characterize the different **flexibility properties** of protein regions, and link this information to sequence and functional annotation.\r\n3. Benchmark **computational methods** that can predict a biophysical description of protein motions.\r\n\r\nThis notebook is part of the third objective, where a list of **computational resources** that are able to predict **protein flexibility** and **conformational ensembles** have been collected, evaluated, and integrated in reproducible and interoperable workflows using the **BioExcel Building Blocks library**. Note that the list is not meant to be exhaustive, it is built following the expertise of the implementation study partners.\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.490.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "490",
        "keep": true,
        "latest_version": 1,
        "license": "other-open",
        "link": "https:/workflowhub.eu/workflows/490?version=1",
        "name": "Galaxy Protein conformational ensembles generation",
        "number_of_steps": 43,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "biobb_flexserv_bd_run_ext",
            "biobb_analysis_cpptraj_convert_ext",
            "biobb_flexserv_pcz_evecs_ext",
            "biobb_analysis_cpptraj_mask_ext",
            "biobb_flexdyn_imod_imode_ext",
            "biobb_flexserv_nma_run_ext",
            "biobb_gromacs_make_ndx_ext",
            "biobb_structure_utils_extract_chain_ext",
            "biobb_flexdyn_imod_imc_ext",
            "biobb_flexdyn_concoord_dist_ext",
            "biobb_io_pdb_ext",
            "zip",
            "biobb_flexserv_pcz_hinges_ext",
            "biobb_analysis_gmx_cluster_ext",
            "biobb_flexserv_pcz_zip_ext",
            "biobb_flexdyn_prody_anm_ext",
            "biobb_flexserv_pcz_animate_ext",
            "biobb_flexdyn_nolb_nma_ext",
            "biobb_flexdyn_concoord_disco_ext",
            "biobb_flexserv_pcz_collectivity_ext",
            "biobb_flexserv_dmd_run_ext",
            "biobb_flexserv_pcz_info_ext",
            "biobb_gromacs_trjcat_ext",
            "biobb_flexserv_pcz_stiffness_ext",
            "biobb_flexserv_pcz_bfactor_ext",
            "biobb_analysis_cpptraj_rms_ext",
            "biobb_structure_utils_extract_model_ext"
        ],
        "type": "Galaxy",
        "update_time": "2023-06-01",
        "versions": 1
    },
    {
        "create_time": "2023-06-06",
        "creators": [
            "Adam Hospital",
            "Gen\u00eds Bayarri"
        ],
        "description": "# Protein Conformational ensembles generation\r\n\r\n## Workflow included in the [ELIXIR 3D-Bioinfo](https://elixir-europe.org/communities/3d-bioinfo) Implementation Study:\r\n\r\n### Building on PDBe-KB to chart and characterize the conformation landscape of native proteins\r\n\r\nThis tutorial aims to illustrate the process of generating **protein conformational ensembles** from** 3D structures **and analysing its **molecular flexibility**, step by step, using the **BioExcel Building Blocks library (biobb)**.\r\n\r\n## Conformational landscape of native proteins\r\n**Proteins** are **dynamic** systems that adopt multiple **conformational states**, a property essential for many **biological processes** (e.g. binding other proteins, nucleic acids, small molecule ligands, or switching between functionaly active and inactive states). Characterizing the different **conformational states** of proteins and the transitions between them is therefore critical for gaining insight into their **biological function** and can help explain the effects of genetic variants in **health** and **disease** and the action of drugs.\r\n\r\n**Structural biology** has become increasingly efficient in sampling the different **conformational states** of proteins. The **PDB** has currently archived more than **170,000 individual structures**, but over two thirds of these structures represent **multiple conformations** of the same or related protein, observed in different crystal forms, when interacting with other proteins or other macromolecules, or upon binding small molecule ligands. Charting this conformational diversity across the PDB can therefore be employed to build a useful approximation of the **conformational landscape** of native proteins.\r\n\r\nA number of resources and **tools** describing and characterizing various often complementary aspects of protein **conformational diversity** in known structures have been developed, notably by groups in Europe. These tools include algorithms with varying degree of sophistication, for aligning the 3D structures of individual protein chains or domains, of protein assemblies, and evaluating their degree of **structural similarity**. Using such tools one can **align structures pairwise**, compute the corresponding **similarity matrix**, and identify ensembles of **structures/conformations** with a defined **similarity level** that tend to recur in different PDB entries, an operation typically performed using **clustering** methods. Such workflows are at the basis of resources such as **CATH, Contemplate, or PDBflex** that offer access to **conformational ensembles** comprised of similar **conformations** clustered according to various criteria. Other types of tools focus on differences between **protein conformations**, identifying regions of proteins that undergo large **collective displacements** in different PDB entries, those that act as **hinges or linkers**, or regions that are inherently **flexible**.\r\n\r\nTo build a meaningful approximation of the **conformational landscape** of native proteins, the **conformational ensembles** (and the differences between them), identified on the basis of **structural similarity/dissimilarity** measures alone, need to be **biophysically characterized**. This may be approached at **two different levels**. \r\n- At the **biological level**, it is important to link observed **conformational ensembles**, to their **functional roles** by evaluating the correspondence with **protein family classifications** based on sequence information and **functional annotations** in public databases e.g. Uniprot, PDKe-Knowledge Base (KB). These links should provide valuable mechanistic insights into how the **conformational and dynamic properties** of proteins are exploited by evolution to regulate their **biological function**. <br><br>\r\n\r\n- At the **physical level** one needs to introduce **energetic consideration** to evaluate the likelihood that the identified **conformational ensembles** represent **conformational states** that the protein (or domain under study) samples in isolation. Such evaluation is notoriously **challenging** and can only be roughly approximated by using **computational methods** to evaluate the extent to which the observed **conformational ensembles** can be reproduced by algorithms that simulate the **dynamic behavior** of protein systems. These algorithms include the computationally expensive **classical molecular dynamics (MD) simulations** to sample local thermal fluctuations but also faster more approximate methods such as **Elastic Network Models** and **Normal Node Analysis** (NMA) to model low energy **collective motions**. Alternatively, **enhanced sampling molecular dynamics** can be used to model complex types of **conformational changes** but at a very high computational cost. \r\n\r\nThe **ELIXIR 3D-Bioinfo Implementation Study** *Building on PDBe-KB to chart and characterize the conformation landscape of native proteins* focuses on:\r\n\r\n1. Mapping the **conformational diversity** of proteins and their homologs across the PDB. \r\n2. Characterize the different **flexibility properties** of protein regions, and link this information to sequence and functional annotation.\r\n3. Benchmark **computational methods** that can predict a biophysical description of protein motions.\r\n\r\nThis notebook is part of the third objective, where a list of **computational resources** that are able to predict **protein flexibility** and **conformational ensembles** have been collected, evaluated, and integrated in reproducible and interoperable workflows using the **BioExcel Building Blocks library**. Note that the list is not meant to be exhaustive, it is built following the expertise of the implementation study partners.\r\n\r\n***\r\n\r\n## Copyright & Licensing\r\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\r\n\r\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\r\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\r\n\r\nLicensed under the\r\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\r\n\r\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")",
        "doi": "10.48546/workflowhub.workflow.488.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "488",
        "keep": true,
        "latest_version": 2,
        "license": "other-open",
        "link": "https:/workflowhub.eu/workflows/488?version=2",
        "name": "CWL Protein conformational ensembles generation",
        "number_of_steps": 40,
        "projects": [
            "BioBB Building Blocks"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Clusters structures from a given GROMACS compatible trajectory.",
            "Extract PCA info (variance, Dimensionality) from a compressed PCZ file.",
            "Run Brownian Dynamics from FlexServ.",
            "Extract PCA collectivity (numerical measure of how many atoms are affected by a given mode) from a compressed PCZ file.",
            "Creates a GROMACS index file (NDX) from an input selection and an input GROMACS structure file.",
            "Extract PCA animations from a compressed PCZ file.",
            "Extracts a model from a 3D structure.",
            "Converts between cpptraj compatible trajectory file formats and/or extracting a selection of atoms or frames.",
            "Wrapper of the Concoord_dist software.",
            "Calculates the Root Mean Square deviation (RMSd) of a given cpptraj compatible trajectory.",
            "Extracts a selection of atoms from a given cpptraj compatible trajectory.",
            "Wrapper of the Nolb software.",
            "Wrapper of the imods_imc software.",
            "Run Discrete Molecular Dynamics from FlexServ.",
            "Wrapper of the imods_imode software.",
            "Extract PCA stiffness from a compressed PCZ file.",
            "Run Normal Mode Analysis from FlexServ.",
            "Extract PCA Eigen Vectors from a compressed PCZ file.",
            "Extract residue bfactors x PCA mode from a compressed PCZ file.",
            "Compute possible hinge regions (residues around which large protein movements are organized) of a molecule from a compressed PCZ file.",
            "Wrapper of the Prody software.",
            "Compress MD simulation trajectories with PCA suite.",
            "Extracts a chain from a 3D structure.",
            "Wrapper of the Concoord_disco software."
        ],
        "type": "Common Workflow Language",
        "update_time": "2024-04-22",
        "versions": 2
    },
    {
        "create_time": "2023-04-27",
        "creators": [
            "Andrea Furlani",
            "Philipp Gormanns"
        ],
        "description": "# **Phenotype similarity analysis**\r\n\r\nA Jupyter Notebook for analyzing phenotyping similarities across user specified genes. Phenotypes are retrieved from the MGI resource\r\n\r\n**_Input_**\r\n\r\nThe tool takes as input a list of gene ids (MGI ids or Gene Symbol ids). The elemnts in the list could be separated by a comma, semicolumn, tab or newline.\r\n\r\n**_Operation_**\r\n\r\nThe Notebook will create a table where row and columns names are the Gene Symbols of the input elements and each cell will contain the name of the common phenotypes shared by those genes. Then an interactive heatmap will be displayed, showing also the count of those phenotypes. Using the slider, adjust the treshold to set the minimum count to be displayed in the heatmap.\r\n\r\nNB: Genes with only counts below the treshold will be not displayed in the heatmap. Also, the labels of the heatmap will use Gene Symbols independently from the type of id used in the input.\r\n\r\n**_Tool access:_** [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AndreaFurlani/Jupyter_alliance/main?urlpath=voila%2Frender%2FAlliance_API_query.ipynb)\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "461",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/461?version=1",
        "name": "Mouse phenotype similarity analyis",
        "number_of_steps": 0,
        "projects": [
            "INFRAFRONTIER workflows"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Jupyter",
        "update_time": "2023-05-25",
        "versions": 1
    },
    {
        "create_time": "2023-05-23",
        "creators": [
            "Henrik Nortamo"
        ],
        "description": "# COVID-19 Multiscale Modelling of the Virus and Patients\u2019 Tissue Workflow\r\n\r\n## Table of Contents\r\n\r\n- [COVID-19 Multiscale Modelling of the Virus and Patients\u2019 Tissue Workflow](#covid-19-multiscale-modelling-of-the-virus-and-patients-tissue-workflow)\r\n  - [Table of Contents](#table-of-contents)\r\n  - [Description](#description)\r\n  - [Contents](#contents)\r\n    - [Building Blocks](#building-blocks)\r\n    - [Workflows](#workflows)\r\n    - [Resources](#resources)\r\n    - [Tests](#tests)\r\n  - [Instructions](#instructions)\r\n    - [Local machine](#local-machine)\r\n      - [Requirements](#requirements)\r\n      - [Usage steps](#usage-steps)\r\n    - [MareNostrum 4](#marenostrum-4)\r\n      - [Requirements in MN4](#requirements-in-mn4)\r\n      - [Usage steps in MN4](#usage-steps-in-mn4)\r\n    - [Mahti or Puhti](#mahti-or-puhti)\r\n      - [Requirements](#requirements)\r\n      - [Steps](#steps)\r\n  - [License](#license)\r\n  - [Contact](#contact)\r\n\r\n## Description\r\n\r\nUses multiscale simulations to predict patient-specific SARS\u2011CoV\u20112 severity subtypes\r\n(moderate, severe or control), using single-cell RNA-Seq data, MaBoSS and PhysiBoSS.\r\nBoolean models are used to determine the behaviour of individual agents as a function\r\nof extracellular conditions and the concentration of different  substrates, including\r\nthe number of virions. Predictions of severity subtypes are based on a meta-analysis of\r\npersonalised model outputs simulating cellular apoptosis regulation in epithelial cells\r\ninfected by SARS\u2011CoV\u20112.\r\n\r\nThe workflow uses the following building blocks, described in order of execution:\r\n\r\n1. High-throughput mutant analysis\r\n2. Single-cell processing\r\n3. Personalise patient\r\n4. PhysiBoSS\r\n5. Analysis of all simulations\r\n\r\nFor details on individual workflow steps, see the user documentation for each building block.\r\n\r\n[`GitHub repository`](<https://github.com/PerMedCoE/covid-19-workflow>)\r\n\r\n\r\n## Contents\r\n\r\n### Building Blocks\r\n\r\nThe ``BuildingBlocks`` folder contains the script to install the\r\nBuilding Blocks used in the COVID-19 Workflow.\r\n\r\n### Workflows\r\n\r\nThe ``Workflow`` folder contains the workflows implementations.\r\n\r\nCurrently contains the implementation using PyCOMPSs and Snakemake (in progress).\r\n\r\n### Resources\r\n\r\nThe ``Resources`` folder contains dataset files.\r\n\r\n### Tests\r\n\r\nThe ``Tests`` folder contains the scripts that run each Building Block\r\nused in the workflow for the given small dataset.\r\nThey can be executed individually for testing purposes.\r\n\r\n## Instructions\r\n\r\n### Local machine\r\n\r\nThis section explains the requirements and usage for the COVID19 Workflow in a laptop or desktop computer.\r\n\r\n#### Requirements\r\n\r\n- [`permedcoe`](https://github.com/PerMedCoE/permedcoe) package\r\n- [PyCOMPSs](https://pycompss.readthedocs.io/en/stable/Sections/00_Quickstart.html) / [Snakemake](https://snakemake.readthedocs.io/en/stable/)\r\n- [Singularity](https://sylabs.io/guides/3.0/user-guide/installation.html)\r\n\r\n#### Usage steps\r\n\r\n1. Clone this repository:\r\n\r\n  ```bash\r\n  git clone https://github.com/PerMedCoE/covid-19-workflow.git\r\n  ```\r\n\r\n2. Install the Building Blocks required for the COVID19 Workflow:\r\n\r\n  ```bash\r\n  covid-19-workflow/BuildingBlocks/./install_BBs.sh\r\n  ```\r\n\r\n3. Get the required Building Block images from the project [B2DROP](https://b2drop.bsc.es/index.php/f/444350):\r\n\r\n  - Required images:\r\n      - MaBoSS.singularity\r\n      - meta_analysis.singularity\r\n      - PhysiCell-COVID19.singularity\r\n      - single_cell.singularity\r\n\r\n  The path where these files are stored **MUST be exported in the `PERMEDCOE_IMAGES`** environment variable.\r\n\r\n  > :warning: **TIP**: These containers can be built manually as follows (be patient since some of them may take some time):\r\n  1. Clone the `BuildingBlocks` repository\r\n     ```bash\r\n     git clone https://github.com/PerMedCoE/BuildingBlocks.git\r\n     ```\r\n  2. Build the required Building Block images\r\n     ```bash\r\n     cd BuildingBlocks/Resources/images\r\n     sudo singularity build MaBoSS.sif MaBoSS.singularity\r\n     sudo singularity build meta_analysis.sif meta_analysis.singularity\r\n     sudo singularity build PhysiCell-COVID19.sif PhysiCell-COVID19.singularity\r\n     sudo singularity build single_cell.sif single_cell.singularity\r\n     cd ../../..\r\n     ```\r\n\r\n**If using PyCOMPSs in local PC** (make sure that PyCOMPSs in installed):\r\n\r\n4. Go to `Workflow/PyCOMPSs` folder\r\n\r\n   ```bash\r\n   cd Workflows/PyCOMPSs\r\n   ```\r\n\r\n5. Execute `./run.sh`\r\n\r\n**If using Snakemake in local PC** (make sure that SnakeMake is installed):\r\n\r\n4. Go to `Workflow/SnakeMake` folder\r\n\r\n   ```bash\r\n   cd Workflows/SnakeMake\r\n   ```\r\n\r\n5. Execute `./run.sh`\r\n  > **TIP**: If you want to run the workflow with a different dataset, please update the `run.sh` script setting the `dataset` variable to the new dataset folder and their file names.\r\n\r\n\r\n### MareNostrum 4\r\n\r\nThis section explains the requirements and usage for the COVID19 Workflow in the MareNostrum 4 supercomputer.\r\n\r\n#### Requirements in MN4\r\n\r\n- Access to MN4\r\n\r\nAll Building Blocks are already installed in MN4, and the COVID19 Workflow available.\r\n\r\n#### Usage steps in MN4\r\n\r\n1. Load the `COMPSs`, `Singularity` and `permedcoe` modules\r\n\r\n   ```bash\r\n   export COMPSS_PYTHON_VERSION=3\r\n   module load COMPSs/3.1\r\n   module load singularity/3.5.2\r\n   module use /apps/modules/modulefiles/tools/COMPSs/libraries\r\n   module load permedcoe\r\n   ```\r\n\r\n   > **TIP**: Include the loading into your `${HOME}/.bashrc` file to load it automatically on the session start.\r\n\r\n   This commands will load COMPSs and the permedcoe package which provides all necessary dependencies, as well as the path to the singularity container images (`PERMEDCOE_IMAGES` environment variable) and testing dataset (`COVID19WORKFLOW_DATASET` environment variable).\r\n\r\n2. Get a copy of the pilot workflow into your desired folder\r\n\r\n   ```bash\r\n   mkdir desired_folder\r\n   cd desired_folder\r\n   get_covid19workflow\r\n   ```\r\n\r\n3. Go to `Workflow/PyCOMPSs` folder\r\n\r\n   ```bash\r\n   cd Workflow/PyCOMPSs\r\n   ```\r\n\r\n4. Execute `./launch.sh`\r\n\r\n  This command will launch a job into the job queuing system (SLURM) requesting 2 nodes (one node acting half master and half worker, and other full worker node) for 20 minutes, and is prepared to use the singularity images that are already deployed in MN4 (located into the `PERMEDCOE_IMAGES` environment variable). It uses the dataset located into `../../Resources/data` folder.\r\n\r\n  > :warning: **TIP**: If you want to run the workflow with a different dataset, please edit the `launch.sh` script and define the appropriate dataset path.\r\n\r\n  After the execution, a `results` folder will be available with with COVID19 Workflow results.\r\n\r\n### Mahti or Puhti\r\n\r\nThis section explains how to run the COVID19 workflow on CSC supercomputers using SnakeMake.\r\n\r\n#### Requirements\r\n\r\n- Install snakemake (or check if there is a version installed using `module spider snakemake`)\r\n- Install workflow, using the same steps as for the local machine. With the exception that containers have to be built elsewhere.\r\n\r\n#### Steps\r\n\r\n\r\n1. Go to `Workflow/SnakeMake` folder\r\n\r\n   ```bash\r\n   cd Workflow/SnakeMake\r\n   ```\r\n\r\n2. Edit `launch.sh` with the correct partition, account, and resource specifications.  \r\n\r\n3. Execute `./launch.sh`\r\n\r\n  > :warning: Snakemake provides a `--cluster` flag, but this functionality should be avoided as it's really not suited for HPC systems.\r\n\r\n## License\r\n\r\n[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\r\n\r\n## Contact\r\n\r\n<https://permedcoe.eu/contact/>\r\n\r\nThis software has been developed for the [PerMedCoE project](https://permedcoe.eu/), funded by the European Commission (EU H2020 [951773](https://cordis.europa.eu/project/id/951773)).\r\n\r\n![](https://permedcoe.eu/wp-content/uploads/2020/11/logo_1.png \"PerMedCoE\")\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "481",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/481?version=1",
        "name": "PerMedCoE Covid19 Pilot workflow (Snakemake)",
        "number_of_steps": 0,
        "projects": [
            "PerMedCoE"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "PhysiCell",
            "MaBoSS",
            "Seurat"
        ],
        "type": "Snakemake",
        "update_time": "2023-05-23",
        "versions": 1
    },
    {
        "create_time": "2023-05-23",
        "creators": [
            "Henrik Nortamo"
        ],
        "description": "# COVID-19 Multiscale Modelling of the Virus and Patients\u2019 Tissue Workflow\r\n\r\n## Table of Contents\r\n\r\n- [COVID-19 Multiscale Modelling of the Virus and Patients\u2019 Tissue Workflow](#covid-19-multiscale-modelling-of-the-virus-and-patients-tissue-workflow)\r\n  - [Table of Contents](#table-of-contents)\r\n  - [Description](#description)\r\n  - [Contents](#contents)\r\n    - [Building Blocks](#building-blocks)\r\n    - [Workflows](#workflows)\r\n    - [Resources](#resources)\r\n    - [Tests](#tests)\r\n  - [Instructions](#instructions)\r\n    - [Local machine](#local-machine)\r\n      - [Requirements](#requirements)\r\n      - [Usage steps](#usage-steps)\r\n    - [MareNostrum 4](#marenostrum-4)\r\n      - [Requirements in MN4](#requirements-in-mn4)\r\n      - [Usage steps in MN4](#usage-steps-in-mn4)\r\n    - [Mahti or Puhti](#mahti-or-puhti)\r\n      - [Requirements](#requirements)\r\n      - [Steps](#steps)\r\n  - [License](#license)\r\n  - [Contact](#contact)\r\n\r\n## Description\r\n\r\nUses multiscale simulations to predict patient-specific SARS\u2011CoV\u20112 severity subtypes\r\n(moderate, severe or control), using single-cell RNA-Seq data, MaBoSS and PhysiBoSS.\r\nBoolean models are used to determine the behaviour of individual agents as a function\r\nof extracellular conditions and the concentration of different  substrates, including\r\nthe number of virions. Predictions of severity subtypes are based on a meta-analysis of\r\npersonalised model outputs simulating cellular apoptosis regulation in epithelial cells\r\ninfected by SARS\u2011CoV\u20112.\r\n\r\nThe workflow uses the following building blocks, described in order of execution:\r\n\r\n1. High-throughput mutant analysis\r\n2. Single-cell processing\r\n3. Personalise patient\r\n4. PhysiBoSS\r\n5. Analysis of all simulations\r\n\r\nFor details on individual workflow steps, see the user documentation for each building block.\r\n\r\n[`GitHub repository`](<https://github.com/PerMedCoE/covid-19-workflow>)\r\n\r\n\r\n## Contents\r\n\r\n### Building Blocks\r\n\r\nThe ``BuildingBlocks`` folder contains the script to install the\r\nBuilding Blocks used in the COVID-19 Workflow.\r\n\r\n### Workflows\r\n\r\nThe ``Workflow`` folder contains the workflows implementations.\r\n\r\nCurrently contains the implementation using PyCOMPSs and Snakemake (in progress).\r\n\r\n### Resources\r\n\r\nThe ``Resources`` folder contains dataset files.\r\n\r\n### Tests\r\n\r\nThe ``Tests`` folder contains the scripts that run each Building Block\r\nused in the workflow for the given small dataset.\r\nThey can be executed individually for testing purposes.\r\n\r\n## Instructions\r\n\r\n### Local machine\r\n\r\nThis section explains the requirements and usage for the COVID19 Workflow in a laptop or desktop computer.\r\n\r\n#### Requirements\r\n\r\n- [`permedcoe`](https://github.com/PerMedCoE/permedcoe) package\r\n- [PyCOMPSs](https://pycompss.readthedocs.io/en/stable/Sections/00_Quickstart.html) / [Snakemake](https://snakemake.readthedocs.io/en/stable/)\r\n- [Singularity](https://sylabs.io/guides/3.0/user-guide/installation.html)\r\n\r\n#### Usage steps\r\n\r\n1. Clone this repository:\r\n\r\n  ```bash\r\n  git clone https://github.com/PerMedCoE/covid-19-workflow.git\r\n  ```\r\n\r\n2. Install the Building Blocks required for the COVID19 Workflow:\r\n\r\n  ```bash\r\n  covid-19-workflow/BuildingBlocks/./install_BBs.sh\r\n  ```\r\n\r\n3. Get the required Building Block images from the project [B2DROP](https://b2drop.bsc.es/index.php/f/444350):\r\n\r\n  - Required images:\r\n      - MaBoSS.singularity\r\n      - meta_analysis.singularity\r\n      - PhysiCell-COVID19.singularity\r\n      - single_cell.singularity\r\n\r\n  The path where these files are stored **MUST be exported in the `PERMEDCOE_IMAGES`** environment variable.\r\n\r\n  > :warning: **TIP**: These containers can be built manually as follows (be patient since some of them may take some time):\r\n  1. Clone the `BuildingBlocks` repository\r\n     ```bash\r\n     git clone https://github.com/PerMedCoE/BuildingBlocks.git\r\n     ```\r\n  2. Build the required Building Block images\r\n     ```bash\r\n     cd BuildingBlocks/Resources/images\r\n     sudo singularity build MaBoSS.sif MaBoSS.singularity\r\n     sudo singularity build meta_analysis.sif meta_analysis.singularity\r\n     sudo singularity build PhysiCell-COVID19.sif PhysiCell-COVID19.singularity\r\n     sudo singularity build single_cell.sif single_cell.singularity\r\n     cd ../../..\r\n     ```\r\n\r\n**If using PyCOMPSs in local PC** (make sure that PyCOMPSs in installed):\r\n\r\n4. Go to `Workflow/PyCOMPSs` folder\r\n\r\n   ```bash\r\n   cd Workflows/PyCOMPSs\r\n   ```\r\n\r\n5. Execute `./run.sh`\r\n\r\n**If using Snakemake in local PC** (make sure that SnakeMake is installed):\r\n\r\n4. Go to `Workflow/SnakeMake` folder\r\n\r\n   ```bash\r\n   cd Workflows/SnakeMake\r\n   ```\r\n\r\n5. Execute `./run.sh`\r\n  > **TIP**: If you want to run the workflow with a different dataset, please update the `run.sh` script setting the `dataset` variable to the new dataset folder and their file names.\r\n\r\n\r\n### MareNostrum 4\r\n\r\nThis section explains the requirements and usage for the COVID19 Workflow in the MareNostrum 4 supercomputer.\r\n\r\n#### Requirements in MN4\r\n\r\n- Access to MN4\r\n\r\nAll Building Blocks are already installed in MN4, and the COVID19 Workflow available.\r\n\r\n#### Usage steps in MN4\r\n\r\n1. Load the `COMPSs`, `Singularity` and `permedcoe` modules\r\n\r\n   ```bash\r\n   export COMPSS_PYTHON_VERSION=3\r\n   module load COMPSs/3.1\r\n   module load singularity/3.5.2\r\n   module use /apps/modules/modulefiles/tools/COMPSs/libraries\r\n   module load permedcoe\r\n   ```\r\n\r\n   > **TIP**: Include the loading into your `${HOME}/.bashrc` file to load it automatically on the session start.\r\n\r\n   This commands will load COMPSs and the permedcoe package which provides all necessary dependencies, as well as the path to the singularity container images (`PERMEDCOE_IMAGES` environment variable) and testing dataset (`COVID19WORKFLOW_DATASET` environment variable).\r\n\r\n2. Get a copy of the pilot workflow into your desired folder\r\n\r\n   ```bash\r\n   mkdir desired_folder\r\n   cd desired_folder\r\n   get_covid19workflow\r\n   ```\r\n\r\n3. Go to `Workflow/PyCOMPSs` folder\r\n\r\n   ```bash\r\n   cd Workflow/PyCOMPSs\r\n   ```\r\n\r\n4. Execute `./launch.sh`\r\n\r\n  This command will launch a job into the job queuing system (SLURM) requesting 2 nodes (one node acting half master and half worker, and other full worker node) for 20 minutes, and is prepared to use the singularity images that are already deployed in MN4 (located into the `PERMEDCOE_IMAGES` environment variable). It uses the dataset located into `../../Resources/data` folder.\r\n\r\n  > :warning: **TIP**: If you want to run the workflow with a different dataset, please edit the `launch.sh` script and define the appropriate dataset path.\r\n\r\n  After the execution, a `results` folder will be available with with COVID19 Workflow results.\r\n\r\n### Mahti or Puhti\r\n\r\nThis section explains how to run the COVID19 workflow on CSC supercomputers using SnakeMake.\r\n\r\n#### Requirements\r\n\r\n- Install snakemake (or check if there is a version installed using `module spider snakemake`)\r\n- Install workflow, using the same steps as for the local machine. With the exception that containers have to be built elsewhere.\r\n\r\n#### Steps\r\n\r\n\r\n1. Go to `Workflow/SnakeMake` folder\r\n\r\n   ```bash\r\n   cd Workflow/SnakeMake\r\n   ```\r\n\r\n2. Edit `launch.sh` with the correct partition, account, and resource specifications.  \r\n\r\n3. Execute `./launch.sh`\r\n\r\n  > :warning: Snakemake provides a `--cluster` flag, but this functionality should be avoided as it's really not suited for HPC systems.\r\n\r\n## License\r\n\r\n[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\r\n\r\n## Contact\r\n\r\n<https://permedcoe.eu/contact/>\r\n\r\nThis software has been developed for the [PerMedCoE project](https://permedcoe.eu/), funded by the European Commission (EU H2020 [951773](https://cordis.europa.eu/project/id/951773)).\r\n\r\n![](https://permedcoe.eu/wp-content/uploads/2020/11/logo_1.png \"PerMedCoE\")\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "480",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/480?version=1",
        "name": "PerMedCoE Covid19 Pilot workflow (Nextflow)",
        "number_of_steps": 0,
        "projects": [
            "PerMedCoE"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "PhysiCell",
            "MaBoSS",
            "Seurat"
        ],
        "type": "Nextflow",
        "update_time": "2023-05-23",
        "versions": 1
    },
    {
        "create_time": "2023-05-23",
        "creators": [
            "Javier Conejero"
        ],
        "description": "# COVID-19 Multiscale Modelling of the Virus and Patients\u2019 Tissue Workflow\r\n\r\n## Table of Contents\r\n\r\n- [COVID-19 Multiscale Modelling of the Virus and Patients\u2019 Tissue Workflow](#covid-19-multiscale-modelling-of-the-virus-and-patients-tissue-workflow)\r\n  - [Table of Contents](#table-of-contents)\r\n  - [Description](#description)\r\n  - [Contents](#contents)\r\n    - [Building Blocks](#building-blocks)\r\n    - [Workflows](#workflows)\r\n    - [Resources](#resources)\r\n    - [Tests](#tests)\r\n  - [Instructions](#instructions)\r\n    - [Local machine](#local-machine)\r\n      - [Requirements](#requirements)\r\n      - [Usage steps](#usage-steps)\r\n    - [MareNostrum 4](#marenostrum-4)\r\n      - [Requirements in MN4](#requirements-in-mn4)\r\n      - [Usage steps in MN4](#usage-steps-in-mn4)\r\n    - [Mahti or Puhti](#mahti-or-puhti)\r\n      - [Requirements](#requirements)\r\n      - [Steps](#steps)\r\n  - [License](#license)\r\n  - [Contact](#contact)\r\n\r\n## Description\r\n\r\nUses multiscale simulations to predict patient-specific SARS\u2011CoV\u20112 severity subtypes\r\n(moderate, severe or control), using single-cell RNA-Seq data, MaBoSS and PhysiBoSS.\r\nBoolean models are used to determine the behaviour of individual agents as a function\r\nof extracellular conditions and the concentration of different  substrates, including\r\nthe number of virions. Predictions of severity subtypes are based on a meta-analysis of\r\npersonalised model outputs simulating cellular apoptosis regulation in epithelial cells\r\ninfected by SARS\u2011CoV\u20112.\r\n\r\nThe workflow uses the following building blocks, described in order of execution:\r\n\r\n1. High-throughput mutant analysis\r\n2. Single-cell processing\r\n3. Personalise patient\r\n4. PhysiBoSS\r\n5. Analysis of all simulations\r\n\r\nFor details on individual workflow steps, see the user documentation for each building block.\r\n\r\n[`GitHub repository`](<https://github.com/PerMedCoE/covid-19-workflow>)\r\n\r\n\r\n## Contents\r\n\r\n### Building Blocks\r\n\r\nThe ``BuildingBlocks`` folder contains the script to install the\r\nBuilding Blocks used in the COVID-19 Workflow.\r\n\r\n### Workflows\r\n\r\nThe ``Workflow`` folder contains the workflows implementations.\r\n\r\nCurrently contains the implementation using PyCOMPSs and Snakemake (in progress).\r\n\r\n### Resources\r\n\r\nThe ``Resources`` folder contains dataset files.\r\n\r\n### Tests\r\n\r\nThe ``Tests`` folder contains the scripts that run each Building Block\r\nused in the workflow for the given small dataset.\r\nThey can be executed individually for testing purposes.\r\n\r\n## Instructions\r\n\r\n### Local machine\r\n\r\nThis section explains the requirements and usage for the COVID19 Workflow in a laptop or desktop computer.\r\n\r\n#### Requirements\r\n\r\n- [`permedcoe`](https://github.com/PerMedCoE/permedcoe) package\r\n- [PyCOMPSs](https://pycompss.readthedocs.io/en/stable/Sections/00_Quickstart.html) / [Snakemake](https://snakemake.readthedocs.io/en/stable/)\r\n- [Singularity](https://sylabs.io/guides/3.0/user-guide/installation.html)\r\n\r\n#### Usage steps\r\n\r\n1. Clone this repository:\r\n\r\n  ```bash\r\n  git clone https://github.com/PerMedCoE/covid-19-workflow.git\r\n  ```\r\n\r\n2. Install the Building Blocks required for the COVID19 Workflow:\r\n\r\n  ```bash\r\n  covid-19-workflow/BuildingBlocks/./install_BBs.sh\r\n  ```\r\n\r\n3. Get the required Building Block images from the project [B2DROP](https://b2drop.bsc.es/index.php/f/444350):\r\n\r\n  - Required images:\r\n      - MaBoSS.singularity\r\n      - meta_analysis.singularity\r\n      - PhysiCell-COVID19.singularity\r\n      - single_cell.singularity\r\n\r\n  The path where these files are stored **MUST be exported in the `PERMEDCOE_IMAGES`** environment variable.\r\n\r\n  > :warning: **TIP**: These containers can be built manually as follows (be patient since some of them may take some time):\r\n  1. Clone the `BuildingBlocks` repository\r\n     ```bash\r\n     git clone https://github.com/PerMedCoE/BuildingBlocks.git\r\n     ```\r\n  2. Build the required Building Block images\r\n     ```bash\r\n     cd BuildingBlocks/Resources/images\r\n     sudo singularity build MaBoSS.sif MaBoSS.singularity\r\n     sudo singularity build meta_analysis.sif meta_analysis.singularity\r\n     sudo singularity build PhysiCell-COVID19.sif PhysiCell-COVID19.singularity\r\n     sudo singularity build single_cell.sif single_cell.singularity\r\n     cd ../../..\r\n     ```\r\n\r\n**If using PyCOMPSs in local PC** (make sure that PyCOMPSs in installed):\r\n\r\n4. Go to `Workflow/PyCOMPSs` folder\r\n\r\n   ```bash\r\n   cd Workflows/PyCOMPSs\r\n   ```\r\n\r\n5. Execute `./run.sh`\r\n\r\n**If using Snakemake in local PC** (make sure that SnakeMake is installed):\r\n\r\n4. Go to `Workflow/SnakeMake` folder\r\n\r\n   ```bash\r\n   cd Workflows/SnakeMake\r\n   ```\r\n\r\n5. Execute `./run.sh`\r\n  > **TIP**: If you want to run the workflow with a different dataset, please update the `run.sh` script setting the `dataset` variable to the new dataset folder and their file names.\r\n\r\n\r\n### MareNostrum 4\r\n\r\nThis section explains the requirements and usage for the COVID19 Workflow in the MareNostrum 4 supercomputer.\r\n\r\n#### Requirements in MN4\r\n\r\n- Access to MN4\r\n\r\nAll Building Blocks are already installed in MN4, and the COVID19 Workflow available.\r\n\r\n#### Usage steps in MN4\r\n\r\n1. Load the `COMPSs`, `Singularity` and `permedcoe` modules\r\n\r\n   ```bash\r\n   export COMPSS_PYTHON_VERSION=3\r\n   module load COMPSs/3.1\r\n   module load singularity/3.5.2\r\n   module use /apps/modules/modulefiles/tools/COMPSs/libraries\r\n   module load permedcoe\r\n   ```\r\n\r\n   > **TIP**: Include the loading into your `${HOME}/.bashrc` file to load it automatically on the session start.\r\n\r\n   This commands will load COMPSs and the permedcoe package which provides all necessary dependencies, as well as the path to the singularity container images (`PERMEDCOE_IMAGES` environment variable) and testing dataset (`COVID19WORKFLOW_DATASET` environment variable).\r\n\r\n2. Get a copy of the pilot workflow into your desired folder\r\n\r\n   ```bash\r\n   mkdir desired_folder\r\n   cd desired_folder\r\n   get_covid19workflow\r\n   ```\r\n\r\n3. Go to `Workflow/PyCOMPSs` folder\r\n\r\n   ```bash\r\n   cd Workflow/PyCOMPSs\r\n   ```\r\n\r\n4. Execute `./launch.sh`\r\n\r\n  This command will launch a job into the job queuing system (SLURM) requesting 2 nodes (one node acting half master and half worker, and other full worker node) for 20 minutes, and is prepared to use the singularity images that are already deployed in MN4 (located into the `PERMEDCOE_IMAGES` environment variable). It uses the dataset located into `../../Resources/data` folder.\r\n\r\n  > :warning: **TIP**: If you want to run the workflow with a different dataset, please edit the `launch.sh` script and define the appropriate dataset path.\r\n\r\n  After the execution, a `results` folder will be available with with COVID19 Workflow results.\r\n\r\n### Mahti or Puhti\r\n\r\nThis section explains how to run the COVID19 workflow on CSC supercomputers using SnakeMake.\r\n\r\n#### Requirements\r\n\r\n- Install snakemake (or check if there is a version installed using `module spider snakemake`)\r\n- Install workflow, using the same steps as for the local machine. With the exception that containers have to be built elsewhere.\r\n\r\n#### Steps\r\n\r\n\r\n1. Go to `Workflow/SnakeMake` folder\r\n\r\n   ```bash\r\n   cd Workflow/SnakeMake\r\n   ```\r\n\r\n2. Edit `launch.sh` with the correct partition, account, and resource specifications.  \r\n\r\n3. Execute `./launch.sh`\r\n\r\n  > :warning: Snakemake provides a `--cluster` flag, but this functionality should be avoided as it's really not suited for HPC systems.\r\n\r\n## License\r\n\r\n[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\r\n\r\n## Contact\r\n\r\n<https://permedcoe.eu/contact/>\r\n\r\nThis software has been developed for the [PerMedCoE project](https://permedcoe.eu/), funded by the European Commission (EU H2020 [951773](https://cordis.europa.eu/project/id/951773)).\r\n\r\n![](https://permedcoe.eu/wp-content/uploads/2020/11/logo_1.png \"PerMedCoE\")\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "476",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/476?version=1",
        "name": "PerMedCoE Covid19 Pilot workflow (PyCOMPSs)",
        "number_of_steps": 0,
        "projects": [
            "PerMedCoE"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "PhysiCell",
            "MaBoSS",
            "Seurat"
        ],
        "type": "COMPSs",
        "update_time": "2023-05-23",
        "versions": 1
    },
    {
        "create_time": "2023-05-23",
        "creators": [
            "Javier Conejero"
        ],
        "description": "# Single drug prediction Workflow\r\n## Table of Contents\r\n\r\n- [Single drug prediction Workflow](#single-drug-prediction-workflow)\r\n  - [Table of Contents](#table-of-contents)\r\n  - [Description](#description)\r\n  - [Contents](#contents)\r\n    - [Building Blocks](#building-blocks)\r\n    - [Workflows](#workflows)\r\n    - [Resources](#resources)\r\n    - [Tests](#tests)\r\n  - [Instructions](#instructions)\r\n    - [Local machine](#local-machine)\r\n      - [Requirements](#requirements)\r\n      - [Usage steps](#usage-steps)\r\n    - [MareNostrum 4](#marenostrum-4)\r\n      - [Requirements in MN4](#requirements-in-mn4)\r\n      - [Usage steps in MN4](#usage-steps-in-mn4)\r\n  - [License](#license)\r\n  - [Contact](#contact)\r\n\r\n## Description\r\n\r\nComplementarily, the workflow supports single drug response predictions to provide a baseline prediction in cases where drug response information for a given drug and cell line is not available. As an input, the workflow needs basal gene expression data for a cell, the drug targets (they need to be known for untested drugs) and optionally CARNIVAL features (sub-network activity predicted with CARNIVAL building block) and predicts log(IC50) values. This workflow uses a custom matrix factorization approach built with Google JAX and trained with gradient descent. The workflow can be used both for training a model, and for predicting new drug responses.\r\n\r\nThe workflow uses the following building blocks in order of execution (for training a model):\r\n\r\n1. Carnival_gex_preprocess\r\n    - Preprocessed the basal gene expression data from GDSC. The input is a matrix of Gene x Sample expression data.\r\n2. Progeny\r\n    - Using the preprocessed data, it estimates pathway activities for each column in the data (for each sample). It returns a matrix of Pathways x Samples with activity values for 11 pathways.\r\n3. Omnipath\r\n    - It downloads latest Prior Knowledge Network of signalling. This building block can be ommited if there exists already a csv file with the network.\r\n4. TF Enrichment\r\n    - For each sample, transcription factor activities are estimated using Dorothea.\r\n5. CarnivalPy\r\n    - Using the TF activities estimated before, it runs Carnival to obtain a sub-network consistent with the TF activities (for each sample).\r\n6. Carnival_feature_merger\r\n    - Preselect a set of genes by the user (if specified) and merge the features with the basal gene expression data.\r\n7. ML Jax Drug Prediction\r\n    - Trains a model using the combined features to predict IC50 values from GDSC.\r\n\r\nFor details on individual workflow steps, please check the scripts that use each individual building block in the workflow [`GitHub repository`](<https://github.com/PerMedCoE/single_drug_prediction>)\r\n\r\n## Contents\r\n\r\n### Building Blocks\r\n\r\nThe ``BuildingBlocks`` folder contains the script to install the\r\nBuilding Blocks used in the Single Drug Prediction Workflow.\r\n\r\n### Workflows\r\n\r\nThe ``Workflow`` folder contains the workflows implementations.\r\n\r\nCurrently contains the implementation using PyCOMPSs.\r\n\r\n### Resources\r\n\r\nThe ``Resources`` folder contains a small dataset for testing purposes.\r\n\r\n### Tests\r\n\r\nThe ``Tests`` folder contains the scripts that run each Building Block\r\nused in the workflow for a small dataset.\r\nThey can be executed individually *without PyCOMPSs installed* for testing\r\npurposes.\r\n\r\n## Instructions\r\n\r\n### Local machine\r\n\r\nThis section explains the requirements and usage for the Single Drug Prediction Workflow in a laptop or desktop computer.\r\n\r\n#### Requirements\r\n\r\n- [`permedcoe`](https://github.com/PerMedCoE/permedcoe) package\r\n- [PyCOMPSs](https://pycompss.readthedocs.io/en/stable/Sections/00_Quickstart.html)\r\n- [Singularity](https://sylabs.io/guides/3.0/user-guide/installation.html)\r\n\r\n#### Usage steps\r\n\r\n1. Clone this repository:\r\n\r\n  ```bash\r\n  git clone https://github.com/PerMedCoE/single-drug-prediction-workflow.git\r\n  ```\r\n\r\n2. Install the Building Blocks required for the COVID19 Workflow:\r\n\r\n  ```bash\r\n  single-drug-prediction-workflow/BuildingBlocks/./install_BBs.sh\r\n  ```\r\n\r\n3. Get the required Building Block images from the project [B2DROP](https://b2drop.bsc.es/index.php/f/444350):\r\n\r\n  - Required images:\r\n      - toolset.singularity\r\n      - carnivalpy.singularity\r\n      - ml-jax.singularity\r\n\r\n  The path where these files are stored **MUST be exported in the `PERMEDCOE_IMAGES`** environment variable.\r\n\r\n  > :warning: **TIP**: These containers can be built manually as follows (be patient since some of them may take some time):\r\n  1. Clone the `BuildingBlocks` repository\r\n     ```bash\r\n     git clone https://github.com/PerMedCoE/BuildingBlocks.git\r\n     ```\r\n  2. Build the required Building Block images\r\n     ```bash\r\n     cd BuildingBlocks/Resources/images\r\n     ## Download new BB singularity files\r\n     wget https://github.com/saezlab/permedcoe/archive/refs/heads/master.zip\r\n     unzip master.zip\r\n     cd permedcoe-master/containers\r\n     ## Build containers\r\n     cd toolset\r\n     sudo /usr/local/bin/singularity build toolset.sif toolset.singularity\r\n     mv toolset.sif ../../../\r\n     cd ..\r\n     cd carnivalpy\r\n     sudo /usr/local/bin/singularity build carnivalpy.sif carnivalpy.singularity\r\n     mv carnivalpy.sif ../../../\r\n     cd ..\r\n     cd ml-jax\r\n     sudo /usr/local/bin/singularity build ml-jax.sif ml-jax.singularity\r\n     mv ml-jax.sif ../../../tf-jax.sif\r\n     cd ..\r\n     cd ../..\r\n     ## Cleanup\r\n     rm -rf permedcoe-master\r\n     rm master.zip\r\n     cd ../../..\r\n     ```\r\n\r\n     > :warning: **TIP**: The singularity containers **can to be downloaded** from: https://cloud.sylabs.io/library/pablormier\r\n\r\n\r\n**If using PyCOMPSs in local PC** (make sure that PyCOMPSs in installed):\r\n\r\n4. Go to `Workflow/PyCOMPSs` folder\r\n\r\n   ```bash\r\n   cd Workflows/PyCOMPSs\r\n   ```\r\n\r\n5. Execute `./run.sh`\r\n\r\n  The execution is prepared to use the singularity images that **MUST** be placed into `BuildingBlocks/Resources/images` folder. If they are located in any other folder, please update the `run.sh` script setting the `PERMEDCOE_IMAGES` to the images folder.\r\n\r\n  > **TIP**: If you want to run the workflow with a different dataset, please update the `run.sh` script setting the `dataset` variable to the new dataset folder and their file names.\r\n\r\n### MareNostrum 4\r\n\r\nThis section explains the requirements and usage for the Single Drug Prediction Workflow in the MareNostrum 4 supercomputer.\r\n\r\n#### Requirements in MN4\r\n\r\n- Access to MN4\r\n\r\nAll Building Blocks are already installed in MN4, and the Single Drug Prediction Workflow available.\r\n\r\n#### Usage steps in MN4\r\n\r\n1. Load the `COMPSs`, `Singularity` and `permedcoe` modules\r\n\r\n   ```bash\r\n   export COMPSS_PYTHON_VERSION=3\r\n   module load COMPSs/3.1\r\n   module load singularity/3.5.2\r\n   module use /apps/modules/modulefiles/tools/COMPSs/libraries\r\n   module load permedcoe\r\n   ```\r\n\r\n   > **TIP**: Include the loading into your `${HOME}/.bashrc` file to load it automatically on the session start.\r\n\r\n   This commands will load COMPSs and the permedcoe package which provides all necessary dependencies, as well as the path to the singularity container images (`PERMEDCOE_IMAGES` environment variable) and testing dataset (`SINGLE_DRUG_PREDICTION_WORKFLOW_DATASET` environment variable).\r\n\r\n2. Get a copy of the pilot workflow into your desired folder\r\n\r\n   ```bash\r\n   mkdir desired_folder\r\n   cd desired_folder\r\n   get_single_drug_prediction_workflow\r\n   ```\r\n\r\n3. Go to `Workflow/PyCOMPSs` folder\r\n\r\n   ```bash\r\n   cd Workflow/PyCOMPSs\r\n   ```\r\n\r\n4. Execute `./launch.sh`\r\n\r\n  This command will launch a job into the job queuing system (SLURM) requesting 2 nodes (one node acting half master and half worker, and other full worker node) for 20 minutes, and is prepared to use the singularity images that are already deployed in MN4 (located into the `PERMEDCOE_IMAGES` environment variable). It uses the dataset located into `../../Resources/data` folder.\r\n\r\n  > :warning: **TIP**: If you want to run the workflow with a different dataset, please edit the `launch.sh` script and define the appropriate dataset path.\r\n\r\n  After the execution, a `results` folder will be available with with Single Drug Prediction Workflow results.\r\n\r\n## License\r\n\r\n[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\r\n\r\n## Contact\r\n\r\n<https://permedcoe.eu/contact/>\r\n\r\nThis software has been developed for the [PerMedCoE project](https://permedcoe.eu/), funded by the European Commission (EU H2020 [951773](https://cordis.europa.eu/project/id/951773)).\r\n\r\n![](https://permedcoe.eu/wp-content/uploads/2020/11/logo_1.png \"PerMedCoE\")\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "478",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/478?version=1",
        "name": "PerMedCoE Single Drug Prediction",
        "number_of_steps": 0,
        "projects": [
            "PerMedCoE"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "PhysiCell",
            "MaBoSS",
            "CARNIVAL",
            "progeny",
            "decoupleR"
        ],
        "type": "COMPSs",
        "update_time": "2023-05-23",
        "versions": 1
    },
    {
        "create_time": "2023-05-19",
        "creators": [],
        "description": "# EukRecover\r\nPipeline to recover eukaryotic MAGs using CONCOCT, metaBAT2 and EukCC's merging algorythm.\r\n\r\nNeeds paired end shotgun metagenomic reads.\r\n\r\n## Environment\r\n\r\nEukrecover requires an environment with snakemake and metaWRAP.\r\n\r\n## Quickstart\r\n\r\nDefine your samples in the file `samples.csv`.\r\nThis file needs to have the columns project and run to identify each metagenome. \r\n\r\nThis pipeline does not support co-binning, but feel free to change it. \r\n\r\nClone this repro wherever you want to run the pipeline:\r\n```\r\ngit clone https://github.com/openpaul/eukrecover/\r\n```\r\n\r\n\r\nYou can then run the snakemake like so\r\n\r\n```\r\nsnakemake --use-singularity\r\n```\r\n\r\nThe pipeline used dockerhub to fetch all tools, so make sure you have singularity installed.\r\n\r\n\r\n\r\n## Prepare databases\r\nThe pipeline will setup databases for you, but if you already have a EukCC or a BUSCO 5 database you can use them \r\nby specifying the location in the file `config/config.yaml`\r\n\r\n\r\n## Output:\r\nIn the folder results you will find a folder `MAGs` which will contain a folder\r\n`fa` containing the actual MAG fastas.\r\nIn addition you will find stats for each MAG in the table `QC.csv`.\r\n\r\nThis table contains the following columns:\r\n\r\nname,eukcc_compl,eukcc_cont,BUSCO_C,BUSCO_M,BUSCO_D,BUSCO_F,BUSCO_tax,N50,bp\r\n\r\n\r\n\r\n## Citation:\r\n\r\nIf you use this pipeline please make sure to cite all used software. \r\n\r\nFor this please reffer to the used rules.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "475",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/475?version=1",
        "name": "EukRecover",
        "number_of_steps": 0,
        "projects": [
            "HoloFood at MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2023-05-19",
        "versions": 1
    },
    {
        "create_time": "2023-05-17",
        "creators": [],
        "description": "The radiation source ELBE (Electron Linac for beams with high Brilliance and low Emittance) at the Helmholtz Centre Dresden Rossendorf (HZDR) can produce several kinds of secondary radiations. THz radiation is one of them and can be used with a typical pulse frequency of 100 kHz as a stimulation source for elementary low-energy degrees of freedom in matter. To sample the whole THz wave the laser path length is modified by moving specific mirrors. The raw data contains for each mirror position a binary file storing the signal spectra and a folder with gray scaled tiff files storing the jitter timing. This Workflow is equivalent to the first part of the standalone jupyter notebook https://github.com/hzdr/TELBE-raw-data-evaluation/blob/main/sorting_binning.ipynb\r\n\r\nIn the job file the folder < FOLDER_BASE> and < FOLDER_SUB> needs to be specified and the parameters as a json string like < PARAMS> = { \"rep\": 100000, \"t_exp\": 1, \"N_sample\": 96, \"offset\": 0, \"pixel_to_ps\": 0.0115, \"Stage_zero\": 0 }\r\n\r\nThe python file which is used is originally published in gitlab https://codebase.helmholtz.cloud/science2workflow/telbe-sorting-binning/-/blob/master/src/ The workflow can automatically be monitored in Heliport if the project number < HELIPORT_PROJECT> is provided.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "473",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/473?version=1",
        "name": "Sorting and registration of Terahertz ELBE raw data",
        "number_of_steps": 0,
        "projects": [
            "Helmholtz Scientific Project Workflow Platform"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Uniform Interface to Computing Resources",
        "update_time": "2023-06-07",
        "versions": 1
    },
    {
        "create_time": "2023-05-16",
        "creators": [
            "Haris Zafeiropoulos",
            "Martin Beracochea"
        ],
        "description": "# metaGOflow: A workflow for marine Genomic Observatories' data analysis\r\n\r\n![logo](https://raw.githubusercontent.com/hariszaf/metaGOflow-use-case/gh-pages/assets/img/metaGOflow_logo_italics.png)\r\n\r\n\r\n## An EOSC-Life project\r\n\r\nThe workflows developed in the framework of this project are based on `pipeline-v5` of the MGnify resource.\r\n\r\n> This branch is a child of the [`pipeline_5.1`](https://github.com/hariszaf/pipeline-v5/tree/pipeline_5.1) branch\r\n> that contains all CWL descriptions of the MGnify pipeline version 5.1.\r\n\r\n## Dependencies\r\n\r\nTo run metaGOflow you need to make sure you have the following set on your computing environmnet first:\r\n\r\n- python3 [v 3.8+]\r\n- [Docker](https://www.docker.com) [v 19.+] or [Singularity](https://apptainer.org) [v 3.7.+]/[Apptainer](https://apptainer.org) [v 1.+]\r\n- [cwltool](https://github.com/common-workflow-language/cwltool) [v 3.+]\r\n- [rdflib](https://rdflib.readthedocs.io/en/stable/) [v 6.+]\r\n- [rdflib-jsonld](https://pypi.org/project/rdflib-jsonld/) [v 0.6.2]\r\n- [ro-crate-py](https://github.com/ResearchObject/ro-crate-py) [v 0.7.0]\r\n- [pyyaml](https://pypi.org/project/PyYAML/) [v 6.0]\r\n- [Node.js](https://nodejs.org/) [v 10.24.0+]\r\n- Available storage ~235GB for databases\r\n\r\n### Storage while running\r\n\r\nDepending on the analysis you are about to run, disk requirements vary.\r\nIndicatively, you may have a look at the metaGOflow publication for computing resources used in various cases.\r\n\r\n## Installation\r\n\r\n### Get the EOSC-Life marine GOs workflow\r\n\r\n```bash\r\ngit clone https://github.com/emo-bon/MetaGOflow\r\ncd MetaGOflow\r\n```\r\n\r\n### Download necessary databases (~235GB)\r\n\r\nYou can download databases for the EOSC-Life GOs workflow by running the\r\n`download_dbs.sh` script under the `Installation` folder.\r\n\r\n```bash\r\nbash Installation/download_dbs.sh -f [Output Directory e.g. ref-dbs] \r\n```\r\nIf you have one or more already in your system, then create a symbolic link pointing\r\nat the `ref-dbs` folder or at one of its subfolders/files.\r\n\r\nThe final structure of the DB directory should be like the following:\r\n\r\n````bash\r\nuser@server:~/MetaGOflow: ls ref-dbs/\r\ndb_kofam/  diamond/  eggnog/  GO-slim/  interproscan-5.57-90.0/  kegg_pathways/  kofam_ko_desc.tsv  Rfam/  silva_lsu/  silva_ssu/\r\n````\r\n\r\n## How to run\r\n\r\n### Ensure that `Node.js` is installed on your system before running metaGOflow\r\n\r\nIf you have root access on your system, you can run the commands below to install it:\r\n\r\n##### DEBIAN/UBUNTU\r\n```bash\r\nsudo apt-get update -y\r\nsudo apt-get install -y nodejs\r\n```\r\n\r\n##### RH/CentOS\r\n```bash\r\nsudo yum install rh-nodejs<stream version> (e.g. rh-nodejs10)\r\n```\r\n\r\n### Set up the environment\r\n\r\n#### Run once - Setup environment\r\n\r\n- ```bash\r\n  conda create -n EOSC-CWL python=3.8\r\n  ```\r\n\r\n- ```bash\r\n  conda activate EOSC-CWL\r\n  ```\r\n\r\n- ```bash\r\n  pip install cwlref-runner cwltool[all] rdflib-jsonld rocrate pyyaml\r\n\r\n  ```\r\n\r\n#### Run every time\r\n\r\n```bash\r\nconda activate EOSC-CWL\r\n``` \r\n\r\n### Run the workflow\r\n\r\n- Edit the `config.yml` file to set the parameter values of your choice. For selecting all the steps, then set to `true` the variables in lines [2-6].\r\n\r\n#### Using Singularity\r\n\r\n##### Standalone\r\n- run:\r\n   ```bash\r\n   ./run_wf.sh -s -n osd-short -d short-test-case -f test_input/wgs-paired-SRR1620013_1.fastq.gz -r test_input/wgs-paired-SRR1620013_2.fastq.gz\r\n   ``\r\n\r\n##### Using a cluster with a queueing system (e.g. SLURM)\r\n\r\n- Create a job file (e.g., SBATCH file)\r\n\r\n- Enable Singularity, e.g. module load Singularity & all other dependencies \r\n\r\n- Add the run line to the job file\r\n\r\n\r\n#### Using Docker\r\n\r\n##### Standalone\r\n- run:\r\n    ``` bash\r\n    ./run_wf.sh -n osd-short -d short-test-case -f test_input/wgs-paired-SRR1620013_1.fastq.gz -r test_input/wgs-paired-SRR1620013_2.fastq.gz\r\n  ```\r\n  HINT: If you are using Docker, you may need to run the above command without the `-s' flag.\r\n\r\n## Testing samples\r\nThe samples are available in the `test_input` folder.\r\n\r\nWe provide metaGOflow with partial samples from the Human Metagenome Project ([SRR1620013](https://www.ebi.ac.uk/ena/browser/view/SRR1620013) and [SRR1620014](https://www.ebi.ac.uk/ena/browser/view/SRR1620014))\r\nThey are partial as only a small part of their sequences have been kept, in terms for the pipeline to test in a fast way. \r\n\r\n\r\n## Hints and tips\r\n\r\n1. In case you are using Docker, it is strongly recommended to **avoid** installing it through `snap`.\r\n\r\n2. `RuntimeError`: slurm currently does not support shared caching, because it does not support cleaning up a worker\r\n   after the last job finishes.\r\n   Set the `--disableCaching` flag if you want to use this batch system.\r\n\r\n3. In case you are having errors like:\r\n\r\n```\r\ncwltool.errors.WorkflowException: Singularity is not available for this tool\r\n```\r\n\r\nYou may run the following command:\r\n\r\n```\r\nsingularity pull --force --name debian:stable-slim.sif docker://debian:stable-sli\r\n```\r\n\r\n## Contribution\r\n\r\nTo make contribution to the project a bit easier, all the MGnify `conditionals` and `subworkflows` under\r\nthe `workflows/` directory that are not used in the metaGOflow framework, have been removed.   \r\nHowever, all the MGnify `tools/` and `utils/` are available in this repo, even if they are not invoked in the current\r\nversion of metaGOflow.\r\nThis way, we hope we encourage people to implement their own `conditionals` and/or `subworkflows` by exploiting the\r\ncurrently supported `tools` and `utils` as well as by developing new `tools` and/or `utils`.\r\n\r\n\r\n<!-- cwltool --print-dot my-wf.cwl | dot -Tsvg > my-wf.svg -->\r\n",
        "doi": "10.48546/workflowhub.workflow.384.3",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "384",
        "keep": true,
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/384?version=3",
        "name": "A workflow for marine Genomic Observatories data analysis",
        "number_of_steps": 0,
        "projects": [
            "emo-bon"
        ],
        "source": "WorkflowHub",
        "tags": [
            "biodiversity"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-05-16",
        "versions": 3
    },
    {
        "create_time": "2023-03-21",
        "creators": [
            "Zavolan Lab"
        ],
        "description": "[![ci](https://github.com/zavolanlab/zarp/workflows/CI/badge.svg?branch=dev)](https://github.com/zavolanlab/zarp/actions?query=workflow%3Aci)\r\n[![GitHub license](https://img.shields.io/github/license/zavolanlab/zarp?color=orange)](https://github.com/zavolanlab/zarp/blob/dev/LICENSE)\r\n[![DOI:10.1101/2021.11.18.469017](http://img.shields.io/badge/DOI-10.1101/2021.11.18.469017-B31B1B.svg)](https://doi.org/10.1101/2021.11.18.469017)\r\n\r\n\r\n<div align=\"left\">\r\n    <img width=\"20%\" align=\"left\" src=https://raw.githubusercontent.com/zavolanlab/zarp/2bdf65deae5d4ffacc4b1a600d7d9ed425614255/images/zarp_logo.svg>\r\n</div> \r\n\r\n\r\n# **ZARP** ([Zavolan-Lab](https://www.biozentrum.unibas.ch/research/researchgroups/overview/unit/zavolan/research-group-mihaela-zavolan/) Automated RNA-Seq Pipeline) \r\n...is a generic RNA-Seq analysis workflow that allows \r\nusers to process and analyze Illumina short-read sequencing libraries with minimum effort. The workflow relies on \r\npublicly available bioinformatics tools and currently handles single or paired-end stranded bulk RNA-seq data.\r\nThe workflow is developed in [Snakemake](https://snakemake.readthedocs.io/en/stable/), a widely used workflow management system in the bioinformatics\r\ncommunity.\r\n\r\nAccording to the current ZARP implementation, reads are analyzed (pre-processed, aligned, quantified) with state-of-the-art\r\ntools to give meaningful initial insights into the quality and composition of an RNA-Seq library, reducing hands-on time for bioinformaticians and giving experimentalists the possibility to rapidly assess their data. Additional reports summarise the results of the individual steps and provide useful visualisations.\r\n\r\n\r\n> **Note:** For a more detailed description of each step, please refer to the [workflow\r\n> documentation](https://github.com/zavolanlab/zarp/blob/main/pipeline_documentation.md).\r\n\r\n\r\n## Requirements\r\n\r\nThe workflow has been tested on:\r\n- CentOS 7.5\r\n- Debian 10\r\n- Ubuntu 16.04, 18.04\r\n\r\n> **NOTE:**\r\n> Currently, we only support **Linux** execution. \r\n\r\n\r\n# Installation\r\n\r\n## 1. Clone the repository\r\n\r\nGo to the desired directory/folder on your file system, then clone/get the \r\nrepository and move into the respective directory with:\r\n\r\n```bash\r\ngit clone https://github.com/zavolanlab/zarp.git\r\ncd zarp\r\n```\r\n\r\n## 2. Conda and Mamba installation\r\n\r\nWorkflow dependencies can be conveniently installed with the [Conda](http://docs.conda.io/projects/conda/en/latest/index.html)\r\npackage manager. We recommend that you install [Miniconda](https://docs.conda.io/en/latest/miniconda.html) \r\nfor your system (Linux). Be sure to select Python 3 option. \r\nThe workflow was built and tested with `miniconda 4.7.12`.\r\nOther versions are not guaranteed to work as expected.\r\n\r\nGiven that Miniconda has been installed and is available in the current shell the first\r\ndependency for ZARP is the [Mamba](https://github.com/mamba-org/mamba) package manager, which needs to be installed in\r\nthe `base` conda environment with:\r\n\r\n```bash\r\nconda install mamba -n base -c conda-forge\r\n```\r\n\r\n## 3. Dependencies installation\r\n\r\nFor improved reproducibility and reusability of the workflow,\r\neach individual step of the workflow runs either in its own [Singularity](https://sylabs.io/singularity/)\r\ncontainer or in its own [Conda](http://docs.conda.io/projects/conda/en/latest/index.html) virtual environemnt. \r\nAs a consequence, running this workflow has very few individual dependencies. \r\nThe **container execution** requires Singularity to be installed on the system where the workflow is executed. \r\nAs the functional installation of Singularity requires root privileges, and Conda currently only provides Singularity\r\nfor Linux architectures, the installation instructions are slightly different depending on your system/setup:\r\n\r\n### For most users\r\n\r\nIf you do *not* have root privileges on the machine you want\r\nto run the workflow on *or* if you do not have a Linux machine, please [install\r\nSingularity](https://sylabs.io/guides/3.5/admin-guide/installation.html) separately and in privileged mode, depending\r\non your system. You may have to ask an authorized person (e.g., a systems\r\nadministrator) to do that. This will almost certainly be required if you want\r\nto run the workflow on a high-performance computing (HPC) cluster. \r\n\r\n> **NOTE:**\r\n> The workflow has been tested with the following Singularity versions:  \r\n>  * `v2.6.2`\r\n>  * `v3.5.2`\r\n\r\nAfter installing Singularity, install the remaining dependencies with:\r\n```bash\r\nmamba env create -f install/environment.yml\r\n```\r\n\r\n\r\n### As root user on Linux\r\n\r\nIf you have a Linux machine, as well as root privileges, (e.g., if you plan to\r\nrun the workflow on your own computer), you can execute the following command\r\nto include Singularity in the Conda environment:\r\n\r\n```bash\r\nmamba env update -f install/environment.root.yml\r\n```\r\n\r\n## 4. Activate environment\r\n\r\nActivate the Conda environment with:\r\n\r\n```bash\r\nconda activate zarp\r\n```\r\n\r\n# Extra installation steps (optional)\r\n\r\n## 5. Non-essential dependencies installation\r\n\r\nMost tests have additional dependencies. If you are planning to run tests, you\r\nwill need to install these by executing the following command _in your active\r\nConda environment_:\r\n\r\n```bash\r\nmamba env update -f install/environment.dev.yml\r\n```\r\n\r\n## 6. Successful installation tests\r\n\r\nWe have prepared several tests to check the integrity of the workflow and its\r\ncomponents. These can be found in subdirectories of the `tests/` directory. \r\nThe most critical of these tests enable you to execute the entire workflow on a \r\nset of small example input files. Note that for this and other tests to complete\r\nsuccessfully, [additional dependencies](#installing-non-essential-dependencies) \r\nneed to be installed. \r\nExecute one of the following commands to run the test workflow \r\non your local machine:\r\n* Test workflow on local machine with **Singularity**:\r\n```bash\r\nbash tests/test_integration_workflow/test.local.sh\r\n```\r\n* Test workflow on local machine with **Conda**:\r\n```bash\r\nbash tests/test_integration_workflow_with_conda/test.local.sh\r\n```\r\nExecute one of the following commands to run the test workflow \r\non a [Slurm](https://slurm.schedmd.com/documentation.html)-managed high-performance computing (HPC) cluster:\r\n\r\n* Test workflow with **Singularity**:\r\n\r\n```bash\r\nbash tests/test_integration_workflow/test.slurm.sh\r\n```\r\n* Test workflow with **Conda**:\r\n\r\n```bash\r\nbash tests/test_integration_workflow_with_conda/test.slurm.sh\r\n```\r\n\r\n> **NOTE:** Depending on the configuration of your Slurm installation you may\r\n> need to adapt file `slurm-config.json` (located directly under `profiles`\r\n> directory) and the arguments to options `--cores` and `--jobs`\r\n> in the file `config.yaml` of a respective profile.\r\n> Consult the manual of your workload manager as well as the section of the\r\n> Snakemake manual dealing with [profiles].\r\n\r\n# Running the workflow on your own samples\r\n\r\n1. Assuming that your current directory is the repository's root directory,\r\ncreate a directory for your workflow run and move into it with:\r\n\r\n    ```bash\r\n    mkdir config/my_run\r\n    cd config/my_run\r\n    ```\r\n\r\n2. Create an empty sample table and a workflow configuration file:\r\n\r\n    ```bash\r\n    touch samples.tsv\r\n    touch config.yaml\r\n    ```\r\n\r\n3. Use your editor of choice to populate these files with appropriate\r\nvalues. Have a look at the examples in the `tests/` directory to see what the\r\nfiles should look like, specifically:\r\n\r\n    - [samples.tsv](https://github.com/zavolanlab/zarp/blob/main/tests/input_files/samples.tsv)\r\n    - [config.yaml](https://github.com/zavolanlab/zarp/blob/main/tests/input_files/config.yaml)\r\n\r\n    - For more details and explanations, refer to the [pipeline-documentation](https://github.com/zavolanlab/zarp/blob/main/pipeline_documentation.md)\r\n\r\n\r\n4. Create a runner script. Pick one of the following choices for either local\r\nor cluster execution. Before execution of the respective command, you need to\r\nremember to update the argument of the `--singularity-args` option of a\r\nrespective profile (file: `profiles/{profile}/config.yaml`) so that\r\nit contains a comma-separated list of _all_ directories\r\ncontaining input data files (samples and any annotation files etc) required for\r\nyour run.\r\n\r\n    Runner script for _local execution_:\r\n\r\n    ```bash\r\n    cat << \"EOF\" > run.sh\r\n    #!/bin/bash\r\n\r\n    snakemake \\\r\n        --profile=\"../../profiles/local-singularity\" \\\r\n        --configfile=\"config.yaml\"\r\n\r\n    EOF\r\n    ```\r\n\r\n    **OR**\r\n\r\n    Runner script for _Slurm cluster exection_ (note that you may need\r\n    to modify the arguments to `--jobs` and `--cores` in the file:\r\n    `profiles/slurm-singularity/config.yaml` depending on your HPC\r\n    and workload manager configuration):\r\n\r\n    ```bash\r\n    cat << \"EOF\" > run.sh\r\n    #!/bin/bash\r\n    mkdir -p logs/cluster_log\r\n    snakemake \\\r\n        --profile=\"../profiles/slurm-singularity\" \\\r\n        --configfile=\"config.yaml\"\r\n    EOF\r\n    ```\r\n\r\n    When running the pipeline with *conda* you should use `local-conda` and\r\n    `slurm-conda` profiles instead.\r\n\r\n5. Start your workflow run:\r\n\r\n    ```bash\r\n    bash run.sh\r\n    ```\r\n\r\n# Sample downloads from SRA\r\n\r\nAn independent Snakemake workflow `workflow/rules/sra_download.smk` is included\r\nfor the download of SRA samples with [sra-tools].\r\n\r\n> Note: as of Snakemake 7.3.1, only profile conda is supported. \r\n> Singularity fails because the *sra-tools* Docker container only has `sh` \r\nbut `bash` is required.\r\n\r\n> Note: The workflow uses the implicit temporary directory \r\nfrom snakemake, which is called with [resources.tmpdir].\r\n\r\nThe workflow expects the following config:\r\n* `samples`, a sample table (tsv) with column *sample* containing *SRR* identifiers,\r\nsee example [here](https://github.com/zavolanlab/zarp/blob/main/tests/input_files/sra_samples.tsv).\r\n* `outdir`, an output directory\r\n* `samples_out`, a pointer to a modified sample table with location of fastq files\r\n* `cluster_log_dir`, the cluster log directory.\r\n\r\nFor executing the example one can use the following\r\n(with activated *zarp* environment):\r\n\r\n```bash\r\nsnakemake --snakefile=\"workflow/rules/sra_download.smk\" \\\r\n          --profile=\"profiles/local-conda\" \\\r\n          --config samples=\"tests/input_files/sra_samples.tsv\" \\\r\n                   outdir=\"results/sra_downloads\" \\\r\n                   samples_out=\"results/sra_downloads/sra_samples.out.tsv\" \\\r\n                   log_dir=\"logs\" \\\r\n                   cluster_log_dir=\"logs/cluster_log\"\r\n```\r\nAfter successful execution, `results/sra_downloads/sra_samples.out.tsv` should contain:\r\n```tsv\r\nsample\tfq1\tfq2\r\nSRR18552868\tresults/sra_downloads/SRR18552868/SRR18552868.fastq.gz\t\r\nSRR18549672\tresults/sra_downloads/SRR18549672/SRR18549672_1.fastq.gz\tresults/sra_downloads/SRR18549672/SRR18549672_2.fastq.gz\r\n```\r\n\r\n\r\n# Metadata completion with HTSinfer\r\nAn independent Snakemake workflow `workflow/rules/htsinfer.smk` that populates the `samples.tsv` required by ZARP with the sample specific parameters `seqmode`, `f1_3p`, `f2_3p`, `organism`, `libtype` and `index_size`. Those parameters are inferred from the provided `fastq.gz` files by [HTSinfer](https://github.com/zavolanlab/htsinfer).\r\n\r\n> Note: The workflow uses the implicit temporary directory \r\nfrom snakemake, which is called with [resources.tmpdir].\r\n\r\n\r\nThe workflow expects the following config:\r\n* `samples`, a sample table (tsv) with column *sample* containing sample identifiers, as well as columns *fq1* and *fq2* containing the paths to the input fastq files\r\nsee example [here](https://github.com/zavolanlab/zarp/blob/main/tests/input_files/sra_samples.tsv). If the table contains further ZARP compatible columns (see [pipeline documentation](https://github.com/zavolanlab/zarp/blob/main/pipeline_documentation.md#read-sample-table)), the values specified there by the user are given priority over htsinfer's results. \r\n* `outdir`, an output directory\r\n* `samples_out`, path to a modified sample table with inferred parameters\r\n* `records`, set to 100000 per default\r\n  \r\nFor executing the example one can use the following\r\n(with activated *zarp* environment):\r\n```bash\r\ncd tests/test_htsinfer_workflow\r\nsnakemake \\\r\n    --snakefile=\"../../workflow/rules/htsinfer.smk\" \\\r\n    --restart-times=0 \\\r\n    --profile=\"../../profiles/local-singularity\" \\\r\n    --config outdir=\"results\" \\\r\n             samples=\"../input_files/htsinfer_samples.tsv\" \\\r\n             samples_out=\"samples_htsinfer.tsv\" \\\r\n    --notemp \\\r\n    --keep-incomplete\r\n```\r\n\r\nHowever, this call will exit with an error, as not all parameters can be inferred from the example files. The argument `--keep-incomplete` makes sure the `samples_htsinfer.tsv` file can nevertheless be inspected. \r\n\r\nAfter successful execution - if all parameters could be either inferred or were specified by the user - `[OUTDIR]/[SAMPLES_OUT]` should contain a populated table with parameters `seqmode`, `f1_3p`, `f2_3p`, `organism`, `libtype` and `index_size` for all input samples as described in the [pipeline documentation](https://github.com/zavolanlab/zarp/blob/main/pipeline_documentation.md#read-sample-table).\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.447.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "447",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/447?version=1",
        "name": "ZARP: An automated workflow for processing of RNA-seq data",
        "number_of_steps": 0,
        "projects": [
            "Zavolan Lab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "ngs",
            "rnaseq",
            "high-throughput",
            "rna",
            "rna-seq"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2023-05-12",
        "versions": 1
    },
    {
        "create_time": "2021-05-05",
        "creators": [
            "Laura Rodriguez-Navas",
            "Jos\u00e9 M\u00aa Fern\u00e1ndez"
        ],
        "description": "# COnSensus Interaction Network InFErence Service\r\nInference framework for reconstructing networks using a consensus approach between multiple methods and data sources.\r\n\r\n![alt text](https://github.com/PhosphorylatedRabbits/cosifer/raw/master/docs/_static/logo.png)\r\n\r\n## Reference\r\n[Manica, Matteo, Charlotte, Bunne, Roland, Mathis, Joris, Cadow, Mehmet Eren, Ahsen, Gustavo A, Stolovitzky, and Mar\u00eda Rodr\u00edguez, Mart\u00ednez. \"COSIFER: a python package for the consensus inference of molecular interaction networks\".Bioinformatics (2020)](https://doi.org/10.1093/bioinformatics/btaa942).",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "118",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/118?version=1",
        "name": "COSIFER",
        "number_of_steps": 1,
        "projects": [
            "iPC: individualizedPaediatricCure"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cancer",
            "cosifer",
            "pediatric",
            "rna-seq"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-04-21",
        "versions": 1
    },
    {
        "create_time": "2023-04-12",
        "creators": [
            "Douglas Lowe"
        ],
        "description": "# WRF/EMEP Linear Workflow\r\n\r\nExample Common Workflow Language (CWL) workflow and tool descriptors for running the \r\nWeather Research and Forecase (WRF) and EMEP models.\r\n\r\nThis workflow is designed for a single model domain. Example datasets for testing this \r\nworkflow can be downloaded from Zenodo.\r\n\r\n\r\n## Requirements:\r\n\r\n* docker or singularity\r\n* conda\r\n* cwltool\r\n* Toil - optional, useful for running on HPC or distributed computing systems\r\n\r\n### CWL / Toil Installation:\r\n\r\nThe workflow runner (either cwltool, or Toil) can be installed using either conda or pip.\r\nEnvironment files for conda are included, and can be used as shown below:\r\n* cwltool only:\r\n  * `conda env create --file install/env_cwlrunner.yml --name cwl`\r\n* Toil & cwltool:\r\n  * `conda env create --file install/env_toil.yml --name toil`\r\n\r\n### Setup for Example Workflow\r\n\r\n* Download the example dataset from Zenodo: https://doi.org/10.5281/zenodo.7817216\r\n* Extract into the `input_files` directory:\r\n  * `tar -zxvf wrf_emep_UK_example_inputs.tar.gz -C input_files --strip-components=1`\r\n\r\n## Running the Workflow\r\n\r\nThe full workflow is broken into several logical steps:\r\n1. ERA5 download\r\n2. WPS 1st step: Geogrid geography file creation\r\n3. WPS process: ungribbing of ERA5 data, and running of metgrid to produce meteorology files.\r\n4. WRF process: generation of WRF input files by REAL, and running of WRF model\r\n5. EMEP model: running of EMEP chemistry and transport model\r\n\r\nSteps 1 and 3 require you to register with the CDS service, in order to download ERA5 data\r\nbefore using in the WPS process.\r\nSteps 2 and 5 require you to download extra input data - the instructions on how to do this\r\nare included in the README.txt files in the relevant input data directories.\r\n\r\nA full workflow for all steps is provided here. But each separate step can by run on it's \r\nown too, following the instructions given below. We recommend running step 4 first, to \r\nexplore how the REAL & WRF workflow works, before trying the other steps.\r\n\r\n### 1. ERA5 download.\r\n\r\nBefore running the ERA5 download tool, ensure that you have reqistered for the CDS service, \r\nsigned the ERA5 licensing agreement, and saved the CDS API key (`.cdsapirc`) in your \r\nworking directory.\r\n\r\nTo run the ERA5 download tool use the following command:\r\n```\r\ncwltool [--cachdir CACHE] [--singularity] workflows/era5_workflow.cwl example_workflow_configurations/era5_download_settings.yaml\r\n```\r\nNote that the `--cachedir CACHE` option sets the working directory cache, which enables the\r\nreuse of any steps previously run (and the restarting of the workflow from this point).\r\nThe `--singularity` option is needed if you are using singularity instead of docker.\r\n\r\n### 2. WPS: Geogrid geography file creation\r\n\r\nBefore running the geogrid tool you will need to download the geography data from the\r\n[UCAR website](https://www2.mmm.ucar.edu/wrf/users/download/get_sources_wps_geog.html).\r\nThese should be extracted into the `input_files/geogrid_geog_input` directory.\r\n\r\nTo run the geogrid program use the following command:\r\n```\r\ncwltool [--cachdir CACHE] [--singularity] workflows/geogrid_workflow.cwl example_workflow_configurations/wps_geogrid_cwl_settings.yaml\r\n```\r\n\r\n### 3. WPS: Creation of meteorology input files\r\n\r\nBefore running the WPS process you will have to download the ERA5 datafiles (which will be\r\ncalled `preslev_[YYYYMMDD].grib` and `surface_[YYYYMMDD].grib`) and copy these to the directory\r\n`input_files/wps_era5_input`. If you have also run geogrid in step 2 you can replace the \r\n`geo_em.d01.nc` file in the `input_files/wps_geogrid_input` directory with the file that \r\ngeogrid created.\r\n\r\nTo run the wps metgrid process use the following command:\r\n```\r\ncwltool [--cachdir CACHE] [--singularity] workflows/wps_workflow.cwl example_workflow_configurations/wps_metgrid_cwl_settings.yaml\r\n```\r\n\r\n### 4. WRF: Creation of WRF input files, and running WRF model\r\n\r\nThe WRF model can be run without any prepreparation, except for the downloading of the \r\ninput data from Zenodo. However, if you have created new meteorology files (`met_em*`) using\r\nWPS you can replace the files in the `input_files/wrf_met_input` directory with these.\r\n\r\nTo run the WRF process (including REAL) use the following command:\r\n```\r\ncwltool [--cachdir CACHE] [--singularity] workflows/wrf_workflow.cwl example_workflow_configurations/wrf_real_cwl_settings.yaml\r\n``` \r\n\r\n### 5. EMEP: Running EMEP chemistry and transport model\r\n\r\nBefore running the EMEP model you will need to download the EMEP input dataset. This can be\r\ndone using the `catalog.py` tool, following the instructions in the `input_files/emep_input/README.txt`\r\nfile. If you have run WRF you can also replace the `wrfout*` data files in the \r\n`input_Files/emep_wrf_input` directory with those you have created.\r\n\r\nTo run the EMEP model use the following command:\r\n```\r\ncwltool [--cachdir CACHE] [--singularity] workflows/emep_workflow.cwl example_workflow_configurations/emep_cwl_settings.yaml\r\n```\r\n\r\n### Full Workflow\r\n\r\nBefore running the full workflow make sure you have carried out the setup tasks described\r\nabove.\r\n\r\nTo run the full workflow use the following command:\r\n```\r\ncwltool [--cachdir CACHE] [--singularity] wrf_emep_full_workflow.cwl example_workflow_configurations/wrf_emep_full_workflow_cwl_settings.yaml\r\n```\r\n\r\n## Notes\r\n\r\n### WRF filenames\r\n\r\nIn order to work with singularity, all filenames need to exclude special characters.\r\nTo ensure that all WRF filenames comply with this requirement, you will need to add the \r\n`nocolons = .true.` option to your WPS, REAL and WRF namelists to ensure this.\r\n\r\n### MPI parallel processing\r\n\r\nThe WPS processes all run in single thread mode. REAL, WRF and EMEP have been compiled with\r\nMPI support. The default cores for each of these is 2, 9 and 9, respectively. The \r\nsettings file can be edited to modify these requirements.\r\n\r\n### Caching intermediate workflow steps\r\n\r\nTo cache the data from individual steps you can use the `--cachedir <cache-dir>` optional flag.\r\n\r\n\r\n## License and Copyright \r\n\r\nThese workflow scripts have been developed by the [Research IT](https://research-it.manchester.ac.uk/) \r\nat the [University of Manchester](https://www.manchester.ac.uk/).\r\n\r\nCopyright 2023 [University of Manchester, UK](https://www.manchester.ac.uk/).\r\n\r\nLicensed under the MIT license, see the LICENSE file for details.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "455",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/455?version=1",
        "name": "WRF / EMEP Linear Workflow",
        "number_of_steps": 5,
        "projects": [
            "Air Quality Prediction"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-04-12",
        "versions": 1
    },
    {
        "create_time": "2023-04-12",
        "creators": [
            "Andrey Prjibelski",
            "Varsha Kale",
            "Anton Korobeynikov"
        ],
        "description": "**Assembly and quantification metatranscriptome using metagenome data**.\r\n\r\nVersion: see VERSION\r\n\r\n## Introduction\r\n\r\n**MetaGT** is a bioinformatics analysis pipeline used for improving and quantification \r\nmetatranscriptome assembly using metagenome data. The pipeline supports Illumina sequencing \r\ndata and complete metagenome and metatranscriptome assemblies. The pipeline involves the \r\nalignment of metatranscriprome assembly to the metagenome assembly with further extracting CDSs,\r\nwhich are covered by transcripts.\r\n\r\nThe pipeline is built using Nextflow, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker containers making installation trivial and results highly reproducible. The Nextflow DSL2 implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies.\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow-%E2%89%A520.04.0-brightgreen.svg)](https://www.nextflow.io/)\r\n\r\n[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg)](https://bioconda.github.io/)\r\n\r\n## Quick Start\r\n\r\n1. Install [`nextflow`](https://nf-co.re/usage/installation)\r\n\r\n2. Install any of [`Conda`](https://conda.io/miniconda.html) for full pipeline reproducibility \r\n\r\n3. Download the pipeline, e.g. by cloning metaGT GitHub repository:\r\n\r\n    ```bash\r\n    git clone git@github.com:ablab/metaGT.git\r\n    ```\r\n   \r\n4. Test it on a minimal dataset by running:\r\n\r\n    ```bash\r\n    nextflow run metaGT -profile test,conda\r\n    ```\r\n   \r\n5. Start running your own analysis!\r\n    > Typical command for analysis using reads:\r\n\r\n    ```bash\r\n    nextflow run metaGT -profile <conda> --dna_reads '*_R{1,2}.fastq.gz' --rna_reads '*_R{1,2}.fastq.gz'\r\n    ```\r\n    > Typical command for analysis using multiple files with reads:\r\n\r\n    ```bash\r\n    nextflow run metaGT -profile <conda> --dna_reads '*.yaml' --rna_reads '*.yaml' --yaml\r\n    ```\r\n    > Typical command for analysis using assemblies:\r\n\r\n    ```bash\r\n    nextflow run metaGT -profile <conda> --genome '*.fasta' --transcriptome '*.fasta'\r\n    ```\r\n## Pipeline Summary\r\nOptionally, if raw reades are used:\r\n\r\n<!-- TODO nf-core: Fill in short bullet-pointed list of default steps of pipeline -->\r\n\r\n* Sequencing quality control (`FastQC`)\r\n* Assembly metagenome or metatranscriptome (`metaSPAdes, rnaSPAdes `)\r\n\r\nBy default, the pipeline currently performs the following:\r\n\r\n* Annotation metagenome (`Prokka`)\r\n* Aligning metatranscriptome on metagenome (`minimap2`)\r\n* Annotation unaligned transcripts (`TransDecoder`)\r\n* Clustering covered CDS and CDS from unaligned transcripts (`MMseqs2`)\r\n* Quantifying abundances of transcripts (`kallisto`)\r\n\r\n## Citation\r\n\r\nMetaGT was developed by Daria Shafranskaya and Andrey Prjibelski.\r\nIf you use it in your research please cite:\r\n\r\n[MetaGT: A pipeline for de novo assembly of metatranscriptomes with the aid of metagenomic data](https://doi.org/10.3389/fmicb.2022.981458)\r\n\r\n## Feedback and bug report\r\n\r\nIf you have any questions, please leave an issue at out [GitHub page](https://github.com/ablab/metaGT/issues).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "454",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/454?version=1",
        "name": "MetaGT: A pipeline for de novo assembly of metatranscriptomes with the aid of metagenomic data",
        "number_of_steps": 0,
        "projects": [
            "HoloFood at MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "multi-omics",
            "expression",
            "metatranscriptomics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-04-13",
        "versions": 1
    },
    {
        "create_time": "2022-04-21",
        "creators": [
            "Bart Nijsse",
            "Jasper Koehorst",
            "Germ\u00e1n Royval"
        ],
        "description": "### Workflow for LongRead Quality Control and Filtering\r\n\r\n- NanoPlot  (read quality control) before and after filtering\r\n- Filtlong  (read trimming)\r\n- Kraken2 taxonomic read classification before and after filtering\r\n- Minimap2 read filtering based on given references<br><br>\r\n\r\nOther UNLOCK workflows on WorkflowHub: https://workflowhub.eu/projects/16/workflows?view=default<br><br>\r\n\r\n**All tool CWL files and other workflows can be found here:**<br>\r\nhttps://gitlab.com/m-unlock/cwl/workflows\r\n\r\n**How to setup and use an UNLOCK workflow:**<br>\r\nhttps://m-unlock.gitlab.io/docs/setup/setup.html<br>\r\n",
        "doi": null,
        "edam_operation": [
            "Sequencing quality control"
        ],
        "edam_topic": [
            "Metagenomic sequencing",
            "Metagenomics",
            "Sequence analysis",
            "Sequencing"
        ],
        "filtered_on": "edam",
        "id": "337",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/337?version=1",
        "name": "LongRead Quality Control and Filtering",
        "number_of_steps": 9,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "cwl",
            "genomics",
            "nanopore"
        ],
        "tools": [
            "Converts the file array to a single file object",
            "Removal of contaminated reads using minimap2 mapping",
            "Preparation of fastp output files to a specific output folder",
            "Quality assessment and report of reads before filter",
            "Merge fastq files",
            "Prepare BBMap references to a single fasta file and unique headers",
            "Visualization of Kraken2 classification with Krona",
            "Taxonomic classification of FASTQ reads"
        ],
        "type": "Common Workflow Language",
        "update_time": "2023-04-07",
        "versions": 1
    },
    {
        "create_time": "2023-03-30",
        "creators": [
            "Ekaterina Sakharova",
            "Martin Beracochea"
        ],
        "description": "The containerised pipeline for profiling shotgun metagenomic data is derived from the [MGnify](https://www.ebi.ac.uk/metagenomics/) pipeline raw-reads analyses, a well-established resource used for analyzing microbiome data.\r\nKey components:\r\n- Quality control and decontamination\r\n- rRNA and ncRNA detection using Rfam database\r\n- Taxonomic classification of SSU and LSU regions \r\n- Abundance analysis with mOTUs",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "450",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/450?version=1",
        "name": "MGnify raw reads taxonomic profiling pipeline",
        "number_of_steps": 0,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "nextflow"
        ],
        "tools": [
            "Biopython",
            "Metagenomic operational taxonomic units (mOTUs)",
            "MAPseq",
            "Infernal"
        ],
        "type": "Nextflow",
        "update_time": "2023-03-30",
        "versions": 1
    },
    {
        "create_time": "2023-03-21",
        "creators": [
            "Stevie Pederson"
        ],
        "description": "# GRAVI: Gene Regulatory Analysis using Variable Inputs\r\n\r\nThis is a `snakemake` workflow for:\r\n\r\n1. Performing sample QC\r\n2. Calling ChIP peaks\r\n3. Performing Differential Binding Analysis\r\n4. Comparing results across ChIP targets\r\n\r\nThe minimum required input is one ChIP target with two conditions.\r\n\r\nFull documentation can be found [here](https://steveped.github.io/GRAVI/)\r\n\r\n\r\n## Snakemake Implementation\r\n\r\nThe basic workflow is written `snakemake`, requiring at least v7.7, and can be called using the following steps.\r\n\r\nFirstly, setup the required conda environments\r\n\r\n```\r\nsnakemake \\\r\n\t--use-conda \\\r\n\t--conda-prefix '/home/steveped/mambaforge/envs/' \\\r\n\t--conda-create-envs-only \\\r\n\t--cores 1\r\n```\r\n\r\nSecondly, create and inspect the rulegraph\r\n\r\n```\r\nsnakemake --rulegraph > workflow/rules/rulegraph.dot\r\ndot -Tpdf workflow/rules/rulegraph.dot > workflow/rules/rulegraph.pdf\r\n```\r\n\r\nFinally, the workflow itself can be run using:\r\n\r\n```\r\nsnakemake \\\r\n\t-p \\\r\n\t--use-conda \\\r\n\t--conda-prefix '/home/steveped/mambaforge/envs/' \\\r\n\t--notemp \\\r\n\t--rerun-triggers mtime \\\r\n\t--keep-going \\\r\n\t--cores 16\r\n```\r\n\r\nNote that this creates common environments able to be called by other workflows and is dependent on the user.\r\nFor me, my global conda environments are stored in `/home/steveped/mambaforge/envs/`.\r\nFor other users, this path will need to be modified.\r\n\r\nIf wishing to tidy the directory after a successful run, you can check which non-essential files can be deleted using `snakemake -n --delete-temp-output --cores 1`.\r\nIf the files earmarked for deletion are considered to be non-essential, they can be deleted by removing the `-n` flag from the above code: `snakemake --delete-temp-output --cores 1`.\r\nAs the bedgraph files produced by `macs2 callpeak` are typically very large, hence their conversion to bigwig files during the workflow, this step can free a considerable amount of disk space.\r\n",
        "doi": "10.48546/workflowhub.workflow.443.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "443",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/443?version=1",
        "name": "GRAVI: Gene Regulatory Analysis using Variable Inputs",
        "number_of_steps": 0,
        "projects": [
            "Black Ochre Data Labs"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bam",
            "bioinformatics",
            "chip-seq"
        ],
        "tools": [
            "MACS"
        ],
        "type": "Snakemake",
        "update_time": "2024-10-24",
        "versions": 1
    },
    {
        "create_time": "2023-03-21",
        "creators": [],
        "description": "# SNP-Calling\r\nGATK Variant calling pipeline for genomic data using Nextflow\r\n\r\n[![nextflow](https://img.shields.io/badge/nextflow-%E2%89%A522.04.5-brightgreen.svg)](http://nextflow.io)\r\n\r\n## Quickstart\r\n\r\nInstall Nextflow using the following command: \r\n\r\n    curl -s https://get.nextflow.io | bash\r\n  \r\nIndex reference genome:\r\n\r\n  `$ bwa index /path/to/reference/genome.fa`\r\n \r\n  `$ samtools faidx /path/to/reference/genome.fa`\r\n  \r\n  `$ gatk CreateSequenceDictionary -R /path/to/genome.fa -O genome.dict`\r\n\r\nLaunch the pipeline execution with the following command:\r\n\r\n    nextflow run jdetras/snp-calling -r main -profile docker\r\n  \r\n## Pipeline Description\r\n\r\nThe variant calling pipeline follows the recommended practices from GATK. The input genomic data are aligned to a reference genome using BWA. The alignemnt files are processed using Picard Tools. Variant calling is done using samtools and GATK. \r\n\r\n## Input files\r\n\r\nThe input files required to run the pipeline:\r\n* Genomic sequence paired reads, `*_{1,2}.fq.gz`\r\n* Reference genome, `*.fa`\r\n\r\n## Pipeline parameters\r\n\r\n### Usage\r\nUsage: `nextflow run jdetras/snp-calling -profile docker [options]`\r\n\r\nOptions:\r\n\r\n* `--reads` \r\n* `--genome`\r\n* `--output`\r\n\r\nExample: \r\n  `$ nextflow run jdetras/snp-calling -profile docker --reads '/path/to/reads/*_{1,2}.fq.gz' --genome '/path/to/reference/genome.fa' --output '/path/to/output'`\r\n\r\n#### `--reads`\r\n\r\n* The path to the FASTQ read files.\r\n* Wildcards (*, ?) can be used to declare multiple reads. Use single quotes when wildcards are used. \r\n* Default parameter: `$projectDir/data/reads/*_{1,2}.fq.gz`\r\n\r\nExample: \r\n  `$ nextflow run jdetras/snp-calling -profile docker --reads '/path/to/reads/*_{1,2}.fq.gz'`\r\n  \r\n#### `--genome`\r\n\r\n* The path to the genome file in fasta format.\r\n* The extension is `.fa`.\r\n* Default parameter: `$projectDir/data/reference/genome.fa`\r\n\r\nExample:\r\n  `$ nextflow run jdetras/snp-calling -profile docker --genome /path/to/reference/genome.fa`\r\n    \r\n#### `--output`\r\n\r\n* The path to the directory for the output files.\r\n* Default parameter: `$projectDir/output`\r\n\r\n## Software\r\n\r\n* [BWA 0.7.17](http://bio-bwa.sourceforge.net/)\r\n* [Samtools 1.3.1](http://www.htslib.org/)\r\n* [GATK 4.2.6.1](https://gatk.broadinstitute.org/) \r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "442",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/442?version=1",
        "name": "SNP-Calling Workflow",
        "number_of_steps": 0,
        "projects": [
            "IRRI Bioinformatics Group"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bwa-mem",
            "gatk4",
            "rice",
            "variant calling"
        ],
        "tools": [
            "GATK",
            "BWA"
        ],
        "type": "Nextflow",
        "update_time": "2023-03-21",
        "versions": 1
    },
    {
        "create_time": "2023-03-21",
        "creators": [
            "Georgina Samaha",
            "Tracy Chew"
        ],
        "description": "# IGVreport-nf \r\n\r\n- [Description](#description)\r\n  - [Diagram](#diagram)\r\n  - [User guide](#user-guide)\r\n  - [Workflow summaries](#workflow-summaries)\r\n      - [Metadata](#metadata)\r\n      - [Component tools](#component-tools)\r\n      - [Required (minimum)\r\n        inputs/parameters](#required-minimum-inputsparameters)\r\n  - [Additional notes](#additional-notes)\r\n  - [Help/FAQ/Troubleshooting](#helpfaqtroubleshooting)\r\n  - [Acknowledgements/citations/credits](#acknowledgementscitationscredits)\r\n\r\n## Description \r\n\r\nQuickly generate [IGV `.html` reports](https://github.com/igvteam/igv-reports) for a genomic region of interest in the human genome (hg38). Bcftools is used to subset a VCF to a region of interest, the subset VCF is then passed to IGV-reports, which generates a report consisting of a table of genomic sites or regions and associated IGV views for each site. The reports can be opened by any web browser as a static page.  \r\n\r\n### Diagram \r\n\r\n```mermaid\r\ngraph LR;\r\n    VCF-->|bcftools view|SubsetVCF;\r\n    SubsetVCF-->|IGVtools|HTMLreport;\r\n    AlignmentBAM-->|IGVtools|HTMLreport;\r\n```\r\n\r\n### User guide\r\n\r\nThis workflow uses containers for all steps and can run using Singularity or Docker. It requires Nextflow and either Singularity or Docker be installed. For instructions on installing Nextflow, see their [documentation](https://www.nextflow.io/docs/latest/getstarted.html).\r\n\r\n**This workflow currently only generates reports for the human reference genome assembly, Hg38.** \r\n\r\nThe workflow runs three processes: \r\n1. The provided VCF file is subset to a region of interest using Bcftools view \r\n2. The Subset VCF file is then indexed using Bcftools index \r\n3. The subset VCF and provided Bam file are used to generate the html report for the region of interest. \r\n\r\nTo start clone this repository: \r\n```\r\ngit clone https://github.com/Sydney-Informatics-Hub/IGVreport-nf.git\r\n```\r\n\r\nFrom the IGVreport-nf directory, run the pipeline: \r\n```\r\nnextflow run main.nf --sample <sampleID> \\\r\n    --bam <path/to/bam> \\\r\n    --vcf <path/to/vcf> \\\r\n    --chr <chrID> --start <begin bp> --stop <end bp>     \r\n```\r\n\r\nThis will create a report in a directory titled `./Report`. You can rename this directory at runtime using the flag `--outDir`. All runtime summary reports will be available in the `./runInfo` directory.  \r\n\r\n### Workflow summaries\r\n\r\n#### Metadata \r\n\r\n|metadata field     | workflow_name / workflow_version  |\r\n|-------------------|:---------------------------------:|\r\n|Version            | 1.0                               |\r\n|Maturity           | under development                 |\r\n|Creators           | Georgie Samaha                    |\r\n|Source             | NA                                |\r\n|License            | GPL-3.0 license                   |\r\n|Workflow manager   | NextFlow                          |\r\n|Container          | None                              |\r\n|Install method     | NA                                |\r\n|GitHub             | github.com/Sydney-Informatics-Hub/IGVreport-nf    |\r\n|bio.tools \t        | NA                                |\r\n|BioContainers      | NA                                | \r\n|bioconda           | NA                                |\r\n\r\n#### Component tools\r\n\r\n* nextflow>=20.07.1\r\n* singularity or docker\r\n* bcftools/1.16\r\n* igv-reports/1.6.1\r\n\r\n#### Required (minimum) inputs/parameters\r\n\r\n* An indexed alignment file in Bam format \r\n* A gzipped and indexed vcf file\r\n\r\n## Additional notes\r\n\r\n## Help/FAQ/troubleshooting \r\n\r\n## Acknowledgements/citations/credits\r\n\r\nThis workflow was developed by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney and the Australian BioCommons which is enabled by NCRIS via Bioplatforms Australia. \r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Genetic variation",
            "Genomics",
            "Mapping",
            "Structural variation"
        ],
        "filtered_on": "metap* in description",
        "id": "440",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/440?version=1",
        "name": "IGVreport-nf",
        "number_of_steps": 0,
        "projects": [
            "Sydney Informatics Hub",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "genomics",
            "mapping",
            "variant calling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-03-21",
        "versions": 1
    },
    {
        "create_time": "2023-03-01",
        "creators": [
            "Jasper Ouwerkerk"
        ],
        "description": "To discover causal mutations of inherited diseases it\u2019s common practice to do a trio analysis. In a trio analysis DNA is sequenced of both the patient and parents. Using this method, it\u2019s possible to identify multiple inheritance patterns. Some examples of these patterns are autosomal recessive, autosomal dominant, and de-novo variants, which are represented in the figure below. To elaborate, the most left tree shows an autosomal dominant inhertitance pattern where the offspring inherits a faulty copy of the gene from one of the parents.\r\n\r\nTo discover these mutations either whole exome sequencing (WES) or whole genome sequencing (WGS) can be used. With these technologies it is possible to uncover the DNA of the parents and offspring to find (shared) mutations in the DNA. These mutations can include insertions/deletions (indels), loss of heterozygosity (LOH), single nucleotide variants (SNVs), copy number variations (CNVs), and fusion genes.\r\n\r\nIn this workflow  we will also make use of the HTSGET protocol, which is a program to download our data securely and savely. This protocol has been implemented in the EGA Download Client Tool: toolshed.g2.bx.psu.edu/repos/iuc/ega_download_client/pyega3/4.0.0+galaxy0 tool, so we don\u2019t have to leave Galaxy to retrieve our data.\r\n\r\nWe will not start our analysis from scratch, since the main goal of this tutorial is to use the HTSGET protocol to download variant information from an online archive and to find the causative variant from those variants. If you want to learn how to do the analysis from scratch, using the raw reads, you can have a look at the Exome sequencing data analysis for diagnosing a genetic disease tutorial.",
        "doi": "10.48546/workflowhub.workflow.363.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "363",
        "keep": true,
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/363?version=2",
        "name": "Trio Analysis",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "variant-analysis"
        ],
        "tools": [
            "regexColumn1",
            "tp_grep_tool",
            "snpEff",
            "bcftools_merge",
            "bcftools_norm",
            "gemini_inheritance",
            "pyega3",
            "\n Filter1",
            "gemini_load",
            "\n CONVERTER_gz_to_uncompressed"
        ],
        "type": "Galaxy",
        "update_time": "2023-09-05",
        "versions": 2
    },
    {
        "create_time": "2023-02-20",
        "creators": [],
        "description": "# MoP2- DSL2 version of Master of Pores\r\n[![Docker Build Status](https://img.shields.io/docker/automated/biocorecrg/nanopore.svg)](https://cloud.docker.com/u/biocorecrg/repository/docker/biocorecrg/nanopore/builds)\r\n[![mop2-CI](https://github.com/biocorecrg/MoP2/actions/workflows/build.yml/badge.svg)](https://github.com/biocorecrg/MoP2/actions/workflows/build.yml)\r\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\r\n[![Nextflow version](https://img.shields.io/badge/Nextflow-21.04.1-brightgreen)](https://www.nextflow.io/)\r\n[![Nextflow DSL2](https://img.shields.io/badge/Nextflow-DSL2-brightgreen)](https://www.nextflow.io/)\r\n[![Singularity version](https://img.shields.io/badge/Singularity-v3.2.1-green.svg)](https://www.sylabs.io/)\r\n[![Docker version](https://img.shields.io/badge/Docker-v20.10.8-blue)](https://www.docker.com/)\r\n\r\n<br/>\r\n\r\n![MOP2](https://github.com/biocorecrg/MoP2/blob/main/img/master_red.jpg?raw=true)\r\n\r\n\r\nInspired by Metallica's [Master Of Puppets](https://www.youtube.com/watch?v=S7blkui3nQc)\r\n\r\n## Install\r\nPlease install nextflow and singularity or docker before.\r\n\r\nThen download the repo:\r\n\r\n```\r\ngit clone --depth 1 --recurse-submodules git@github.com:biocorecrg/MOP2.git\r\n```\r\n\r\nYou can use INSTALL.sh to download the version 3.4.5 of guppy or you can replace it with the version you prefer. Please consider that the support of VBZ compression of fast5 started with version 3.4.X. \r\n\r\n```\r\ncd MoP2; sh INSTALL.sh\r\n```\r\n\r\n## Testing\r\nYou can replace ```-with-singularity``` with ```-with-docker``` if you want to use the docker engine.\r\n\r\n```\r\ncd mop_preprocess\r\nnextflow run mop_preprocess.nf -with-singularity -bg -profile local > log\r\n\r\n```\r\n\r\n## Reference\r\nIf you use this tool, please cite our papers:\r\n\r\n[\"Nanopore Direct RNA Sequencing Data Processing and Analysis Using MasterOfPores\"\r\nCozzuto L, Delgado-Tejedor A, Hermoso Pulido T, Novoa EM, Ponomarenko J. *N. Methods Mol Biol. 2023*;2624:185-205. doi: 10.1007/978-1-0716-2962-8_13.](https://link.springer.com/protocol/10.1007/978-1-0716-2962-8_13)\r\n\r\n[\"MasterOfPores: A Workflow for the Analysis of Oxford Nanopore Direct RNA Sequencing Datasets\"\r\nLuca Cozzuto, Huanle Liu, Leszek P. Pryszcz, Toni Hermoso Pulido, Anna Delgado-Tejedor, Julia Ponomarenko, Eva Maria Novoa.\r\n*Front. Genet., 17 March 2020.* https://doi.org/10.3389/fgene.2020.00211](https://www.frontiersin.org/articles/10.3389/fgene.2020.00211/full)\r\n\r\n\r\n## Documentation\r\nThe documentation is available at [https://biocorecrg.github.io/MOP2/docs/](https://biocorecrg.github.io/MOP2/docs/about.html)\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Metatranscriptomics",
            "Transcriptomics"
        ],
        "filtered_on": "edam",
        "id": "438",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/438?version=1",
        "name": "Master of Pores 2",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics Unit @ CRG"
        ],
        "source": "WorkflowHub",
        "tags": [
            "ont",
            "transcriptomics",
            "drnaseq",
            "metatranscriptomics",
            "nanopore"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-02-20",
        "versions": 1
    },
    {
        "create_time": "2023-02-15",
        "creators": [
            "Diego Garrido-Mart\u00edn",
            "Roderic Guig\u00f3"
        ],
        "description": "# sqtlseeker2-nf\r\n\r\n[![nextflow](https://img.shields.io/badge/nextflow-%E2%89%A50.27.0-blue.svg)](http://nextflow.io)\r\n[![CI-checks](https://github.com/guigolab/sqtlseeker2-nf/actions/workflows/ci.yaml/badge.svg)](https://github.com/guigolab/sqtlseeker2-nf/actions/workflows/ci.yaml)\r\n\r\nA pipeline for splicing quantitative trait loci (sQTL) mapping.\r\n\r\nThe pipeline performs the following analysis steps:\r\n\r\n* Index the genotype file\r\n* Preprocess the transcript expression data\r\n* Test for association between splicing ratios and genetic variants in *cis* (nominal pass)\r\n* Obtain an empirical P-value for each phenotype (permutation pass, optional)\r\n* Control for multiple testing \r\n\r\nFor details on each step, please read [sQTLseekeR2](https://github.com/guigolab/sQTLseekeR2) documentation.\r\n\r\nThe pipeline uses [Nextflow](http://www.nextflow.io) as the execution backend. Please check [Nextflow documentation](http://www.nextflow.io/docs/latest/index.html) for more information.\r\n\r\n## Requirements\r\n\r\n- Unix-like operating system (Linux, MacOS, etc.)\r\n- Java 8 or later \r\n- [Docker](https://www.docker.com/) (v1.10.0 or later) or [Singularity](http://singularity.lbl.gov) (v2.5.0 or later)\r\n\r\n## Quickstart (~2 min)\r\n\r\n1. Install Nextflow:\r\n    ```\r\n    curl -fsSL get.nextflow.io | bash\r\n    ```\r\n\r\n2. Make a test run:\r\n    ```\r\n    ./nextflow run guigolab/sqtlseeker2-nf -with-docker\r\n    ```\r\n\r\n    **Note**: set `-with-singularity` to use Singularity instead of Docker. \r\n\r\n## Pipeline usage\r\n\r\nLaunching the pipeline with the `--help` parameter shows the help message:\r\n\r\n```\r\nnextflow run sqtlseeker2-nf --help\r\n```\r\n\r\n```\r\nN E X T F L O W  ~  version 0.27.2\r\nLaunching `sqtlseeker2.nf` [admiring_lichterman] - revision: 28c86caf1c\r\n\r\nsqtlseeker2-nf ~ A pipeline for splicing QTL mapping\r\n----------------------------------------------------\r\nRun sQTLseekeR2 on a set of data.\r\n\r\nUsage: \r\n    sqtlseeker2-nf [options]\r\n\r\nOptions:\r\n--genotype GENOTYPE_FILE    the genotype file\r\n--trexp EXPRESSION_FILE     the transcript expression file\r\n--metadata METADATA_FILE    the metadata file\r\n--genes GENES_FILE          the gene location file\r\n--dir DIRECTORY             the output directory\r\n--mode MODE                 the run mode: nominal or permuted (default: nominal)\r\n--win WINDOW                the cis window in bp (default: 5000)\r\n--covariates COVARIATES     include covariates in the model (default: false)\r\n--fdr FDR                   false discovery rate level (default: 0.05)\r\n--min_md MIN_MD             minimum effect size reported (default: 0.05)\r\n--svqtl SVQTLS              report svQTLs (default: false)\r\n\r\nAdditional parameters for mode = nominal:\r\n--ld LD                     threshold for LD-based variant clustering (default: 0, no clustering)\r\n--kn KN                     number of genes per batch in nominal pass (default: 10)\r\n\r\nAdditional parameters for mode = permuted:\r\n--kp KP                     number of genes per batch in permuted pass (default: 10)\r\n--max_perm MAX_PERM         maximum number of permutations (default: 1000)\r\n```\r\n\r\n## Input files and format\r\n\r\n`sqtlseeker2-nf` takes as input files the following:\r\n\r\n* **Genotype file.**\r\nContains the genotype of each sample, coded as follows: 0 for REF/REF, 1 for REF/ALT, 2 for ALT/ALT, -1 for missing value.\r\nThe first four columns should be: `chr`, `start`, `end` and `snpId`. This file needs to be sorted by coordinate.\r\n\r\n* **Transcript expression file.**\r\nContains the expression of each transcript in each sample (e.g. read counts, RPKM, TPM).\r\nIt is not recommended to use transformed (log, quantile, or any non-linear transformation) expression.\r\nColumns `trId` and `geneId`, corresponding to the transcript and gene IDs, are required. \r\n\r\n* **Metadata file.** Contains the covariate information for each sample. \r\nIn addition, it defines the groups or conditions for which sQTL mapping will be performed.\r\nThe first columns should be: `indId`, `sampleId`, `group`, followed by the covariates.\r\nThis file defines which samples will be tested.\r\n\r\n* **Gene location file.**\r\nContains the location of each gene. Columns `chr`, `start`, `end` and `geneId` are required. \r\nThis file defines which genes will be tested.\r\n\r\nExample [data](data) is available for the test run.\r\n\r\n## Pipeline results\r\n\r\nsQTL mapping results are saved into the folder specified with the `--dir` parameter. By default it is the `result` folder within the current working directory.\r\n\r\nOutput files are organinzed into subfolders corresponding to the different `groups` specified in the metadata file: \r\n\r\n```\r\nresult\r\n\u2514\u2500\u2500 groups\r\n    \u251c\u2500\u2500 group1                            \r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 all-tests.nominal.tsv          \r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 all-tests.permuted.tsv         \r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 sqtls-${level}fdr.nominal.tsv      \r\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 sqtls-${level}fdr.permuted.tsv     \r\n    \u251c\u2500\u2500 group2\r\n   ...\r\n```\r\n\r\nNote: if only a nominal pass was run, files `*.permuted.tsv` will not be present.\r\n\r\nOutput files contain the following information:\r\n\r\n`all-tests.nominal.tsv`\r\n\r\n* geneId: gene name\t\r\n* snpId: variant name\r\n* F: test statistic\r\n* nb.groups: number of genotype groups\r\n* md: maximum difference in relative expression between genotype groups (sQTL effect size)\r\n* tr.first/tr.second: the transcript IDs of the two transcripts that change the most, in opposite directions\r\n* info: number of individuals in each genotype group, including missing values (-1,0,1,2)\r\n* pv: nominal P-value\r\n\r\nif `--svqtl true`\r\n* F.svQTL: svQTL test statistic\r\n* nb.perms.svQTL: number of permutations for svQTL test\r\n* pv.svQTL: svQTL nominal P-value \r\n\r\nif `--ld ${r2}`\r\n* LD: other variants in linkage disequilibrium with snpId above a given r<sup>2</sup> threshold > 0\r\n\r\n`sqtls-${level}fdr.nominal.tsv` (in addition to the previous)\r\n\r\n* fdr: false discovery rate (computed across all nominal tests)\r\n* fdr.svQTL: svQTL FDR\r\n\r\n`all-tests.permuted.tsv`\r\n\r\n* geneId: gene name\r\n* variants.cis: number of variants tested in *cis*\r\n* LD: median linkage disequilibrium in the region (r<sup>2</sup>)\r\n* best.snp: ID of the top variant\r\n* best.nominal.pv: P-value of the top variant\r\n* shape1: first parameter value of the fitted beta distribution\r\n* shape2: second parameter value of the fitted beta distribution (effective number of independent tests in the region)\r\n* nb.perm: number of permutations\r\n* pv.emp.perm: empirical P-value, computed based on permutations\r\n* pv.emp.beta: empirical P-value, computed based on the fitted beta distribution\r\n* runtime: run time in minutes\r\n\r\n`sqtls-${level}fdr.nominal.tsv` (in addition to the previous)\r\n\r\n* fdr: false discovery rate (computed across empirical P-values)\r\n* p_tn: gene-level threshold for nominal P-values\r\n\r\n## Cite sqtlseeker2-nf\r\n\r\nIf you find `sqtlseeker2-nf` useful in your research please cite the related publication:\r\n\r\nGarrido-Mart\u00edn, D., Borsari, B., Calvo, M., Reverter, F., Guig\u00f3, R. Identification and analysis of splicing quantitative trait loci across multiple tissues in the human genome. *Nat Commun* 12, 727 (2021). [https://doi.org/10.1038/s41467-020-20578-2](https://doi.org/10.1038/s41467-020-20578-2)\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "435",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/435?version=1",
        "name": "sqtlseeker2-nf",
        "number_of_steps": 0,
        "projects": [
            "Guig\u00f3 lab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alternative splicing",
            "nextflow",
            "qtl mapping",
            "snps",
            "rna-seq"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-02-15",
        "versions": 1
    },
    {
        "create_time": "2023-02-15",
        "creators": [
            "Diego Garrido-Mart\u00edn",
            "Roderic Guig\u00f3"
        ],
        "description": "# mvgwas-nf\r\n\r\n[![nextflow](https://img.shields.io/badge/nextflow-%E2%89%A520.04.1-blue.svg)](http://nextflow.io)\r\n[![CI-checks](https://github.com/guigolab/sqtlseeker2-nf/actions/workflows/ci.yaml/badge.svg)](https://github.com/guigolab/sqtlseeker2-nf/actions/workflows/ci.yaml)\r\n\r\nA pipeline for multi-trait genome-wide association studies (GWAS) using [MANTA](https://github.com/dgarrimar/manta).\r\n\r\nThe pipeline performs the following analysis steps:\r\n\r\n* Split genotype file \r\n* Preprocess phenotype and covariate data\r\n* Test for association between phenotypes and genetic variants\r\n* Collect summary statistics\r\n\r\nThe pipeline uses [Nextflow](http://www.nextflow.io) as the execution backend. Please check [Nextflow documentation](http://www.nextflow.io/docs/latest/index.html) for more information.\r\n\r\n## Requirements\r\n\r\n- Unix-like operating system (Linux, MacOS, etc.)\r\n- Java 8 or later \r\n- [Docker](https://www.docker.com/) (v1.10.0 or later) or [Singularity](http://singularity.lbl.gov) (v2.5.0 or later)\r\n\r\n## Quickstart (~2 min)\r\n\r\n1. Install Nextflow:\r\n    ```\r\n    curl -fsSL get.nextflow.io | bash\r\n    ```\r\n\r\n2. Make a test run:\r\n    ```\r\n    nextflow run dgarrimar/mvgwas-nf -with-docker\r\n    ```\r\n\r\n**Notes**: move the `nextflow` executable to a directory in your `$PATH`. Set `-with-singularity` to use Singularity instead of Docker. \r\n\r\n(*) Alternatively you can clone this repository:\r\n```\r\ngit clone https://github.com/dgarrimar/mvgwas-nf\r\ncd mvgwas-nf\r\nnextflow run mvgwas.nf -with-docker\r\n```\r\n\r\n## Pipeline usage\r\n\r\nLaunching the pipeline with the `--help` parameter shows the help message:\r\n\r\n```\r\nnextflow run mvgwas.nf --help\r\n```\r\n\r\n```\r\nN E X T F L O W  ~  version 20.04.1\r\nLaunching `mvgwas.nf` [amazing_roentgen] - revision: 56125073b7\r\n\r\nmvgwas-nf: A pipeline for multivariate Genome-Wide Association Studies\r\n==============================================================================================\r\nPerforms multi-trait GWAS using using MANTA (https://github.com/dgarrimar/manta)\r\n\r\nUsage:\r\nnextflow run mvgwas.nf [options]\r\n\r\nParameters:\r\n--pheno PHENOTYPES          phenotype file\r\n--geno GENOTYPES            indexed genotype VCF file\r\n--cov COVARIATES            covariate file\r\n--l VARIANTS/CHUNK          variants tested per chunk (default: 10000)\r\n--t TRANSFOMATION           phenotype transformation: none, sqrt, log (default: none)\r\n--i INTERACTION             test for interaction with a covariate: none, <covariate> (default: none)\r\n--ng INDIVIDUALS/GENOTYPE   minimum number of individuals per genotype group (default: 10)\r\n--dir DIRECTORY             output directory (default: result)\r\n--out OUTPUT                output file (default: mvgwas.tsv)\r\n```\r\n\r\n## Input files and format\r\n\r\n`mvgwas-nf` requires the following input files:\r\n\r\n* **Genotypes.** \r\n[bgzip](http://www.htslib.org/doc/bgzip.html)-compressed and indexed [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) genotype file.\r\n\r\n* **Phenotypes.**\r\nTab-separated file with phenotype measurements (quantitative) for each sample (i.e. *n* samples x *q* phenotypes).\r\nThe first column should contain sample IDs. Columns should be named.\r\n\r\n* **Covariates.**\r\nTab-separated file with covariate measurements (quantitative or categorical) for each sample (i.e. *n* samples x *k* covariates). \r\nThe first column should contain sample IDs. Columns should be named. \r\n\r\nExample [data](data) is available for the test run.\r\n\r\n## Pipeline results\r\n\r\nAn output text file containing the multi-trait GWAS summary statistics (default: `./result/mvgwas.tsv`), with the following information:\r\n\r\n* `CHR`: chromosome\r\n* `POS`: position\r\n* `ID`: variant ID\r\n* `REF`: reference allele\r\n* `ALT`: alternative allele\r\n* `F`: pseudo-F statistic\r\n* `R2`: fraction of variance explained by the variant\r\n* `P`: P-value\r\n\r\nThe output folder and file names can be modified with the `--dir` and `--out` parameters, respectively.\r\n\r\n## Cite mvgwas-nf\r\n\r\nIf you find `mvgwas-nf` useful in your research please cite the related publication:\r\n\r\nGarrido-Mart\u00edn, D., Calvo, M., Reverter, F., Guig\u00f3, R. A fast non-parametric test of association for multiple traits. *bioRxiv* (2022). [https://doi.org/10.1101/2022.06.06.493041](https://doi.org/10.1101/2022.06.06.493041)\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "436",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/436?version=1",
        "name": "mvgwas-nf",
        "number_of_steps": 0,
        "projects": [
            "Statistical genetics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gwas",
            "multivariate",
            "nextflow",
            "non-parametric"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-02-15",
        "versions": 1
    },
    {
        "create_time": "2022-04-20",
        "creators": [
            "Bart Nijsse",
            "Jasper Koehorst",
            "Germ\u00e1n Royval"
        ],
        "description": "#### - Deprecated -\r\n#### See our updated hybrid assembly workflow: https://workflowhub.eu/workflows/367\r\n#### And other workflows: https://workflowhub.eu/projects/16#workflows\r\n# \r\n**Workflow for sequencing with ONT Nanopore data, from basecalled reads to (meta)assembly and binning**\r\n- Workflow Nanopore Quality\r\n- Kraken2 taxonomic classification of FASTQ reads\r\n- Flye (de-novo assembly)\r\n- Medaka (assembly polishing)\r\n- metaQUAST (assembly quality reports)\r\n\r\n**When Illumina reads are provided:** \r\n  - Workflow Illumina Quality: https://workflowhub.eu/workflows/336?version=1\t\r\n  - Assembly polishing with Pilon<br>\r\n  - Workflow binnning https://workflowhub.eu/workflows/64?version=11\r\n      - Metabat2\r\n      - CheckM\r\n      - BUSCO\r\n      - GTDB-Tk\r\n\r\n**All tool CWL files and other workflows can be found here:**<br>\r\n  Tools: https://git.wur.nl/unlock/cwl/-/tree/master/cwl<br>\r\n  Workflows: https://git.wur.nl/unlock/cwl/-/tree/master/cwl/workflows<br>",
        "doi": null,
        "edam_operation": [
            "Sequence assembly",
            "Sequencing quality control"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Metagenomic sequencing",
            "Metagenomics",
            "Sequence assembly",
            "Sequencing"
        ],
        "filtered_on": "edam",
        "id": "254",
        "keep": true,
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/254?version=3",
        "name": "Nanopore Assembly Workflow - Deprecated -",
        "number_of_steps": 22,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genomics",
            "metagenomics",
            "nanopore"
        ],
        "tools": [
            "Quality and filtering workflow for illumina reads",
            "evaluation of polished assembly with metaQUAST",
            "Preparation of Flye output files to a specific output folder",
            "Illumina reads assembly polishing with Pilon",
            "Binning workflow to create bins",
            "Preparation of pilon output files to a specific output folder",
            "Preparation of Medaka output files to a specific output folder",
            "Sam file conversion to a sorted bam file",
            "Illumina read mapping on pilon assembly for binning",
            "Preparation of Kraken2 output files to a specific output folder",
            "Medaka for polishing of assembled genome",
            "Preparation of metaQUAST output files to a specific output folder",
            "Taxonomic classification of Nanopore reads",
            "Preparation of quast output files to a specific output folder",
            "Preparation of QUAST output files to a specific output folder",
            "De novo assembly of single-molecule reads with Flye",
            "Taxonomic classification of FASTQ reads",
            "Illumina evaluation of pilon polished assembly with metaQUAST",
            "Compress large kraken2 report file",
            "Visualization of kraken2 with Krona",
            "Quality and filtering workflow for nanopore reads"
        ],
        "type": "Common Workflow Language",
        "update_time": "2023-02-02",
        "versions": 3
    },
    {
        "create_time": "2023-01-17",
        "creators": [],
        "description": "# TronFlow BAM preprocessing pipeline\r\n\r\n![GitHub tag (latest SemVer)](https://img.shields.io/github/v/release/tron-bioinformatics/tronflow-bam-preprocessing?sort=semver)\r\n[![Automated tests](https://github.com/TRON-Bioinformatics/tronflow-bam-preprocessing/actions/workflows/automated_tests.yml/badge.svg)](https://github.com/TRON-Bioinformatics/tronflow-bam-preprocessing/actions/workflows/automated_tests.yml)\r\n[![DOI](https://zenodo.org/badge/358400957.svg)](https://zenodo.org/badge/latestdoi/358400957)\r\n[![License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT)\r\n[![Powered by Nextflow](https://img.shields.io/badge/powered%20by-Nextflow-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://www.nextflow.io/)\r\n\r\nThe TronFlow BAM preprocessing pipeline is part of a collection of computational workflows for tumor-normal pair \r\nsomatic variant calling. These workflows are implemented in the Nextflow (Di Tommaso, 2017) framework.\r\n\r\nFind the documentation here [![Documentation Status](https://readthedocs.org/projects/tronflow-docs/badge/?version=latest)](https://tronflow-docs.readthedocs.io/en/latest/?badge=latest)\r\n\r\n\r\nThe aim of this workflow is to preprocess BAM files based on Picard and GATK (DePristo, 2011) best practices.\r\n\r\n\r\n## Background\r\n\r\nIn order to have a variant calling ready BAM file there are a number of operations that need to be applied on the BAM. \r\nThis pipeline depends on the particular variant caller, but there are some common operations.\r\n\r\nGATK has been providing a well known best practices document on BAM preprocessing, the latest best practices for \r\nGATK4 (https://software.broadinstitute.org/gatk/best-practices/workflow?id=11165) does not perform anymore realignment around indels as opposed to best practices for GATK3 (https://software.broadinstitute.org/gatk/documentation/article?id=3238). This pipeline is based on both Picard and GATK. These best practices have been implemented a number of times, see for instance this implementation in Workflow Definition Language https://github.com/gatk-workflows/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl.\r\n\r\n\r\n## Objectives\r\n\r\nWe aim at providing a single implementation of the BAM preprocessing pipeline that can be used across different \r\nuse cases. \r\nFor this purpose there are some required steps and some optional steps.  \r\n\r\nThe input can be either a tab-separated values file (`--input_files`) where each line corresponds to one input BAM or a single BAM (`--input_bam` and `--input_name`).\r\n\r\n## Implementation\r\n\r\nSteps:\r\n\r\n* **Clean BAM**. Sets the mapping quality to 0 for all unmapped reads and avoids soft clipping going beyond the reference genome boundaries. Implemented in Picard\r\n* **Reorder chromosomes**. Makes the chromosomes in the BAM follow the same order as the reference genome. Implemented in Picard\r\n* **Add read groups**. GATK requires that some headers are adde to the BAM, also we want to flag somehow the normal and tumor BAMs in the header as some callers, such as Mutect2 require it. Implemented in Picard.\r\n* **Mark duplicates** (optional). Identify the PCR and the optical duplications and marks those reads. This uses the parallelized version on Spark, it is reported to scale linearly up to 16 CPUs.\r\n* **Realignment around indels** (optional). This procedure is important for locus based variant callers, but for any variant caller doing haplotype assembly it is not needed. This is computing intensive as it first finds regions for realignment where there are indication of indels  and then it performs a local realignment over those regions. Implemented in GATK3, deprecated in GATK4\r\n* **Base Quality Score Recalibration (BQSR)** (optional). It aims at correcting systematic errors in the sequencer when assigning the base call quality errors, as these scores are used by variant callers it improves variant calling in some situations. Implemented in GATK4\r\n* **Metrics** (optional). A number of metrics are obtained from the BAM file with Picard's CollectMetrics, CollectHsMetrics and samtools' coverage and depth.\r\n\r\n![Pipeline](figures/bam_preprocessing2.png)\r\n\r\n\r\n## How to run it\r\n\r\n```\r\n$ nextflow run tron-bioinformatics/tronflow-bam-preprocessing --help\r\n\r\nN E X T F L O W  ~  version 19.07.0\r\nLaunching `main.nf` [intergalactic_shannon] - revision: e707c77d7b\r\n\r\nUsage:\r\n    main.nf --input_files input_files\r\n\r\nInput:\r\n    * --input_bam: the path to a single BAM (this option is not compatible with --input_files)\r\n    * --input_files: the path to a tab-separated values file containing in each row the sample name, sample type (eg: tumor or normal) and path to the BAM file (this option is not compatible with --input_bam)\r\n    Sample type will be added to the BAM header @SN sample name\r\n    The input file does not have header!\r\n    Example input file:\r\n    name1       tumor   tumor.1.bam\r\n    name1       normal  normal.1.bam\r\n    name2       tumor   tumor.2.bam\r\n    * --reference: path to the FASTA genome reference (indexes expected *.fai, *.dict)\r\n\r\nOptional input:\r\n    * --input_name: the name of the sample. Only used when --input_bam is provided (default: normal)\r\n    * --dbsnp: path to the dbSNP VCF (required to perform BQSR)\r\n    * --known_indels1: path to a VCF of known indels (optional to perform realignment around indels)\r\n    * --known_indels2: path to a second VCF of known indels (optional to perform realignment around indels)\r\n    * --intervals: path to a BED file to collect coverage and HS metrics from (default: None)\r\n    * --collect_hs_minimum_base_quality: minimum base quality for a base to contribute coverage (default: 20).\r\n    * --collect_hs_minimum_mapping_quality: minimum mapping quality for a read to contribute coverage (default: 20).\r\n    * --skip_bqsr: optionally skip BQSR (default: false)\r\n    * --skip_realignment: optionally skip realignment (default: false)\r\n    * --skip_deduplication: optionally skip deduplication (default: false)\r\n    * --remove_duplicates: removes duplicate reads from output BAM instead of flagging them (default: true)\r\n    * --skip_metrics: optionally skip metrics (default: false)\r\n    * --output: the folder where to publish output (default: ./output)\r\n    * --platform: the platform to be added to the BAM header. Valid values: [ILLUMINA, SOLID, LS454, HELICOS and PACBIO] (default: ILLUMINA)\r\n\r\nComputational resources:\r\n    * --prepare_bam_cpus: (default: 3)\r\n    * --prepare_bam_memory: (default: 8g)\r\n    * --mark_duplicates_cpus: (default: 16)\r\n    * --mark_duplicates_memory: (default: 64g)\r\n    * --realignment_around_indels_cpus: (default: 2)\r\n    * --realignment_around_indels_memory: (default: 31g)\r\n    * --bqsr_cpus: (default: 3)\r\n    * --bqsr_memory: (default: 4g)\r\n    * --metrics_cpus: (default: 1)\r\n    * --metrics_memory: (default: 8g)\r\n\r\n Output:\r\n    * Preprocessed and indexed BAMs\r\n    * Tab-separated values file with the absolute paths to the preprocessed BAMs, preprocessed_bams.txt\r\n\r\nOptional output:\r\n    * Recalibration report\r\n    * Deduplication metrics\r\n    * Realignment intervals\r\n    * GATK multiple metrics\r\n    * HS metrics\r\n    * Horizontal and vertical coverage metrics\r\n```\r\n\r\n### Input table\r\n\r\nThe table with FASTQ files expects two tab-separated columns **without a header**\r\n\r\n| Sample name          | Sample type                      | BAM                  |\r\n|----------------------|---------------------------------|------------------------------|\r\n| sample_1             | normal      |    /path/to/sample_1.normal.bam   |\r\n| sample_1             | tumor      |    /path/to/sample_1.tumor.bam   |\r\n| sample_2             | normal      |    /path/to/sample_2.normal.bam   |\r\n| sample_2             | tumor      |    /path/to/sample_2.tumor.bam   |\r\n\r\nThe values used in `sample type` are arbitrary. These will be set in the BAM header tag @RG:SM for sample. There may be some downstream constraints, eg: Mutect2 pipeline requires that the sample type between normal and tumor samples of the same pair are not the same.\r\n\r\n### References\r\n\r\nThe BAM preprocessing workflow requires the human reference genome (`--reference`)\r\nBase Quality Score Recalibration (BQSR) requires dbSNP to avoid extracting error metrics from polymorphic sites (`--dbsnp`)\r\nRealignment around indels requires a set of known indels (`--known_indels1` and `--known_indels2`).\r\nThese resources can be fetched from the GATK bundle https://gatk.broadinstitute.org/hc/en-us/articles/360035890811-Resource-bundle.\r\n\r\nOptionally, in order to run Picard's CollectHsMetrics a BED file will need to be provided (`--intervals`).\r\nThis BED file will also be used for `samtools coverage`.\r\n\r\n## Troubleshooting\r\n\r\n### Too new Java version for MarkDuplicatesSpark\r\n\r\nWhen using Java 11 the cryptic error messsage `java.lang.IllegalArgumentException: Unsupported class file major version 55` has been observed.\r\nThis issue is described here and the solution is to use Java 8 https://gatk.broadinstitute.org/hc/en-us/community/posts/360056174592-MarkDuplicatesSpark-crash.\r\n\r\n\r\n\r\n## Bibliography\r\n\r\n* DePristo M, Banks E, Poplin R, Garimella K, Maguire J, Hartl C, Philippakis A, del Angel G, Rivas MA, Hanna M, McKenna A, Fennell T, Kernytsky A, Sivachenko A, Cibulskis K, Gabriel S, Altshuler D, Daly M. (2011). A framework for variation discovery and genotyping using next-generation DNA sequencing data. Nat Genet, 43:491-498. DOI: 10.1038/ng.806.\r\n* Di Tommaso, P., Chatzou, M., Floden, E. W., Barja, P. P., Palumbo, E., & Notredame, C. (2017). Nextflow enables reproducible computational workflows. Nature Biotechnology, 35(4), 316\u2013319. 10.1038/nbt.3820\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "419",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/419?version=1",
        "name": "TronFlow BAM preprocessing pipeline",
        "number_of_steps": 0,
        "projects": [
            "TRON gGmbH"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "gatk4",
            "sambamba"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-17",
        "versions": 1
    },
    {
        "create_time": "2023-01-17",
        "creators": [],
        "description": "# TronFlow alignment pipeline\r\n\r\n![GitHub tag (latest SemVer)](https://img.shields.io/github/v/release/tron-bioinformatics/tronflow-bwa?sort=semver)\r\n[![Run tests](https://github.com/TRON-Bioinformatics/tronflow-bwa/actions/workflows/automated_tests.yml/badge.svg?branch=master)](https://github.com/TRON-Bioinformatics/tronflow-bwa/actions/workflows/automated_tests.yml)\r\n[![DOI](https://zenodo.org/badge/327943420.svg)](https://zenodo.org/badge/latestdoi/327943420)\r\n[![License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT)\r\n[![Powered by Nextflow](https://img.shields.io/badge/powered%20by-Nextflow-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://www.nextflow.io/)\r\n\r\nThe TronFlow alignment pipeline is part of a collection of computational workflows for tumor-normal pair \r\nsomatic variant calling.\r\n\r\nFind the documentation here [![Documentation Status](https://readthedocs.org/projects/tronflow-docs/badge/?version=latest)](https://tronflow-docs.readthedocs.io/en/latest/?badge=latest)\r\n\r\nThis pipeline aligns paired and single end FASTQ files with BWA aln and mem algorithms and with BWA mem 2.\r\nFor RNA-seq STAR is also supported. To increase sensitivity of novel junctions use `--star_two_pass_mode` (recommended for RNAseq variant calling).\r\nIt also includes an initial step of read trimming using FASTP.\r\n\r\n\r\n## How to run it\r\n\r\nRun it from GitHub as follows:\r\n```\r\nnextflow run tron-bioinformatics/tronflow-alignment -profile conda --input_files $input --output $output --algorithm aln --library paired\r\n```\r\n\r\nOtherwise download the project and run as follows:\r\n```\r\nnextflow main.nf -profile conda --input_files $input --output $output --algorithm aln --library paired\r\n```\r\n\r\nFind the help as follows:\r\n```\r\n$ nextflow run tron-bioinformatics/tronflow-alignment  --help\r\nN E X T F L O W  ~  version 19.07.0\r\nLaunching `main.nf` [intergalactic_shannon] - revision: e707c77d7b\r\n\r\nUsage:\r\n    nextflow main.nf --input_files input_files [--reference reference.fasta]\r\n\r\nInput:\r\n    * input_fastq1: the path to a FASTQ file (incompatible with --input_files)\r\n    * input_files: the path to a tab-separated values file containing in each row the sample name and two paired FASTQs (incompatible with --fastq1 and --fastq2)\r\n    when `--library paired`, or a single FASTQ file when `--library single`\r\n    Example input file:\r\n    name1\tfastq1.1\tfastq1.2\r\n    name2\tfastq2.1\tfastq2.2\r\n    * reference: path to the indexed FASTA genome reference or the star reference folder in case of using star\r\n\r\nOptional input:\r\n    * input_fastq2: the path to a second FASTQ file (incompatible with --input_files, incompatible with --library paired)\r\n    * output: the folder where to publish output (default: output)\r\n    * algorithm: determines the BWA algorithm, either `aln`, `mem`, `mem2` or `star` (default `aln`)\r\n    * library: determines whether the sequencing library is paired or single end, either `paired` or `single` (default `paired`)\r\n    * cpus: determines the number of CPUs for each job, with the exception of bwa sampe and samse steps which are not parallelized (default: 8)\r\n    * memory: determines the memory required by each job (default: 32g)\r\n    * inception: if enabled it uses an inception, only valid for BWA aln, it requires a fast file system such as flash (default: false)\r\n    * skip_trimming: skips the read trimming step\r\n    * star_two_pass_mode: activates STAR two-pass mode, increasing sensitivity of novel junction discovery, recommended for RNA variant calling (default: false)\r\n    * additional_args: additional alignment arguments, only effective in BWA mem, BWA mem 2 and STAR (default: none) \r\n\r\nOutput:\r\n    * A BAM file \\${name}.bam and its index\r\n    * FASTP read trimming stats report in HTML format \\${name.fastp_stats.html}\r\n    * FASTP read trimming stats report in JSON format \\${name.fastp_stats.json}\r\n```\r\n\r\n### Input tables\r\n\r\nThe table with FASTQ files expects two tab-separated columns without a header\r\n\r\n| Sample name          | FASTQ 1                      | FASTQ 2                  |\r\n|----------------------|---------------------------------|------------------------------|\r\n| sample_1             | /path/to/sample_1.1.fastq      |    /path/to/sample_1.2.fastq   |\r\n| sample_2             | /path/to/sample_2.1.fastq      |    /path/to/sample_2.2.fastq   |\r\n\r\n\r\n### Reference genome\r\n\r\nThe reference genome has to be provided in FASTA format and it requires two set of indexes:\r\n* FAI index. Create with `samtools faidx your.fasta`\r\n* BWA indexes. Create with `bwa index your.fasta`\r\n\r\nFor bwa-mem2 a specific index is needed:\r\n```\r\nbwa-mem2 index your.fasta\r\n```\r\n\r\nFor star a reference folder prepared with star has to be provided. In order to prepare it will need the reference\r\ngenome in FASTA format and the gene annotations in GTF format. Run a command as follows:\r\n```\r\nSTAR --runMode genomeGenerate --genomeDir $YOUR_FOLDER --genomeFastaFiles $YOUR_FASTA --sjdbGTFfile $YOUR_GTF\r\n```\r\n\r\n## References\r\n\r\n* Li H. and Durbin R. (2010) Fast and accurate long-read alignment with Burrows-Wheeler Transform. Bioinformatics, Epub. https://doi.org/10.1093/bioinformatics/btp698 \r\n* Shifu Chen, Yanqing Zhou, Yaru Chen, Jia Gu; fastp: an ultra-fast all-in-one FASTQ preprocessor, Bioinformatics, Volume 34, Issue 17, 1 September 2018, Pages i884\u2013i890, https://doi.org/10.1093/bioinformatics/bty560\r\n* Vasimuddin Md, Sanchit Misra, Heng Li, Srinivas Aluru. Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore Systems. IEEE Parallel and Distributed Processing Symposium (IPDPS), 2019.\r\n* Dobin A, Davis CA, Schlesinger F, Drenkow J, Zaleski C, Jha S, Batut P, Chaisson M, Gingeras TR. STAR: ultrafast universal RNA-seq aligner. Bioinformatics. 2013 Jan 1;29(1):15-21. doi: 10.1093/bioinformatics/bts635. Epub 2012 Oct 25. PMID: 23104886; PMCID: PMC3530905.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "418",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/418?version=1",
        "name": "TronFlow alignment pipeline",
        "number_of_steps": 0,
        "projects": [
            "TRON gGmbH"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "bwa",
            "bioinformatics",
            "star",
            "fastp"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-17",
        "versions": 1
    },
    {
        "create_time": "2023-01-17",
        "creators": [
            "Pablo Riesgo Ferreiro",
            "Thomas Bukur",
            "Patrick Sorn"
        ],
        "description": "![CoVigator logo](images/CoVigator_logo_txt_nobg.png \"CoVigator logo\")\r\n\r\n# CoVigator pipeline: variant detection pipeline for Sars-CoV-2\r\n\r\n[![DOI](https://zenodo.org/badge/374669617.svg)](https://zenodo.org/badge/latestdoi/374669617)\r\n[![Run tests](https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline/actions/workflows/automated_tests.yml/badge.svg?branch=master)](https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline/actions/workflows/automated_tests.yml)\r\n[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-Nextflow-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://www.nextflow.io/)\r\n[![License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT)\r\n\r\n\r\n\r\nThe Covigator pipeline processes SARS-CoV-2 FASTQ or FASTA files into annotated and normalized analysis ready VCF files.\r\nIt also classifies samples into lineages using pangolin.\r\nThe pipeline is implemented in the Nextflow framework (Di Tommaso, 2017), it is a stand-alone pipeline that can be\r\nused independently of the CoVigator dashboard and knowledge base.\r\n\r\nAlthough it is configured by default for SARS-CoV-2 it can be employed for the analysis of other microbial organisms \r\nif the required references are provided.\r\n\r\nThe result of the pipeline is one or more annotated VCFs with the list of SNVs and indels ready for analysis.\r\n\r\nThe results from the CoVigator pipeline populate our CoVigator dashboard [https://covigator.tron-mainz.de](https://covigator.tron-mainz.de) \r\n\r\n**Table of Contents**\r\n\r\n1. [Two pipelines in one](#id1)\r\n2. [Implementation](#id2)\r\n3. [How to run](#id3)\r\n4. [Understanding the output](#id4)\r\n6. [Annotation resources](#id5)\r\n7. [Future work](#id6)\r\n8. [Bibliography](#id7)\r\n\r\n\r\n## Two pipelines in one\r\n\r\nIn CoVigator we analyse samples from two different formats, FASTQ files (e.g.: as provided by the European Nucleotide \r\nArchive) and FASTA files containing a consensus assembly. While from the first we get the raw reads, \r\nfrom the second we obtain already assembled genomes. Each of these formats has to be \r\nanalysed differently. Also, the output data that we can obtain from each of these is different.\r\n\r\n![CoVigator pipeline](images/pipeline.drawio.png)\r\n\r\n### Pipeline for FASTQ files\r\n\r\nWhen FASTQ files are provided the pipeline includes the following steps:\r\n- **Trimming**. `fastp` is used to trim reads with default values. This step also includes QC filtering.\r\n- **Alignment**. `BWA mem 2` is used for the alignment of single or paired end samples.\r\n- **BAM preprocessing**. BAM files are prepared and duplicate reads are marked using GATK and Sambamba tools.\r\n- **Primer trimming**. When a BED with primers is provided, these are trimmed from the reads using iVar. This is applicable to the results from all variant callers.\r\n- **Coverage analysis**. `samtools coverage` and `samtools depth` are used to compute the horizontal and vertical \r\n  coverage respectively.\r\n- **Variant calling**. Four different variant callers are employed: BCFtools, LoFreq, iVar and GATK. \r\n  Subsequent processing of resulting VCF files is independent for each caller.\r\n- **Variant normalization**. `bcftools norm` is employed to left align indels, trim variant calls and remove variant duplicates.\r\n- **Technical annotation**. `VAFator` is employed to add VAF and coverage annotations from the reads pileup.\r\n- **Phasing**. Clonal mutations (ie: VAF >= 0.8) occurring in the same amino acid are merged for its correct functional annotation.\r\n- **Biological annotation**. `SnpEff` is employed to annotate the variant consequences of variants and\r\n  `bcftools annotate` is employed to add additional SARS-CoV-2 annotations.\r\n- **Lineage determination**. `pangolin` is used for this purpose, this runs over the results from each of the variant callers separately.\r\n\r\nBoth single end and paired end FASTQ files are supported.\r\n\r\n### Pipeline for FASTA files\r\n\r\nWhen a FASTA file is provided with a single assembly sequence the pipeline includes the following steps:\r\n- **Variant calling**. A Smith-Waterman global alignment is performed against the reference sequence to call SNVs and \r\n  indels. Indels longer than 50 bp and at the beginning or end of the assembly sequence are excluded. Any mutation where\r\n  either reference or assembly contain an N is excluded.\r\n- **Variant normalization**. Same as described above.\r\n- **Phasing**. mutations occurring in the same amino acid are merged for its correct annotation.\r\n- **Biological annotation**. Same as described above.\r\n- **Lineage determination**. `pangolin` is used for this purpose.\r\n\r\nThe FASTA file is expected to contain a single assembly sequence. \r\nBear in mind that only clonal variants can be called on the assembly.\r\n\r\n### Pipeline for VCF files\r\n\r\nWhen a VCF file is provided the pipeline includes the following steps:\r\n- **Variant normalization**. Same as described above.\r\n- **Technical annotation**. Same as described above (optional if BAM is provided)\r\n- **Phasing**. mutations occurring in the same amino acid are merged for its correct annotation.\r\n- **Biological annotation**. Same as described above\r\n- **Lineage determination**. `pangolin` is used for this purpose.\r\n\r\n## Implementation\r\n\r\nThe pipeline is implemented as a Nextflow workflow with its DSL2 syntax.\r\nThe dependencies are managed through a conda environment to ensure version traceability and reproducibility.\r\nThe references for SARS-CoV-2 are embedded in the pipeline.\r\nThe pipeline is based on a number of third-party tools, plus a custom implementation based on biopython (Cock, 2009) \r\nfor the alignment and subsequent variant calling over a FASTA file.\r\n\r\nAll code is open sourced in GitHub [https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline](https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline)\r\nand made available under the MIT license. We welcome any contribution. \r\nIf you have troubles using the CoVigator pipeline or you find an issue, we will be thankful if you would report a ticket \r\nin GitHub.\r\n\r\nThe alignment, BAM preprocessing and variant normalization pipelines are based on the implementations in additional \r\nNextflow pipelines within the TronFlow initiative [https://tronflow-docs.readthedocs.io/](https://tronflow-docs.readthedocs.io/). \r\n\r\n\r\n### Variant annotations\r\n\r\nThe variants derived from a FASTQ file are annotated on the `FILTER` column using the VAFator \r\n(https://github.com/TRON-Bioinformatics/vafator) variant allele frequency \r\n(VAF) into `LOW_FREQUENCY`, `SUBCLONAL`, `LOW_QUALITY_CLONAL` and finally `PASS` variants correspond to clonal variants. \r\nBy default, variants with a VAF < 2 % are considered `LOW_FREQUENCY`, variants with a VAF >= 2 % and < 50 % are \r\nconsidered `SUBCLONAL` and variants with a VAF >= 50 % and < 80 % are considered `LOW_QUALITY_CLONAL`. \r\nThis thresholds can be changed with the parameters `--low_frequency_variant_threshold`,\r\n`--subclonal_variant_threshold` and `--low_quality_clonal_variant_threshold` respectively.\r\n\r\nVAFator technical annotations:\r\n\r\n- `INFO/vafator_af`: variant allele frequency of the mutation \r\n- `INFO/vafator_ac`: number of reads supporting the mutation \r\n- `INFO/vafator_dp`: total number of reads at the position, in the case of indels this represents the number of reads in the previous position\r\n\r\nSnpEff provides the functional annotations. And all mutations are additionally annotated with the following SARS-CoV-2 specific annotations:\r\n- ConsHMM conservation scores as reported in (Kwon, 2021)\r\n- Pfam domains as reported in Ensemble annotations.\r\n\r\nBiological annotations: \r\n\r\n- `INFO/ANN` are the SnpEff consequence annotations (eg: overlapping gene, effect of the mutation). \r\nThis are described in detail here [http://pcingola.github.io/SnpEff/se_inputoutput/](http://pcingola.github.io/SnpEff/se_inputoutput/) \r\n- `INFO/CONS_HMM_SARS_COV_2` is the ConsHMM conservation score in SARS-CoV-2\r\n- `INFO/CONS_HMM_SARBECOVIRUS` is the ConsHMM conservation score among Sarbecovirus\r\n- `INFO/CONS_HMM_VERTEBRATE_COV` is the ConsHMM conservation score among vertebrate Corona virus\r\n- `INFO/PFAM_NAME` is the Interpro name for the overlapping Pfam domains\r\n- `INFO/PFAM_DESCRIPTION` is the Interpro description for the overlapping Pfam domains\r\n- `INFO/problematic` contains the filter provided in DeMaio et al. (2020) for problematic mutations\r\n\r\nAccording to DeMaio et al. (2020), mutations at the beginning (ie: POS <= 50) and end (ie: POS >= 29,804) of the \r\ngenome are filtered out\r\n\r\nThis is an example of biological annotations of a missense mutation in the spike protein on the N-terminal subunit 1 domain.\r\n```\r\nANN=A|missense_variant|MODERATE|S|gene-GU280_gp02|transcript|TRANSCRIPT_gene-GU280_gp02|protein_coding|1/1|c.118G>A|\r\np.D40N|118/3822|118/3822|40/1273||;CONS_HMM_SARS_COV_2=0.57215;CONS_HMM_SARBECOVIRUS=0.57215;CONS_HMM_VERTEBRATE_COV=0;\r\nPFAM_NAME=bCoV_S1_N;PFAM_DESCRIPTION=Betacoronavirus-like spike glycoprotein S1, N-terminal\r\n```\r\n\r\n\r\n### Phasing limitations\r\n\r\nThe phasing implementation is applicable only to clonal mutations. It assumes all clonal mutations are in phase and \r\nhence it merges those occurring in the same amino acid.\r\nIn order to phase intrahost mutations we would need to implement a read-backed phasing approach such as in WhatsHap \r\nor GATK's ReadBackedPhasing. Unfortunately these tools do not support the scenario of a haploid organism with an\r\nundefined number of subclones.\r\nFor this reason, phasing is implemented with custom Python code at `bin/phasing.py`.\r\n\r\n### Primers trimming\r\n\r\nWith some library preparation protocols such as ARTIC it is recommended to trim the primers from the reads.\r\nWe have observed that if primers are not trimmed spurious mutations are being called specially SNVs with lower frequencies and long deletions.\r\nAlso the variant allele frequencies of clonal mutations are underestimated.\r\n\r\nThe BED files containing the primers for each ARTIC version can be found at https://github.com/artic-network/artic-ncov2019/tree/master/primer_schemes/nCoV-2019.\r\n\r\nIf the adequate BED file is provided to the CoVigator pipeline with `--primers` the primers will be trimmed with iVar. \r\nThis affects the output of every variant caller, not only iVar.\r\n\r\n### Reference data\r\n\r\nThe default SARS-CoV-2 reference files correspond to Sars_cov_2.ASM985889v3 and were downloaded from Ensembl servers.\r\nNo additional parameter needs to be provided to use the default SARS-CoV-2 reference genome.\r\n\r\n#### Using a custom reference genome\r\n\r\nThese references can be customised to use a different SARS-CoV-2 reference or to analyse a different virus.\r\nTwo files need to be provided:\r\n- Use a custom reference genome by providing the parameter `--reference your.fasta`.\r\n- Gene annotation file in GFFv3 format `--gff your.gff`. This is only required to run iVar\r\n\r\nAdditionally, the FASTA needs bwa indexes, .fai index and a .dict index.\r\nThese indexes can be generated with the following two commands:\r\n```\r\nbwa index reference.fasta\r\nsamtools faidx reference.fasta\r\ngatk CreateSequenceDictionary --REFERENCE your.fasta\r\n```\r\n\r\n**NOTE**: beware that for Nextflow to find these indices the reference needs to be passed as an absolute path.\r\n\r\nThe SARS-CoV-2 specific annotations will be skipped when using a custom genome.\r\n\r\nIn order to have SnpEff functional annotations available you will also need to provide three parameters:\r\n- `--snpeff_organism`: organism to annotate with SnpEff (ie: as registered in SnpEff)\r\n- `--snpeff_data`: path to the SnpEff data folder\r\n- `--snpeff_config`: path to the SnpEff config file\r\n\r\n### Intrahost mutations\r\n\r\nSome mutations may be observed in a subset of the virus sample, this may arise through intrahost virus evolution or\r\nco-infection. Intrahost mutations can only be detected when analysing the raw reads (ie: the FASTQs) \r\nas in the assembly (ie: the FASTA file) a single virus consensus sequence is represented. \r\nBCFtools and GATK do not normally capture intrahost mutations; on the other hand LoFreq and iVar both capture\r\nmutations that deviate from a clonal-like VAF. \r\nNevertheless, mutations with lower variant allele frequency (VAF) are challenging to distinguish from sequencing and\r\nanalytical errors.  \r\n\r\nMutations are annotated on the `FILTER` column using the VAF into three categories: \r\n- `LOW_FREQUENCY`: subset of intrahost mutations with lowest frequencies, potentially enriched with false positive calls (VAF < 2 %).\r\n- `SUBCLONAL`: subset of intrahost mutations with higher frequencies (2 % <= VAF < 50 %).\r\n- `LOW_QUALITY_CLONAL`: subset of clonal mutations with lower frequencies (50 % <= VAF < 80 %).\r\n- `PASS` clonal mutations (VAF >= 80 %)\r\n\r\nOther low quality mutations are removed from the output.\r\n\r\nThe VAF thresholds can be changed with the parameters `--low_frequency_variant_threshold`,\r\n`--subclonal_variant_threshold` and `--low_quality_clonal_variant_threshold`.\r\n\r\n## How to run\r\n\r\n### Requirements\r\n\r\n- Nextflow >= 19.10.0\r\n- Java >= 8\r\n- Conda >=4.9\r\n\r\n### Testing\r\n\r\nTo run the workflow on a test assembly dataset run:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline -profile conda,test_fasta\r\n```\r\n\r\nFind the output in the folder `covigator_test_fasta`.\r\n\r\nTo run the workflow on a test raw reads dataset run:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline -profile conda,test_fastq\r\n```\r\n\r\nFind the output in the folder `covigator_test_fastq`.\r\n\r\nThe above commands are useful to create the conda environments beforehand.\r\n\r\n**NOTE**: pangolin is the most time-consuming step of the whole pipeline. To make it faster, locate the conda \r\nenvironment that Nextflow created with pangolin (eg: `find $YOUR_NEXTFOW_CONDA_ENVS_FOLDER -name pangolin`) and run\r\n`pangolin --decompress-model`.\r\n\r\n### Running\r\n\r\nFor paired end reads:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\r\n[-r v0.10.0] \\\r\n[-profile conda] \\\r\n--fastq1 <FASTQ_FILE> \\\r\n--fastq2 <FASTQ_FILE> \\\r\n--name example_run \\\r\n--output <OUTPUT_FOLDER> \\\r\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\r\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\n\r\nFor single end reads:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\r\n[-r v0.10.0] \\\r\n[-profile conda] \\\r\n--fastq1 <FASTQ_FILE> \\\r\n--name example_run \\\r\n--output <OUTPUT_FOLDER> \\\r\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\r\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\n\r\nFor assembly:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\r\n[-r v0.10.0] \\\r\n[-profile conda] \\\r\n--fasta <FASTA_FILE> \\\r\n--name example_run \\\r\n--output <OUTPUT_FOLDER> \\\r\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\r\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\n\r\nFor VCF:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\r\n[-r v0.10.0] \\\r\n[-profile conda] \\\r\n--vcf <VCF_FILE> \\\r\n--name example_run \\\r\n--output <OUTPUT_FOLDER> \\\r\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\r\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\n\r\nAs an optional input when processing directly VCF files you can provide BAM files to annotate VAFs:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\r\n[-r v0.10.0] \\\r\n[-profile conda] \\\r\n--vcf <VCF_FILE> \\\r\n--bam <BAM_FILE> \\\r\n--bai <BAI_FILE> \\\r\n--name example_run \\\r\n--output <OUTPUT_FOLDER> \\\r\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\r\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\n\r\nFor batch processing of reads use `--input_fastqs_list` and `--name`.\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline [-profile conda] --input_fastqs_list <TSV_FILE> --library <paired|single> --output <OUTPUT_FOLDER> [--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] [--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\nwhere the TSV file contains two or three columns tab-separated columns **without header**. Columns: sample name, path to FASTQ 1 and optionally path to FASTQ 2. \r\n\r\n| Sample    | FASTQ 1                       | FASTQ 2 (optional column)     |\r\n|-----------|-------------------------------|-------------------------------|\r\n| sample1   | /path/to/sample1_fastq1.fastq | /path/to/sample1_fastq2.fastq |\r\n| sample2   | /path/to/sample2_fastq1.fastq | /path/to/sample2_fastq2.fastq |\r\n| ...       | ...                           | ...                           |\r\n\r\n\r\nFor batch processing of assemblies use `--input_fastas_list`.\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline [-profile conda] --input_fastas_list <TSV_FILE> --library <paired|single> --output <OUTPUT_FOLDER> [--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] [--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\nwhere the TSV file contains two columns tab-separated columns **without header**. Columns: sample name and path to FASTA.\r\n\r\n| Sample    | FASTA                  | \r\n|-----------|------------------------|\r\n| sample1   | /path/to/sample1.fasta |\r\n| sample2   | /path/to/sample2.fasta |\r\n| ...       | ...                    |\r\n\r\nFor batch processing of VCFs use `--input_vcfs_list`.\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline [-profile conda] --input_vcfs_list <TSV_FILE> --output <OUTPUT_FOLDER> [--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] [--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\nwhere the TSV file contains two columns tab-separated columns **without header**. Columns: sample name and path to VCF.\r\n\r\n| Sample    | FASTA                  |\r\n|-----------|------------------------|\r\n| sample1   | /path/to/sample1.vcf |\r\n| sample2   | /path/to/sample2.vcf |\r\n| ...       | ...                    |\r\n\r\nOptionally, provide BAM files for batch processing of VCFs using `--input_bams_list`.\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline [-profile conda] \\\r\n  --input_vcfs_list <TSV_FILE> \\\r\n  --input_bams_list <TSV_FILE> \\\r\n  --output <OUTPUT_FOLDER> \\\r\n  [--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\r\n  [--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\nwhere the BAMs TSV file contains three columns tab-separated columns **without header**. Columns: sample name, \r\npath to BAM and path to BAI.\r\n\r\n| Sample    | BAM                  | BAI                  |\r\n|-----------|----------------------|----------------------|\r\n| sample1   | /path/to/sample1.bam | /path/to/sample1.bai |\r\n| sample2   | /path/to/sample2.bam | /path/to/sample2.bai |\r\n| ...       | ...                  | ...                  |\r\n\r\n\r\n\r\n### Getting help\r\n\r\nYou can always contact us directly or create a GitHub issue, otherwise see all available options using `--help`:\r\n```\r\n$ nextflow run tron-bioinformatics/covigator-ngs-pipeline -profile conda --help\r\n\r\nUsage:\r\n    nextflow run tron-bioinformatics/covigator-ngs-pipeline -profile conda --help\r\n\r\nInput:\r\n    * --fastq1: the first input FASTQ file (not compatible with --fasta, nor --vcf)\r\n    * --fasta: the FASTA file containing the assembly sequence (not compatible with --fastq1, nor --vcf)\r\n    * --vcf: the VCF file containing mutations to analyze (not compatible with --fastq1, nor --fasta)\r\n    * --bam: the BAM file containing reads to annotate VAFs on a VCF (not compatible with --fastq1, nor --fasta)\r\n    * --bai: the BAI index for a BAM file (not compatible with --fastq1, nor --fasta)\r\n    * --name: the sample name, output files will be named after this name\r\n    * --output: the folder where to publish output\r\n    * --input_fastqs_list: alternative to --name and --fastq1 for batch processing\r\n    * --library: required only when using --input_fastqs\r\n    * --input_fastas_list: alternative to --name and --fasta for batch processing\r\n    * --input_vcfs_list: alternative to --name and --vcf for batch processing\r\n    * --input_bams_list: alternative to --name, --vcf, --bam and --bai for batch processing\r\n\r\nOptional input only required to use a custom reference:\r\n    * --reference: the reference genome FASTA file, *.fai, *.dict and bwa indexes are required.\r\n    * --gff: the GFFv3 gene annotations file (required to run iVar and to phase mutations from all variant callers)    \r\n    * --snpeff_data: path to the SnpEff data folder, it will be useful to use the pipeline on other virus than SARS-CoV-2\r\n    * --snpeff_config: path to the SnpEff config file, it will be useful to use the pipeline on other virus than SARS-CoV-2\r\n    * --snpeff_organism: organism to annotate with SnpEff, it will be useful to use the pipeline on other virus than SARS-CoV-2\r\n\r\nOptional input:\r\n    * --fastq2: the second input FASTQ file\r\n    * --primers: a BED file containing the primers used during library preparation. If provided primers are trimmed from the reads.\r\n    * --min_base_quality: minimum base call quality to take a base into account for variant calling (default: 20)\r\n    * --min_mapping_quality: minimum mapping quality to take a read into account for variant calling (default: 20)\r\n    * --vafator_min_base_quality: minimum base call quality to take a base into account for VAF annotation (default: 0)\r\n    * --vafator_min_mapping_quality: minimum mapping quality to take a read into account for VAF annotation (default: 0)\r\n    * --low_frequency_variant_threshold: VAF threshold to mark a variant as low frequency (default: 0.02)\r\n    * --subclonal_variant_threshold: VAF superior threshold to mark a variant as subclonal  (default: 0.5)\r\n    * --lq_clonal_variant_threshold: VAF superior threshold to mark a variant as loq quality clonal (default: 0.8)\r\n    * --memory: the ammount of memory used by each job (default: 3g)\r\n    * --cpus: the number of CPUs used by each job (default: 1)\r\n    * --skip_lofreq: skips calling variants with LoFreq\r\n    * --skip_gatk: skips calling variants with GATK\r\n    * --skip_bcftools: skips calling variants with BCFTools\r\n    * --skip_ivar: skips calling variants with iVar\r\n    * --skip_pangolin: skips lineage determination with pangolin\r\n    * --match_score: global alignment match score, only applicable for assemblies (default: 2)\r\n    * --mismatch_score: global alignment mismatch score, only applicable for assemblies (default: -1)\r\n    * --open_gap_score: global alignment open gap score, only applicable for assemblies (default: -3)\r\n    * --extend_gap_score: global alignment extend gap score, only applicable for assemblies (default: -0.1)\r\n    * --skip_sarscov2_annotations: skip some of the SARS-CoV-2 specific annotations (default: false)\r\n    * --keep_intermediate: keep intermediate files (ie: BAM files and intermediate VCF files)\r\n    * --args_bcftools_mpileup: additional arguments for bcftools mpileup command (eg: --args_bcftools_mpileup='--ignore-overlaps')\r\n    * --args_bcftools_call: additional arguments for bcftools call command (eg: --args_bcftools_call='--something')\r\n    * --args_lofreq: additional arguments for lofreq command (eg: --args_lofreq='--something')\r\n    * --args_gatk: additional arguments for gatk command (eg: --args_gatk='--something')\r\n    * --args_ivar_samtools: additional arguments for ivar samtools mpileup command (eg: --args_ivar_samtools='--ignore-overlaps')\r\n    * --args_ivar: additional arguments for ivar command (eg: --args_ivar='--something')\r\n\r\nOutput:\r\n    * Output a VCF file for each of BCFtools, GATK, LoFreq and iVar when FASTQ files are\r\n    provided or a single VCF obtained from a global alignment when a FASTA file is provided.\r\n    * A pangolin results file for each of the VCF files.\r\n    * Only when FASTQs are provided:\r\n      * FASTP statistics\r\n      * Depth and breadth of coverage analysis results\r\n      \r\n```\r\n\r\n## Understanding the output\r\n\r\nAlthough the VCFs are normalized for both pipelines, the FASTQ pipeline runs four variant callers, while the FASTA\r\npipeline runs a single variant caller. Also, there are several metrics in the FASTQ pipeline that are not present\r\nin the output of the FASTA pipeline. Here we will describe these outputs.\r\n\r\n### FASTQ pipeline output\r\n\r\nFind in the table below a description of each of the expected files and a link to a sample file for the FASTQ pipeline.\r\nThe VCF files will be described in more detail later.\r\n\r\n| Name                            | Description                                                    | Sample file                                                                                                                                       |\r\n|---------------------------------|----------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\r\n| $NAME.fastp_stats.json          | Output metrics of the fastp trimming process in JSON format    | [ERR4145453.fastp_stats.json](_static/covigator_pipeline_sample_output_reads/ERR4145453.fastp_stats.json)                                         |\r\n| $NAME.fastp_stats.html          | Output metrics of the fastp trimming process in HTML format    | [ERR4145453.fastp_stats.html](_static/covigator_pipeline_sample_output_reads/ERR4145453.fastp_stats.html)                                         |\r\n| $NAME.deduplication_metrics.txt | Deduplication metrics                                          | [ERR4145453.deduplication_metrics.txt](_static/covigator_pipeline_sample_output_reads/ERR4145453.deduplication_metrics.txt)                       |\r\n| $NAME.coverage.tsv              | Coverage metrics (eg: mean depth, % horizontal coverage)       | [ERR4145453.coverage.tsv](_static/covigator_pipeline_sample_output_reads/ERR4145453.coverage.tsv)                                                 |\r\n| $NAME.depth.tsv                 | Depth of coverage per position                                 | [ERR4145453.depth.tsv](_static/covigator_pipeline_sample_output_reads/ERR4145453.depth.tsv)                                                       |\r\n| $NAME.bcftools.vcf.gz           | Bgzipped, tabix-indexed and annotated output VCF from BCFtools | [ERR4145453.bcftools.normalized.annotated.vcf.gz](_static/covigator_pipeline_sample_output_reads/ERR4145453.bcftools.normalized.annotated.vcf.gz) |\r\n| $NAME.gatk.vcf.gz               | Bgzipped, tabix-indexed and annotated output VCF from GATK     | [ERR4145453.gatk.normalized.annotated.vcf.gz](_static/covigator_pipeline_sample_output_reads/ERR4145453.gatk.normalized.annotated.vcf.gz)         |\r\n| $NAME.lofreq.vcf.gz             | Bgzipped, tabix-indexed and annotated output VCF from LoFreq   | [ERR4145453.lofreq.normalized.annotated.vcf.gz](_static/covigator_pipeline_sample_output_reads/ERR4145453.lofreq.normalized.annotated.vcf.gz)     |\r\n| $NAME.ivar.vcf.gz               | Bgzipped, tabix-indexed and annotated output VCF from LoFreq   | [ERR4145453.ivar.tsv](_static/covigator_pipeline_sample_output_reads/ERR4145453.ivar.tsv)                                                         |\r\n| $NAME.lofreq.pangolin.csv       | Pangolin CSV output file derived from LoFreq mutations         | [ERR4145453.lofreq.pangolin.csv](_static/covigator_pipeline_sample_output_reads/ERR4145453.lofreq.pangolin.csv)                                              |\r\n\r\n\r\n### FASTA pipeline output\r\n\r\nThe FASTA pipeline returns a single VCF file. The VCF files will be described in more detail later.\r\n\r\n| Name                        | Description                                                  | Sample file                                                                                          |\r\n|-----------------------------|--------------------------------------------------------------|------------------------------------------------------------------------------------------------------|\r\n| $NAME.assembly.vcf.gz | Bgzipped, tabix-indexed and annotated output VCF | [ERR4145453.assembly.normalized.annotated.vcf.gz](_static/covigator_pipeline_sample_output_assembly/hCoV-19_NTXX.assembly.normalized.annotated.vcf.gz) |\r\n\r\n\r\n## Annotations resources\r\n\r\nSARS-CoV-2 ASM985889v3 references were downloaded from Ensembl on 6th of October 2020:\r\n- ftp://ftp.ensemblgenomes.org/pub/viruses/fasta/sars_cov_2/dna/Sars_cov_2.ASM985889v3.dna.toplevel.fa.gz\r\n- ftp://ftp.ensemblgenomes.org/pub/viruses/gff3/sars_cov_2/Sars_cov_2.ASM985889v3.101.gff3.gz\r\n\r\nConsHMM mutation depletion scores downloaded on 1st of July 2021:\r\n- https://github.com/ernstlab/ConsHMM_CoV/blob/master/wuhCor1.mutDepletionConsHMM.bed\r\n- https://github.com/ernstlab/ConsHMM_CoV/blob/master/wuhCor1.mutDepletionSarbecovirusConsHMM.bed\r\n- https://github.com/ernstlab/ConsHMM_CoV/blob/master/wuhCor1.mutDepletionVertebrateCoVConsHMM.bed\r\n\r\nGene annotations including Pfam domains downloaded from Ensembl on 25th of February 2021 from:\r\n- ftp://ftp.ensemblgenomes.org/pub/viruses/json/sars_cov_2/sars_cov_2.json\r\n\r\n\r\n## Future work\r\n\r\n- Primer trimming on an arbitrary sequencing library.\r\n- Pipeline for Oxford Nanopore technology.\r\n- Variant calls from assemblies contain an abnormally high number of deletions of size greater than 3 bp. This\r\nis a technical artifact that would need to be avoided.\r\n\r\n## Bibliography\r\n\r\n- Di Tommaso, P., Chatzou, M., Floden, E. W., Barja, P. P., Palumbo, E., & Notredame, C. (2017). Nextflow enables reproducible computational workflows. Nature Biotechnology, 35(4), 316\u2013319. https://doi.org/10.1038/nbt.3820\r\n- Vasimuddin Md, Sanchit Misra, Heng Li, Srinivas Aluru. Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore Systems. IEEE Parallel and Distributed Processing Symposium (IPDPS), 2019.\r\n- Adrian Tan, Gon\u00e7alo R. Abecasis and Hyun Min Kang. Unified Representation of Genetic Variants. Bioinformatics (2015) 31(13): 2202-2204](http://bioinformatics.oxfordjournals.org/content/31/13/2202) and uses bcftools [Li, H. (2011). A statistical framework for SNP calling, mutation discovery, association mapping and population genetical parameter estimation from sequencing data. Bioinformatics (Oxford, England), 27(21), 2987\u20132993. 10.1093/bioinformatics/btr509\r\n- Danecek P, Bonfield JK, Liddle J, Marshall J, Ohan V, Pollard MO, Whitwham A, Keane T, McCarthy SA, Davies RM, Li H. Twelve years of SAMtools and BCFtools. Gigascience. 2021 Feb 16;10(2):giab008. doi: 10.1093/gigascience/giab008. PMID: 33590861; PMCID: PMC7931819.\r\n- Van der Auwera GA, Carneiro M, Hartl C, Poplin R, del Angel G, Levy-Moonshine A, Jordan T, Shakir K, Roazen D, Thibault J, Banks E, Garimella K, Altshuler D, Gabriel S, DePristo M. (2013). From FastQ Data to High-Confidence Variant Calls: The Genome Analysis Toolkit Best Practices Pipeline. Curr Protoc Bioinformatics, 43:11.10.1-11.10.33. DOI: 10.1002/0471250953.bi1110s43.\r\n- Martin, M., Patterson, M., Garg, S., O Fischer, S., Pisanti, N., Klau, G., Sch\u00f6enhuth, A., & Marschall, T. (2016). WhatsHap: fast and accurate read-based phasing. BioRxiv, 085050. https://doi.org/10.1101/085050\r\n- Danecek, P., & McCarthy, S. A. (2017). BCFtools/csq: haplotype-aware variant consequences. Bioinformatics, 33(13), 2037\u20132039. https://doi.org/10.1093/bioinformatics/btx100\r\n- Wilm, A., Aw, P. P. K., Bertrand, D., Yeo, G. H. T., Ong, S. H., Wong, C. H., Khor, C. C., Petric, R., Hibberd, M. L., & Nagarajan, N. (2012). LoFreq: A sequence-quality aware, ultra-sensitive variant caller for uncovering cell-population heterogeneity from high-throughput sequencing datasets. Nucleic Acids Research, 40(22), 11189\u201311201. https://doi.org/10.1093/nar/gks918\r\n- Grubaugh, N. D., Gangavarapu, K., Quick, J., Matteson, N. L., De Jesus, J. G., Main, B. J., Tan, A. L., Paul, L. M., Brackney, D. E., Grewal, S., Gurfield, N., Van Rompay, K. K. A., Isern, S., Michael, S. F., Coffey, L. L., Loman, N. J., & Andersen, K. G. (2019). An amplicon-based sequencing framework for accurately measuring intrahost virus diversity using PrimalSeq and iVar. Genome Biology, 20(1), 8. https://doi.org/10.1186/s13059-018-1618-7\r\n- Shifu Chen, Yanqing Zhou, Yaru Chen, Jia Gu; fastp: an ultra-fast all-in-one FASTQ preprocessor, Bioinformatics, Volume 34, Issue 17, 1 September 2018, Pages i884\u2013i890, https://doi.org/10.1093/bioinformatics/bty560\r\n- Kwon, S. Bin, & Ernst, J. (2021). Single-nucleotide conservation state annotation of the SARS-CoV-2 genome. Communications Biology, 4(1), 1\u201311. https://doi.org/10.1038/s42003-021-02231-w\r\n- Cock, P. J., Antao, T., Chang, J. T., Chapman, B. A., Cox, C. J., Dalke, A., et al. (2009). Biopython: freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics, 25(11), 1422\u20131423.\r\n- Artem Tarasov, Albert J. Vilella, Edwin Cuppen, Isaac J. Nijman, Pjotr Prins, Sambamba: fast processing of NGS alignment formats, Bioinformatics, Volume 31, Issue 12, 15 June 2015, Pages 2032\u20132034, https://doi.org/10.1093/bioinformatics/btv098\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "417",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/417?version=1",
        "name": "CoVigator pipeline: variant detection pipeline for Sars-CoV-2 (and other viruses...)",
        "number_of_steps": 0,
        "projects": [
            "TRON gGmbH"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "nextflow",
            "sars-cov-2",
            "covid-19",
            "variant calling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-17",
        "versions": 1
    },
    {
        "create_time": "2023-01-13",
        "creators": [
            "Luc  Cornet"
        ],
        "description": "\r\n# Github: https://github.com/Lcornet/GENERA\r\n\r\n# BCCM GEN-ERA tools repository\r\n\r\nPlease visit the wiki for tutorials and access to the tools:\r\nhttps://github.com/Lcornet/GENERA/wiki  \r\n\r\n# NEWS\r\nMantis is now installed in a singularity container for the Metabolic workflow (install is no longer necessary).  \r\n\r\n# Information about the GEN-ERA project\r\nPlease visit  \r\nhttps://bccm.belspo.be/content/bccm-collections-genomic-era  \r\n\r\n# Publications\r\n1. ToRQuEMaDA: tool for retrieving queried Eubacteria, metadata and dereplicating assemblies.  \r\n   L\u00e9onard, R. R., Leleu, M., Vlierberghe, M. V., Cornet, L., Kerff, F., and Baurain, D. (2021).  \r\n   PeerJ 9, e11348. doi:10.7717/peerj.11348.  \r\n   https://peerj.com/articles/11348/  \r\n2. The taxonomy of the Trichophyton rubrum complex: a phylogenomic approach.  \r\n   Cornet, L., D\u2019hooge, E., Magain, N., Stubbe, D., Packeu, A., Baurain, D., and Becker P. (2021).  \r\n   Microbial Genomics 7, 000707. doi:10.1099/mgen.0.000707.  \r\n   https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000707  \r\n3. ORPER: A Workflow for Constrained SSU rRNA Phylogenies.  \r\n   Cornet, L., Ahn, A.-C., Wilmotte, A., and Baurain, D. (2021).  \r\n   Genes 12, 1741. doi:10.3390/genes12111741.  \r\n   https://www.mdpi.com/2073-4425/12/11/1741/html  \r\n4. AMAW: automated gene annotation for non-model eukaryotic genomes.  \r\n   Meunier, L., Baurain, D., Cornet, L. (2021)  \r\n   https://www.biorxiv.org/content/10.1101/2021.12.07.471566v1  \r\n5. Phylogenomic analyses of Snodgrassella isolates from honeybees and bumblebees reveals taxonomic and functional diversity.  \r\n   Cornet, L.,  Cleenwerck, I., Praet, J., Leonard, R., Vereecken, N.J., Michez, D., Smagghe, G., Baurain, D., Vandamme, P. (2021)  \r\n   https://www.biorxiv.org/content/10.1101/2021.12.10.472130v1  \r\n6. Contamination detection in genomic data: more is not enough.   \r\n   Cornet, L & Baurain, D (2022)   \r\n   Genome Biology. 2022;23:60.  \r\n   https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02619-9  \r\n7. The GEN-ERA toolbox: unified and reproducible workflows for research in microbial genomics  \r\n   Cornet, L., Durieu, B., Baert, F., D\u2019hooge, E., Colignon, D., Meunier, L., Lupo, V., Cleenwerck I.,\r\n   Daniel, HM., Rigouts, L., Sirjacobs, D., Declerck, D., Vandamme, P., Wilmotte, A., Baurain, D., Becker P (2022).  \r\n   https://www.biorxiv.org/content/10.1101/2022.10.20.513017v1  \r\n8. CRitical Assessment of genomic COntamination detection at several Taxonomic ranks (CRACOT)    \r\n   Cornet, L., Lupo, V., Declerck, S., Baurain, D. (2022).   \r\n   https://www.biorxiv.org/content/10.1101/2022.11.14.516442v1  \r\n\r\n# Copyright and License\r\n\r\nThis softwares is copyright (c) 2017-2021 by University of Liege / Sciensano / BCCM collection by Luc CORNET\r\nThis is free softwares; you can redistribute it and/or modify.\r\n\r\n![BCCM](https://github.com/Lcornet/GENERA/blob/main/images/GENERA-logo.png)  \r\n",
        "doi": "10.48546/workflowhub.workflow.416.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "416",
        "keep": true,
        "latest_version": 1,
        "license": "LGPL-3.0",
        "link": "https:/workflowhub.eu/workflows/416?version=1",
        "name": "GEN-ERA toolbox",
        "number_of_steps": 0,
        "projects": [
            "BCCM_ULC"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-03-14",
        "versions": 1
    },
    {
        "create_time": "2023-01-03",
        "creators": [
            "Max Schubach"
        ],
        "description": "The Regulatory Mendelian Mutation (ReMM) score was created for relevance prediction of non-coding variations (SNVs and small InDels) in the human genome (GRCh37) in terms of Mendelian diseases. This project updates the ReMM score for the genome build GRCh38 and combines GRCh37 and GRCh38 into one workflow.\r\n\r\n## Pre-requirements\r\n\r\n### Conda\r\nWe use Conda as software and dependency management tool. Conda installation guidelines can be found here:\r\n\r\nhttps://conda.io/projects/conda/en/latest/user-guide/install/index.html\r\n\r\n### Additional programs\r\nThese programs are used during the workflow. They usually need to be compiled, however, the repository already contains the executables or generated files.\r\n\r\n- [AttributeDB](https://github.com/visze/attributedb)\r\n- [Jannovar](https://github.com/charite/jannovar) \r\n- [parSMURF](https://github.com/AnacletoLAB/parSMURF)\r\n\r\n### Snakemake\r\n\r\nThe workflow is managed by Snakemake - a workflow management system used to create reproducible and scalable data analyses. To install Snakemake as well as all other required packages, you need to create a working environment according to the description in the file env/ReMM.yaml. For that, first\r\n\r\nClone the repository\r\n```\r\ngit clone https://github.com/kircherlab/ReMM\r\ncd ReMM\r\n```\r\n\r\nCreate a working environment and activate it\r\n\r\n```\r\nconda env create -n ReMM --file workflow/envs/ReMM.yaml\r\nconda activate ReMM\r\n```\r\n\r\nAll paths are relative to the Snakemake file so you do not need to change any path variables. Additionally, Snakemake creates all missing directories, so no need to create any aditional folders either.\r\n\r\n## Workflow\r\n\r\nThe workflow consists of four main parts:\r\n\r\n- Download of feature data\r\n- Data processing and cleaning\r\n- Model training and validation\r\n- Calculation of ReMM for the whole genome\r\n\r\nThe `workflow` folder contains a graph of the workflow and more detailed information on the most important steps.\r\n\r\nTo launch a snakemake workflow, you need to tell snakemake which file you want to generate. We defined all rules for multiple steps. They can be found here: `workflow/Snakefile`. For example, you want to generate all feature sets defined in a config file you can run:\r\n\r\n```\r\nsnakemake -c1 all_feature_sets\r\n```\r\n\r\nTo execute any step separately (see `README.md` in the `workflow` folder for details on workflow steps), you need to look up the name of the desired output file in the scripts and call Snakemake with the exact name. Using a flag `-n`, you can initiate a 'dry run': Snakemake will check the consistency of all rules and files and show the number of steps. However, a clean dry run does not necessarily mean that no errors will occur during a normal run. ReMM score is not allele-specific so that you get only one score independent of the variant itself. The workflow from the download of data up to computing the scores may take several days or weeks depending on the computing power and internet connection.\r\n\r\n\r\n### The config files\r\n\r\nThe main config file can be found in `config/config.yaml`. This config file was used to generate the ReMM score. Here most of the configuration magic happens. There is a second config file `config/features.yaml` where all features are listed (with additional description). Config files are controled via [json-schema](http://json-schema.org). \r\n\r\nWe also provide a slurm config file for runtimes, memory and number of threads per rule: `config/slurm.yaml`.\r\n",
        "doi": "10.48546/workflowhub.workflow.414.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "414",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/414?version=1",
        "name": "ReMM score",
        "number_of_steps": 0,
        "projects": [
            "KircherLab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "remm",
            "regulatory mendelian mutation score",
            "snakemake",
            "non-coding",
            "pathogenicity score",
            "variant pathogenicity prediction"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-11-24",
        "creators": [
            "Saskia Hiltemann",
            "Willem de Koning"
        ],
        "description": "Workflow for the GTN training \"Antibiotic resistance detection\"",
        "doi": null,
        "edam_operation": [
            "Antimicrobial resistance prediction"
        ],
        "edam_topic": [
            "Microbiology"
        ],
        "filtered_on": "metage* in tags",
        "id": "406",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/406?version=1",
        "name": "GTN Training - Antibiotic Resistance Detection",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "nanoplot",
            "unicycler",
            "racon",
            "gfa_to_fa",
            "minimap2",
            "miniasm",
            "bandage_image",
            "PlasFlow",
            "staramr_search"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2022-11-18",
        "creators": [
            "Yvan Le Bras"
        ],
        "description": "This Galaxy-E workflow was made from the [\"Cleaning GBIF data for the use in biogeography\" tutorial](https://ropensci.github.io/CoordinateCleaner/articles/Cleaning_GBIF_data_with_CoordinateCleaner.html) and allows to:\r\n- Use CoordinateCleaner to automatically flag problematic records\r\n- Use GBIF provided meta-data to improve coordinate quality, tailored to your downstream analyses\r\n- Use automated cleaning algorithms of CoordinateCleaner to identify problematic contributing datasets\r\n- Visualize data on a map",
        "doi": "10.48546/workflowhub.workflow.404.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "404",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/404?version=1",
        "name": "GBIF data Quality check and filtering workflow Feb-2020",
        "number_of_steps": 11,
        "projects": [
            "PNDB"
        ],
        "source": "WorkflowHub",
        "tags": [
            "biodiversity",
            "ecology"
        ],
        "tools": [
            "\n Summary_Statistics1",
            "spocc_occ",
            "\n Filter1",
            "gdal_ogr2ogr",
            "\n Count1"
        ],
        "type": "Galaxy",
        "update_time": "2024-12-09",
        "versions": 1
    },
    {
        "create_time": "2022-01-27",
        "creators": [],
        "description": "## Introduction\r\n\r\n**vibbits/rnaseq-editing** is a bioinformatics pipeline that can be used to analyse RNA sequencing data obtained from organisms with a reference genome and annotation followed by a prediction step of editing sites using RDDpred.\r\n\r\nThe pipeline is largely based on the [nf-core RNAseq pipeline](https://nf-co.re/rnaseq/).\r\n\r\nThe initial nf-core pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\n## Pipeline summary\r\n\r\n1. Merge re-sequenced FastQ files ([`cat`](http://www.linfo.org/cat.html))\r\n2. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\r\n3. Adapter and quality trimming ([`Trimmomatics`](https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/))\r\n4. Use of STAR for multiple alignment and quantification: [`STAR`](https://github.com/alexdobin/STAR)\r\n5. Sort and index alignments ([`SAMtools`](https://sourceforge.net/projects/samtools/files/samtools/))\r\n6. Prediction of editing sites using RDDpred ([`RDDpred`](https://github.com/vibbits/RDDpred))\r\n7. Extensive quality control:\r\n    1. [`RSeQC`](http://rseqc.sourceforge.net/)\r\n    2. [`Qualimap`](http://qualimap.bioinfo.cipf.es/)\r\n    3. [`dupRadar`](https://bioconductor.org/packages/release/bioc/html/dupRadar.html)\r\n8. Present QC for raw read, alignment, gene biotype, sample similarity, and strand-specificity checks ([`MultiQC`](http://multiqc.info/), [`R`](https://www.r-project.org/))\r\n\r\n## Quick Start\r\n\r\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=21.04.0`)\r\n\r\n2. Install [`Docker`](https://docs.docker.com/engine/installation/) on a Linux operating system.\r\n   Note: This pipeline does not currently support running with macOS.\r\n\r\n3. Download the pipeline via git clone, download the associated training data files for RDDpred into the assets folder, download the reference data to \r\n\r\n    ```console\r\n    git clone https://github.com/vibbits/rnaseq-editing.git\r\n    cd $(pwd)/rnaseq-editing/assets\r\n    # download training data file for RDDpred\r\n    wget -c \r\n    # download reference data for your genome, we provide genome and indexed genome for STAR 2.7.3a\r\n    \r\n    ```\r\n\r\n    > * Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\r\n\r\n4. Start running your own analysis using Docker locally!\r\n\r\n    ```console\r\n    nextflow run vibbits/rnaseq-editing \\\r\n        --input samplesheet.csv \\\r\n        --genome hg19 \\\r\n        -profile docker\r\n    ```\r\n\r\n    * An executable Python script called [`fastq_dir_to_samplesheet.py`](https://github.com/nf-core/rnaseq/blob/master/bin/fastq_dir_to_samplesheet.py) has been provided if you would like to auto-create an input samplesheet based on a directory containing FastQ files **before** you run the pipeline (requires Python 3 installed locally) e.g.\r\n\r\n        ```console\r\n        wget -L https://raw.githubusercontent.com/nf-core/rnaseq/master/bin/fastq_dir_to_samplesheet.py\r\n        ./fastq_dir_to_samplesheet.py <FASTQ_DIR> samplesheet.csv --strandedness reverse\r\n        ```\r\n\r\n    * The final analysis has been executed on the Azure platform using Azure Kubernetes Services (AKS). AKS has to be set up on the Azure platform by defining a standard node pool called sys next to the scalable node pool cpumem using Standard_E8ds_v4 as node size for calculation.\r\n      Furthermore, persistent volume claims (PVCs) have been setup for input and work folders of the nextflow runs. In the PVC `input` the reference data as well as the fastqc files have been stored where the PVC `work`, the temporary nextflow files for the individual runs as well as the output files have been stored.\r\n    * The config file for the final execution run for [RNAseq editing for the human samples and reference genome hg19](https://github.com/vibbits/rnaseq-editing/blob/master/nextflow.config.as-executed).    \r\n\r\n## Documentation\r\n\r\nThe nf-core/rnaseq pipeline comes with documentation about the pipeline [usage](https://nf-co.re/rnaseq/usage), [parameters](https://nf-co.re/rnaseq/parameters) and [output](https://nf-co.re/rnaseq/output).\r\n\r\n## Credits\r\nThese scripts were written to provide a reproducible data analysis pipeline until the downstream processing using dedicated R scripts for exploratory analysis and plotting. The general structure of pipeline is based on the data analysis steps of the our recent paper [ADAR1 interaction with Z-RNA promotes editing of endogenous double-stranded RNA and prevents MDA5-dependent immune activation](https://pubmed.ncbi.nlm.nih.gov/34380029/).\r\n\r\nNote: The nf-core scripts this pipeline is based on were originally written for use at the [National Genomics Infrastructure](https://ngisweden.scilifelab.se), part of [SciLifeLab](http://www.scilifelab.se/) in Stockholm, Sweden, by Phil Ewels ([@ewels](https://github.com/ewels)) and Rickard Hammar\u00e9n ([@Hammarn](https://github.com/Hammarn)).\r\n\r\nThe RNAseq pipeline was re-written in Nextflow DSL2 by Harshil Patel ([@drpatelh](https://github.com/drpatelh)) from [The Bioinformatics & Biostatistics Group](https://www.crick.ac.uk/research/science-technology-platforms/bioinformatics-and-biostatistics/) at [The Francis Crick Institute](https://www.crick.ac.uk/), London.\r\n\r\n## Citations\r\n\r\nThe `nf-core` publication is cited here as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "264",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/264?version=1",
        "name": "RNA sequencing data obtained from organisms with a reference genome and annotation followed by a prediction step of editing sites using RDDpred",
        "number_of_steps": 0,
        "projects": [
            "VIB Bioinformatics Core"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-07-07",
        "creators": [
            "Bart Nijsse",
            "Jasper Koehorst"
        ],
        "description": "### Workflow for Metagenomics from bins to metabolic models (GEMs)\r\n\r\n**Summary**\r\n  - Prodigal gene prediction\r\n  - CarveMe genome scale metabolic model reconstruction\r\n  - MEMOTE for metabolic model testing\r\n  - SMETANA Species METabolic interaction ANAlysis\r\n\r\nOther UNLOCK workflows on WorkflowHub: https://workflowhub.eu/projects/16/workflows?view=default<br>\r\n\r\n**All tool CWL files and other workflows can be found here:**<br>\r\nTools: https://gitlab.com/m-unlock/cwl<br>\r\nWorkflows: https://gitlab.com/m-unlock/cwl/workflows\r\n\r\n**How to setup and use an UNLOCK workflow:**<br>\r\nhttps://m-unlock.gitlab.io/docs/setup/setup.html<br>\r\n",
        "doi": null,
        "edam_operation": [
            "Metabolic pathway prediction"
        ],
        "edam_topic": [
            "Metagenomics",
            "Sequence analysis"
        ],
        "filtered_on": "edam",
        "id": "372",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/372?version=1",
        "name": "Metagenomic GEMs from Assembly",
        "number_of_steps": 11,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gem",
            "genomics",
            "metagenomics",
            "carveme",
            "memote"
        ],
        "tools": [
            "Take a snapshot of a model's state and generate a report.",
            "Preparation of workflow output files to a specific output folder",
            "Compress CarveMe GEM",
            "Species METabolic interaction ANAlysis",
            "Compress prodigal protein files",
            "Genome-scale metabolic models reconstruction with CarveMe",
            "prodigal gene/protein prediction",
            "CarveMe GEM statistics",
            "MEMOTE run analsis"
        ],
        "type": "Common Workflow Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-04-20",
        "creators": [
            "Woosub Shin",
            "Friederike Ehrhart",
            "Juma Bayjan",
            "Cenna Doornbos",
            "Ozan Ozisik"
        ],
        "description": "In this analysis, we created an extended pathway, using the WikiPathways repository (Version 20210110) and the three -omics datasets. For this, each of the three -omics datasets was first analyzed to identify differentially expressed elements, and pathways associated with the significant miRNA-protein links were detected. A miRNA-protein link is deemed significant, and may possibly be implying causality, if both a miRNA and its target are significantly differentially expressed. \r\n\r\nThe peptidome and the proteome datasets were quantile normalized and log2 transformed (Pan and Zhang 2018; Zhao, Wong, and Goh 2020). Before transformation, peptide IDs were mapped to protein IDs, using the information provided by the data uploaders, and were summarized into single protein-level values using geometric mean. The miRNome dataset was already normalized and transformed; thus, the information of their targeting genes was simply added to each miRNA ID, using the information provided by miTaRBase (Huang et al. 2019). As a result, all three datasets had been mapped to their appropriate gene product-level (or, protein-level) identifiers. ",
        "doi": null,
        "edam_operation": [
            "Enrichment analysis"
        ],
        "edam_topic": [
            "Rare diseases"
        ],
        "filtered_on": "ITS in description",
        "id": "331",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/331?version=1",
        "name": "EJP-RD WP13 case-study: CAKUT proteome, peptidome and miRNome data analysis using WikiPathways",
        "number_of_steps": 0,
        "projects": [
            "EJPRD WP13 case-studies workflows"
        ],
        "source": "WorkflowHub",
        "tags": [
            "pathway analysis",
            "proteomics",
            "mirna prediction",
            "protein",
            "rare diseases",
            "workflow"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-10-12",
        "creators": [
            "Georgina Samaha"
        ],
        "description": "# IndexReferenceFasta-nf\r\n===========\r\n\r\n  - [Description](#description)\r\n  - [Diagram](#diagram)\r\n  - [User guide](#user-guide)\r\n  - [Benchmarking](#benchmarking)\r\n  - [Workflow summaries](#workflow-summaries)\r\n      - [Metadata](#metadata)\r\n      - [Component tools](#component-tools)\r\n      - [Required (minimum)\r\n        inputs/parameters](#required-minimum-inputsparameters)\r\n  - [Additional notes](#additional-notes)\r\n  - [Help/FAQ/Troubleshooting](#helpfaqtroubleshooting)\r\n  - [Acknowledgements/citations/credits](#acknowledgementscitationscredits)\r\n\r\n---\r\n\r\n## Description\r\nThis is a flexible pipeline for generating common reference genome index files for WGS data analysis. IndexReferenceFasta-nf is a Nextflow (DSL2) pipeline that runs the following tools using Singularity containers:\r\n* Samtools faidx\r\n* BWA index\r\n* GATK CreateSequenceDictionary \r\n\r\n## Diagram\r\n<p align=\"center\"> \r\n<img src=\"https://user-images.githubusercontent.com/73086054/189310509-375fea4f-11fb-41ca-ba52-90760e9a5aa3.png\" width=\"80%\">\r\n</p> \r\n\r\n## User guide\r\n**1. Set up**\r\n\r\nClone this repository by running:\r\n```\r\ngit clone https://github.com/Sydney-Informatics-Hub/IndexReferenceFasta-nf.git\r\ncd IndexReferenceFasta-nf\r\n``` \r\n\r\n**2. Generate indexes**  \r\n\r\nUsers can specify which index files to create by using the `--samtools`, `--bwa`, and/or `--gatk` flags. All are optional. Run the pipeline with:\r\n\r\n```\r\nnextflow run main.nf /path/to/ref.fasta --bwa --samtools --gatk \r\n```\r\n\r\n## Benchmarking\r\n\r\n### Human hg38 reference assembly @ Pawsey's Nimbus (NCPU/task = 1)\r\n|task_id|hash     |native_id|name          |status   |exit|submit |duration  |realtime  |%cpu   |peak_rss|peak_vmem|rchar  |wchar  |\r\n|-------|---------|---------|--------------|---------|----|-------|----------|----------|-------|--------|---------|-------|-------|\r\n|3      |27/33fffc|131621   |samtools_index|COMPLETED|0   |55:44.9|12.2s     |12s       |99.20% |6.3 MB  |11.8 MB  |3 GB   |19.1 KB|\r\n|1      |80/f03e46|131999   |gatk_index    |COMPLETED|0   |55:46.7|22.6s     |22.3s     |231.90%|3.8 GB  |37.1 GB  |3.1 GB |726 KB |\r\n|2      |ea/e29535|131594   |bwa_index     |COMPLETED|0   |55:44.9|1h 50m 16s|1h 50m 15s|99.50% |4.5 GB  |4.5 GB   |12.1 GB|8.2 GB |\r\n\r\n## Workflow summaries\r\n\r\n### Metadata\r\n|metadata field     | workflow_name / workflow_version  |\r\n|-------------------|:---------------------------------:|\r\n|Version            | workflow_version                  |\r\n|Maturity           | under development                 |\r\n|Creators           | Georgie Samaha                    |\r\n|Source             | NA                                |\r\n|License            | GPL-3.0 license                   |\r\n|Workflow manager   | NextFlow                          |\r\n|Container          | None                              |\r\n|Install method     | Manual                            |\r\n|GitHub             | Sydney-Informatics-Hub/IndexReferenceFasta-nf                                |\r\n|bio.tools          | NA                                |\r\n|BioContainers      | NA                                | \r\n|bioconda           | NA                                |\r\n\r\n### Component tools\r\n\r\n* samtools/1.15.1\r\n* gatk/4.2.6.1 \r\n* bwa/0.7.17\r\n\r\n### Required (minimum) inputs/parameters\r\n\r\n* A reference genome file in fasta format.\r\n\r\n## Additional notes\r\n\r\n### Help/FAQ/Troubleshooting\r\n\r\n## Acknowledgements/citations/credits\r\n### Authors \r\n- Georgie Samaha (Sydney Informatics Hub, University of Sydney)   \r\n\r\n### Acknowledgements \r\n\r\n- This pipeline was built using the [Nextflow DSL2 template](https://github.com/Sydney-Informatics-Hub/Nextflow_DSL2_template).  \r\n- Documentation was created following the [Australian BioCommons documentation guidelines](https://github.com/AustralianBioCommons/doc_guidelines).  \r\n\r\n### Cite us to support us! \r\nAcknowledgements (and co-authorship, where appropriate) are an important way for us to demonstrate the value we bring to your research. Your research outcomes are vital for ongoing funding of the Sydney Informatics Hub and national compute facilities. We suggest including the following acknowledgement in any publications that follow from this work:  \r\n\r\nThe authors acknowledge the technical assistance provided by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney and the Australian BioCommons which is enabled by NCRIS via Bioplatforms Australia. \r\n",
        "doi": "10.48546/workflowhub.workflow.393.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "393",
        "keep": true,
        "latest_version": 1,
        "license": "LGPL-3.0",
        "link": "https:/workflowhub.eu/workflows/393?version=1",
        "name": "IndexReferenceFasta-nf",
        "number_of_steps": 0,
        "projects": [
            "Sydney Informatics Hub",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bwa",
            "bioinformatics",
            "gatk",
            "genomics",
            "nextflow",
            "samtools",
            "wgs",
            "index",
            "referencegenome"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-09-26",
        "creators": [
            "Jeanette Reinshagen"
        ],
        "description": "This workflow can be used to fit dose-response curves from normalised cell-based assay data (%confluence) using the KNIME HCS extension. The workflow expects triplicates for each of eight test concentrations. This workflow needs R-Server to run in the back-end. Start R and run the following command: library(Rserve); Rserve(args = \"--vanilla\"). \r\nThree types of outliers can be removed: 1 - Outliers from triplicate measurement (standard deviation cut-off can be selected), 2 - inactive and weekly active compounds (% confluence cut-offs can be selected), 3 - toxic concentrations (cut-off for reduction in confluence with stepwise increasing concentration can be selected)\r\nOutput are two dose-response curve fits per compound for pre and post outlier removal with graphical representation and numerical fit parameters. \r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Compound libraries and screening",
            "Preclinical and clinical studies"
        ],
        "filtered_on": "ITS in description",
        "id": "388",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/388?version=1",
        "name": "DRC_cellbased_OutlierDetection",
        "number_of_steps": 0,
        "projects": [
            "EU-Openscreen"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "KNIME",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-09-26",
        "creators": [
            "Jeanette Reinshagen"
        ],
        "description": "Generates Dose-response curve fits on cell-based toxicity data. Outliers of replicate data-sets can be removed by setting a threshold for standard deviation (here set to 25). Curve fits for compounds showing low response can be removed by setting a threshold for minimum activity (here set to 75% confluence).\r\nThis workflow needs R-Server to run in the back-end. Start R and run the following command: library(Rserve); Rserve(args = \"--vanilla\")",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Compound libraries and screening",
            "Preclinical and clinical studies"
        ],
        "filtered_on": "ITS in description",
        "id": "387",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/387?version=1",
        "name": "DRC_template_toxicity",
        "number_of_steps": 0,
        "projects": [
            "EU-Openscreen"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "KNIME",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-01-31",
        "creators": [
            "Luca Pireddu"
        ],
        "description": "# Snakemake workflow: FAIR CRCC - image conversion\r\n\r\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22656.3.0-brightgreen.svg)](https://snakemake.github.io)\r\n[![GitHub actions status](https://github.com/crs4/fair-crcc-img-convert/workflows/Tests/badge.svg?branch=main)](https://github.com/crs4/fair-crcc-img-convert/actions?query=branch%3Amain+workflow%3ATests)\r\n\r\n\r\nA Snakemake workflow for converting whole-slide images (WSI) from the [CRC\r\nCohort](https://www.bbmri-eric.eu/scientific-collaboration/colorectal-cancer-cohort/)\r\nfrom vendor-specific image formats to open image formats (at the moment,\r\nOME-TIFF).  The workflow also encrypts the new image files with\r\n[Crypt4GH](https://doi.org/10.1093/bioinformatics/btab087).\r\n\r\n\r\n## What's the CRC Cohort?\r\n\r\nThe CRC Cohort is a collection of clinical data and digital high-resolution\r\ndigital pathology images pertaining to tumor cases.  The collection has been\r\nassembled from a number of participating biobanks and other partners through the\r\n[ADOPT BBMRI-ERIC](https://www.bbmri-eric.eu/scientific-collaboration/adopt-bbmri-eric/) project.\r\n\r\nResearchers interested in using the data for science can [apply for\r\naccess](https://www.bbmri-eric.eu/services/access-policies/).\r\n\r\n\r\n## Usage\r\n\r\nThe usage of this workflow is described in the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog/?usage=crs4%2Ffair-crcc-img-convert).\r\n\r\nIf you use this workflow in a paper, don't forget to give credits to the authors by citing the URL of this repository and its DOI (see above).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "266",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/266?version=1",
        "name": "FAIR CRCC - image conversion",
        "number_of_steps": 0,
        "projects": [
            "CRC Cohort"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2025-03-14",
        "versions": 1
    },
    {
        "create_time": "2022-09-02",
        "creators": [
            "Johannes K\u00f6ster"
        ],
        "description": "# Snakemake workflow: dna-seq-varlociraptor\r\n\r\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22656.3.0-brightgreen.svg)](https://snakemake.github.io)\r\n[![GitHub actions status](https://github.com/snakemake-workflows/dna-seq-varlociraptor/workflows/Tests/badge.svg?branch=master)](https://github.com/snakemake-workflows/dna-seq-varlociraptor/actions?query=branch%3Amaster+workflow%3ATests)\r\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4675661.svg)](https://doi.org/10.5281/zenodo.4675661)\r\n\r\n\r\nA Snakemake workflow for calling small and structural variants under any kind of scenario (tumor/normal, tumor/normal/relapse, germline, pedigree, populations) via the unified statistical model of [Varlociraptor](https://varlociraptor.github.io).\r\n\r\n\r\n## Usage\r\n\r\nThe usage of this workflow is described in the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog/?usage=snakemake-workflows%2Fdna-seq-varlociraptor).\r\n\r\nIf you use this workflow in a paper, don't forget to give credits to the authors by citing the URL of this (original) repository and its DOI (see above).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "382",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/382?version=1",
        "name": "dna-seq-varlociraptor workflow",
        "number_of_steps": 0,
        "projects": [
            "Snakemake-Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-08-17",
        "creators": [
            "Cali Willet",
            "Tracy Chew",
            "Georgina Samaha",
            "Rosemarie Sadsad"
        ],
        "description": "Fastq-to-BAM @ NCI-Gadi is a genome alignment workflow that takes raw FASTQ files, aligns them to a reference genome and outputs analysis ready BAM files. This workflow is designed for the National Computational Infrastructure's (NCI) Gadi supercompter, leveraging multiple nodes on NCI Gadi to run all stages of the workflow in parallel, either massively parallel using the scatter-gather approach or parallel by sample. It consists of a number of stages and follows the BROAD Institute's best practice recommendations. \r\n\r\nInfrastructure\\_deployment\\_metadata: Gadi (NCI)",
        "doi": "10.48546/workflowhub.workflow.146.1",
        "edam_operation": [
            "Genetic mapping",
            "Mapping",
            "Sequence alignment"
        ],
        "edam_topic": [
            "Genetic variation",
            "Genomics",
            "Population genomics",
            "Whole genome sequencing"
        ],
        "filtered_on": "metap* in description",
        "id": "146",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/146?version=1",
        "name": "Fastq-to-bam @ NCI-Gadi",
        "number_of_steps": 0,
        "projects": [
            "Australian BioCommons",
            "Sydney Informatics Hub"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "broad",
            "bwa-mem",
            "dna",
            "gadi",
            "genomics",
            "nci",
            "pbs",
            "wgs",
            "genome",
            "mapping",
            "scalable"
        ],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2025-07-25",
        "versions": 1
    },
    {
        "create_time": "2021-08-18",
        "creators": [
            "Tracy Chew",
            "Rosemarie Sadsad"
        ],
        "description": "RNASeq-DE @ NCI-Gadi processes RNA sequencing data (single, paired and/or multiplexed) for differential expression (raw FASTQ to counts). This pipeline consists of multiple stages and is designed for the National Computational Infrastructure's (NCI) Gadi supercompter, leveraging multiple nodes to run each stage in parallel. \r\n\r\nInfrastructure\\_deployment\\_metadata: Gadi (NCI)",
        "doi": "10.48546/workflowhub.workflow.152.1",
        "edam_operation": [
            "Differential gene expression profiling",
            "Expression analysis",
            "Gene expression profiling"
        ],
        "edam_topic": [
            "Gene expression",
            "Gene transcripts",
            "Transcriptomics"
        ],
        "filtered_on": "metap* in description",
        "id": "152",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/152?version=1",
        "name": "RNASeq-DE @ NCI-Gadi",
        "number_of_steps": 0,
        "projects": [
            "Sydney Informatics Hub"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bbduk",
            "de",
            "fastq",
            "fastqc",
            "gadi",
            "htseq",
            "multiqc",
            "nci",
            "nci-gadi",
            "pbs",
            "rnaseq",
            "rseqc",
            "samtools",
            "star",
            "bash",
            "counts",
            "differential expression",
            "differential_expression",
            "expression",
            "illumina",
            "parallel",
            "rna",
            "rna-seq",
            "scalable",
            "workflow"
        ],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-06-07",
        "creators": [
            "Ekaterina Sakharova",
            "Varsha Kale",
            "Martin Beracochea"
        ],
        "description": "MGnify (http://www.ebi.ac.uk/metagenomics) provides a free to use platform for the assembly, analysis and archiving of microbiome data derived from sequencing microbial populations that are present in particular environments. Over the past 2 years, MGnify (formerly EBI Metagenomics) has more than doubled the number of publicly available analysed datasets held within the resource. Recently, an updated approach to data analysis has been unveiled (version 5.0), replacing the previous single pipeline with multiple analysis pipelines that are tailored according to the input data, and that are formally described using the Common Workflow Language, enabling greater provenance, reusability, and reproducibility. MGnify's new analysis pipelines offer additional approaches for taxonomic assertions based on ribosomal internal transcribed spacer regions (ITS1/2) and expanded protein functional annotations. Biochemical pathways and systems predictions have also been added for assembled contigs. MGnify's growing focus on the assembly of metagenomic data has also seen the number of datasets it has assembled and analysed increase six-fold. The non-redundant protein database constructed from the proteins encoded by these assemblies now exceeds 1 billion sequences. Meanwhile, a newly developed contig viewer provides fine-grained visualisation of the assembled contigs and their enriched annotations.\r\n\r\nDocumentation: https://docs.mgnify.org/en/latest/analysis.html#raw-reads-analysis-pipeline",
        "doi": "10.48546/workflowhub.workflow.362.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "362",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/362?version=1",
        "name": "MGnify - raw-reads analysis pipeline",
        "number_of_steps": 4,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cwl",
            "metagenomics",
            "workflows"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-06-07",
        "creators": [
            "Alex L Mitchell",
            " Alexandre Almeida",
            " Martin Beracochea",
            " Miguel Boland",
            " Josephine Burgin",
            " Guy Cochrane",
            " Michael R Crusoe",
            " Varsha Kale",
            " Simon C Potter",
            " Lorna J Richardson",
            " Ekaterina Sakharova",
            " Maxim Scheremetjew",
            " Anton Korobeynikov",
            " Alex Shlemov",
            " Olga Kunyavskaya",
            " Alla Lapidus",
            " Robert D Finn"
        ],
        "description": "MGnify (http://www.ebi.ac.uk/metagenomics) provides a free to use platform for the assembly, analysis and archiving of microbiome data derived from sequencing microbial populations that are present in particular environments. Over the past 2 years, MGnify (formerly EBI Metagenomics) has more than doubled the number of publicly available analysed datasets held within the resource. Recently, an updated approach to data analysis has been unveiled (version 5.0), replacing the previous single pipeline with multiple analysis pipelines that are tailored according to the input data, and that are formally described using the Common Workflow Language, enabling greater provenance, reusability, and reproducibility. MGnify's new analysis pipelines offer additional approaches for taxonomic assertions based on ribosomal internal transcribed spacer regions (ITS1/2) and expanded protein functional annotations. Biochemical pathways and systems predictions have also been added for assembled contigs. MGnify's growing focus on the assembly of metagenomic data has also seen the number of datasets it has assembled and analysed increase six-fold. The non-redundant protein database constructed from the proteins encoded by these assemblies now exceeds 1 billion sequences. Meanwhile, a newly developed contig viewer provides fine-grained visualisation of the assembled contigs and their enriched annotations.\r\n\r\nDocumentation: https://docs.mgnify.org/en/latest/analysis.html#amplicon-analysis-pipeline\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "361",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/361?version=1",
        "name": "MGnify - amplicon analysis pipeline",
        "number_of_steps": 3,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cwl",
            "metagenomics",
            "rna",
            "workflow"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-06-07",
        "creators": [
            " Alex L Mitchell",
            " Alexandre Almeida",
            " Martin Beracochea",
            " Miguel Boland",
            " Josephine Burgin",
            " Guy Cochrane",
            " Michael R Crusoe",
            " Varsha Kale",
            " Simon C Potter",
            " Lorna J Richardson",
            " Ekaterina Sakharova",
            " Maxim Scheremetjew",
            " Anton Korobeynikov",
            " Alex Shlemov",
            " Olga Kunyavskaya",
            " Alla Lapidus",
            " Robert D Finn"
        ],
        "description": "MGnify (http://www.ebi.ac.uk/metagenomics) provides a free to use platform for the assembly, analysis and archiving of microbiome data derived from sequencing microbial populations that are present in particular environments. Over the past 2 years, MGnify (formerly EBI Metagenomics) has more than doubled the number of publicly available analysed datasets held within the resource. Recently, an updated approach to data analysis has been unveiled (version 5.0), replacing the previous single pipeline with multiple analysis pipelines that are tailored according to the input data, and that are formally described using the Common Workflow Language, enabling greater provenance, reusability, and reproducibility. MGnify's new analysis pipelines offer additional approaches for taxonomic assertions based on ribosomal internal transcribed spacer regions (ITS1/2) and expanded protein functional annotations. Biochemical pathways and systems predictions have also been added for assembled contigs. MGnify's growing focus on the assembly of metagenomic data has also seen the number of datasets it has assembled and analysed increase six-fold. The non-redundant protein database constructed from the proteins encoded by these assemblies now exceeds 1 billion sequences. Meanwhile, a newly developed contig viewer provides fine-grained visualisation of the assembled contigs and their enriched annotations.\r\n\r\nDocumentation: https://docs.mgnify.org/en/latest/analysis.html#assembly-analysis-pipeline\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in tags",
        "id": "360",
        "keep": true,
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/360?version=2",
        "name": "MGnify - assembly analysis pipeline",
        "number_of_steps": 4,
        "projects": [
            "MGnify",
            "HoloFood at MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "cwl",
            "metagenomics",
            "workflow"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-04-28",
        "versions": 2
    },
    {
        "create_time": "2022-05-10",
        "creators": [],
        "description": "# HiFi *de novo* genome assembly workflow\r\n\r\nHiFi-assembly-workflow is a bioinformatics pipeline that can be used to analyse Pacbio CCS reads for *de novo* genome assembly using PacBio Circular Consensus Sequencing (CCS)  reads. This workflow is implemented in Nextflow and has 3 major sections. \r\n \r\nPlease refer to the following documentation for detailed description of each workflow section:\r\n \r\n- [Pre-assembly quality control (QC)](https://github.com/AusARG/hifi-assembly-workflow/blob/master/recommendations.md#stage-1-pre-assembly-quality-control)\r\n- [Assembly](https://github.com/AusARG/hifi-assembly-workflow/blob/master/recommendations.md#stage-2-assembly)\r\n- [Post-assembly QC](https://github.com/AusARG/hifi-assembly-workflow/blob/master/recommendations.md#stage-3-post-assembly-quality-control)\r\n\r\n## HiFi assembly workflow flowchart\r\n\r\n![](https://github.com/AusARG/hifi-assembly-workflow/blob/master/workflow.png?raw=true)\r\n\r\n# Quick Usage:\r\nThe pipeline has been tested  on NCI Gadi and AGRF balder cluster. If needed to run on AGRF cluster, please contact us at bioinformatics@agrf.org.au.\r\nPlease note for running this on NCI Gadi you need access. Please refer to Gadi guidelines for account creation and usage: these can be found at https://opus.nci.org.au/display/Help/Access.\r\n\r\nHere is an example that can be used to run a phased assembly on Gadi:\r\n\r\n```\r\nModule load nextflow/21.04.3\r\nnextflow run Hifi_assembly.nf \u2013bam_folder <PATH TO THE BAM FOLDER> -profile gadi \r\n\r\nThe workflow accepts 2 mandatory arguments:\r\n--bam_folder     --    Full Path to the CCS bam files\r\n-profile         --    gadi/balder/local\r\n```\r\n\r\nPlease note that you can either run jobs interactively or submit jobs to the cluster. This is determined by the -profile flag. By passing the gadi tag to the profile argument, the jobs are submitted and run on the cluster.\r\n\r\n# General recommendations for using the HiFi *de novo* genome assembly workflow\r\n\r\n## Example local profile usage\r\n\r\n```\r\nStart a screen, submit a job, and run the workflow \r\nScreen -S \u2018name\u2019\r\n\r\nqsub -I -qnormal -Pwz54 -lwalltime=48:00:00,ncpus=4,mem=200GB,storage=scratch/wz54+gdata/wz54,wd\r\nexport MODULEPATH=/apps/Modules/modulefiles:/g/data/wz54/groupResources/modules\r\n\r\nmodule load nextflow/21.04.3\r\nnextflow run /g/data/wz54/groupResources/scripts/pl/hifi_assembly.nf  --bam_folder  <bam-folder_path> -profile local\r\n\r\n#This load the scripts directory to the environmental PATH and load nextflow module\r\nmodule load hifi_assembly/1.0.0 \r\n```\r\n\r\n# Outputs\r\n\r\nPipeline generates various files and folders here is a brief description: \r\nThe pipeline creates a folder called `secondary_analysis` that contains two sub folders named:\r\n\r\n- `exeReport`     \r\n- `Results`       -- Contains preQC, assembly and postQC analysis files\r\n\r\n## exeReport\r\nThis folder contains a computation resource usage summary in various charts and a text file. \r\n`report.html` provides a comprehensive summary.\r\n\r\n## Results\r\nThe `Results` folder contains three sub-directories preQC, assembly and postqc. As the name suggests, outputs from the respective workflow sections are placed in each of these folders.\r\n\r\n### preQC\r\nThe following table contains list of files and folder from preQC results\r\n\r\n| Output folder/file | File             | Description                                                                    |\r\n| ------------------ | ---------------- | ------------------------------------------------------------------------------ |\r\n| <sample>.fa        |                  | Bam files converted to fasta format                                            |\r\n| kmer\\_analysis     |                  | Folder containing kmer analysis outputs                                        |\r\n|                    | <sample>.jf      | k-mer counts from each sample                                                  |\r\n|                    | <sample>.histo   | histogram of k-mer occurrence                                                  |\r\n| genome\\_profiling  |                  | genomescope profiling outputs                                                  |\r\n|                    | summary.txt      | Summary metrics of genome scope outputs                                        |\r\n|                    | linear\\_plot.png | Plot showing no. of times a k-mer observed by no. of k-mers with that coverage |\r\n\r\n\r\n### Assembly\r\nThis folder contains final assembly results in <FASTA> format.\r\n\r\n- `<sample>_primary.fa` - Fasta file containing primary contigs\r\n- `<sample>_associate.fa` - Fasta file containing associated contigs\r\n\r\n### postqc\r\n \r\nThe postqc folder contains two sub folders \r\n\r\n- `assembly_completeness`\r\n- `assembly_evaluation`\r\n\r\n#### assembly_completeness\r\nThis contains BUSCO evaluation results for primary and associate contig.\r\n\r\n#### assembly_evaluation\r\nAssembly evaluation folder contains various file formats, here is a brief description for each of the outputs.\r\n\r\n| File        | Description                                                                               |\r\n| ----------- | ----------------------------------------------------------------------------------------- |\r\n| report.txt  | Assessment summary in plain text format                                                   |\r\n| report.tsv  | Tab-separated version of the summary, suitable for spreadsheets (Google Docs, Excel, etc) |\r\n| report.tex  | LaTeX version of the summary                                                              |\r\n| icarus.html | Icarus main menu with links to interactive viewers                                        |\r\n| report.html | HTML version of the report with interactive plots inside                                  |\r\n\r\n\r\n# Infrastructure usage and recommendations\r\n\r\n### NCI facility access\r\nOne should have a user account set with NCI to access gadi high performance computational facility. Setting up a NCI account is mentioned in detail at the following URL: https://opus.nci.org.au/display/Help/Setting+up+your+NCI+Account \r\n  \r\nDocumentation for a specific infrastructure should go into a infrastructure documentation template\r\nhttps://github.com/AustralianBioCommons/doc_guidelines/blob/master/infrastructure_optimisation.md\r\n\r\n\r\n## Compute resource usage across tested infrastructures\r\n\r\n|                                       | Computational resource for plant case study |\r\n| ------------------------------------- | ------------------------------------------- |\r\n|                                       | Time                                        | CPU | Memory | I/O |\r\n| Process                               | duration                                    | realtime | %cpu | peak\\_rss | peak\\_vmem | rchar | wchar |\r\n| Converting bam to fasta for sample    | 12m 54s                                     | 12m 48s | 99.80% | 5.2 MB | 197.7 MB | 43.3 GB | 50.1 GB |\r\n| Generating k-mer counts and histogram | 26m 43s                                     | 26m 36s | 1725.30% | 19.5 GB | 21 GB | 77.2 GB | 27.1 GB |\r\n| Profiling genome characteristics      | 34.7s                                       | 13.2s | 89.00% | 135 MB | 601.2 MB | 8.5 MB | 845.9 KB |\r\n| Denovo assembly                       | 6h 51m 15s                                  | 6h 51m 11s | 4744.40% | 84.7 GB | 225.6 GB | 1.4 TB | 456 GB |\r\n| evaluate\\_assemblies                  | 5m 18s                                      | 4m 54s | 98.20% | 1.6 GB | 1.9 GB | 13.6 GB | 2.8 GB |\r\n| assemblies\\_completeness              | 25m 57s                                     | 25m 53s | 2624.20% | 22 GB | 25.2 GB | 624.9 GB | 2.9 GB |\r\n\r\n\r\n|                                       | Computational resource for bird case study |\r\n| ------------------------------------- | ------------------------------------------ |\r\n|                                       | Time                                       | CPU | Memory | I/O |\r\n| Process                               | duration                                   | realtime | %cpu | peak\\_rss | peak\\_vmem | rchar | wchar |\r\n| Converting bam to fasta for sample    | 12m 54s                                    | 7m 9s | 86.40% | 5.2 MB | 197.8 MB | 21.5 GB | 27.4 GB |\r\n| Generating k-mer counts and histogram | 26m 43s                                    | 15m 34s | 1687.70% | 10.1 GB | 11.7 GB | 44 GB | 16.6 GB |\r\n| Profiling genome characteristics      | 34.7s                                      | 1m 15s | 15.30% | 181.7 MB | 562.2 MB | 8.5 MB | 819.1 KB |\r\n| De novo assembly                      | 6h 51m 15s                                 | 9h 2m 47s | 1853.50% | 67.3 GB | 98.4 GB | 1 TB | 395.6 GB |\r\n| evaluate assemblies                   | 5m 18s                                     | 2m 48s | 97.50% | 1.1 GB | 1.4 GB | 8.7 GB | 1.8 GB |\r\n| assemblies completeness               | 25m 57s                                    | 22m 36s | 2144.00% | 22.2 GB | 25 GB | 389.7 GB | 1.4 GB |\r\n\r\n\r\n# Workflow summaries\r\n\r\n## Metadata\r\n\r\n| Metadata field   | Pre-assembly quality control                                                      | Primary assembly   | Post-assembly quality control |\r\n| ---------------- | --------------------------------------------------------------------------------- | ------------------ | ----------------------------- |\r\n| Version          | 1.0                                                                               | 1.0                | 1.0                           |\r\n| Maturity         | Production                                                                        | Production         | production                    |\r\n| Creators         | Naga, Kenneth                                                                     | Naga, Kenneth      | Naga, Kenneth                 |\r\n| Source           | [AusARG/hifi-assembly-workflow](https://github.com/AusARG/hifi-assembly-workflow) |\r\n| License          |  MIT License                                                                       | MIT License         | MIT License                     |\r\n| Workflow manager | NextFlow                                                                          | NextFlow           | NextFlow                      |\r\n| Container        | No containers used                                                                | No containers used | No containers used            |\r\n| Install method   | Manual                                                                            | Manual             | Manual                        |\r\n\r\n\r\n## Component tools\r\n\u200b\r\n| Workflow element                  | Workflow element version | Workflow title                |\r\n| --------------------------------- | ------------------------ | ----------------------------- |\r\n| Samtools, jellyfish, genomescope  | 1.0                      | Pre-assembly quality control  |\r\n| Improved phased assembler (pbipa) | 1.0                      | Primary assembly              |\r\n| Quast and busco                   | 1.0                      | Post-assembly quality control |\r\n\r\n\r\n## Required (minimum) inputs/parameters\r\n \r\nPATH to HIFI bam folder is the minimum requirement for the processing the pipeline.\r\n\r\n## Third party tools / dependencies\r\n\r\nThe following packages are used by the pipeline.\r\n\r\n- `nextflow/21.04.3`\r\n- `samtools/1.12`\r\n- `jellyfish/2.3.0`\r\n- `genomescope/2.0`\r\n- `ipa/1.3.1`\r\n- `quast/5.0.2`\r\n- `busco/5.2.2`\r\n\r\nThe following paths contain all modules required for the pipeline.\r\n\r\n- `/apps/Modules/modulefiles`\r\n- `/g/data/wz54/groupResources/modules`\r\n\r\n---\r\n\r\n# Help/FAQ/Troubleshooting\r\n\r\nDirect training and help is available if you are new to HPC and/or new to NCI/Gadi.\r\n\r\n- Basic information to get started with the NCI Gadi for bioinformatics can be found at https://github.com/AusARG/ABLeS/wiki/temppage.\r\n- For NCI support, contact the NCI helpdesk directly at https://www.nci.org.au/users/nci-helpdesk\r\n- Queue limits and structure explained at https://opus.nci.org.au/display/Help/4.+PBS+Jobs\r\n\r\n---\r\n\r\n# 3rd party Tutorials \r\n\r\nA tutorial by Andrew Severin on running GenomeScope 1.0 is available here:\r\nhttps://github.com/AusARG/hifi-assembly-workflow.git\r\n\r\nImproved Phased Assembler tutorial is available at \r\nhttps://github.com/PacificBiosciences/pbbioconda/wiki/Improved-Phased-Assembler\r\n\r\nBusco tutorial\r\nhttps://wurmlab.com/genomicscourse/2016-SIB/practicals/busco/busco_tutorial\r\n\r\n---\r\n\r\n# Licence(s)\r\n\r\nMIT License\r\n\r\nCopyright (c) 2022 AusARG\r\n\r\nPermission is hereby granted, free of charge, to any person obtaining a copy\r\nof this software and associated documentation files (the \"Software\"), to deal\r\nin the Software without restriction, including without limitation the rights\r\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\ncopies of the Software, and to permit persons to whom the Software is\r\nfurnished to do so, subject to the following conditions:\r\n\r\nThe above copyright notice and this permission notice shall be included in all\r\ncopies or substantial portions of the Software.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\nSOFTWARE.\r\n\r\n---\r\n\r\n# Acknowledgements/citations/credits\r\n\r\n> Jung, H. et al. Twelve quick steps for genome assembly and annotation in the classroom. PLoS Comput. Biol. 16, 1\u201325 (2020).\r\n\r\n> 2020, G. A. W. No Title. https://ucdavis-bioinformatics-training.github.io/2020-Genome_Assembly_Workshop/kmers/kmers.\r\n\r\n> Sovi\u0107, I. et al. Improved Phased Assembly using HiFi Data. (2020).\r\n\r\n> Gurevich, A., Saveliev, V., Vyahhi, N. & Tesler, G. QUAST: Quality assessment tool for genome assemblies. Bioinformatics 29, 1072\u20131075 (2013).\r\n\r\n> Waterhouse, R. M. et al. BUSCO applications from quality assessments to gene prediction and phylogenomics. Mol. Biol. Evol. 35, 543\u2013548 (2018).\r\n\r\n---\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "340",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/340?version=1",
        "name": "HiFi de novo genome assembly workflow",
        "number_of_steps": 0,
        "projects": [
            "AGRF BIO"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-06-06",
        "creators": [
            "Miguel Roncoroni"
        ],
        "description": "This workflow begins from a set of genome assemblies of different samples, strains, species. The genome is first annotated with Funnanotate. Predicted proteins are furtner annotated with Busco. Next, 'ProteinOrtho' finds orthologs across the samples and makes orthogroups. Orthogroups where all samples are represented are extracted. Orthologs in each orthogroup are aligned with ClustalW. Test dataset: https://zenodo.org/record/6610704#.Ypn3FzlBw5k",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "358",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/358?version=1",
        "name": "preparing genomic data for phylogeny recostruction (GTN)",
        "number_of_steps": 12,
        "projects": [
            "usegalaxy.be workflows"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "phylogenetics",
            "phylogenomics"
        ],
        "tools": [
            "proteinortho",
            "tp_replace_in_line",
            "funannotate_predict",
            "repeatmasker_wrapper",
            "clustalw",
            "collapse_dataset",
            "\n Filter1",
            "regex1",
            "proteinortho_grab_proteins",
            "glimmer_gbk_to_orf",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2022-06-04",
        "creators": [
            "Anton Nekrutenko"
        ],
        "description": "Generic variation analysis on WGS PE data\n-------------------------------------------\n\nThis workflows performs paired end read mapping with bwa-mem followed by\nsensitive variant calling across a wide range of AFs with lofreq and variant\nannotation with snpEff. The reference genome can be provided as a GenBank file.\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "357",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/357?version=1",
        "name": "generic-variant-calling-wgs-pe/main",
        "number_of_steps": 12,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "generic",
            "mpxv"
        ],
        "tools": [
            "fastp",
            "samtools_stats",
            "samtools_view",
            "lofreq_filter",
            "snpEff",
            "lofreq_viterbi",
            "snpEff_build_gb",
            "lofreq_call",
            "picard_MarkDuplicates",
            "lofreq_indelqual",
            "bwa_mem",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2022-05-31",
        "creators": [
            "Anna Syme"
        ],
        "description": "# workflow-partial-ustacks-only\r\n\r\n\r\nThese workflows are part of a set designed to work for RAD-seq data on the Galaxy platform, using the tools from the Stacks program. \r\n\r\nGalaxy Australia: https://usegalaxy.org.au/\r\n\r\nStacks: http://catchenlab.life.illinois.edu/stacks/\r\n\r\n\r\nFor the full de novo workflow see https://workflowhub.eu/workflows/348\r\n\r\nYou may want to run ustacks with different batches of samples. \r\n* To be able to combine these later, there are some necessary steps - we need to keep track of how many samples have already run in ustacks, so that new samples can be labelled with different identifying numbers.  \r\n* In ustacks, under \"Processing options\" there is an option called \"Start identifier at\". \r\n* The default for this is 1, which can be used for the first batch of samples. These will then be labelled as sample 1, sample 2 and so on. \r\n* For any new batches of samples to process in ustacks, we will want to start numbering these at the next available number. e.g. if there were 10 samples in batch 1, this should then be set to start at 11. \r\n\r\nTo combine multiple outputs from ustacks, providing these have been given appropriate starting identifiers:\r\n* Find the ustacks output in the Galaxy history. This will be a list of samples. \r\n* Click on the cross button next to the filename to delete, but select \"Collection only\". This releases the items from the list, but they will now be hidden in the Galaxy history.\r\n* In the history panel, click on \"hidden\" to reveal any hidden files. Unhide the samples. \r\n* Do this for all the batches of ustacks outputs that are needed. \r\n* Click on the tick button, tick all the samples needed, then \"For all selected\" choose \"Build dataset list\"\r\n* This is now a combined set of samples for input into cstacks. \r\n",
        "doi": null,
        "edam_operation": [
            "Sequence clustering"
        ],
        "edam_topic": [
            "Population genomics"
        ],
        "filtered_on": "binn* in description",
        "id": "349",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/349?version=1",
        "name": "Partial de novo workflow: ustacks only",
        "number_of_steps": 1,
        "projects": [
            "Galaxy Australia",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "stacks2_ustacks"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-30",
        "versions": 1
    },
    {
        "create_time": "2022-05-05",
        "creators": [
            "Georgina Samaha",
            "Tracy Chew",
            "Cali Willet",
            "Nandan Deshpande"
        ],
        "description": "# GermlineShortV_biovalidation\r\n\r\n - [Description](#description)\r\n  - [Diagram](#diagram)\r\n  - [User guide](#user-guide)\r\n      - [Quick start guide](#quick-start-guide)\r\n  - [Benchmarking](#benchmarking)\r\n  - [Workflow summaries](#workflow-summaries)\r\n      - [Metadata](#metadata)\r\n      - [Component tools](#component-tools)\r\n      - [Required (minimum)\r\n        inputs/parameters](#required-minimum-inputsparameters)  \r\n        [Preparing your own input files](#preparing-input-files)\r\n  - [Additional notes](#additional-notes)\r\n      - [Understanding your outputs](#understanding-your-outputs)  \r\n      - [Performance metrics explained](#performance-metrics-explained)   \r\n  - [Help/FAQ/Troubleshooting](#helpfaqtroubleshooting)\r\n  - [Acknowledgements/citations/credits](#acknowledgementscitationscredits)\r\n\r\n## Description \r\nPopulation-scale WGS cohorts are essential resources for genetic analyses including heritable diseases, evolutionary genomics, conservation biology, and population genomics. Processing raw reads into analysis-ready variants remains challenging. Various mapping and variant calling pipelines have been made publicly available in recent decades. Designing a mapping and variant calling pipeline to meet your needs is dependent on the compute infrastructure you\u2019re working on, the types of variants you\u2019re primarily interested in, and the sequencing technology you use to generate raw sequencing data. Keep in mind that the tools you use to build your pipeline can affect variant calling accuracy. Further, optimisation and customisation of these tools\u2019 commands can also affect their performance. Best-practice recommendations for variant calling pipelines vary dramatically between species and research questions, depending on the availability of genomic resources for the population of interest, genome structure, and clinical relevance of the resulting variant dataset. It is important to not only design a robust variant calling pipeline but also fine-tune it to achieve optimal performance for your dataset and research question. \r\n\r\nThere are various measurements that you can apply to evaluate the biological accuracy of your germline variant calling pipeline. Currently, no best practice methods for interrogating joint-called variant sets exist in the literature. A number of publicly available, human \u2018gold standard\u2019 truth datasets including Platinum Genomes and Genome in a Bottle (GIAB) are useful for benchmarking across high confidence regions of the genome and evaluating the recall and precision of the pipeline. We recommend individuals working with human datasets benchmark their germline variant calling pipelines using one of these datasets. Unfortunately, these resources are not typically available for non-human organisms. \r\n\r\nHere, we present protocols for benchmarking and validating germline short variant (SNVs and indels) datasets using a combination of methods that can capture the quality of your variant sets for human, non-human model, and non-model organisms. The process you can apply will depend on the organism you\u2019re working with and the genomic resources available to that organism. \r\n\r\n## Diagram \r\n\r\n<p align=\"center\"> \r\n<img src=\"https://github.com/Sydney-Informatics-Hub/GermlineShortV_biovalidation/blob/main/Benchmarking%20and%20validation%20protocol.png\" width=\"70%\" height=\"70%\">  \r\n</p> \r\n\r\n## User guide \r\n###  Quick start guide \r\n\r\nThese bash scripts were written for the University of Sydney\u2019s high performance computer, Artemis. They can be run on the command line or submitted as PBS jobs. These scripts assume your input is a gzipped multi-sample (cohort) VCF file. Before running, edit the PBS project directive and define the variables at the top of the script. All software used in this protocol is installed on Artemis- to use alternate versions or run on a different compute infrastructure, edit the modules according to your needs.  \r\n\r\n#### Human datasets \r\nFor human datasets, we recommend you benchmark your germline variant calling pipeline using a gold standard dataset such as Platinum Genomes. Raw sequence data in FASTQ format for these datasets can be downloaded along with their high confidence variant calls and regions from public repositories. See [Preparing input files]() for more information on how to download and prepare these files.    \r\n\r\n##### 1. Collect vcf summary metrics  \r\nEdit the PBS -P directive and variables for your dataset in `vcfstat.sh`. Then run script with: \r\n\r\n```\r\nqsub vcfstat.sh (or bash vcfstat.sh)\r\n```\r\nThis will produce summary and quality metrics reports and plots for your cohort. It will also produce summary and detail files for known variant representation. BCFtools stats plots will be housed in a directory labelled `${cohort}_vcfplots`. \r\n\r\n##### 2. Biological benchmarking using a truth set  \r\n\r\nEdit the PBS -P directive and variables for your files. Then run script with:  \r\n\r\n```\r\nqsub run_happy.sh\r\n```\r\nThis script will subset your multi-sample VCF into individual samples, prepare them for hap.py, and output a number of files including summary metrics (including recall, precision and F1-score) and ROC count files that can be used to produce ROC curves, separately for SNVs and indels. See the [hap.py user guide](https://github.com/Illumina/hap.py/blob/master/doc/happy.md) for more information on how to interpret hap.py output. ROC curves of Hap.py runs can be plotted using the script [rocplot.Rscript](https://github.com/Illumina/hap.py/blob/master/src/R/rocplot.Rscript).   \r\n\r\n#### Non-human model organism datasets\r\n\r\n##### 1. Collect vcf summary metrics  \r\nEdit the PBS -P directive and variables for your dataset in `vcfstat.sh`. We recommend you use the set of known variants used for base quality score recalibration to validate population level variants. If you used trio data, unhash the Mendelian error command within the script. Then run script with: \r\n\r\n```\r\nqsub vcfstat.sh (or bash vcfstat.sh)\r\n```\r\nThis will produce summary and quality metrics reports and plots for your cohort. It will also produce summary and detail files for known variant representation. BCFtools stats plots will be housed in a directory labelled `${cohort}_vcfplots`.  \r\n#### Non-model organism datasets \r\n\r\n##### 1. Collect vcf summary metrics  \r\n\r\nEdit the PBS -P directive and variables for your dataset in `vcfstat_nonmodel.sh`. Then run script with: \r\n\r\n```\r\nqsub vcfstat_nonmodel.sh (or bash vcfstat_nonmodel.sh)\r\n```\r\n\r\nThis will produce summary and quality metrics reports and plots for your cohort. It will also produce summary and detail files for known variant representation. BCFtools stats plots will be housed in a directory labelled `${cohort}_vcfplots`. \r\n\r\n## Benchmarking \r\nComing soon!  \r\n\r\n## Workflow summaries \r\n### Metadata \r\n|metadata field     | workflow_name / workflow_version  |\r\n|-------------------|:---------------------------------:|\r\n|Version            | 1.0                 |\r\n|Maturity           | stable                            |\r\n|Creators           | Georgie Samaha, Tracy Chew, Cali Willet                 |\r\n|Source             | NA                                |\r\n|License            | NA                                |\r\n|Workflow manager   | NA                          |\r\n|Container          | None                              |\r\n|Install method     | Manual                            |\r\n|GitHub             | NA                                |\r\n|bio.tools \t        | NA                                |\r\n|BioContainers      | NA                                | \r\n|bioconda           | NA                                |\r\n\r\n### Component tools \r\n\r\nbcftools/1.14  \r\nhtslib/1.14  \r\npython/3.8.2  \r\nR/4.1.1  \r\nhap.py/0.3.14  \r\n\r\n### Required (minimum) inputs/parameters \r\n\r\n- Multi-sample or single sample VCF file (VCF.gz format)\r\n- List of sample IDs that match the VCF (.txt format)\r\n- Known variant dataset (VCF format. Human and non-human model organisms only)\r\n- Pedigree file (format: mother,father,offspring. Trios or Platinum Genomes only)\r\n- Truth set variant calls (VCF.gz format. Human, Platinum Genomes only)\r\n- High confidence call regions (BED format. Human, Platinum Genomes only)\r\n\r\n### Preparing input files \r\n\r\n#### Gold standard variant truth sets  \r\n\r\nThe benchmarking protocol for human datasets assumes you have performed mapping and germline variant calling on a gold standard truth set. These datasets contain millions of variants that have been confirmed using orthologous technologies [Eberle et al. 2017](https://doi.org/10.1101/gr.210500.116).   \r\n\r\nWe recommend you use the Platinum Genomes dataset for benchmarking germline variant calling pipelines that include joint genotyping of multiple samples. Six members, comprising two trios, of the Platinum Genomes dataset can be downloaded from the Illumina BaseSpace Sequence Hub, the ENA, or dbGaP. The Platinum Genomes dataset contains multiple files including the following files you will need for running `run_happy.sh`: \r\n- Paired-end FASTQ files for each sample\r\n- High-confidence germline variant VCF files for each sample\r\n- High-confidence genomic regions (BED format)\r\n\r\nCurrently, these files are available for Hg19 (GRCh37) and Hg38 (GRCh38) . Links to raw data are [here](https://github.com/Illumina/PlatinumGenomes). BaseSpace offers a command line tool for downloading files, see [here](https://developer.basespace.illumina.com/docs/content/documentation/cli/cli-examples) for instructions. \r\n\r\n#### Providing your own \u2018truth set\u2019 \r\n*A word of caution*- testing the performance of your pipeline using a truth set is only intended to estimate the overall quality of your pipeline and detect any potential sources of error in your method. It is not intended to test the truthfulness of your variant set. See [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035531572-Evaluating-the-quality-of-a-germline-short-variant-callset) for further discussion of the assumptions we make about truth sets. Most non-human organisms do not have access to gold standard truth set resources like the Platinum Genomes dataset. However there are a few alternative options you could try: \r\n - Genotyping arrays: if you have genotyping data for the same samples you tested your germline variant calling pipeline with, you can reformat these to VCF using a tool like [PLINK\u2019s recode](https://www.cog-genomics.org/plink/1.9/data#recode) and use it as a truth set. \r\n - Known variant datasets: if your organism of interest has a set of known population-level variants you can use these as a truth-set. Just remember that these variants might not always be validated (i.e. dbSNP). \r\n\r\nUsing this method you will need to also provide your own high-confidence regions file in BED format. The location and size of these regions will depend on your dataset, organism, reference assembly and sequencing method. Typically these regions would exclude centromeres, telomeres and repetitive parts of the genome that are likely to complicate variant calling.   \r\n\r\n\r\n## Additional notes \r\n\r\nTest data for Hap.py can be found [here](https://github.com/Illumina/hap.py/blob/master/doc/microbench.md)  \r\n\r\nInstructions on how to install Hap.py can be found [here](https://github.com/Illumina/hap.py#installation)   \r\n\r\nThis warning may be thrown by Hap.py and can be ignored: `WARNING  No reference file found at default locations. You can set the environment variable 'HGREF' or 'HG19' to point to a suitable Fasta file.`  \r\n\r\n\r\n### Understanding your outputs \r\nThe following files will be produced and stored in your designated working directory. They will all be labelled with your specified cohort name.  \r\n\r\n#### Variant based metrics \r\nProduced by BCFtools stats command. Output file:\r\n- ${cohort}.bcftools.metrics  \r\n- ${cohort}_bcftools.metrics_vcfstatplots (directory and files)  \r\n\r\n#### Sample based metrics   \r\nProduced by BCFtools smplstats and mendelian commands. Output files:\r\n- ${cohort}.smplstats\r\n- ${cohort}.smplstats.pdf\r\n- ${cohort}.Mendelianerr\r\n\r\n#### Known variant concordance \r\nProduced by GATK CollectVariantCallingMetrics command. Output files:\r\n- ${cohort}.known.variant_calling_summary_metrics\r\n- ${cohort}.known.variant_calling_detail_metrics\r\n\r\n#### Biological validation using a truth set \r\nProduced by Hap.py. Output files:\r\n- ${sample}.happy.metrics.json.gz\r\n- ${sample}.happy.roc.all.csv.gz\r\n- ${sample}.happy.roc.Locations.INDEL.csv.gz\r\n- ${sample}.happy.roc.Locations.INDEL.PASS.csv.gz\r\n- ${sample}.happy.roc.Locations.SNP.csv.gz\r\n- ${sample}.happy.roc.Locations.SNP.PASS.csv.gz\r\n- ${sample}.happy.roc.tsv\r\n- ${sample}.happy.runinfo.json\r\n- ${sample}.happy.summary.csv\r\n\r\n### Performance metrics explained  \r\n\r\n|Metric                                |Expected/ideal value                                |Tool           |Relevance                                                                                                      |\r\n|--------------------------------------|----------------------------------------------------|---------------|---------------------------------------------------------------------------------------------------------------|\r\n|Number of SNVs and indels (per sample)|Human WGS: ~4.4M, Human WES: ~41k, Species dependent|bcftools stats |Population, sequencing approach, and genomic region dependent. Alone, this metric cannot indicate data quality.|\r\n|Indel length distribution             |Indel length range is 1-10,000bp.                   |bcftools stats |Increased length is conflated with reduced mapping quality. Distribution is dataset dependent. Recommend filtering for high quality.|\r\n|Depth of coverage                     |Depends on the sequencing coverage of samples.      |bcftools stats |Dramatic deviation from expected distribution can indicate artifactual bias.                                   |\r\n|Substitution type counts              |See TiTv ratio.                                     |bcftools stats |Twice as many possible transversions as transitions. See [here](https://dx.doi.org/10.1093%2Fbioinformatics%2Fbtu668)  |\r\n|TiTv ratio (genome wide)              |For mammals: WGS: 2.0-2.1, WES: 3.0-3.3             |bcftools stats |Dramatic deviation from expected ratio can indicate artifactual bias. Typically elevated in coding regions where transversions are more likely to occur. |\r\n|Base quality distribution             |Dataset dependent.                                  |bcftools stats |This will reflect the quality based filtering you performed. Dramatic deviation from expected ratio can indicate artifactual bias.|\r\n|Indel ratio                           |Common: ~1.0, Rare: 0.2-0.5                         |GATK CollectVariantCallingMetrics|This should be evaluated after custom filtering variants for your needs. Dramatic deviation from expected ratio can indicate artifactual bias.|\r\n|Het/hom(non-ref)                      |~2.0 assuming Hardy-Weinberg equilibrium.           |GATK CollectVariantCallingMetrics|Ancestry dependent, can vary dramatically. See [Wang et al. 2015](https://dx.doi.org/10.1093%2Fbioinformatics%2Fbtu668)|\r\n|Mendelian error                       |0                                                   |BCFtools +mendelian|Mendelian inheritance errors are likely erroneous genotype calls. See [Pilipenko et al. 2014](https://dx.doi.org/10.1186%2F1753-6561-8-S1-S21)|\r\n|True positives                        |Dataset dependent.                                  |Hap.py         |Number of query variants that are present in the truth set.                                                    |\r\n|False negatives                       |Dataset dependent.                                  |Hap.py         |Number of variants in truth set, not present in query VCF.                                                     |\r\n|False positives                       |Dataset dependent.                                  |Hap.py         |Number of variants in query VCF, not present in truth set.                                                     |\r\n|Recall                                |1                                                   |Hap.py         |Absence of false negatives. See [Krusche et al. 2019](https://doi.org/10.1038/s41587-019-0054-x)               |\r\n|Precision                             |1                                                   |Hap.py         |Absence of false positives. See [Krusche et al. 2019](https://doi.org/10.1038/s41587-019-0054-x)               |\r\n|F1-score                              |1                                                   |Hap.py         |Harmonic mean of recall and precision. See [Krusche et al. 2019](https://doi.org/10.1038/s41587-019-0054-x)    |\r\n|Genotype errors (FP.GT)               |Dataset dependent.                                  |Hap.py         |Number of query variants with incorrect genotype                                                               |\r\n\r\n### Resources and references \r\n\r\nEberle, M. A., Fritzilas, E., Krusche, P., K\u00e4llberg, M., Moore, B. L., Bekritsky, M. A., Iqbal, Z., Chuang, H. Y., Humphray, S. J., Halpern, A. L., Kruglyak, S., Margulies, E. H., McVean, G., & Bentley, D. R. (2017). A reference data set of 5.4 million phased human variants validated by genetic inheritance from sequencing a three-generation 17-member pedigree. Genome research, 27(1), 157\u2013164. https://doi.org/10.1101/gr.210500.116   \r\n\r\nKoboldt, D.C. Best practises for variant calling in clinical sequencing. Genome Med 12, 91 (2020). https://doi.org/10.1186/s13073-020-00791-w  \r\n\r\nKrusche, P., Trigg, L., Boutros, P.C. et al. Best practices for benchmarking germline small-variant calls in human genomes. Nat Biotechnol 37, 555\u2013560 (2019). https://doi.org/10.1038/s41587-019-0054-x  \r\n\r\nMarshall, C.R., Chowdhury, S., Taft, R.J. et al. Best practices for the analytical validation of clinical whole-genome sequencing intended for the diagnosis of germline disease. npj Genom. Med. 5, 47 (2020). https://doi.org/10.1038/s41525-020-00154-9   \r\n\r\nPilipenko, V.V., He, H., Kurowski, B.G. et al. Using Mendelian inheritance errors as quality control criteria in whole genome sequencing data set. BMC Proc 8, S21 (2014). https://doi.org/10.1186/1753-6561-8-S1-S21   \r\n\r\nWang, J., Raskin, J., Samuels, D., Shyr, Y., Guo, Y., Genome measures used for quality control are dependent on gene function and ancestry, Bioinformatics 31, 318\u2013323 (2015)  https://doi.org/10.1093/bioinformatics/btu668  \r\n\r\n\r\n## Help/FAQ/Troubleshooting\r\n\r\nIf Hap.py throws an error, search the [issues at Hap.py GitHub repository](https://github.com/Illumina/hap.py/issues) and attempt to resolve it before submitting an issue here.    \r\n\r\n## Acknowledgements/citations/credits  \r\n\r\n### Authors \r\n- Georgie Samaha (Sydney Informatics Hub, University of Sydney)   \r\n- Tracy Chew (Sydney Informatics Hub, University of Sydney)  \r\n- Cali Willet (Sydney Informatics Hub, University of Sydney)  \r\n- Nandan Deshpande (Sydney Informatics Hub, University of Sydney)\r\n\r\nAcknowledgements (and co-authorship, where appropriate) are an important way for us to demonstrate the value we bring to your research. Your research outcomes are vital for ongoing funding of the Sydney Informatics Hub and national compute facilities. We suggest including the following acknowledgement in any publications that follow from this work:  \r\n\r\nThe authors acknowledge the technical assistance provided by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney and the Australian BioCommons which is enabled by NCRIS via Bioplatforms Australia.  \r\n",
        "doi": "10.48546/workflowhub.workflow.339.1",
        "edam_operation": [
            "Indel detection",
            "SNP detection",
            "Validation"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Genetic variation",
            "Whole genome sequencing"
        ],
        "filtered_on": "metap* in description",
        "id": "339",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/339?version=1",
        "name": "GermlineShortV_biovalidation",
        "number_of_steps": 0,
        "projects": [
            "Sydney Informatics Hub"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-05-03",
        "creators": [
            "Marlene Rezk"
        ],
        "description": "Objective. Biomarkers have become important for the prognosis and diagnosis of various diseases. High-throughput methods such as RNA-sequencing facilitate the detection of differentially expressed genes (DEGs), hence potential biomarker candidates. Individual studies suggest long lists of DEGs, hampering the identification of clinically relevant ones. Concerning preeclampsia, a major obstetric burden with high risk for adverse maternal and/or neonatal outcomes, limitations in diagnosis and prediction are still important issues. Therefore, we developed a workflow to facilitate the screening for biomarkers.\r\nMethods. Based on the tool DeSeq2, we established a comprehensive workflow for the identification of  DEGs, analyzing data from multiple publicly available RNA-sSequencing studies. We applied it to four RNA-sSequencing datasets (one blood, three placenta) analyzing patients with preeclampsia and normotensive controls. We compared our results with other published approaches and evaluated their performance. \r\nResults. We identified 110 genes dysregulated in preeclampsia, observed in \u22653 of the analyzed studies, six even in all four studies. Among them were FLT-1, TREM-1, and FN1 which either represent established biomarkers on protein level, or promising candidates based on recent studies. In comparison, using a published meta-analysis approach we obtained 5,240  DEGs.\r\nConclusions. We present a data analysis workflow for preeclampsia biomarker screening, capable of identifying significant biomarker candidates, while drastically decreasing the numbers of candidates. Moreover, we were also able to confirm its performance for heart failure. Our approach can be applied to additional diseases for biomarker identification and the set of identified DEGs in preeclampsia represents a resource for further studies.\r\n",
        "doi": "10.48546/workflowhub.workflow.338.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "338",
        "keep": true,
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/338?version=1",
        "name": "Biomarker screening in preeclampsia",
        "number_of_steps": 7,
        "projects": [
            "Gyn Department"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "fastq_dump",
            "fastqc",
            "featurecounts",
            "bowtie2",
            "deseq2"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2022-04-20",
        "creators": [],
        "description": "# ESCALIBUR\r\n\r\nEscalibur Population Genomic Analysis Pipeline is able to explore key aspects centering the population genetics of organisms, and automates three key bioinformatic components in population genomic analysis using Workflow Definition Language (WDL: https://openwdl.org/), and customised R, Perl, Python and Unix shell scripts. Associated programs are packaged into a platform independent singularity image, for which the definition file is provided.\r\n\r\nThe workflow for analysis using Escalibur consists of three steps - each step can be run in a separate workflow in a sequential manner; step 2 is optional.\r\n\r\n    1. Trimming and mapping the raw data - selection of the best reference genome;\r\n    2. Removing the contamination from mapped data;\r\n    3. Recalibration, variant calling and filtering;\r\n\r\nThis implementation runs both locally and in a distributed environment that uses SLURM job scheduler.\r\n\r\n## Dependencies\r\nFollowing software dependencies are required:\r\n\r\n* Git\r\n* SLURM scheduler required for distributed HPC environment (https://slurm.schedmd.com/documentation.html)\r\n* Python3.7: (https://www.python.org/)\r\n* Perl 5.26.2: (https://www.perl.org/)\r\n* Java 1.8\r\n* Singularity 3.7.3: (https://sylabs.io/singularity/)\r\n\r\n## Step 1: Installation\r\n\r\nTypically, the installation of Singularity requires root rights. You should therefore contact your administrator to get it correctly installed. Minimum Linux kernel version requirement is 3.8, thought >= 3.18 would be preferred (https://sylabs.io/guides/3.5/admin-guide/installation.html).\r\n\r\nClone the git repository to a directory on your cluster or stand-alone server.\r\n```\r\n> git clone --depth 1 -b v0.3-beta https://gitlab.unimelb.edu.au/bioscience/escalibur.git\r\n> cd escalibur\r\n```\r\n\r\n### Description of Files\r\n* `workflow-main.local.config`: main configuration file for stand alone server runtime environment\r\n* `workflow-main.slurm.config`: main configuration file for HPC runtime environment that support Slurm job scheduler\r\n* `workflow-mapping.json`: defines location of input files, has behavioral settings and sets resource allocations\r\n* `workflow-cleaning.json`:  defines location of input files and sets resource allocations\r\n* `workflow-variants.json`:  defines location of input files, has behavioral settings and sets resource allocations\r\n* `workflow-mapping.wdl`: main workflow file to trim and map PE reads into the genome\r\n* `workflow-cleaning.wdl`: main workflow file to clean contamination from mapped PE reads against genomes representing putative contamination\r\n* `workflow-variants.wdl`: main workflow file to call variants using mapped and cleaned reads\r\n* `workflow-mapping.outputs.json`: defines location for resultant outputs and logs from mapping workflow\r\n* `workflow-cleaning.outputs.json`: defines location for resultant outputs and logs from cleaning workflow\r\n* `workflow-variants.outputs.json`: defines location for resultant outputs and logs from variants workflow\r\n* `inputReads.txt`: example input file for fastq read files to mapping step\r\n* `cleanup.conf`: example configuration file for putative host contamination to cleaning step\r\n* `inputBams.txt`: example input file for resultant BAM files to variant calling step\r\n* `references.txt`: contains list of example references genomes\r\n* `perl_scripts`: contains Perl scripts used by the pipeline\r\n* `scripts`: contains Python scripts used by the pipeline\r\n* `R_scripts`: contains R scripts used by the pipeline\r\n* `sub_workflows`: sub-workflows, one for each of the workflow steps\r\n* `tasks`: workflow tasks\r\n* `cromwell-50.jar`: java archive file required to run the workflow.\r\n\r\nTwo config files have been created. One for stand alone server (`workflow-runtime.local.config`) and another one for HPC environment that supports Slurm scheduler (`workflow-runtime.slurm.config`).\r\nThese files have already been optimised. For slurm configuration you only need to define the HPC partition in line 35: \"String rt_queue\"\r\nChange this to the partition you have access to on HPC environment.\r\n\r\nFiles `workflow-mapping.outputs.json`, `workflow-cleaning.outputs.json` and `workflow-variants.outputs.json` define the directories to copy the result files to. Modify if you want to change default output directories `outputMapping`, `outputCleaning` and `outputVariants`. These output directories are generated to the directory `escalibur`.\r\n#### NOTE: delete output directories from previous runs. If you have files there already and a name matches during the copy, the workflow may fail.\r\n\r\n`Singularity` directory contains the definition file for the software used in Escalibur. Pre-built singularity image can be downloaded from `library://pakorhon/workflows/escalibur:0.0.1-beta`.\r\n```\r\n> singularity pull escalibur.sif library://pakorhon/workflows/escalibur:0.0.1-beta\r\n```\r\n\r\n## Step 2: Test run\r\n\r\nTo confirm correct function of the workflows (`mapping`, `cleaning` and `variant calling`), fix the required absolute paths, marked by three dots `...` in `workflow-mapping.json`, `workflow-cleaning.json` and `workflow-variants.json` and configuration files `cleanup.conf` and `inputBams.txt`, and run the workflow with the provided test and configuration files, and parameter settings.\r\n```\r\n> java -Dconfig.file=./workflow-runtime.local.config  -jar ./cromwell-50.jar run workflow-mapping.wdl -i workflow-mapping.json -o workflow-mapping.outputs.json > out.mapping 2> err.mapping\r\n> java -Dconfig.file=./workflow-runtime.local.config  -jar ./cromwell-50.jar run workflow-cleaning.wdl -i workflow-cleaning.json -o workflow-cleaning.outputs.json > out.cleaning 2> err.cleaning\r\n> java -Dconfig.file=./workflow-runtime.local.config  -jar ./cromwell-50.jar run workflow-variants.wdl -i workflow-variants.json -o workflow-variants.outputs.json > out.variants 2> err.variants\r\n```\r\nSlurm file templates `runMapping.slurm`, `runCleaning.slurm` and `runVariants.slurm` are available for each workflow.\r\n#### NOTE: default parameter settings for run-times, memory usage and module loading may require adjustment in these files if run in HPC environment using slurm. Current settings should account for the test run.\r\n\r\nAfter the runs are complete, the results will be at the output directories: `outputMapping`, `outputCleaning` and `outputVariants`.\r\nYou can compare the result of `outputVariants/full_genotype_output.vcf` to that or pre-run `TestResults/full_genotype_output.vcf`.\r\n\r\n## Step 3: Mapping\r\n\r\nMake a directory for your fastq files e.g. `Reads` and copy your paired end raw data in there.\r\n```\r\n> mkdir Reads\r\n```\r\n\r\nIt should look something like below\r\n```\r\n> ls TestReads/\r\n1-1_r1.fastq.gz  32-1_r1.fastq.gz  44-1_r1.fastq.gz\r\n1-1_r2.fastq.gz  32-1_r2.fastq.gz  44-1_r2.fastq.gz\r\n```\r\nRun the python script to create a file of your input samples and edit the resulting file to match your sample identifiers and libraries.\r\n```\r\n> python3 scripts/inputArgMaker.py -d Reads/ -p -ps 33 -pq 20 -pl ILLUMINA -ml 50 -o inputReads.txt \r\n```\r\n\r\nThe edited output file is shown below. The script will automatically sort the files by size.\r\n```\r\n> cat inputReads.txt\r\n# Prefix PE/SE\tMinLen\tPhredS\tSequencer\tPhredQ\tLibrary\tRead Group ID\tSample\tPlatform Unit\tFirst pair of PE reads\t\tSecond pair of PE reads\r\ntest1\t PE\t50\t33\tILLUMINA\t28\tLIB1\tCL100082180L1\tSM1\tCL100082180L1\t./TestReads/1-1_r1.fastq.gz\t./TestReads/1-1_r2.fastq.gz\r\ntest2\t PE\t50\t33\tILLUMINA\t20\tLIB2\tCL100082180L1\tSM2\tCL100082180L1\t./TestReads/44-1_r1.fastq.gz\t./TestReads/44-1_r2.fastq.gz\r\ntest3\t PE\t50\t33\tILLUMINA\t20\tLIB3\tCL100034574L1\tSM2\tCL100034574L1\t./TestReads/32-1_r1.fastq.gz\t./TestReads/32-1_r2.fastq.gz\r\n```\r\n#### NOTE: If several libraries are embedded in a single read file, library-specific reads have to be separated into own files before create the inputReads.txt file. In contrast, inputReads.txt file format can accommodate multiple library files to a single sample.\r\n\r\n* `Prefix`: Prefix for the resultant files from trimming.\r\n* `PE/SE`: Paired-End/Single-End reads as input.\r\n* `MinLen`: Minimum Length of reads after trimming.\r\n* `PhredS`: Used Phred coding by the sequencer (33 or 64).\r\n* `Sequencer`: Name of the sequencer.\r\n* `PhredQ`: Phred cut-off score used in trimming.\r\n* `Library`: Identifier for the library.\r\n* `Read Group ID`: Identifier for the read groups required by GATK (inputArgMaker tries to find this from FASTQ reads). Refer to (https://gatk.broadinstitute.org/hc/en-us/articles/360035890671-Read-groups).\r\n* `Sample`: Identifier for the sample. Defined prefix for resultant sample specific files.\r\n* `Platform Unit (optional)`: Information about flow cell, lane and sample. Helps GATK in recalibration (inputArgMaker copies Read Group ID here). Refer to (https://gatk.broadinstitute.org/hc/en-us/articles/360035890671-Read-groups).\r\n* `First pair of PE reads`: Relative path to the forward pair of PE reads.\r\n* `Second pair of PE reads`: Relative path to the reverse pair of PE reads.\r\n\r\nCreate a file listing reference genomes and configure `workflow-mapping.json` file.\r\nAn example reference file (`references.txt`) has been created for you. Use this as an example to create your own.\r\nEnsure there are no whitespaces at the end of the line or else the cromwell engine will throw an error.\r\nReads are mapped to these reference files and the best matching reference will be selected for variant calling.\r\n```\r\n> cat references.txt\r\nscf00001\t./TestReferences/scf00001.fa\r\nscf00013\t./TestReferences/scf00013.fa\r\n```\r\n#### NOTE: Reference label (e.g. `scf00001`) must be a substring found in the reference fasta file (`scf00001.fa`)\r\n\r\nThe figure below illustrates the flow of the information, and appearance of labels (`Prefix`, `Sample`, `Label`) in file names, as defined in `inputReads.txt` and `references.txt`.\r\n![](figures/labelFlow.png)\r\n\r\n### workflow-mapping.json config file\r\nAdd the path of your fastq and reference genome input files and change parameters as appropriate, and adjust the absolute paths for singularity image. If `mapping_workflow.readQc` is set to `yes`, reads are trimmed both for quality and the adapters. Adapters to trim are given in `mapping_workflow.pe_filtering_workflow.trimmomatic_pe_task.truseq_pe_adapter`. If you want to use custom adapters, copy them to `adapters` directory and instead of default `TruSeq3-PE.fa`, refer to your custom file. If you don't want to use adapters, use `empty.fa` file instead. For BGISEQ adapters, refer to (https://en.mgitech.cn/Download/download_file/id/71).\r\n```\r\n{\r\n  \"## CONFIG FILE\": \"WDL\",\r\n  \"mapping_workflow.inputSampleFile\": \"./inputReads.txt\",\r\n  \"mapping_workflow.inputReferenceFile\": \"./references.txt\",\r\n\r\n  \"## Parameters for samtools read filtering\": \"-F 4 does filters unmapped reads from resultant files\",\r\n  \"mapping_workflow.samtoolsParameters\": \"-F 4\",\r\n  \r\n  \"## Is read QC required\": \"yes or no\",\r\n  \"mapping_workflow.readQc\": \"yes\",\r\n  \"## What is the ploidy of given genome\": \"1 for haploid, 2 for diploid, etc.\",\r\n  \"mapping_workflow.ploidy\": 2,\r\n  \r\n  \"## Singularity parameters\": \"absolute paths to the container and the directory to bind visible inside singularity\",\r\n  \"mapping_workflow.singularityContainerPath\": \"/home/.../escalibur/escalibur.sif\",\r\n  \"mapping_workflow.singularityBindPath\": \"/home/.../escalibur/\",\r\n\r\n  \"## trimmomatic adapters\": \"\",\r\n  \"mapping_workflow.pe_filtering_workflow.trimmomatic_pe_task.truseq_pe_adapter\":\"./adapters/TruSeq3-PE.fa\",\r\n  \"mapping_workflow.pe_filtering_workflow.trimmomatic_se_task.truseq_se_adapter\":\"./adapters/TruSeq3-SE.fa\",\r\n  \r\n  \"## Indexing sub workflow task parameters\": \"Samtools index run time parameters\",\r\n  \"mapping_workflow.index_sub_workflow.indexing_sam_task.IST_minutes\": 300,\r\n  \"mapping_workflow.index_sub_workflow.indexing_sam_task.IST_threads\": 16,\r\n  \"mapping_workflow.index_sub_workflow.indexing_sam_task.IST_mem\": 30000,\r\n  .\r\n  .\r\n  .\r\n}\r\n```\r\n\r\nRun the mapping workflow.\r\n```\r\n> java -Dconfig.file=./workflow-runtime.local.config  -jar ./cromwell-50.jar run workflow-mapping.wdl -i workflow-mapping.json -o workflow-mapping.outputs.json > out.mapping 2> err.mapping\r\n```\r\nThe resultant BAM files will be copied to `outputMapping` directory.\r\n\r\n## Step 4 (optional): Cleaning\r\n\r\nIf you suspect 'host' contamination in your data, you can remove that using the cleaning workflow.\r\nDefine the file representing the contamination. First column defines the sample identifier, second the resultant BAM file from mapping workflow and third the putative contaminant genome assembly.\r\n```\r\n> cat cleanup.conf\r\nSM1\t/home/.../escalibur/outputMapping/SM1.scf00001.MarkDup.bam\t/home/.../escalibur/Hosts/host1.fa\r\nSM2\t/home/.../escalibur/outputMapping/SM2.scf00001.MarkDup.bam\t/home/.../escalibur/Hosts/host1.fa\r\n```\r\n#### NOTE: you have to use absolute paths both to BAM files and the contaminant reference genome (here `host1.fa` and `host2.fa`).\r\n\r\n### workflow-cleaning.json config file\r\nAdd the path of your cleaning config file (here `cleanup.conf`) and adjust the absolute paths for singularity image.\r\n```\r\n{\r\n  \"## CONFIG FILE\": \"WDL\",\r\n  \"cleaning_workflow.inputContaminantFile\": \"./cleanup.conf\",\r\n  \r\n  \"## Singularity parameters\": \"absolute paths to the container and the directory to bind visible inside singularity\",\r\n  \"cleaning_workflow.singularityContainerPath\": \"/home/.../escalibur/escalibur.sif\",\r\n  \"cleaning_workflow.singularityBindPath\": \"/home/.../escalibur/\",\r\n\r\n  \"cleaning_workflow.indexing_bwa_task.IBT_minutes\": 60,\r\n  \"cleaning_workflow.indexing_bwa_task.IBT_threads\": 1,\r\n  \"cleaning_workflow.indexing_bwa_task.IBT_mem\": 16000,\r\n\r\n  \"######################################\":\"########################################\",\r\n  \"CLEANING\":\"PARAMETERS\",\r\n  \"######################################\":\"########################################\",\r\n  \"cleaning_workflow.clean_bams_workflow.cleanBams_task.CLEAN_BAMS_minutes\": 600,\r\n  \"cleaning_workflow.clean_bams_workflow.cleanBams_task.CLEAN_BAMS_threads\": 4,\r\n  \"cleaning_workflow.clean_bams_workflow.cleanBams_task.CLEAN_BAMS_mem\": 32000,\r\n\r\n  \"cleaning_workflow.create_cleaned_bams_workflow.createCleanedBams_task.CREATE_CLEAN_BAMS_minutes\": 300,\r\n  \"cleaning_workflow.create_cleaned_bams_workflow.createCleanedBams_task.CREATE_CLEAN_BAMS_threads\": 4,\r\n  \"cleaning_workflow.create_cleaned_bams_workflow.createCleanedBams_task.CREATE_CLEAN_BAMS_mem\": 32000,\r\n\r\n  \"cleaning_workflow.refsBySample.RBS_minutes\": 5,\r\n  \"cleaning_workflow.refsBySample.RBS_threads\": 1,\r\n  \"cleaning_workflow.refsBySample.RBS_mem\": 4000\r\n}\r\n```\r\n\r\nRun the cleaning workflow.\r\n```\r\n> java -Dconfig.file=./workflow-runtime.local.config  -jar ./cromwell-50.jar run workflow-cleaning.wdl -i workflow-cleaning.json -o workflow-cleaning.outputs.json > out.cleaning 2> err.cleaning\r\n```\r\nThe resultant cleaned BAM files will be copied to `outputCleaning` directory. You can repeat the workflow if you suspect that there may be more than one contaminant genomes per each sample. In that case you have to take care of the properly configured `cleanup.conf` file that should describe the BAM files from previous cleaning round but also define new output directory for each round in `workflow-cleaning.outputs.json` file.\r\n\r\n## Step 5: Variant calling\r\n\r\nDefine the file listing the BAM files used for variant calling. First column defines the sample identifier, and second the resultant BAM file either from mapping of cleaning workflow.\r\n```\r\n> cat inputBams.txt\r\nSM1\t/home/.../escalibur/outputMapping/SM1.scf00001.MarkDup.bam\r\nSM2\t/home/.../escalibur/outputCleaned/SM2.scf00001.MarkDup.cleaned.bam\r\n```\r\n\r\n### workflow-variants.json config file\r\nAdd the path of your file listing the locations of BAM files (here `inputBams.txt`), and add the location to selected reference genome (found in `outputMapping/best.ref`) and it's label, as defined in `references.txt` file. Adjust the absolute paths for singularity image and adjust other parameters, especially define if you want to recalibrate the BAM files by selecting value \"independent\" to \"variants_workflow.call_type\".\r\n```\r\n{\r\n  \"## CONFIG FILE\": \"WDL\",\r\n  \"variants_workflow.inputSampleFile\": \"./inputBams.txt\",\r\n  \"variants_workflow.selectedRefFile\": \"TestReferences/scf00001.fa\",\r\n  \"variants_workflow.selectedRefLabel\": \"scf00001\",\r\n  \r\n  \"## Singularity parameters\": \"absolute paths to the container and the directory to bind visible inside singularity\",\r\n  \"variants_workflow.singularityContainerPath\": \"/home/.../escalibur/escalibur.sif\",\r\n  \"variants_workflow.singularityBindPath\": \"/home/.../escalibur/\",\r\n\r\n  \"## Which variant call workflow to use\": \"fast or independent\",\r\n  \"variants_workflow.call_type\": \"fast\",\r\n  \r\n  \"## Variant filtering expressions\": \"For SNPs and INDELs\",\r\n  \"variants_workflow.SNP_filt_exp\": \"QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0\",\r\n  \"variants_workflow.INDEL_filt_exp\": \"QD < 2.0 || FS > 200.0 || ReadPosRankSum < -20.0\",\r\n\r\n  \"## Variant Filter params\": \"Variant filter, indel, snps, report making: Safe to leave as default\",\r\n  \"variants_workflow.ploidy\": 2,\r\n  \"variants_workflow.maxIndelSize\": 60,\r\n  \"variants_workflow.scafNumLim\": 95,\r\n  \"variants_workflow.scafNumCo\": 2,\r\n  \"variants_workflow.scafLenCutOff\": 0,\r\n  \"variants_workflow.ldWinSize\": 10,\r\n  \"variants_workflow.ldWinStep\": 5,\r\n  \"variants_workflow.ldCutOff\": 0.3,\r\n  \"variants_workflow.snp_indel_var_filtering_workflow.indelFilterName\": \"Indel_filter\",\r\n  \"variants_workflow.snp_indel_var_filtering_workflow.indelFilterExpression\": \"QD < 2.0 || FS > 200.0 || ReadPosRankSum < -20.0\",\r\n  \"variants_workflow.snp_indel_var_filtering_workflow.snpFilterName\": \"Snp_filter\",\r\n  \"variants_workflow.snp_indel_var_filtering_workflow.snpFilterExpression\": \"QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0\",\r\n  \"variants_workflow.snp_indel_var_filtering_workflow.vfindel_tk.selectType\": \"\",\r\n  \"variants_workflow.snp_indel_var_filtering_workflow.vfsnp_tk.selectType\": \"\",\r\n\r\n  \"## Build chromosome map\":\"map_def_scf_lim_task\",\r\n  \"variants_workflow.snp_indel_var_filtering_workflow.map_def_scf_lim_task.scafLenCutOff\": 1000000,\r\n  \"variants_workflow.snp_indel_var_filtering_workflow.map_def_scf_lim_task.scafNumCo\": 3,\r\n\r\n  \"## Indexing sub workflow task parameters\": \"Samtools index run time parameters\",\r\n  \"variants_workflow.ref_index.IST_minutes\": 300,\r\n  \"variants_workflow.ref_index.IST_threads\": 2,\r\n  \"variants_workflow.ref_index.IST_mem\": 8000,\r\n  .\r\n  .\r\n  .\r\n}\r\n```\r\n\r\nRun the variant calling workflow.\r\n```\r\n> java -Dconfig.file=./workflow-runtime.local.config  -jar ./cromwell-50.jar run workflow-variants.wdl -i workflow-variants.json -o workflow-variants.outputs.json > out.variants 2> err.variants\r\n```\r\nThe resultant files will be copied to `outputVariants` directory. That includes filtered variants calls (`full_genotype_output.vcf`) and recalibrated BAM files (if independent call_type is selected).\r\n\r\n## Other considerations\r\n\r\n### Resource allocation in HPC environment\r\nWall time, memory usage and thread count (`_minutes`, `_mem`, `_threads`) given in `.json` files for each workflow can vary substantially and may require adjusting in HPC environment and slurm. This may lead to frequent restarting of the workflow after each adjustment. We have automated this task by providing scripts that automatically check the failed resource allocations and double them for each round. These scripts are located in `Automation` directory and can be run as follows:\r\n```\r\n> cd Automation\r\n> sh init.sh # Copies the content of ../tasks directory to tasksOrig directory\r\n> sbatch runMapping.slurm # Runs runLoopMapping.sh in a worker node\r\n> sbatch runCleaning.slurm # Runs runLoopCleaning.sh in a worker node\r\n> sbatch runVariants.slurm # Runs runLoopVariants.sh in a worker node\r\n```\r\nScripts `runLoop*.sh` copy resource allocations from collective `runtimes.json` file to the files in `../tasks` directory, run the workflow and double the failed resource allocations in `../tasks` files, and reruns the workflow until it succeeds or until ten rounds have passed. Copying of resource allocations directly to the files in `../tasks` directory is necessary to guarantee proper function of call-caching.\r\n#### NOTE: automated resource allocation adjustment is experimental, should be monitored when running and may require modifications to scripts to function properly.\r\n\r\n### Disk usage\r\nCromwell will create duplicate copies of files while running the workflows. It is therefore recommended to remove `cromwell-executions` directory after each workflow is run, if disk space is getting sparse.\r\n```\r\n> rm -r cromwell-executions\r\n```\r\nEspecially, if there are hundreds of samples that may sum up to terabytes of data, disk space might become an issue if unused files are not removed.\r\n\r\n### Troubleshooting\r\nIf the output text does not reveal the error, you can try to find an error message using command(s):\r\n```\r\n> find cromwell-executions/ -name stderr -exec cat {} \\; | grep -i fatal\r\n> find cromwell-executions/ -name stderr -exec cat {} \\; | less\r\n```\r\n\r\nMost commonly encountered error cases:\r\n\r\n* Singularity is not running correctly. Typically you require help from your administrator to get singularity properly installed.\r\n* Singularity image `escalibur.sif` was not downloaded\r\n* Check that you are using correct runtime configuration file `workflow-runtime.local.config` or `workflow-runtime.slurm.config` when calling `cromwell-50.jar`\r\n* Absolute file paths for Singularity/Trimmomatic, input files or contaminant genomes are not updated or are wrong in `workflow-*.json`, `inputBams.txt` or `cleanup.conf` configuration files, respectively.\r\n* Defined run-time and memory requirements for some tasks are not sufficient in `.json` configuration files to run the pipeline in HPC environment.\r\n* If you are using slurm job scheduler and want to run the pipeline in HPC environment, you have to create the related configuration file yourselves.\r\n* Pipeline has not been tested in other environments but Linux and we expect that users encounter challenges if trying to run the pipeline e.g. in Mac environment.\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "335",
        "keep": true,
        "latest_version": 1,
        "license": "BSD-3-Clause",
        "link": "https:/workflowhub.eu/workflows/335?version=1",
        "name": "Escalibur",
        "number_of_steps": 0,
        "projects": [
            "Workflows Australia"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Workflow Description Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-04-20",
        "creators": [
            "Pasi Korhonen"
        ],
        "description": "## CWL based workflow to assemble haploid/diploid eukaryote genomes of non-model organisms\r\nThe workflow is designed to use both PacBio long-reads and Illumina short-reads. The workflow first extracts, corrects, trims and decontaminates the long reads. Decontaminated trimmed reads are then used to assemble the genome and raw reads are used to polish it. Next, Illumina reads are cleaned and used to further polish the resultant assembly. Finally, the polished assembly is masked using inferred repeats and haplotypes are eliminated. The workflow uses BioConda and DockerHub to install required software and is therefore fully automated. In addition to final assembly, the workflow produces intermediate assemblies before and after polishing steps. The workflow follows the syntax for CWL v1.0.\r\n\r\n### Dependencies\r\n# Programs\r\nThe pipeline can be run either using [Cromwell](https://cromwell.readthedocs.io/en/stable) or [cwltool reference](https://github.com/common-workflow-language/cwltool) implementation and docker containers can be run either using [Singularity](https://singularity.lbl.gov) or [udocker](https://singularity.lbl.gov).\r\n\r\nCromwell implementation\r\n* [cromwell v44](https://github.com/broadinstitute/cromwell/releases/tag/44)\r\n* [java-jdk v8.0.112](https://www.java.com/en)\r\n\r\nReference implementation\r\n* [cwltool v1.0.20181012180214](https://github.com/common-workflow-language/cwltool)\r\n* [nodejs v10.4.1 required by cwltool](https://nodejs.org/en)\r\n* [Python library galaxy-lib v18.5.7](https://pypi.org/project/galaxy-lib)\r\n\r\nSingularity software packages have to be installed server-wide by administrator\r\n* [Singularity v3.2.1](https://singularity.lbl.gov)\r\n* [squashfs-tools v4.3.0](https://github.com/plougher/squashfs-tools)\r\n\r\nUdocker software package can be installed locally\r\n* [udocker v1.1.2](https://github.com/indigo-dc/udocker)\r\n\r\n# Data\r\n* [Illumina adapters converted to FASTA format](http://sapac.support.illumina.com/downloads/illumina-adapter-sequences-document-1000000002694.html)\r\n* [NCBI nucleotide non-redundant sequences for decontamination with Centrifuge](http://www.ccb.jhu.edu/software/centrifuge)\r\n* [RepBase v17.02 file RMRBSeqs.embl](https://www.girinst.org/repbase)\r\n\r\n### Installation\r\nInstall miniconda using installation script ```installConda.sh```.\r\nTo install CWL, use either installation script ```installCromwell.sh``` or ```installCwltool.sh```.\r\nTo install udocker, use installation script ```installUdocker.sh```.\r\nTo install singularity, ask your system administrator.\r\n\r\n```\r\n# First confirm that you have the program 'git' installed in your system\r\n> cd\r\n> git clone -b 'v0.1.3-beta' --single-branch --depth 1 https://github.com/vetscience/Assemblosis\r\n> cd Assemblosis\r\n> bash installConda.sh\r\n> bash installCromwell.sh # or bash installCwltool.sh\r\n> bash installUdocker.sh # if singularity cannot be installed or does not run\r\n\r\n```\r\nFor data dependencies: download and extract [RepBase database](https://www.girinst.org/repbase), download Centrifuge version of [NCBI nt database](http://www.ccb.jhu.edu/software/centrifuge) and create [Illumina adapter FASTA file](http://sapac.support.illumina.com/downloads/illumina-adapter-sequences-document-1000000002694.html) to your preferred locations. If your reads are clean from adapters, the adapter FASTA file can be empty.\r\nGive the location of these data in the configuration (.yml) file (see **Usage**).\r\n\r\n### Usage\r\nYou have to create a YAML (.yml) file for each assembly. This file defines the required parameters and the location for both PacBio and Illumina raw-reads.\r\n```\r\n> cd\r\n> export PATH=~/miniconda3/bin:$PATH\r\n> cd Assemblosis/Run\r\n> cp ../Examples/assemblyCele.yml .\r\n\r\n\"Edit assemblyCele.yml to fit your computing environment and to define the location for the read files, databases and Illumina adapters\"\r\n\r\n\"Running docker images using Cromwell and singularity:\"\r\n> java -Dconfig.file=cromwell.udocker.conf -jar cromwell-44.jar run -t CWL -v v1.0 assembly.cwl -i assemblyCele.yml\r\n\r\n\"Running docker images using Cromwell and udocker:\"\r\n> java -Dconfig.file=cromwell.singularity.conf -jar cromwell-44.jar run -t CWL -v v1.0 assembly.cwl -i assemblyCele.yml\r\n\r\n\"Running docker images using Cwltool and singularity:\"\r\n> cwltool --tmpdir-prefix /home/<username>/Tmp --beta-conda-dependencies --cachedir /home/<username>/Cache --singularity --leave-tmpdir assembly.cwl assemblyCele.yml\r\n\r\n\"Running docker images using Cwltool and udocker:\"\r\n> cwltool --tmpdir-prefix /home/<username>/Tmp --beta-conda-dependencies --cachedir /home/<username>/Cache --user-space-docker-cmd udocker --leave-tmpdir assembly.cwl assemblyCele.yml\r\n```\r\n\r\nAn annotated example of the YAML file for Caenorhabditis elegans assembly.\r\n```\r\n## Directory, which contains the PacBio raw data\r\n# NOTE! The software looks for all .h5 file (or bam files if bacBioInBam below is defined true) in given directory\r\npacBioDataDir:\r\n  class: Directory\r\n  location: /home/<username>/Dna\r\n\r\n## PacBio files are in bam format as returned from Sequel platform\r\npacBioInBam: true\r\n\r\n## Prefix for the resultant assembly files\r\nprefix: cele\r\n\r\n## Maximum number of threads used in the pipeline\r\nthreads: 24\r\n\r\n## Minimum number of threads per job used in canu assembler\r\nminThreads: 4\r\n\r\n## Number of concurrent jobs in canu assembler (recommended to use threads / minThreads)\r\ncanuConcurrency: 6\r\n\r\n### Parameters for the program Canu are described in https://canu.readthedocs.io/en/latest/parameter-reference.html\r\n## Expected genome size. This parameter is forwarded to Canu assembler.\r\ngenomeSize: 100m\r\n\r\n## Minimum length for the PacBio reads used for the assembly. This parameter is forwarded to Canu assembler.\r\n# The maximum resolvable repeat regions becomes 2 x minReadLength\r\nminReadLen: 6000\r\n\r\n## Parameter for Canu assembler to adjust to GC-content. Should be 0.15 for high or low GC content.\r\ncorMaxEvidenceErate: 0.20\r\n\r\n### Parameters for the program Trimmomatic are described in http://www.usadellab.org/cms/?page=trimmomatic\r\n## Paired-end (PE) reads of Illumina raw data. These files are given to the program Trimmomatic.\r\n# NOTE! Data for two paired libraries is given below.\r\nreadsPe1:\r\n  - class: File\r\n    format: edam:format_1930  # fastq\r\n    path: /home/<username>/Dna/SRR2598966_1.fastq.gz\r\n  - class: File\r\n    format: edam:format_1930  # fastq\r\n    path: /home/<username>/Dna/SRR2598967_1.fastq.gz\r\nreadsPe2:\r\n  - class: File\r\n    format: edam:format_1930  # fastq\r\n    path: /home/<username>/Dna/SRR2598966_2.fastq.gz\r\n  - class: File\r\n    format: edam:format_1930  # fastq\r\n    path: /home/<username>/Dna/SRR2598967_2.fastq.gz\r\n\r\n## Phred coding of Illumina data. This parameter is forwarded to Trimmomatic.\r\n# NOTE! Each read-pair needs one phred value.\r\nphredsPe: ['33','33']\r\n\r\n## Sliding window and illuminaClip parameters for Trimmomatic\r\nslidingWindow:\r\n    windowSize: 4\r\n    requiredQuality: 25\r\nilluminaClip:\r\n    adapters:\r\n        class: File\r\n        path: <path to Illumina adapter file>\r\n    seedMismatches: 2\r\n    palindromeClipThreshold: 30\r\n    simpleClipThreshold: 10\r\n    minAdapterLength: 20\r\n    keepBothReads: true\r\n## Further parameters for Trimmomatic\r\n# Required phred-quality for leading 5 nucleotides\r\nleading: 25\r\n# Required phred-quality for trailing 5 nucleotides\r\ntrailing: 25\r\n# Minimum accepted read-length to keep the read after trimming\r\nminlen: 40\r\n\r\n### Parameters for the program bowtie2 are described in http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml\r\n## Illumina PE fragment length. Program bowtie2 parameter -X.\r\n# NOTE! Each read-pair needs one phred value.\r\nmaxFragmentLens: [500, 600]\r\n# Orientation of pair-end reads e.g. 'fr', 'rf', 'ff': Program bowtie2 parameters --fr, --rf or --ff\r\norientation: 'fr'\r\n\r\n### Parameters for the program Pilon are described in https://github.com/broadinstitute/pilon/wiki/Requirements-&-Usage\r\n# Prefix for the resultant pilon polished assembly. Pilon parameter --output\r\npolishedAssembly: celePilon\r\n# This is set 'true' for an organism with diploid genome: Pilon parameter --diploid\r\ndiploidOrganism: true\r\n# Value 'bases' fixes snps and indels: Pilon parameter --fix\r\nfix: bases\r\n\r\n### Parameters for the program centrifuge are described in http://www.ccb.jhu.edu/software/centrifuge/manual.shtml\r\n# Path to the directory, that contains NCBI nt database in nt.?.cf files. Centrifuge parameter -x\r\ndatabase:\r\n  class: Directory\r\n  path:  /home/<username>/ntDatabase\r\n# Lenght of the identical match in nucleotides required to infer a read as contaminant. Centrifuge parameter --min-hitlen\r\npartialMatch: 100\r\n# NCBI taxon root identifers for the species considered contaminants: e.g. bacteria (=2), viruses (=10239), fungi (=4751), mammals (=40674), artificial seqs (=81077). Pipeline specific parameter.\r\ntaxons: [2,10239,4751,40674,81077]\r\n\r\n## Parameters for the RepeatModeler and RepeatMasker are described in http://www.repeatmasker.org\r\nrepBaseLibrary:\r\n  class: File\r\n  # This is the RepBase file from https://www.girinst.org/repbase. RepeatMasker parameter -lib\r\n  path: /home/<username>/RepBaseLibrary/RMRBSeqs.embl\r\n# Constant true and false values for repeat masker\r\ntrueValue: true\r\nfalseValue: false\r\n\r\n```\r\n### Runtimes and hardware requirements\r\nThe workflow was tested in Linux environment (CentOS Linux release 7.2.1511) in a server with 24 physical CPUs (48 hyperthreaded CPUs) and 512 GB RAM.\r\n\r\n| Assembly | Runtime in CPU hours | RAM usage (GB) |\r\n| --- | --- | --- |\r\n| *Caenorhabditis elegans* | 1537 | 134.1 |\r\n| *Drosophila melanogaster* | 6501 | 134.1 |\r\n| *Plasmodium falciparum* | 424 | 134.1 |\r\n\r\nMaximum memory usage of 134.1 GB was claimed by the program Centrifuge for each assembly.\r\n\r\n### Software tools used in this pipeline\r\n* [Dextractor v1.0](https://github.com/thegenemyers/DEXTRACTOR)\r\n* [Trimmomatic v0.36](http://www.usadellab.org/cms/?page=trimmomatic)\r\n* [Centrifuge v1.0.3](http://www.ccb.jhu.edu/software/centrifuge)\r\n* [Canu v1.8](http://canu.readthedocs.io/en/latest/index.html)\r\n* [Arrow in SmrtLink v7.0.1](https://www.pacb.com/support/software-downloads)\r\n* [Bowtie 2 v2.2.8](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml)\r\n* [SAMtools v1.6](http://samtools.sourceforge.net)\r\n* [Pilon v1.22](https://github.com/broadinstitute/pilon)\r\n* [RepeatMasker v4.0.6](http://www.repeatmasker.org)\r\n* [RepeatModeler v1.0.11](http://www.repeatmasker.org)\r\n* [RepBase v17.02](https://www.girinst.org/repbase)\r\n* [HaploMerger2 build_20160512](https://github.com/mapleforest/HaploMerger2)\r\n\r\n### Cite\r\nIf you use the pipeline, please cite:\r\nKorhonen, Pasi K., Ross S. Hall, Neil D. Young, and Robin B. Gasser. \"Common Workflow Language (CWL)-based software pipeline for de novo genome assembly from long-and short-read data.\" GigaScience 8, no. 4 (2019): giz014.\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "334",
        "keep": true,
        "latest_version": 1,
        "license": "BSD-3-Clause",
        "link": "https:/workflowhub.eu/workflows/334?version=1",
        "name": "Assemblosis",
        "number_of_steps": 23,
        "projects": [
            "Workflows Australia"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-04-07",
        "creators": [
            "Cali Willet",
            "Rosemarie Sadsad",
            "Tracy Chew"
        ],
        "description": "# Shotgun Metagenomics Analysis\r\nAnalysis of metagenomic shotgun sequences including assembly, speciation, ARG discovery and more\r\n\r\n## Description\r\nThe input for this analysis is paired end next generation sequencing data from metagenomic samples. The workflow is designed to be modular, so that individual modules can be run depending on the nature of the metagenomics project at hand. More modules will be added as we develop them - this repo is a work in progress!\r\n\r\nThese scripts have been written specifically for NCI Gadi HPC, wich runs PBS Pro, however feel free to use and modify for anothre system if you are not a Gadi user. \r\n\r\n### Part 1. Setup and QC\r\nDownload the repo. You will see directories for `Fastq`, `Inputs`, `Reference` and `Logs`. You will need to copy or symlink your fastq to `Fastq`, sample configuration file (see below) to `Inputs` and the reference genome sequence of your host species (if applicable) to `Reference` for host contamination removal.\r\n \r\n\r\n#### Fastq inputs\r\nThe scripts assume all fastq files are paired, gzipped, and all in the one directory named 'Fastq'. If your fastq are within a convoluted directory structure (eg per-sample directories) or you would simply like to link them from an alternate location, please use the script `setup_fastq.sh`.\r\n\r\nTo use this script, parse the path name of your fastq as first argument on the command line, and run the script from the base working directory (<your_path>/Shotgun-Metagenomics-Analysis) which will from here on be referred to as `workdir`. Note that this script looks for `f*q.gz` files (ie fastq.gz or fq.gz) - if yours differ in suffix, please adjust the script accordingly.\r\n\r\n```\r\nbash ./Scripts/setup_fastq.sh </path/to/your/parent/fastq/directory>\r\n```\r\n\r\n#### Configuration/sample info\r\nThe only required input configuration file should be named <cohort>.config, where <cohort> is the name of the current batch of samples you are processing, or some other meaningful name to your project; it will be used to name output files. The config file should be placed inside the $workdir/Inputs directory, and include the following columns, in this order:\r\n\r\n```\r\n1. Sample ID - used to identify the sample, eg if you have 3 lanes of sequencing per sample, erach of those 6 fastq files should contain this ID that si in column 1\r\n2. Lab Sample ID - can be the same as column 1, or different if you have reason to change the IDs eg if the seq centre applies an in-house ID. Please make sure IDs are unique within column 1 and unique within column 2\r\n3. Group - eg different time points or treatment groups. If no specific group structure is relevant, please set this to 1 (do not leave blank!) \r\n3. Platform - should be Illumina; other sequencing platforms are not tested on this workflow\r\n4. Sequencing centre name\r\n5. Library - eg if you have 2 sequencing libraries for the same sample. Can be left blank, or assigned to 1. Blank will be assigned libray ID of 1 during processing.\r\n```\r\n\r\nPlease do not have spaces in any of the values for the config file. \r\n\r\n\r\n#### General setup\r\n\r\nAll scripts will need to be edited to reflect your NCI project code at the `-P <project>` and `-l <storage> directive. Please run the script create_project.sh and follow the prompts to complete some of the setup for you. \r\n\r\nNote that you will need to manually edit the PDS resource requests for each PBS script; guidelines/example resources will be given at each step to help you do this. As the 'sed' commands within this script operate on .sh and .pbs files, this setup script has been intentionally named .bash (easiest solution).\r\n\r\nRemember to submit all scripts from your `workdir`. \r\n\r\n`bash ./Scripts/create_project.sh`\r\n\r\nFor jobs that execute in parallel, there are 3 scripts: one to make the 'inputs' file listing hte details of each parallel task, one job execution shell script that is run over each task in parallel, and one PBS launcher script. The process is to submit the make input script, check it to make sure your job details are correct, edit the resources directives depending on the number and size of your parallel tasks, then submit the PBS launcher script with `qsub`. \r\n\r\n#### QC\r\n\r\nRun fastQC over each fastq file in parallel. Adjust the resources as per your project. To run all files in parallel, set the number of NCPUS requested equal to the number of fastq files (remember that Gadi can only request <1 node or multiples of whole nodes). The make input script sorts the fastq files largest to smallest, so if you have a discrpeancy in file size, optimal efficiency can be achieved by requested less nodes than the total required to run all your fastq in parallel.\r\n\r\nFastQC does not multithread on a single file, so CPUs per parallel task is set to 1. Example walltimes on Gadi 'normal' queue:  one 1.8 GB fastq = 4 minutes; one 52 GB fastq file = 69.5 minutes.\r\n\r\nMake the fastqc parallel inputs file by running (from `workdir`):\r\n`bash ./Scripts/fastqc_make_inputs.sh`\r\n\r\nEdit the resource requests in `fastqc_run_parallel.pbs` according to your number of fastq files and their size, then submit:\r\n`qsub fastqc_run_parallel.pbs`\r\n\r\nTo ease manual inspection of the fastQC output, running `multiqc` is recommended. This will collate the individual fastQC reports into one report. This can be done on the login node for small sample numbers, or using the below script for larger cohorts. Edit the PBS directives, then run:\r\n\r\n`qsub multiqc.pbs`\r\n\r\nSave a copy of ./MultiQC/multiqc_report.html to your local disk then open in a web browser to inspect the results. \r\n\r\n#### Quality filtering and trimming\r\n\r\nWill be added at a later date. This is highly dependent on the quality of your data and your individual project needs so will be a guide only. \r\n\r\n### Part 2. Removal of host contamination. \r\n\r\nIf you have metagenomic data extracted from a host, you will need a copy of the host reference genome sequence in order to remove any DNA sequences belonging to the host. Even if your wetlab protocol included a host removal step, it is still important to run bioinformatic host removal.\r\n\r\n\r\n#### Prepare the reference\r\nEnsure you have a copy of the reference genome (or symlink) in ./Fasta. This workflow requires BBtools(tested with version 37.98). As of writing, BBtools is not available as a global app on Gadi. Please install locally and make \"module loadable\", or else edit the scripts to point directly to your local BBtools installation.\r\n\r\nBBtools repeat masking will use all available threads on machine and 85% of available mem by default. For a mammalian genome, 2 hours on one Gadi 'normal' node is sufficient for repeat masking. \r\n\r\nUpdate the name of your reference fastq in the `bbmap_prep.pbs` script (and BBtools, see note above), then run:\r\n`qsub ./Scripts/bbmap_prep.pbs`\r\n\r\n#### Host contamination removal\r\n\r\nTBC 1/4/22... \r\n",
        "doi": "10.48546/workflowhub.workflow.327.1",
        "edam_operation": [],
        "edam_topic": [
            "Metagenomic sequencing",
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "327",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/327?version=1",
        "name": "Shotgun-Metagenomics-Analysis",
        "number_of_steps": 0,
        "projects": [
            "Sydney Informatics Hub",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "diamond",
            "metagenomics",
            "abricate",
            "antimicrobial resistance",
            "bbmap",
            "braken",
            "humann2",
            "kraken",
            "prokka",
            "shotgun",
            "whole genome sequencing"
        ],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-04-05",
        "creators": [
            "Delphine Lariviere"
        ],
        "description": "Create Meryl Database used for the estimation of assembly parameters and quality control with Merqury. Part of the VGP pipeline.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in name",
        "id": "309",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/309?version=1",
        "name": "VGP genome profile analysis",
        "number_of_steps": 6,
        "projects": [
            "usegalaxy-eu"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "galaxy",
            "vgp"
        ],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-03-29",
        "creators": [],
        "description": "# eQTL-Catalogue/qtlmap\r\n**Portable eQTL analysis and statistical fine mapping workflow used by the eQTL Catalogue**\r\n\r\n### Introduction\r\n\r\n**eQTL-Catalogue/qtlmap** is a bioinformatics analysis pipeline used for QTL Analysis.\r\n\r\nThe workflow takes phenotype count matrix (normalized and quality controlled) and genotype data as input, and finds associations between them with the help of sample metadata and phenotype metadata files (See [Input formats and preparation](docs/inputs_expl.md) for required input file details). To map QTLs, pipeline uses [QTLTools's](https://qtltools.github.io/qtltools/) PCA and RUN methods. For manipulation of files [BcfTools](https://samtools.github.io/bcftools/bcftools.html), [Tabix](http://www.htslib.org/doc/tabix.html) and custom [Rscript](https://www.rdocumentation.org/packages/utils/versions/3.5.3/topics/Rscript) scripts are used.\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a bioinformatics workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\r\n\r\n\r\n### Documentation\r\nThe eQTL-Catalogue/qtlmap pipeline comes with documentation about the pipeline, found in the `docs/` directory:\r\n\r\n1. [Installation](docs/installation.md)\r\n2. Pipeline configuration\r\n    * [Local installation](docs/configuration/local.md)\r\n    * [Adding your own system](docs/configuration/adding_your_own.md)\r\n3. [Input formats and preparation](docs/inputs_expl.md)\r\n4. [Running the pipeline](docs/usage.md)\r\n5. [Troubleshooting](docs/troubleshooting.md)\r\n\r\n<!-- TODO nf-core: Add a brief overview of what the pipeline does and how it works -->\r\n\r\n### Pipeline Description\r\nMapping QTLs is a process of finding statistically significant associations between phenotypes and genetic variants located nearby (within a specific window around phenotype, a.k.a cis window)\r\nThis pipeline is designed to perform QTL mapping. It is intended to add this pipeline to the nf-core framework in the future.\r\nHigh level representation of the pipeline is shown below:\r\n\r\n### Results\r\nThe output directory of the workflow contains the following subdirectories:\r\n\r\n1. PCA - genotype and gene expression PCA values used as covariates for QTL analysis.\r\n2. sumstats - QTL summary statistics from nominal and permutation passes.\r\n3. susie - SuSiE fine mapping credible sets.\r\n4. susie_full - full set of susie results for all tested variants (very large files).\r\n5. susie_merged - susie credible sets merged with summary statistics from univariate QTL analysis.\r\n\r\nColumn names of the output files are explained [here](https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/tabix/Columns.md).\r\n\r\n\r\n# Contributors\r\n* Nurlan Kerimov\r\n* Kaur Alasoo\r\n* Masahiro Kanai\r\n* Ralf Tambets\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "300",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/300?version=1",
        "name": "eQTL-Catalogue/qtlmap",
        "number_of_steps": 0,
        "projects": [
            "CINECA"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-02-08",
        "creators": [
            "Jean-Marie Burel"
        ],
        "description": "\r\n# Summary \r\n\r\nThis notebook demonstrates how to recreate lineages published in the paper [Live imaging of remyelination in the adult mouse corpus callosum](https://www.pnas.org/content/118/28/e2025795118) and available at [idr0113-bottes-opcclones](https://idr.openmicroscopy.org/search/?query=Name:idr0113).\r\n\r\nThe lineage is created from the metadata associated to the specified image.\r\n\r\nTo load the data from the Image Data Resource, we use:\r\n\r\n* the [Python API](https://docs.openmicroscopy.org/omero/latest/developers/Python.html)\r\n* the [JSON API](https://docs.openmicroscopy.org/omero/latest/developers/json-api.html)\r\n\r\nLPC-induced focal demyelination and in vivo imaging of genetically targeted OPCs and their progeny to describe the cellular dynamics of OPC-mediated remyelination in the CC.\r\n\r\nLongitudinal observation of OPCs and their progeny for up to two months reveals functional inter- and intraclonal heterogeneity and provides insights into the cell division capacity and the migration/differentiation dynamics of OPCs and their daughter cells in vivo.\r\n\r\nThe majority of the clones remained quiescent or divided only few times. Some OPCs were highly proliferative. Large clones showed longer times between consecutive divisions compared to low proliferating clones.\r\n\r\nOPCs show distinct modes of cell division: from symmetric proliferative, to symmetric differentiating and also asymmetric cell division, where the OPC is self-renewed while the other daughter cell differentiates.\r\n\r\nOnly 16.46% of OPC-derived cells differentiated into mature, remyelinating oligodendrocytes, with OPCs born at early divisions showing a higher probability to survive and to terminally differentiate.\r\n\r\nCell death was associated with distinct cell division histories of different clones, with higher probability of death when generated at later divisions.\r\n\r\nMigratory behaviour was restricted to progenitors. Successfully differentiating progenitors moved shorter distances per day compared to dying cells.\r\n\r\n# Inputs\r\nParameters needed to configure the workflow:\r\n\r\n**imageId**: Identifier of an image in IDR.\r\n\r\n# Ouputs\r\nOutput file generated:\r\n\r\n**lineage_imageId.pdf**: A PDF with the generated lineage. Options to save as `png` or `svg` are also available.\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "267",
        "keep": true,
        "latest_version": 1,
        "license": "BSD-2-Clause",
        "link": "https:/workflowhub.eu/workflows/267?version=1",
        "name": "Cell Lineage in the adult mouse corpus callosum",
        "number_of_steps": 0,
        "projects": [
            "OME"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Jupyter",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-09-24",
        "creators": [
            "Valentin Tilloy",
            "Pierre Cuzin",
            "Laura Leroi",
            "Patrick Durand"
        ],
        "description": "ASPICov was developed to provide a rapid, reliable and complete analysis of NGS SARS-Cov2 samples to the biologist. This broad application tool allows to process samples from either capture or amplicon strategy and Illumina or Ion Torrent technology. To ensure FAIR data analysis, this Nextflow pipeline follows nf-core guidelines and use Singularity containers. \r\n\r\nAvailability and Implementation:\u00a0https://gitlab.com/vtilloy/aspicov\r\n\r\nCitation: Valentin Tilloy, Pierre Cuzin, Laura Leroi, Emilie Gu\u00e9rin, Patrick Durand, Sophie Alain\r\n\t\t\t\t\t\t\tASPICov: An automated pipeline for identification of SARS-Cov2 nucleotidic variants\r\n\t\t\t\t\t\t\tPLoS One 2022 Jan 26;17(1):e0262953: https://pubmed.ncbi.nlm.nih.gov/35081137/",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amplicon in description",
        "id": "192",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/192?version=1",
        "name": "ASPICov",
        "number_of_steps": 0,
        "projects": [
            "CHU Limoges - UF9481 Bioinformatique / CNR Herpesvirus"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-01-27",
        "creators": [
            "Luca Pireddu"
        ],
        "description": "# Snakemake workflow: FAIR CRCC - send data\r\n\r\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22656.3.0-brightgreen.svg)](https://snakemake.github.io)\r\n[![GitHub actions status](https://github.com/crs4/fair-crcc-send-data/workflows/Tests/badge.svg?branch=main)](https://github.com/crs4/fair-crcc-send-data/actions?query=branch%3Amain+workflow%3ATests)\r\n\r\n\r\nA Snakemake workflow for securely sharing Crypt4GH-encrypted sensitive data from\r\nthe [CRC\r\nCohort](https://www.bbmri-eric.eu/scientific-collaboration/colorectal-cancer-cohort/)\r\nto a destination approved through a successful [access\r\nrequest](https://www.bbmri-eric.eu/services/access-policies/).\r\n\r\nThe recommendation is to create a directory for the request that has been\r\napproved;  it will be used as the working directory for the run.  Copy there the\r\nrecipient's crypt4gh key and prepare the run configuration.  The configuration\r\nwill specify the repository, the destination of the data, and the list of\r\nfiles/directories to transfer.\r\n\r\n\r\n## What's the CRC Cohort?\r\n\r\nThe CRC Cohort is a collection of clinical data and digital high-resolution\r\ndigital pathology images pertaining to tumor cases.  The collection has been\r\nassembled from a number of participating biobanks and other partners through the\r\n[ADOPT BBMRI-ERIC](https://www.bbmri-eric.eu/scientific-collaboration/adopt-bbmri-eric/) project.\r\n\r\nResearchers interested in using the data for science can file an application for\r\naccess.  If approved, the part of the dataset required for the planned and\r\napproved work can be copied to the requester's selected secure storage location\r\n(using this workflow).\r\n\r\n\r\n## Usage\r\n\r\n### Example\r\n\r\n    mkdir request_1234 && cd request_1234\r\n    # Now write the configuration, specifying crypt4gh keys, destination and files to send.\r\n    # Finally, execute workflow.\r\n    snakemake --snakefile ../fair-crcc-send-data/workflow/Snakefile --profile ../profile/ --configfile config.yml --use-singularity --cores\r\n\r\n\r\n#### Run configuration example\r\n\r\n```\r\nrecipient_key: ./recipient_key\r\nrepository:\r\n  path: \"/mnt/rbd/data/sftp/fair-crcc/\"\r\n  private_key: bbmri-key\r\n  public_key: bbmri-key.pub\r\nsources:\r\n  glob_extension: \".tiff.c4gh\"\r\n  items:\r\n  - some/directory/to/glob\r\n  - another/individual/file.tiff.c4gh\r\ndestination:\r\n  type: \"S3\"\r\n  root_path: \"my-bucket/prefix/\"\r\n  connection:  # all elements will be passed to the selected snakemake remote provider\r\n    access_key_id: \"MYACCESSKEY\"\r\n    secret_access_key: \"MYSECRET\"\r\n    host: http://localhost:9000\r\n    verify: false # don't verify ssl certificates\r\n```\r\n\r\n\r\nTODO\r\n\r\nThe usage of this workflow is described in the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog/?usage=crs4%2Ffair-crcc-send-data).\r\n\r\nIf you use this workflow in a paper, don't forget to give credits to the authors by citing the URL of this (original) fair-crcc-send-datasitory and its DOI (see above).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil* in description",
        "id": "265",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/265?version=1",
        "name": "FAIR CRCC - send data",
        "number_of_steps": 0,
        "projects": [
            "CRC Cohort"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-09-15",
        "versions": 1
    },
    {
        "create_time": "2022-01-17",
        "creators": [
            "Ryan Patterson-Cross"
        ],
        "description": "# polya_liftover - sc/snRNAseq Snakemake Workflow\r\n\r\nA [Snakemake][sm] workflow for using PolyA_DB and UCSC Liftover with Cellranger.\r\n\r\nSome genes are not accurately annotated in the reference genome.\r\nHere,\r\nwe use information provide by the [PolyA_DB v3.2][polya] to update the coordinates,\r\nthen the [USCS Liftover][liftover] tool to update to a more recent genome.\r\nNext,\r\nwe use [Cellranger][cr] to create the reference and count matrix.\r\nFinally,\r\nby taking advantage of the integrated [Conda][conda] and [Singularity][sing] support,\r\nwe can run the whole thing in an isolated environment.\r\n\r\nPlease see our [README][readme] for the full details!\r\n\r\n\r\n[sm]: https://snakemake.readthedocs.io/en/stable/index.html \"Snakemake\"\r\n[polya]: https://exon.apps.wistar.org/polya_db/v3/index.php \"PolyA_DB\"\r\n[liftover]: https://genome.ucsc.edu/cgi-bin/hgLiftOver \"Liftover\"\r\n[cr]: https://github.com/alexdobin/STAR \"Cellranger\"\r\n[conda]: https://docs.conda.io/en/latest/ \"Conda\"\r\n[sing]: https://sylabs.io/singularity/ \"Singularity\"\r\n[readme]: https://github.com/IMS-Bio2Core-Facility/polya_liftover/blob/main/README.md",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "263",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/263?version=1",
        "name": "polya_liftover",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics and Biostatistics (BIO2 ) Core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cellranger",
            "fair workflows",
            "fastqc",
            "liftover",
            "multiqc",
            "snakemake",
            "transcriptomics",
            "scrna-seq"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-10-07",
        "creators": [
            "Sergi Sayols"
        ],
        "description": "# RNA-Seq pipeline\r\nHere we provide the tools to perform paired end or single read RNA-Seq analysis including raw data quality control, differential expression (DE) analysis and functional annotation. As input files you may use either zipped fastq-files (.fastq.gz) or mapped read data (.bam files). In case of paired end reads, corresponding fastq files should be named using *.R1.fastq.gz* and *.R2.fastq.gz* suffixes.\r\n\r\n\r\n## Pipeline Workflow\r\nAll analysis steps are illustrated in the pipeline [flowchart](https://www.draw.io/?lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1&title=NGSpipe2go_RNAseq_pipeline.html#R7R1Zk5s489e4Knmwi%2Ft4nDOz2WSyO5OtbPYlJZCw%2BYLBATzXr%2F90AOYQNtgY8Ex2UjuDACG1Wn13ayJfLJ8%2BhGC1%2BBxA5E0kAT5N5MuJJImKJE3IPwE%2BsxZd11nDPHRh8tCm4d59QUmjkLSuXYiiwoNxEHixuyo22oHvIzsutIEwDB6LjzmBV%2FzqCsxRpeHeBl619ZsL40XSKmrm5sYNcueL5NOGlMzPAvbPeRis%2FeR7fuAjdmcJ0m6SOUYLAIPHXJN8NZEvwiCI2V%2FLpwvkEbCmEGPvXdfczYYcIj9u8sLiRdX%2F%2FXT3%2FOR9Xj7NbwX7r3%2FtaTa4%2BDmFBYIYNMllEMaLYB74wLvatJ7T%2BSLSrYCv%2FrdertLn52CFWzZvfQoC3HApksdQHD8nKw%2FWcYCbFvHSS%2B6iJzf%2Bl3Q4U5Or77k7l0%2FJt%2BjFc3rhx%2BFz7iVy%2BT1%2Fb%2FMavUrfi%2BIw%2BJmtM16H8yooE%2BhGwTq0E7g82Ld%2F339bf1xfvvz39Y%2Bv19%2Bjh%2F%2BmcoIgMQjnKN7yoJIAmkA394lkqT6gYInwGPEDIfJA7D4UERMk%2BD3PntssNP4jWWv%2But8IH53FrRF7D4s76%2Fbm%2FvrHy8003XgPwFsnn5pImodncA7dB%2FJFz5379Ib2a01Q9Dyk6J9d4r%2FmyW%2F6mhWWW%2FDAaF9pawnPcqvvAQt559lWugi8IKQPydf0P%2B6iOYEfJ%2FgkamTcIFpQpBQLKEquHNfz8p1K5CfrNL1D9658Pg8BdDEilJrtYOnaCQbNPRBFKTal21qgn43x4gUEcFNT2IZZDyiM0dNWTEjuGmpCaxLaOjU1dv2Yo1RGQoAXOSKlpIt8CPZwkVl80%2BhzpZGfY6BPB9giGUIBWzIWm8MWRTCr2KJ1gS0v3%2BLLr8K%2F%2F328vrsKlyL8%2BuH5j6l8MIt5SwylIT%2BRlaH4CXeNVfG1LzLy4RmRNPGlTfYv2c2k8dr10uFU8aBAVHpACnkopBDlx9sb9zE4%2B3O%2B%2BvLjW%2FDxXBGnEodJ9I8kXJAcdx0S%2FtgD3LcNk8OcCdMrLEDKjsmNaURBeIYfEJXVE4dXX0iTs%2FMlcH0CP3eFPBfzNtx4vp2Ns68Wm5mYsO9AuplO0ku0An5FHCk3ALLublz6TuiDCP2apbCYYQEkeHjOfypdqbQFqMCWFc1GpiVA3QKCaiqm4uiOo5qaIAlT2wRQRbbqiFDSZR0auqBrjgUdVRUECymKAnUgI6vwkUWInMJnFnFMVOczgkjS9dyNF2trhmUQfOEuLSfACIz%2FvP1wT4YuzQN8YXmBRdANRDHCq3idTirCf9%2FdnuF54j92T5iBrfa5FCcAB08qQOdiE2%2B5apCsLEKOBvWmlCK5MbDw27m9JAkQOa7vUh1CEt5Z5M771ptr1PP2AgAjSgujFbIpNZQEOlFiNQng2iMod%2FrzQ1GEGYwLCLkGPswmNyXTdh0sP1AmBDDNJ%2Fttjwl3gINPyF7HBN45HKwZSb3%2B1YXaYooFtcWoai2yztFaROlYOq6yW3jButuK%2FInp6gpreWTyTBZJRUAjbchshwoXXFu5eGMYihWQpcDJQyxtayyKJN%2F7K3AptiUfm0pqYcH0YgeB40RYRCovQjboA2wP5pCaRvr394LWsUvTKOgZG7WjB3VSa6o5DKU4bBt1rXWpSvGs4IkQPNefM5JnBSFE4RQ34wsmIyX7ghJEAW8CNbuTN%2BKT2xNJRgr5yZ5YAQizvqVt%2FKMiw%2FDEFb4waWNyG2LpK5rFTwVLWVmGhKYGRV2Fmg5EYAiKaIgSFiNFxZIMgGQw1R1bMETTAVARJB0CSVJkU9ZsQVRtLGealiqogoNFxkFkyNp5MiCVb2%2BTGGtkwcoi8PlnKn5V0akeV8qsFS0LQLQxduNB2RRZch0KlacylMo9JdXjVWwF8LnSWBEIY1hucdMGvGD405dXVBwB3nPk4lXB%2B1OYMxGMeq6EFPoE6ckSCkwmwM9pYEmcO74VrdgssFQ%2FY1D488tD9O0rnsq7P79Mv319P8vB3eWsRXWU1RausFt9jDRWodAGLh%2Fwzl%2FhSfhYEosYNcM7zs%2BoZhkPxzqhhv2%2FkYGRRt6WIe1s29fItUTecm3gnSUejJhICOepP8NDDnkrwE85HjUGOtT%2Bl5cVHhdujO4xESA9PoZgtY2DtxCRi4Z9WZQ48p5YFfiMo3mB1N%2BSWHNJLF2u0Yhi%2F0Qo%2FGL9j4RTYCpPXGhvR9QqUvZaQUu1DV3XHMlxREUxVUeVbRGopmTqjgMdxZqKGJUtyTItLFxZQFZMXVRUJBkOhIajiwKybSQpUB1G0KqZJQMQn7n9FrJaCVkRlom8zGSyN6Nzy%2F1RYaS1zNGaQ2Pe1eHgQwSgjbc2oSekZzYHISd3vlv77q81xYa1NcXUlawNIReU2cDg%2FbYpH0tMmTP5rysozHPiJJGqHTLbApr0ObkQrTwszcSdLnPWJaGXR5pUO%2Fltu5wi5RhoIticoMCXBSPuEvjEPYQBfJmTB1oIgZLxWwhsLgSmRuSTEQJXXM69xMPHGjJlzsJqI%2Fex9indQeSelruHkT6eJrvqjLJnn%2Fk9dkl3f22cYzx3SUUM4VKdVbltUXlqr6ky8kBuGnxR9ZwKuW3l08x3NHsAYdTArWxZqmBAKIuyqEhYaIVAFIGlSw6CyNEgmiITmhDaClINBe9QoBu6ZGL6AQSkmo4sQQgVU5Hso0uqmYdvI6funC0DWd1jjWXXRONogybHQYoSR12FwYMLCYLnfYY8ZN%2FKYukIKHRv8YsFIBVmzvkue43MleubFN4Fq3jWXjrLgXMnl5a2c%2Bnx8eMs6WEXPzb11mR5f3bMi%2F3p1W2ZfnC0fkuluIrFDo7nt5R%2B%2By0369W%2F%2FMPHBimNhE6wQUv3T9oFk%2BiStzZIUe3IKMYviErJ6spmXOnoLAzBc%2B6xFXkg2jJgVeMOuHZcpefTcW2Qm42gW1TnxU6MUZ60A99x5%2BuQ5mzkg454tq1RiJCgkVSGBflZ6K1mPiFltZLZdCOdQfRAMvquGUBIO3lsxi5rBTL%2BQ7XiWI0dkRt%2BGfKnXydB9wGSVYjA0sKiay08OE%2FsYVftSQbtEav4xukg8OpBWbpbC8aq2NkCywYT%2BCsNNOqTUKTHIPxJAghp3z6YU%2Bl7m9zNazukIUSM5UUtvvuuuwFiQIC1RxiYm4wggWXkrcOCF4BELrjEuOoH5HEfISIo4cXb%2FRGCv0noY5LTlXbrBTbRvXIL2Ghe7ydpiCfvLkfNyihF9iJBeGKCwQNImBFWusji%2F0SI4OQGMFg%2BjV1%2FHu2MsihpX6NWplRZ52gBHDVA1EoSWmfu7DSgcUDVSWkLxVenOnGT1MXqQgyapD6MMtVVLlnjZDLtKDpYW41HF4wC1kmatlXjKT8vG%2BohGs8uA3mmKVyDKP77gkoBTWzTv9aYCMfPkyTijhJ%2F6isMwSOFKU0iIPPAktPPZHN0aX3Fj%2BH%2FO%2FjGL3vGmjN5a6exjiFRrbGORyZLPCfx33%2BiV5eyVsNpaJZ20rEw4eT1HcSE1DJuyZzUeo1DUTWhHssPMOnxU2sPJn49kbrjkSylIclSjmM2akuylFIOvpxYmepIVuV5xZwcQLKaodWOPArkWcHjb5Saqp3XbekEpRRVOj6K7CgO9BtFEqpjjhNFxD5QZHvdhzeCIsd2U6hiaWlTZ1LH3oUKI9J6YESpQjL%2B0iGd1nlpTF06V8MOK%2BazPd7rJDZ8ppNnF6xTaZdS3sdy68OQFE3rh6QoRrckhWsw0oekKCNztXeHmnLncs6eQYmZWeX%2B69ldY4vLHUrcKqsVDTs%2Fom0lc3JFGKjtjStKDqzHNK4ct4SDlNZrGMiYwodt1ciPF2t6T1ZL2EQGlKgHiTXIQTkJNShapwgUy6HkSxdCSlxKC7JpuUvAIE9qygnSipI04J3t%2FITe4mGp5%2FgfBt0FIRfqJfGIqefi5hr%2FI4%2BH8UXg466BS1cWYWx9RFE8qVZA7AAHZIN8vIAFSgUJuH4dWWmNBA1XvL6YwG5fLy2cknh8hT2CT76s49Wa5sMEHiQoc1ahYfjmL3ucoSeZuzLIpuEynyXL6dmMu0n05%2BEpF3mETUt4XoOl6xFcu0HeAyK9TvpwX%2BpKsSirrKrN8HwPYtcQz%2FWR4blQQfUQRWsvHmmcVT5zLdo2nGN8HLqOg8Is9Bs9rTCsIlYErAq1Xob04QsZiB%2B69mKJ%2FM33x7vVj7CtVaEqw3C39dGS7FN9e8Tb2gvmI93TeGTTlGXZgedhYZHGCDH2hYC9mGyCcl49MityjzwqejGMp2vBVeX%2FxPnl8i5a3vijO22gnaq%2Bs%2F4vvko%2BScTpxICCQpdm0dxnCetdK%2FBpqPlOBd4czJS4ddyjIW1VyTxERIEaKXXbN7nw8opNa3a3hPleKzmF0BINZDuSbkuOYQFoObZkAmSqOhJMQZyKliWZJoIahrJsAMG0LUMVBc2EDrB1UVQUAxi6WvzIMXIKabQw%2Fp0t13W0cP3nH6wK7Q%2FWjDffDxZleV0HAQa90t22IdxtY9d5CzSj48fDmC2Qt0JhNLvbtlJANGwk6tAwdFs2bVGWbVWWoQmRIUBoy1PbkgQJiQCJoi7aUHdMVdaBaMqGZTqKSRZMEnTdGuFK7QBFumT8x8Yafv8XHiim9BmFIXQoJIDMlwKmksssh0FZkO%2FnACsMhFBTDBVYGQ%2FM1mMiz9jrKKZdbaq8bXI5cwHHeHmjBSZ%2BWc2zNIR6z6Di16fUa5wqtPygZFk6nEfyg78OL6%2BQl5gSo2L%2FIlPy4Y28dFS%2F%2FJHiwDixpsX0wTRwa5cTrcOwU9cnhoOnFRUz%2FbixNwQSarBkhIZYhylNSLO%2FWWAqDUf1XCsECSIf3V1Smkxrz4nSV1jqccNQZXFYzwmfEh2evzwcJSq5%2F9uSpQMokToMJWoaIdQhJao0RGuLhLFLWLaLn1eIr0W19N5SKy2rGsbK02LaQYSZ5xXqh0SVJ9WeRqmvg0aZI6RRqRNx%2FKFkYzhqTGlaHF4Z7KwxbnSPNuQin1p0T%2BNFlo8TeNY6XiylF1n61o7cidLz6mG5E425XcIHfjC34aFczEEgXof9srD2nEs7Sc5VjkuS1RFyrh3VgcYRVju6XNbUXrOTuKXVecYTulhpsMDygljHyEnlhwjKNt7btDar5c6%2FufM%2Bkkjx0O1k6O2Jiv4qiIpqGrOqd3VwsiKfAlnpVNwxBhN3DqUIH778uMrCbRpv902ETtHin6u6PUe09E4fokVhDu1pgfE6aMHA5rvzG1G8mL%2B4yvJGhnfXcPnpCU5PpvrgcKa646Rx7jTVZTFm%2FZnq4Hp1ByAI9%2FIW4Jdp%2FfckOpEKG33QFvzdkAy6PVkxT5KslC1uysD5FPzSPIcnb55caZ4CheqSAqlNA7lGWppH3WGrGao0DxGAzgP4jPUrqTHJC1NTTaqTCeSvRJwiH0T%2BnMRb9EH6yCfJYRh4MFJr8qfmz6I%2FXfKnjtFsIw9anOfkHA6Nk6CNnpSzZkzuZCowjcLh0DideJwOB0XbzsSGcjhcXv24vLpHv5pzsMstSUUbS0EvojvCv9tzLuV1JEIr8rCca6tFfDTB99W8ojgE9s%2BRxt4z4zqZcjLIxOD24EaksGOUqMdvMWWOawk%2FWp4RP1bt1YcINMhGasHUj37MRqnas14%2BHaPxMRumVOior2M29OKxGeM4ZsMY9TEbd4gKHzbRlT8gP1jy3ZoNSWT%2FeVW0tgw5CB6iJ5o0Oi9Mopzlk0VwCLSWv0vpP%2F3GPHb6Ti%2BpJIxsStAnuSL5OvP0rPtdh6vtdeDXqEvO62K15LyYRjUWsjuEDvgUfwcPXnI%2BoyFvt%2BQ8P1N5e13L3u3aRVNPJkb0LVMMlOFscoIYucsmjivD2ayvPURPSucQfit4IiefUzgS%2Bm4FIdaQprg54wH156ZbWBeZUyxMVSoZKeQneyI7EJ3clrbxjpzhIUmHpWi5XRt7a%2Be%2FM5AU6yPhX%2B8wiuNVY7UmWF5nMak4zdX8iQWF8P3mGksO1FaT1XxhSViAohrvyACWt5WL%2B4hitIryJ8M0Odv7WKePt%2Bz%2FjQxs0upo81M4qdxMi4emibMm5xyF1OLUuWjFNwEMWgN0P89M5%2By6Sx4simqVCQ96dnkzHpyNexBL5x9%2Bkyoj4BGCGIzT0klPiJlsSv%2B9e3FXKwTfbxtZLzqtD5YshQJzwweiu1LHBjPDroDLuC9KRKGmNQ16HH8ElisPibM7cUZhPJu%2FjHuc0h7jPGnTdslgkF7urAZxNHtB6kgasdOGlFcuFBEZESmzwHJSKvD4plwzvIrM%2FRaAO%2FnC7HqfoRSpH3l3vEznhxQdmsxAqzHdJUWemgYsJAAhHJ3Zqikvx3v7JwweWSlUViyJjProWZJJPSkyg9aRC2Ye%2FqcbuaArVYLRZ%2BTCjfDRWdwasfewuLNub%2B6vf7zcTOsDFyqoRegsl4%2BUDUwKj118QnMiPm5Ql3VXYRdWLbtI6sbnFolb451wiVJTpdR8bWl5HlYUSWYnnEQuchK5ghdKGq%2FJcwMcwki4SCBXkIAeRj%2FZlHNt4EbZveW6AJ1Rio8wqluK50Epe7Y7A90hXmOtLEPm7K0pNDeNFK48BQNrFESBsyLyK1fxbrM4g8uKewqIbYxxR0CuzNiRxy6Fh13qkbCrT4%2FmVvR%2Buw5NLlhO0BOVGMOvmVNly3Z8a16nrcB4s26V0Q5sQH9PByym7NvhyS88DtPFgT5cSjbkOScuM%2BdfByVb2Cij2N6AhauMnKJYPW6KL%2F9oR8LO%2BuM6Mkv5HzEGp50PbN8Y0el6riOaaJggW1aX%2BdUql2UhVpOaLeLRtMuqmf2S5fxSsxNiQZOQJMETGYMXl0HDMN7QkvEsy%2BnBvgXfiHn4knEtyydT36K7HL1qgGHtyuZNzd9N7%2BNX948r6%2F7%2BKXgOnj9%2B%2F%2FLPtKmlOT2OaOjM83Isr1zIJN%2F5vKL1k3meHDpEbRyfyDFJjat4Batnuh7RT%2FIJesKSgNEvF2uWNKYSyBFt4ks3ImoN8FGwJtfJrKjFLbONS1zreD2ynZxxvJyQrgvDFvzipipzzC0Dxy23oo34IgsnLtPLtod17%2BmJ42Js05zm1H8yksR1aWTVWTrEhhL3lPtHhqHz1o3iQbspi%2BOytOrx72ViVpal9y5C1UxcS%2FW0V4iZJTo1MJniQ38oKrVt1I1cBuvQez4nKdco3s30N%2FjCrmKWlS1fTs2WkG8uJSi6XDoBO9uoBd2oKibIegc%2BGf5yH17A5IhFuqS21UFbVQhstJn62SQ1qkm5HKB5nJRmXSyqQJrZjwr0ee3F7t8XzYOA1sslCAkKSQJ%2BLYvX26Hf4CHMQm8184n7rFbHmW70HIgekFdQc1gx4yUZ7y97W8hPPbqcnFJTjvjhHXnWp1KzNauv1nlZDT0I6UjbGcULbsQSvcxBnSL7eebfTA11E0m%2Bpv9xaVbePq6VaViRT%2BWtf7jTK4381JkK5yGALl7%2BUjNGM5IOQingnCSHpMR0ASCllQKPIXaAT6YszLRiLQRFUbKmPAtUjZnICRkSDXV2LEYo1aeC1CFSbQwLfWvtlVu8zJb%2Fd9ksTA9i9AKW1sfCGWmJQY9Roi2Y6VWi7DctyZESJO6apUNEZFWjuOhh2NbDPxf3hM6mRRoEjBiPzANRrMCz79B%2BrYEfk2PldhfF2dbbp%2FQkOhom5BEhgfG%2FCEXRshg0tHVU9MQomjJF14BUxcJvFwa3o4sPrAQkc%2BrmSkQ2evme5lOkggOCPqIDwbt%2BTQ7iYyeUW6Tjz5f3OZRhqZ4LBGK81vsvyraqYO8ur95POGXEk3MB6cgoMoMo3n8EV7tLlmf5r3t%2B4gxCWoEDeMVPuL7trSHbJ5TRp%2BkKdC%2B2%2FR5urBKAHYzkcBdQIi4M6wAySwGhilgNC5BEns9uj7hGfImlsDgvmuJpLT4HkEidV%2F8H). Specify the desired analysis details for your data in the *essential.vars.groovy* file (see below) and run the pipeline *rnaseq.pipeline.groovy* as described [here](https://gitlab.rlp.net/imbforge/NGSpipe2go/-/blob/master/README.md). A markdown file *DEreport.Rmd* will be generated in the output reports folder after running the pipeline. Subsequently, the *DEreport.Rmd* file can be converted to a final html report using the *knitr* R-package.\r\n\r\n\r\n### The pipelines includes\r\n- quality control of rawdata with FastQC and MultiQC\r\n- Read mapping to the reference genome using STAR\r\n- generation of bigWig tracks for visualisation of alignment with deeptools\r\n- Characterization of insert size for paired-end libraries\r\n- Read quantification with featureCounts (Subread) \r\n- Library complexity assessment with dupRadar\r\n- RNA class representation\r\n- Check for strand specificity\r\n- Visualization of gene body coverage\r\n- Illustration of sample relatedness with MDS plots and heatmaps\r\n- Differential Expression Analysis for depicted group comparisons with DESeq2\r\n- Enrichment analysis for DE results with clusterProfiler and ReactomePA\r\n- Additional DE analysis including multimapped reads\r\n\r\n\r\n### Pipeline parameter settings\r\n- targets.txt: tab-separated txt-file giving information about the analysed samples. The following columns are required \r\n  - sample: sample identifier for use in plots and and tables\r\n  - file: read counts file name (a unique sub-string of the file name is sufficient, this sub-string is grebbed against the count file names produced by the pipeline) \r\n  - group: variable for sample grouping (e.g. by condition)\r\n  - replicate: replicate number of samples belonging to the same group\r\n- contrasts.txt: indicate intended group comparisions for differential expression analysis, e.g. *KOvsWT=(KO-WT)* if targets.txt contains the groups *KO* and *WT*. Give 1 contrast per line.  \r\n- essential.vars.groovy: essential parameter describing the experiment including: \r\n  - ESSENTIAL_PROJECT: your project folder name\r\n  - ESSENTIAL_STAR_REF: path to STAR indexed reference genome\r\n  - ESSENTIAL_GENESGTF: genome annotation file in gtf-format\r\n  - ESSENTIAL_PAIRED: either paired end (\"yes\") or single read (\"no\") design\r\n  - ESSENTIAL_STRANDED: strandness of library (no|yes|reverse)\r\n  - ESSENTIAL_ORG: UCSC organism name\r\n  - ESSENTIAL_READLENGTH: read length of library\r\n  - ESSENTIAL_THREADS: number of threads for parallel tasks\r\n- additional (more specialized) parameter can be given in the var.groovy-files of the individual pipeline modules \r\n\r\n\r\n## Programs required\r\n- Bedtools\r\n- DEseq2\r\n- deeptools\r\n- dupRadar (provided by another project from imbforge)\r\n- FastQC\r\n- MultiQC\r\n- Picard\r\n- R packages DESeq2, clusterProfiler, ReactomePA\r\n- RSeQC\r\n- Samtools\r\n- STAR\r\n- Subread\r\n- UCSC utilities\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "58",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/58?version=1",
        "name": "RNA-Seq",
        "number_of_steps": 0,
        "projects": [
            "IMBforge"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bpipe",
            "groovy",
            "rna-seq"
        ],
        "tools": [],
        "type": "Bpipe",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-10-07",
        "creators": [
            "Sergi Sayols"
        ],
        "description": "# ChIP-Seq pipeline\r\nHere we provide the tools to perform paired end or single read ChIP-Seq analysis including raw data quality control, read mapping, peak calling, differential binding analysis and functional annotation. As input files you may use either zipped fastq-files (.fastq.gz) or mapped read data (.bam files). In case of paired end reads, corresponding fastq files should be named using *.R1.fastq.gz* and *.R2.fastq.gz* suffixes.\r\n\r\n\r\n## Pipeline Workflow\r\nAll analysis steps are illustrated in the pipeline [flowchart](https://www.draw.io/?lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1&title=NGSpipe2go_ChIPseq_pipeline.html#R7R1Zc6M489e4KvNgF4fPx0kyzmSPTCbJfLOzLykMsq0NBgLYOX79p5NTYLC5kkyyNRuEAKnV6m712VPPNs8Xruas%2F7YNYPYUyXjuqec9RZGHitLD%2F0nGC22ZDGe0YeVCg3UKG27hK2CNEmvdQgN4sY6%2BbZs%2BdOKNum1ZQPdjbZrr2k%2FxbkvbjH%2FV0VYg1XCra2a69Sc0%2FDVrlcez8MZXAFdr9umpMqE3Fpr%2BsHLtrcW%2BZ9kWoHc2Gn8Nm6O31gz7KdKkfumpZ65t%2B%2FSvzfMZMDFYOcToc%2FOMu8GQXWD5RR44%2F%2B%2FFO%2F%2Fxv8eH1%2FXZ5Hopg0tf6geD8184LICBQMMubddf2yvb0swvYespmS%2FAr5XQ1X%2FbjcP7rzQHtYRP%2FWXbqOFcxt2A77%2Bwlde2vo2a1v7GZHfBM%2FT%2FwS8cjNjVr8id82f2LXLxwi8s332JPIQvf0XvhY%2BRK%2F6c52uuz0Yyxtf21tXBNXDhBvjAvXU0HVqr4FEj0hU9uQJ%2BRlfPd%2B2HAH%2FQ%2Bp6ml4itGv0kbdrpV99vf27%2F2J6%2F%2Fnt3eTf%2F5e3%2B7asM8egHczoO2QLiVYt8gqHABbDRQN0X1MEFpubDXRzhNbZvVkG%2FEIHQHwyHxPj0Vfpjub6a%2BuZufbO4%2Bno7v3%2F92ucbeqeZW%2FapnjI20QxODbjDXzThyiI3xo9bjPqnLtlWwSX6a8X%2BTx5buMkWNDDyLt6awN8IVpnaApinwRY9s03bJZ3UOfkRLtrStjh2yHjNDc1bE2SXY6iPr5bQNKMvlfBv8FJ%2Bh9AE9XTlagZEiJBo1u0N1BkGrUzN8zg2cXIhkc%2F6aPFsDLj%2BTMrDrB1wffCciwns7nSsDpQRfYqRbbR806DtKUIIJ2xZ1xEaOJSGg9HxaCTEavlD49GXMf6tA4%2FgBjGyz55DeahcDSIpUzmGRfJISmHQUJqlMWjM0eoY%2FHn96Z%2FfSf%2F8%2B8f85ou7kY27i5fLvno0V%2Bs8D6uQ1xRkNeqwalZDHv3sutpLpINjQ8v3Im%2B%2Bxg0hug2nihDd5hn91Uluf%2FQHHUGIbcFUCiHg%2FGasX%2Fxzvr4Bfz3qy18vd9%2FOXvvy6L1jYIyctYKOajfRkZ2FstAx1X%2BYj76jmZTX%2F2j0zVuDLO6b4KzJBsz5YrjPeTK%2B0fcI9n5GHeSh8yxg2GdK73OUSdPXpXh31Z9NzuZsfXndvwWP6Ilr6AATIjaLhnaaOwahlJExAQEoqWxz5AyOg0OfbCnoawv0NNrBfOrocA%2BW0IJEDEXog%2F%2FZM8dqFsRzNKtIm4alCxcsY19a%2Bz5WIXzG%2B0WZr6CPRLmBazoDC9EWZQ43i6WN6Az68%2BriFs9VWdnooo%2FX1rQXeJXADis75hwQiBTMMWp4GDPm%2Bho66K8BvztAEqK9e0nPJrsjh6AmQBHRPLObG8azTqOwaWuGhxU0GOBYHWMbWxMvnqRZBkZxz0OcCWom5VIaOc9XtIsr2IDPQN%2F6ZLiRDYhQFb8WWiuyNU888An9jzQ6GnQBmReZ3YlDbhnAw6ek4rPKPgJVck5IcDI1fUyQBQdNmZ8dKj9mDgVsLgECdHxy8J%2FoeOWggxaePBXCuLAz5Q2BfnAoBFcuny0MQzkFMkUAMd5WTvJJiR79hHpgEn%2BBvVx6mIomFqEC8aNd6Zn%2F%2FSsmSe%2BTnmOycyhKN3B%2Bk5WiEnPVAnPG5vvhAffb4j%2BsZECEGOtOBDKkgDwu7GdMHYlGFdPHhe0awO2jZixrYBBxxCfUU0JYPgruRDXxn4lsooIh%2Fg16OJphBO9W8thPK6IEXT5v4D8LtFjxm3kig4AfcVkuDfFscCZZFdhEByXpaLG9QPsd3pJSvQKoR3op2aD3F7bxkmpMcS7f4C2X17n8LeyYfpS3wOi7ECJt8PwhAZmFBIPI22ChL%2BDG9Ihz5xD9TkXz8LSNk5qD8N11zerq%2BsddhZOynK2ftz6Nz63iRWPza33dMBV1Dp0Fe5gNXrKXkRlVMXDcKCIRhIASMheVZfOlDCXC%2FphYEpE4sFAGdc38zCwNPhYjTrndwQRL%2FBkb9VqaRL%2BOlfi9mEBRhcycUK3LM4EAKJKZuVBQgm2jywjnLiGuKdPf4lpxcY0fMt6MuOYIRYcNGj60qHQgOaGERtv7ZLfge%2BPIPYT0fp%2FtoM9EPrDoqXufLu46PASnDuuF1XBOsm2d6nXQVCkpwDenYqHyNKIq64zmKlCADHaa62XqrbK6pUVQJnXnLECG2iq1LvWsQoJROa69gwbYpwoScavo5E4D3dIVejAGpBhABN%2Blj%2FWxNRYuoR7%2FunRiO%2F7gU9FhiIC5lwEq%2BQywDKt7WkMfYHcY%2FOyTS9yAqmd%2Fw6FI%2FyFgf7PyjgmHcz%2BRsaRRLRL%2FYGfVSMP4KtagRhI6t8lyaiHenVzSlCtbzNx7uPCjFrbujjsi%2FQSyxVzz%2FO9nhKoXETwet4h%2B%2BniEuo3WzDb5acjVnsiwiX0CDxbx5oce07cneP56uxggmpHD7xmv36DBYbYxD%2BwcuBvW76Mbj%2FqANgfsey9roOuUyRpE9CzBLph26C9yda6OM5gE8ZJiL5YqPz6NuOmAn5%2FG0xTlk8dymvSNy3smFWEgQmclfqbLIFPAXNhPrVOo%2BijCsCBF4F5lrft7xK1YQ3b6rcofQ4gie9y0f6MIs%2BfNOoki6rQJFMkXdj4IilS88mlvLDlxIpETRlEmVtGnwjU%2FGoVG1aLQXOS02KoaT4qh0eQgcVndKy5X6KZYmChVjppFfR6Eiyz%2F9o0%2BcpWFYG3quFJskdXjF7nSsK44j5h0X0lfeHerHRE5xnF%2BobCDT6YXcrI%2FO2%2FXyl%2F4mb6bWHkYUu5jOfW5yRelTB1B0KTz32yYj6CJ%2Fspo1gCC5p%2FEfyNoPQLS5F1g6LhiRYAQQ4f5ioC3gaFyHEOVfYy9AyhauXjXEooqx6BoYVX5wn7yIVAK68p5S8R38QZoRg8bdx0HmyX2WJaz4hBo04lHXJGwVdWhgSzYLf5T1vN7dPIH2%2BFD1XxohWeQKq%2BfH0awpE79fMXuTDMljsBjtgNj%2BniBKbImfXyu6UkQ%2FbbfM4GEdDD%2FBOkA15dvW5%2B6AeIcJqhDuIUi%2B0d61PM2Vnu%2BLhSN8WYLpgFp1A2NXQnHXcR54XhnvGgMPo%2FJn2sbaGL8%2BwrMHcBv7TXhyjAZTmOor0rjNOrz7XEk6hfE80nH8FxKoboLvK2JOWIXkf0EIRmaGzA%2BEcaiPYTo3qxv0zlcLoEb%2BjDRoZjQ8xsfyjX9tmZZPGNJx3Z3DTt5OBZkTBHtZB75Xf1OnnZ%2BJ5v2qqPbGI2sz7mUbpsmkhZIJCXlWEDT1z3uq%2FcBkHmsFkTmKtjSn7uLe%2Bfs7od%2FtQHj6832yr%2B%2Fajd3SxNOW4efV%2Fnpf%2B95ddaaOSJ33J2hT2mJ2gUOQpuOkqi4C3nN7uI48SI%2BpAYQmXtraL3cswwG97QdHR3vcU92mqWNg5uNkR596n5m%2BGLSsbrLMx6QPujWYA1MB7je4EY8dXHHvBDOdhzi8RohmhTsBbxjXOx7GEtBEJVvCweltTMhxLPh8oWwMh%2FzcH3r%2BXQ%2B3M0%2BTAfhI%2FTy1ohA4CaI3TQtADBHKTyxd39gncgFJQNZqiAt4Fyo%2FT7eobt97XdJaaE25ffw7bpiX1qoJ9snBVXMeIo4axF%2BEEMHP8sJ3FjbYESwFh7dOvEQ49TXaXIV0UeZmFOl8jjTsZtOBc%2BjtPJ42JRzd2TLJFJcAvLTS6a4RHfUsTpTjV4kcWVNvuAqv47Ss5EoTWU9umdxyMrwvZ9%2BEvSs%2B6mYh6M3SyedtWYhkYcoBIsSyu9nWOJCI4C6R6kfpY70X0wcoWVQk5y%2BRoKUjoAPPT%2FonaCVXhat3EMoj4l8Cc1rEQCUp5Kj31QS%2B64LLHQNUkmhHMj1nd2UAw90b5y0JgiOCxK46h11jiVw4NkHLOFTEdr2hXan0Xf41cBakaMhWlg0wN2KiITaagOsiJDYDaJGpT464fK0bNw8LYtRpvKErQJalvQ2GEuTNC1r0NtAHLfzFqL%2FqnPJrijeh5OivTSL56Zv2XNroiY9sY7K4lzcE0vbnGEtE6l%2FU5BK6og6%2BiQBle1uEBnwiDlsAVdPEBNIH8l8D02ELqOx62zs5QnepHmCV4P0Neb5UFuiWD8t9eHp9PHu35%2FX9zfWn456ubv%2FXVyhFKGapgmVEKqV06mjNK0812s3JewDBewmA4hE657rhdW2a3EiIGi4z7U40X%2FEMKZuhoaP8vcR96LCTA1Res2FHkleT20%2BO%2BjhHB2vzE%2BpGYVEfPjl%2Bdq0W4L8coR%2FRYL8mPz0qAkrKuCTn3oE%2FGHL7sRila7SJr%2F8rdLNyRPxFlW6C70wzbs%2Bu0FvO7V93wQW0LFj5pkNlkuoQ8DSEzRA8Rblsw%2FR9fnoqtfhSGBwb91A9e5T8785ajYq6hXYPWoGHQQIqoQtStQc08ZvpSnO%2BcPNkLJwtKUp2kjqNU7RaiBJI255aYkkiYs9vvtae5XKPoVjXqcNUYuCGolO2%2FzegkaicEqTjmok5HIpTZrSSGw03Sse6syCs3TNNIkTRSOMiwyxvBT%2BPmKM1anAz6vtGGNulupMRETays3tPF0MiFjA1c%2BYMYpFazGVnhcLPexuqHEF2J700h6NBTbuuuK3FpOtcWFf%2FeOATf%2F7zcyf6Rfj%2Frs%2FIFbJlKeFmXJbwljusA8iX7VXC7kBJBxaxzbsC2DZsRJeOby5fbpGw55Ov%2F28u%2FxC%2BDPaFc%2FE%2FL6KzSMZKMXvSjjdN46WQnx%2FSyHQdBAOmQC0cTmLaBQ42h36A%2FZCoKWh8mOI2q4Iwjz%2FTVsnoycxQXgO%2B4qhHFSio2MVN%2BLC00SYpUJw4Jfl2mjNSEBrEoCrt75GQO0%2Bbn0NYaR2q7EKcb1LwNxL6V2CRGQNcPqZwMtWqOjgWv%2BOsPpZdjakztdxDXh9JASZoFL%2BoeajlVClIInnbEL%2FO0FIbgBaVpxFAidDuXlo7YMFffdTeL2EFklFE%2BTvoaVPNIJuosoo1CNDQ0%2B9eNAjgAaON4gsSpEqlAWLcPbKVsQs%2Bf4PMrBeqVKclVbWrEn4mfFSMLkhz8IS9XWFPKdlndZO028gbbcsFw7W6xSTDcbdij6QVx7OT5CiPRkaPlt2UR9Iqlz1wmyDJ6%2FQcUhitpbPlOisy2IlbWuHz47kLEyVlY4GKXNNhTB1J4cGLd0sD27kAYHxYPXa7XEqB4zzTSuAlTjLkkVHT1GFzFlNpIznUO%2BwZQMnFY6ldukQJQtTTxRWkb1p9E3aLxBYBCKXwH4xqk3iOt6A0cHg4ialsJlcUAjjeeG645BGcnXdsDxgRW36DCCYyVP1MWHvaL8%2FGPaT1QsTauFRV2Hsz8sozrON4RmUNvnPovB%2Fuyb%2FybBdk%2F9X6Y%2Fl%2Bmrqm7v1zeLq6%2B38%2FvUrD%2FAS8MUUamHaK%2BQtSZXSUMRC%2FgIrEuUeoC59XYqFLDJZCGa38UWKe0FbaPF7cZdp1sSxgTFo9TTJjTbQMAjFFGFFnIpWwV24byyXjtQUXgxF0pFSHi%2BKIUG6VvgC7%2B5eJMNsT1GfP%2FdoemmTnA9IPjrbwYcHzfyUWq3DdmUV0E0WKRTsuqEAusO6oHuMVXqcFD0jSlgOzbCRwFV0LImlEoskTgwXp3UR80C5sox%2BrgrkmsRdj7nabS9yVSAXCpGrSTNkLnZ%2FXCukECxv0D7F1ONzamjJ2Y0fzRKVC4wPa2jp7MBatABVwGFmk8PEF3WYTRuP4jBt1mOB1AYwtxMatE46yX0AvVgSOQuLP%2BOakDO7xEigXb%2F0ETT1qMt4qHgny7n1MEZJDNeCFNvv9vQ5ies25aIUprbjZ1o3f06jXYleClBHRwNBjOR%2FEjlqEL%2BM97xkcSlWFmiSFOGSVWBPEbrTv%2FuEwAdqnoXAKqp4bsr6v0%2Fv3AFn%2BeuojsJawtXW5U7PXbSPxQuDoFOLlfxqzcVCKJBwO%2B42oJehK3jSnCfsxGEoKv4hmlKz3vtNA9JxgbZBpwUvE4qCHnkgbMcFIQ48kddBqqFmCHP1Y9xARKrdZII6cTcPzN3x%2Fkg1GGBJSdqT7T6YNislbGkrIn%2FumcP%2BdSvR4ALKtLwS3z2pboAIENrWJKUv2AgYLD1z68a0QDjtOy18Y0dlc8nb%2FxFSWgM8Ax3JEW5snXEEjRlbwELzwp7E1CNYdDeM0wnKzQYEIniQVmqKhfCcELPsAwAYJ0PAeEgSgtbK%2BzTYM7zE0TJXasu3o3bA1TXhNjQapuN8hH5D8kgpLQUVMZkKoagIzpx1qdzz5MkPrHKfC%2Bsw5eesfpO5OJo8phTODVp5CpaDMnEkk1dPWMadrEwcqWTX41ECC%2BvJxMGqtxKz61%2B43mzx5KDOC1kPjxROJqVqaU2AQB3KGrlWVOnVlrZjAz1satEsYG89IieTWZGTRODRowh9erKR7c259IzkOIOaSIJQjAZdesR0MM2PWqWD0qAUJUQXQXq6JHWU26OOohgOIfh5cvSOZKjiBfXePzYobaNDrttpV9ChYwnLKkSHhOiktowNeTJW88iQO%2BxCR5mta76c4sRGeIL7eG2IMfSK50U%2F789Kwr44cx5ORoNR%2FOgxkwaiWMk0f1YntWUIfIenkrr89vNKULzVBIGjSaymTmcSBBpwucRFEwsfR%2FADOIERzjTTixZcDOPj95w%2FqigzG2qM%2BQTywgmyEarZo0d19QmqTu%2BdDFIYqTMRyRSeacqTTMGZZh%2FCdtZdsmZzSYD7xPYRwXfixoZpoXcfbAH%2FWeD3ndErz3jy0Tw7b%2BEGmppLyNYDCLiMR0GFjQ8LomdhCUTwaGi8A8aQMGcq8x9aYt3MSWCjoAFXGPoZ5FKyLfMFK%2FilNLWNLEqF%2FpiBAeEaDZ7iJ8uG0gsSrLCkKonp5YxPZJWoa8xndDhk4DhjW4%2FlcGlktL1SjqX5tWJzeVBnU5EocWahKoqAWYgtNNJkUD4G4je%2FqIRfEEc5dH8fx8js95tn5NClvXT%2BjJB65stEAEwoFmU7uB13QALsgE79z2877%2BcdGv%2FJn9%2F6P%2B%2F2WoDroLUXCNmdXpAVE%2B9Zlm0yxiE7RlXza6O%2FFaqqDAtmy5g0aFRot15Yh8tZCKFVNKtiU6XLC6qq5KPXuHOqqpo0VUVXPTcZTcc0Vcq4XCkLhVVuqltThejwwn7yIVC2FnzcgsIaKxdsEDlHb9%2BgAw7s4wQ7lDW7QDNqUFflJcBITiJPa5WNXM1qrfJKbBaoRFeDdkoYDde2vV0%2Bnmx%2BINY4KUgkeWGgrvDG4%2BWfD80biy57R3mjOixXePpI3lgMJysoMde9nGD7zPaJQpgtEKbKw7QOw9BkmpvRHukt2b8hO%2BPN5nzrFHd6DGQ2Y%2BuYJOTTqE9gE9gXUVf0r7tBn89NVJaNQs3KaOhlUOf0unPymiq3W%2FpczMqlNuW1dhh3jGoeTiNnRbl452r6nmqb72eFqZAoxpzYmcJEqHup0THe1pQKoY895pYpz16jt%2BdZnajpK1KCytzjoC3KMTqectQocCndIiYVy0dpgSaRRXE8m8Rfwaqi06dyJKMJjzLjTEtNZCthNdOTLyodsCJNhQPOHFeiv9qQvu1vrC0rQSy97WajuRhXFen7WS%2FIE12%2Feo2SSaLdO4BQUkf9N0cpkzLWeKp0T8YaZVeOoAb6dPJKl4y0XF6lmO05QZkjUCfYfhrY8CPC75z8CKlcNMXSOEn14v7NCVn7yxj%2FpmRtlltm5WoGBKF3H2uOCvIrU%2FO8pFAvVYM%2Bs6RmQ1SIWFRNRpUqSOIkRpZ9yRJLZDolT23NZIsZhr5rT%2Fh0SYqGCHMJOabtR9OMmCnbduRlgAXxc8uChx3dPT%2BebyrvDZhAhpWCf5zdYgLKy4NKCM%2BeaHaqoKQwCQBkFYUJyWVj0LHlmpSyzNkveSOBFvoSCT2n701PpfQrNV0HnkchY8KFq7kU1LgqxjOF%2BwL%2Fc312g%2F%2B0fd8EFtCJJ5cNlkuow3hu2ryPYZHDcqG%2B3pBn%2BDoyyEaHcnlNpoc20Qq79RV6u%2B7ankfG5VK5hZYPZd9ILYqz1iw%2F8KMrigzEfc0IH5K4Iw3NHvA%2FYOGPGlBDNGQTzo2WGcFHFIQCpuYUXZ1YIdQYtJZA87cuUcMgFHDhYsu63c17UR8X09ZhMDzug5dAxD2jwL13EDylxlDA37wUWqLGNF3Yw0OOzyDGJIUj8ofFtDpRLlEFM%2BCnrCDZpCDhgixgBsqs%2FFkd69Vs24%2FKq2i%2B679tA4uiX%2F4P). Specify the desired analysis details for your data in the *essential.vars.groovy* file (see below) and run the pipeline *chipseq.pipeline.groovy* as described [here](https://gitlab.rlp.net/imbforge/NGSpipe2go/-/blob/master/README.md). A markdown file *ChIPreport.Rmd* will be generated in the output reports folder after running the pipeline. Subsequently, the *ChIPreport.Rmd* file can be converted to a final html report using the *knitr* R-package.\r\n\r\n\r\n### The pipelines includes\r\n- raw data quality control with FastQC, BamQC and MultiQC\r\n- mapping reads or read pairs to the reference genome using bowtie2 (default) or bowtie1\r\n- filter out multimapping reads from bowtie2 output with samtools (optional)\r\n- identify and remove duplicate reads with Picard MarkDuplicates (optional) \r\n- generation of bigWig tracks for visualisation of alignment with deeptools bamCoverage. For single end design, reads are extended to the average fragment size\r\n- characterization of insert size using Picard CollectInsertSizeMetrics (for paired end libraries only)\r\n- characterize library complexity by PCR Bottleneck Coefficient using the GenomicAlignments R-package (for single read libraries only) \r\n- characterize phantom peaks by cross correlation analysis using the spp R-package (for single read libraries only)\r\n- peak calling of IP samples vs. corresponding input controls using MACS2\r\n- peak annotation using the ChIPseeker R-package (optional)\r\n- differential binding analysis using the diffbind R-package (optional). For this, input peak files must be given in *NGSpipe2go/tools/diffbind/targets_diffbind.txt* and contrasts of interest in *NGSpipe2go/tools/diffbind/contrasts_diffbind.txt* (see below)\r\n\r\n\r\n### Pipeline-specific parameter settings\r\n- targets.txt: tab-separated txt-file giving information about the analysed samples. The following columns are required: \r\n  - IP: bam file name of IP sample\r\n  - IPname: IP sample name to be used in plots and tables \r\n  - INPUT: bam file name of corresponding input control sample\r\n  - INPUTname: input sample name to be used in plots and tables \r\n  - group: variable for sample grouping (e.g. by condition)\r\n\r\n- essential.vars.groovy: essential parameter describing the experiment including: \r\n  - ESSENTIAL_PROJECT: your project folder name\r\n  - ESSENTIAL_BOWTIE_REF: full path to bowtie2 indexed reference genome (bowtie1 indexed reference genome if bowtie1 is selected as mapper)\r\n  - ESSENTIAL_BOWTIE_GENOME: full path to the reference genome FASTA file\r\n  - ESSENTIAL_BSGENOME: Bioconductor genome sequence annotation package\r\n  - ESSENTIAL_TXDB: Bioconductor transcript-related annotation package\r\n  - ESSENTIAL_ANNODB: Bioconductor genome annotation package\r\n  - ESSENTIAL_BLACKLIST: files with problematic 'blacklist regions' to be excluded from analysis (optional)\r\n  - ESSENTIAL_PAIRED: either paired end (\"yes\") or single read (\"no\") design\r\n  - ESSENTIAL_READLEN: read length of library\r\n  - ESSENTIAL_FRAGLEN: mean length of library inserts and also minimum peak size called by MACS2\r\n  - ESSENTIAL_THREADS: number of threads for parallel tasks\r\n  - ESSENTIAL_USE_BOWTIE1: if true use bowtie1 for read mapping, otherwise bowtie2 by default\r\n\r\n- additional (more specialized) parameter can be given in the var.groovy-files of the individual pipeline modules \r\n\r\nIf differential binding analysis is selected it is required additionally:\r\n\r\n- contrasts_diffbind.txt: indicate intended group comparisions for differential binding analysis, e.g. *KOvsWT=(KO-WT)* if targets.txt contains the groups *KO* and *WT*. Give 1 contrast per line.  \r\n- targets_diffbind.txt: \r\n  - SampleID: IP sample name (as IPname in targets.txt)\r\n  - Condition: variable for sample grouping (as group in targets.txt)\r\n  - Replicate: number of replicate\r\n  - bamReads: bam file name of IP sample (as IP in targets.txt but with path relative to project directory)\r\n  - ControlID: input sample name (as INPUTname in targets.txt)\r\n  - bamControl: bam file name of corresponding input control sample (as INPUT in targets.txt but with path relative to project directory)\r\n  - Peaks: peak file name opbatined from peak caller (path relative to project directory)\r\n  - PeakCaller: name of peak caller (e.g. macs)\r\n\r\n## Programs required\r\n- Bedtools\r\n- Bowtie2\r\n- deepTools\r\n- encodeChIPqc (provided by another project from imbforge)\r\n- FastQC\r\n- MACS2\r\n- MultiQC\r\n- Picard\r\n- R with packages ChIPSeeker, diffbind, GenomicAlignments, spp and genome annotation packages\r\n- Samtools\r\n- UCSC utilities\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "59",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/59?version=1",
        "name": "ChIP-seq",
        "number_of_steps": 0,
        "projects": [
            "IMBforge"
        ],
        "source": "WorkflowHub",
        "tags": [
            "chip-seq",
            "bpipe",
            "groovy"
        ],
        "tools": [],
        "type": "Bpipe",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-10-07",
        "creators": [
            "Sergi Sayols"
        ],
        "description": "# DNA-Seq pipeline\r\nHere we provide the tools to perform paired end or single read DNA-Seq analysis including raw data quality control, read mapping, variant calling and variant filtering. \r\n\r\n\r\n## Pipeline Workflow\r\nAll analysis steps are illustrated in the pipeline [flowchart](https://www.draw.io/?lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1&title=NGSpipe2go_DNAseq_pipeline.html#R7R1bd5s489f4nObBPoDvj3ESO%2BnXZpuk3Wz3pUcG2WaDgQJ2Lr%2F%2B0wgJIxAY29jGabZ7TowAIc2MRnNXrXkxfxl5yJ19dQxs1TTFeKk1L2uaprY0rQb%2FK8Zr2NLtdsOGqWca7KFVw4P5hlmjwloXpoF94cHAcazAdMVG3bFtrAdCG%2FI851l8bOJY4lddNMWphgcdWenWR9MIZqxV7fRXN66xOZ2xT%2Fc0Nr8x0p%2BmnrOw2fdsx8bhnTni3bA5%2BjNkOM%2BxpuZVrXnhOU4Q%2Fpq%2FXGALwMohFr43zLgbDdnDdlDkhaV%2Be%2FfwuPi8uHz79%2FvN9%2BFPf%2FlvXeUYWCJrwYBR0zoW6XFgmEsAr2VObXqj83sBYx14FA7RJfk1ZX%2Fpa2Mv2ULGRPvirRQawSsH%2FiyYW%2BSXSu5ZaIytQQTTC8dyPPpQc0j%2FI4%2F4gec8RVgiUBxMHDtgJKV2YNzIn2GD9Uj7ia4mpmXFOr3qwL%2BoU36HIrE5mHrIMAlwE826Mzd1cqnAIxbyffY7wq8STTKOHIavJfYC%2FBJrYsgaYWeOA%2B%2BVPMLuaj32CltTmsJQ9byi0JbCCHQWI84OfxCxVTGN%2Bl6RB%2FnBKEROLf9i21w%2BXM8nj8355L63%2FPEjqNc7Kdxhg6wjdul4wcyZOjayrlatMQQAXP5bzF3%2B%2FBS5pGX11hfHcRme%2FsNB8MpwihaBQ5piVIJfzOAf6LDRZlc%2FWffw%2B%2FIlfvHKL2wCgNhLcPmT9wcXq9foFX8vTW4hDGDimQuRNfnOwtPZU%2F3P6rBl%2FfwyuhtMHr%2BPTHXx2Ku3GatD3hQHOXBvygnFwxYKzKU4jl2Qfq18nsxue4G1nN2Pb68fhr%2FerusfHGIvHMJzAoI8BwBX75fEMnrtrsAy6n0txTLUnpZmGS1F2516rkZfR%2BdB%2F9v8WRuN0C%2F%2F4n%2Bvn%2BvdnVlGaQyC35EziKyFvmZdS3fVZj%2B9sKUPthQpNznAUn97DC6%2FK%2F%2F8%2B3l4f%2BXNVeP76PUm5DOngKwsbr7i35tx862QXBDHzValcNxW3zuSsW2cg05ALnXgvsCLoXFoWnw4aToQtoQDEEXzWESRN2zJFg9bp0AtfFOHG3Wf4vucPKC23JecHT9quNBq54PL2%2FP6A%2F4NMDZdbJlk1yTtg7g4MC4kNISjE5tDoWTHAe827TrFrBmgMXk7NkmiIOOJaZt03ycosddMpRzg%2By6Cr%2BlMElk9qPt%2BPfAWto4CGFv8ss4oWNKpEht02HUhtFRyLmGnCKQhD0%2BEDmdBAMaHc1hY2nBqBkTybHiW27Chq6E5H08c0i35eTt6AAxrU4dc1IF2LWcMEi1egplkyNHvk9%2BE8n0g%2FKFhI%2FKjwe81iDjrLF%2FTY8x8jsMaSeC%2FAVqSknel1pDlIMMHKwtMH2wqjrGwAI4Ksg1YY75POLSJrJBbI8IMsXfKE8YvWF8EdIYxpkEIDbo17SllJ598fEb%2B0EYXmR6moKAA%2BeTSWwb2QSXbBhDlLNPUJ7MVulIsJYqg9nQ6aUMJ12LjWk9k%2FCp9P21J9tMECIgy6MJPoiy6RG2EyYcyFJdKerwhMjy2pODK3dALw1BNgUyTQIy3FZZK2Pe%2BOSYlK%2FaxutYWENYVO3AmEx%2BYbAIJ0aC3x4vWO6bwy3%2F%2FFAThdcKvIPquJOEDaDicatdLswcSZn%2F42Ptr%2FB94AMjmAJYgUVh1pZxrToZPRC3KthTgWqG1RQnb6wFgEe51YvfIqgnqzLAF93QCPLq1rBNwv63YtmxHKibWusm2WeqpraZq4UkQ3uzJuXdcvJMJEceQlKJNvrFEnp8pKGU9toWclAL%2FfoAd9m3yBtdzlqaBC4g1Zh4RDSI56Za8KACDDibnu%2BFrdd%2FFujkhWrPwdeVT3giyGxw3aMjePCs6IRlatLWMXotxILYzxJg%2BN1mHOBo4ZJ%2BcWNRiMKFGgjj3fp6ZAX5wEWWVzx51WpQhtCS9O7IdWCa0KK2N%2BSa5jLHOTfZLmWHgoIIM%2F2BlJZmWgMYEbkqRZPqfvd7T01B%2Fsr4P6uhypGL1S11tv3dJRrDIlWq4LWyk61REromkhiHyg7sLysiLiBS%2FF4TRBTBCnWhjngNU5UxgwOiZDpuq1zBYsus%2BcdWsrN080tbhSdAIyeB%2F642wOdqg17LyZtz7mWblMvaTYO9km9eJ8vyFXl02OxlMnXoJWceKSPcl8Ps2d9Vzht9qphiV2pFwqs7mSmoRfi%2FlKmnu%2Fs58A6XykVZBPtKqHB8ZPJ4XZiKCiSxsuieMgwq6rkvWVUyoXO%2FZT%2FX1yceRMQufZb22fx4V6RvjZ7Q5j2odiEeVbDjj8QGcJ%2FW6R%2BVJubuwxDm1Xi%2BjVkqmnSlb6Pd%2FLQJ3AasJQgrJA6tFE1sxym89bykdT6EPyRiWVzQNM7SfhyZlGLfyiQkV3Kg%2BOv%2F%2BPwpuz0RU1MUAeRqrcpa7zhP6WYz%2BgSJNHVnnjOapySW5AtJKWDxsh4fxDNHctIBcr7G1xNDrQbS1bqsnrJRmP21jlqpr3BZdvsu2W7FloaRWhof9hRX41VwbS30SWw%2FPJsFiRPTQQsjVop6V8etqUVwj13KCVxdfwF1qFbm%2BOIuWzg%2FbnJjYGGGbPkTv%2Fxid5U12H1P7%2B%2B7hy1%2BX0LHueNj%2FWLRcOe92Gu1iy7YfPVn%2Bwu1VfuFazrSiq5aMrM7XrO6QNagHdJGG%2BxlGOqxivu1Vi8j3QNDtfkFy3kJeKxbeyT2o1TdDlan58bDNtZpf%2F1CaX0G3dHYod1VEaQ%2B7hCAqyn1EB9menWGQAAWKaQSRoT8z7ddfYUTQr7CZqIy%2F4EFyl8kuYXvjfm5IZB7JI5n%2BsaRTqdrTbtCnyN3GDFtE9PIb95nzlz%2Bb5yg8iNj2jYyIMKNoEcBS8Zx5TYwFgs2vUZP48b46hjkBXkLxSh9cCbb6wg9oV4jw2Vff9Gsx594qoCogiPVnZH36IfvRFBtjYNU5vrqq7K8HEiI73X6xPVfVSth0pVbajzD9TfbqTnqvlnvUDrRVFzbS3s8vF4ypFrHTfkXeE%2BWbc7KeSNfGwrVMiH81agd39Xhz8nV%2FYzNqiKuTN6M2O12Jnnl05456VL5x0i7jdlGBv3ou479DmeeKyPwbuHzawEDGD7ffahpYhqfMokW1blt3PAPZBGD0Hg70xiE4S%2BSgYVIcmKU35jBtpXaKHCbpPG42laNzGEk%2B88lH1zY3FkzSCe07iCqFHcq9Y5kVpGjvf6C9bLTnlQs4PNalRQlaO2O91GIQIh1o2xGCtm14%2FablIHZb%2F2F%2B%2BQ6Ip6%2Beex56jT3gQmijH%2Bs5EevY4lsJtz4z1WpY8Pmm2k0QXTiCgnGRxYNbkI%2FvsU727bHHSiwUlHtWb4GkA%2F2QP3c0iM6kLgfI2FQG519rkefwgHKPF5%2FT5rrVqYaoiHHS7WY6vveQcs%2B4uzBGzu0%2FLp7X7%2B77QV8fdervPha3TF2qV5jLHWu7yx32Vr6Tvacd3eMJJqgAnUyB8IM5LqrpVcOnAgGJwF4N%2FELtVVNhDklXBIvGuMeM2DRlTBZTmBxLWGKDKq%2B%2B7TaW%2BuQs%2FPaT7TzbtXhYE2Pgm0DkwHlAzIRvOTpl%2BdEGtC7Vaat0moplx4gWtbYkq0PtS2qfqWp%2FX%2Bu%2FLVn%2FCcDtNxUm4kB%2FbiqMVBFR04j4KEu3ix7Sl7hMpJDfVQ3ZaUH2s%2BOCs6o5jJ0XKGEAMeuUIY8dz8BenTRHTJutFVriALx67ehOvNRnGDXVxC34Fz3hIsOI%2BtYK1cMRPfKUmPIjHyTB9LzMTHq%2B2ZNJVnPA8%2FhgidxhWUzcFzpUUk9Fc449pWVPPBg7xmuqMbU9BQZvCUEiBjDDNk%2BomqAujP0KneSS4Abu%2FX6yzcA7W11PTJsm10YBqmFWFKJEJ0uaCr3jMd%2B5H2DXj%2Be1rkacnkPuNpx%2BDBrTEMmB0Yb9%2FyEDg0YZtUF7uGLKjVnYv1DU7ybqnPRbaamoJ9nh%2ByVEIsiLyZ1MecdtNd%2Btisltv%2FWqqqQm7Alox9G4jxJaeGMXiSxEzwYK0Jr99UhaMM2NXSmlyqc303WxcfDUgZQ2aqN5mOJHNsMlaJ20ct0kXXxKsLGuCRI74Ph9NHctrDbu1QaFcWP6Vu1xaluMs5wN7EhB7alCGBKlVVoHY1%2FFu3j13spESacTNCDzVojKrBArG6N5jJFRwZrlZYDQDLwtyhuG8JEoTI3uVXMH4lTojRs7PGXiHlOwzYHC6A3mlxHcIR%2BZidH6KZoT0uvtvn7k9phjyoSiN6R7WHdIOdYXfgBKseLhRwwto2H09zkJGzFBLFqWkcled4xVcUvCWp6M0ELPg98P6Fnl%2BQAwkY0dq%2F34%2Bjwlx6poYpcqkwd0rMrPnsjciVMb3m6FS7%2FgKa2gGhFxRsXUceZOAhu8iCTpkRHx8yVYE6cGJhI0B8lNaW4aBuWSMqoQOWcpG4p4iIQmqaYq20%2FKSHeQEsExE93NULMcOgmxrJKO1KpLQCUQZ9IEpvLA2DjXakmos1VCHQYpdWZnc0dK201AwKnHt8iVPkfxufBprQBGbFHO1bvlMD0xmllt94ohcW8sJq3xXYapBFQEYaFfBldTZB4C6hB4zygTnbtqpy8rpSBFWgkeeWnU1e4BqO8z6koKrKJKxUcp50jEi5VyJut8Yk4XHo%2FDqaLd5aBZ2iFAoB0ea4SXmbWZ5Q8dPeH6OBBzPYzmY8jizgKX5IkKwCoh1IjAk9moj1EsnFYPyARs4m62uJwCc3Z9a3qiD820d7wnOLCEgtRGUyrSrbHhb1BHe32Dx8Ih%2FQ2%2Bm1vUe7MBEkCghQXs3GQjYND3rYUnhJeAEdcEu7DtxMVdxV%2F%2FEUwL0tFTUtgRjbxbiJK0BJQXmteqeJfsbrI2Omy%2BfHFGL4aFKIQwzU%2FUqPWEqb17BRifiBamPfXPpBXRs9W1XDEo3%2FxUgbCFhIOnyesvr%2FXwdDYXK4pYmqRQ1CRqXEK6LC26M09A%2BwjuFJ0JaTScmJDf3VzKz0ou3NS9IKWzXlFFoHccwT%2BSqFl5twHs81%2BgIF3RLC7dcV%2FpDP0nypahlh1h0rVYFCFr5Ma8%2Fbsa5qYP0ZLIxs7Cp9IgnRwVjCOXgyZ1OmRj8eR8Dm1VtN51Osf1OUhL2h25us5GPIdcfMOeGR7RkuBDqsiHtk5t3craUDSKjTs7Dx%2FGJsX97gfcVwP3O%2Be3l4v6vI2oPMxvl9ScyC%2FtsOPRt0xSLiZsSoMls6TNhWe9DjykPwE01zFp8TD71BnzGyC6OFdv9UQQttqSKvKSXK0mP4m%2B9AJpR7UMV7hAWl7ds%2FXhJkc7x9xEw7vl893NTXPYt97uH2%2FsN62EnOs9lp84cPWJfaKdR0Mem08nikm0WJ5D1YpP0KjFeMhi8doToYSeqC%2BhIPgij4V0Hd9kNqb96yyxwhPRdPKio7LJ7OQUlWRwVIsXgKtStS21QoGWlXKA5hVPWs%2Fvugfa5jat4AdHTvwYFeYnF2E2Y%2BxEi4UfZnKkT6k4IC9h49HpZPK5SRYST46bJKv3taXBE8fnJ39mLf%2FCBtEqM4br4odCZjOG1Ok2x%2BEL1xebF7c6UXtogjG0%2Bq0jMwZpdFz6EGBqt66tvKJZtouN4F0CQLs9MRmmIxHbZF6y1ubQLAa6E6xRwZLjh2GlhZwgiT%2BtFEUuMP7YMguVHVjtePUfSuBk%2FY64NXTSWRjSEOlma3dOJj%2FpKTsXZ328WSeZhhFbuhywsVAeALFs3aI5YMIe%2B%2FDHjYdn8n0m8Uz6WsklLWno5gYjTFBSBYJvkjuiPGFCk2yJJSRMyAlJkxBSAQFiq1CbfFL%2Bc2NtpNVctHzXN7bGzvPRdb8SjedlFvEufBh4Re3pzbaSIKnd%2FJ5Sn83u7vU4ylimzOnR3fZUJjvM6KSorN%2BLU9m%2BvTbFQgx3r2H1DspHCkfnSGlnLcVKgdssSLDCwSwHP8tg94NLKudMPlQNi52wHtpUD29CZcUCJkgXdZZV4vL69DDeENaxkBYj2KR%2FZsx9gCO7a1D1Jn6Awdq6BMIgsizByiYHg4sfOrwl%2BBcd46%2FChx5k02HJZuFCOltKtZCso%2BJmYVU7slk4u%2Fpvjm0zrTZ7dKCbVS4QrIwJJh1DBV3kg8j8ybOpa1pzSP%2BTcrx4EYNOchMUQ%2BniKdqk06sO%2FKtl5HNPPWSYBNmJZkJnUD6Sst8pFJPkrHiGDGoVUGpCzJ5SDjklaybw0BmxbKjEBKB223vab3OMSVm0k2kUom8trGSLtcqLQ8%2Bkx7D2ozR3H7xe8aReK5WSFesMsww%2FVsKMkCNBlx%2BI9R3yevg7OoEAnF20k5z38ibGjkFI%2B%2FMLjYO9nXL65bxMGvPGQ3rQF1ZYIyG11bBCT5MFEYtoaiRZUQH2QiDSKnErj2QOOygwo%2FiJh%2FEDH9KYKvgF6bTXsKbdKz%2BwHWjnug%2B78Y5WU%2BQd%2FXRsr9QWpm7hUoNQM8cJ4todmdbsqwPBb82r%2FwM%3D). In case of paired end reads, corresponding fastq files should be named using *.R1.fastq.gz* and *.R2.fastq.gz* suffixes. Specify the desired analysis details for your data in the *essential.vars.groovy* file (see below) and run the pipeline *dnaseq.pipeline.groovy* as described [here](https://gitlab.rlp.net/imbforge/NGSpipe2go/-/blob/master/README.md). A markdown file *variantreport.Rmd* will be generated in the output reports folder after running the pipeline. Subsequently, the *variantreport.Rmd* file can be converted to a final html report using the *knitr* R-package.\r\nGATK requires chromosomes in bam files to be karyotypically ordered. Best you use an ordered genome fasta file as reference for the pipeline (assigned in *essential.vars.groovy*, see below).\r\n\r\n\r\n### The pipelines includes\r\n- quality control of rawdata with FastQC\r\n- Read mapping to the reference genome using BWA\r\n- identify and remove duplicate reads with Picard MarkDuplicates\r\n- Realign BAM files at Indel positions using GATK\r\n- Recalibrate Base Qualities in BAM files using GATK\r\n- Variant calling using GATK UnifiedGenotyper and GATK HaplotypeCaller\r\n- Calculate VQSLOD scores for further filtering variants using GATK VariantRecalibrator and ApplyRecalibration\r\n- Calculate the basic properties of variants as triplets for \"all\", \"known\" ,\"novel\" variants in comparison to dbSNP using GATK VariantEval\r\n\r\n\r\n### Pipeline parameter settings\r\n- essential.vars.groovy: essential parameter describing the experiment including: \r\n  - ESSENTIAL_PROJECT: your project folder name\r\n  - ESSENTIAL_BWA_REF: path to BWA indexed reference genome\r\n  - ESSENTIAL_CALL_REGION: bath to bed file containing region s to limit variant calling to (optional)\r\n  - ESSENTIAL_PAIRED: either paired end (\"yes\") or single end (\"no\") design\r\n  - ESSENTIAL_KNOWN_VARIANTS: dbSNP from GATK resource bundle (crucial for BaseQualityRecalibration step)\r\n  - ESSENTIAL_HAPMAP_VARIANTS: variants provided by the GATK bundle (essential for Variant Score Recalibration)\r\n  - ESSENTIAL_OMNI_VARIANTS: variants provided by the GATK bundle (essential for Variant Score Recalibration)\r\n  - ESSENTIAL_MILLS_VARIANTS: variants provided by the GATK bundle (essential for Variant Score Recalibration)\r\n  - ESSENTIAL_THOUSAND_GENOMES_VARIANTS: variants provided by the GATK bundle (essential for Variant Score Recalibration)\r\n  - ESSENTIAL_THREADS: number of threads for parallel tasks\r\n- additional (more specialized) parameter can be given in the var.groovy-files of the individual pipeline modules \r\n\r\n\r\n## Programs required\r\n- Bedtools\r\n- BWA\r\n- FastQC\r\n- GATK\r\n- Picard\r\n- Samtools\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "60",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/60?version=1",
        "name": "DNA-seq",
        "number_of_steps": 0,
        "projects": [
            "IMBforge"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dna-seq",
            "gatk3",
            "bpipe",
            "groovy"
        ],
        "tools": [],
        "type": "Bpipe",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-10-07",
        "creators": [
            "Sergi Sayols"
        ],
        "description": "# scRNA-Seq pipelines\r\n\r\nHere we forge the tools to analyze single cell RNA-Seq experiments. The analysis workflow is based on the Bioconductor packages [*scater*](https://bioconductor.org/packages/devel/bioc/vignettes/scater/inst/doc/overview.html) and [*scran*](https://bioconductor.org/packages/devel/bioc/vignettes/scran/inst/doc/scran.html) as well as the Bioconductor workflows by Lun ATL, McCarthy DJ, & Marioni JC [*A step-by-step workflow for low-level analysis of single-cell RNA-seq data.*](http://doi.org/10.12688/f1000research.9501.1) F1000Res. 2016 Aug 31 [revised 2016 Oct 31];5:2122 and Amezquita RA, Lun ATL et al. [*Orchestrating Single-Cell Analysis with Bioconductor*](https://osca.bioconductor.org/index.html) Nat Methods. 2020 Feb;17(2):137-145.\r\n\r\n## Implemented protocols\r\n - MARS-Seq (massively parallel single-cell RNA-sequencing): The protocol is based on the publications of Jaitin DA, et al. (2014). *Massively parallel single-cell RNA-seq for marker-free decomposition of tissues into cell types.* Science (New York, N.Y.), 343(6172), 776\u2013779. https://doi.org/10.1126/science.1247651 and Keren-Shaul H., et al. (2019). *MARS-seq2.0: an experimental and analytical pipeline for indexed sorting combined with single-cell RNA sequencing.* Nature Protocols. https://doi.org/10.1038/s41596-019-0164-4. The MARS-Seq library preparation protocol is given [here](https://github.com/imbforge/NGSpipe2go/blob/master/resources/MARS-Seq_protocol_Step-by-Step_MML.pdf). The sequencing reads are demultiplexed according to the respective pool barcodes before they are used as input for the analysis pipeline.  \r\n- Smart-seq2: Libraries are generated using the [Smart-seq2 kit](http://www.nature.com/nmeth/journal/v10/n11/full/nmeth.2639.html). \r\n\r\n## Pipeline Workflow\r\nAll analysis steps are illustrated in the pipeline [flowchart](https://www.draw.io/?lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1&title=scRNA-Seq#R7R3ZcpvK8mtUlTxIxSIh6dF2oiyVODm2Uzk5LykEI4kYAWGxrXz9nZ6FdUBIQoDjm%2BTeIwYYZqZ7eu%2BegXq1fXrn697ms2sie6BI5tNAfTNQFFmdTPB%2FoGVHW6bKjDasfctkDyUNt9YfxBol1hpZJgoyD4aua4eWl200XMdBRphp033ffcw%2BtnLt7Fc9fY0KDbeGbhdbv1tmuGGtsjZPbrxH1nrDPj1TpvTGUjfu174bOex7jusgemer827YHIONbrqPqSb17UC98l03pL%2B2T1fIhmXlK0bfW5TcjYfsIyes88J76eNqcz0L7YfNzfL6%2Fe3i55%2F3Qw6AB92O2FoMFM3GHV6a1gOsrm2tHXJD%2Bx3BUC99sgzxJf61Zv8lry39fAseEumLt5LFCHd87Tfh1sa%2FZHzP1pfIvoyX9Mq1XZ88pC7IH%2FxIEPrufQwkvIiXK9cJGUbJGoxbDzbIZD2SfuKrlWXbqU4R%2BRN3yu8QGKqXhru1DHwp4d9rWw8C9juGo0T6D%2FXQcmGFhnMpnl4aKgxQD8gP0VOqiUHpHXK3KPR3%2BBF2dzbR6CtsMw1nDAsfU6g5ZW2bFFaOJZVtCbYd1nHfCV7gHww1xGjyYFz%2Fc%2Fs9%2Bhi9%2BfPf3Ye7xY%2Fg4b%2Bh%2FKLx5K0Gf8vwZO3rpoWhfQz6NIAtykzKYEtMU1PYMpYE2KLxB0%2FBlj%2Ffwzd30r%2F%2FfVzcvPW3snn3bvdhOJELwEMmprfs0vXDjbt2Hd1%2Bm7SmIAALkzzzyXU9BpZfKAx3DIR6FLq4KYUU6MkK%2F4XXRxN29SN1580T65lc7PiFg%2Bebegkuf6TvJa%2BRq%2BQ98wLYDr40ALYAaWhcWDYfThEBMwhXCvnAjXwDVexFtsdD3V%2BjsOo59iAsfSUi%2BcjGNOwhywpFSEFexfPWd6kHPNdywiDV81doSPBzPJtk8VMZZ9lU7vnJbFb1PP5BR5CgZzyVWhj75e7L%2Fa%2FN7Ofvf%2B9CZfHz%2BrusLYez54KwRaw6PyKNZ00j0km8SN3DiRIgctYDlH8YEDBc4AdkzXsq50vwcK1exsJerpTBxeVWtxwAiuUh28LcADdeptkc%2FcY%2Bflh7JJXzCTzdybfpwAp8tMr0uglDkLYvADrKYm2FmMuOfNsbORg7lIW1Xa5cjCn45%2FW7W5iasnbxxRD%2Fb2m7S%2BDe6AH0ggWfN6YKi8C4ub4I0G%2F4udX9EP8c8fsjzL7dh1167JLBWGXSiFF%2BGPqRY%2Bgh3m2FeVb0Gq%2FrIlkAKfUvhocuEEVE61YCurzg0gU2VvbSDbwxYILGwV3e6UnQbh6sYvLQCJEZEmJshfoSv50iM1j%2FRivLsYhCokivlnDndaXA%2Fezmbbu6GRBeE3jIINwGa%2BEwUdC5XTOyAQ2f%2F%2FwQxnIntHRgh7pjxpMbwrStFRY4CdPXMU9F%2FlETbgAHn5ARhbDeKRwsGUm5MncGHUgTqEBc20mrQPK4ARVILDMJhJTcEmBF0IOfWFH0sMoIk6cCI5fuZrwhtjyNhctVKSXVXkO5sGSKYMV422E6Q0HIHypZpWCa7cBdrQJgQjkgHCboiw0Zky4lff77R0bq36eaDtKKaaKnlqimTWoIslJX12xJQ%2FgWIP%2FL8heYgDGpBCtQPQ1g6T4BRbOcNaVpS9c3kT%2FEzSAaEEmBIT6heBLG8kl8J23jvSCihIrG8Dd%2BwtNNM%2B5b6Z8kRuEXjMKnjEHucOkr29FJEpeAI3GBpgi%2FcuDkmRXaZmeIUQdL8gaBTqpDqfBUDMPUU0o5IMOla%2B4KjQURKzRjPUXfejYaVAljycPF13mLxRsMSu14t5KDRYFUX1at%2FqGxOOaKWSx133DN5qcR9yu98tHvyPIB%2BzBwcf%2FS54ub2%2BEtIHL57GoO5OD5eq5rNz5Z2qn04U3ldF93Ml9MnxuELuuOTdWJtkvkdzIvsNU2PivSqfTKxry%2Fo2kZTWInnxbuNNo6HQOMEO7m5iZhfWkhk%2F%2BDn9IrA%2FMNn2zDR%2FhSJ3MEocJrbo6sOwn8EWtni5ywi0mNRqPmpqQ7IDW64QZ0XMnUQ%2F1cU4JGEUsn0hMRS9KabLWOoaSEX6aUpPQNUMksQ7cvmMc0BCXikvtPbbSCz7j4qZVNHEwr4lNKqxOPGytEt1iqgR4ffd2Lx3WaFq1kdDJ5pglUQpFOKM8PFuTxZUqWP0CBUzp11Tw3BY6D9NkocJ5Q%2FN%2Fi4VsOlfAlL9HZaPuQ7CC4p6XuYaQPh2xXXRAZ36GWsrJ4BN7wNTGnigxsy1pUx8u3bQpPHTVVSh7g5kysZl6mNK9ulM3Ycjl60P2gEaN%2FWZcnKaAFEJ0HIHkhy3cfLKLopC28IkSrZG%2BXsUH4Gr%2BYWRZmoyj9Ln0N5iq0JEuvXC8cHS4PpZZzL4dUqjnkqbyQKfyfyOuNxdlkueNYntbjjvIRQVnHc0eRc7xVuzP%2FYG8Nz%2BMsGM9geBZGmMjqy5Nbmgp4qh3xpPVEiollhIUehP9cEZJcR4D4HWHSF%2B6IDs7VU3cFA9YfybCJbxAGi%2Fnu%2FYB50nL8fBMtR3hvV%2FByxse3eHBA8Rex2xQeA88dvvHbGNHmmNvupeoUTqVUXUR3cpQ%2BS7hVrYS%2Bk0hO1rGURfgGCP1EygWgjYuUXtYEJEo73JlYh9ALQyy5ylVCTpC9dB9boCRKNSk5H0kY1yQJE6lpknBkEGTWQT3WpBxWnBbUKMSReDVfNo40DPpiwKqcg62a82ZTJGVvJUA%2FFYfU6bRRHBKKLT1yl%2B%2FBozKxRe1MbKlLo8Z9CdTO0ygljV%2F78XE8OT8%2Bys8mtaDRSO3aqNSWBFyAlqb52q%2FVxtve6xvVfHKt%2B2t1qPWHehzKhFoAljrvxb6fqrl9n%2BUr%2B59vOEHj9uHi8Z186%2F78cjn%2F8k19%2BrrZ3g97pD3%2FvXxo2gt8PJUPTWYn8aHauv3t3cVNbc3%2BBqvuxGjseRBkdE4dPsm3wBA%2FXIkfpyBzTiW%2B4QjguZxFmo6VdvHaMmk6ZZ0lfhMa3CR9TfwpOVoHvqzUQjNXVtYQAguZdy1vLdMkpDAHk6Tlhq2EOijJbiUJzsQBTskU2714WJNL%2FA%2Bv3hUw1wke5hW%2BlpNr%2FA8e98Mr18Fd6xYBLsII%2B4iCcFBMyG0ADdQZfDxrvSnggdBMr44PxoN6MtALlVi1ukyncQPNacmFWmGL1ncPkkQJ5iSUjnBFf4lCLwK2A%2FUrYINfFJgOvvnbqGI53fmeY7%2BiG0%2FDIhyJxTUm467jPzw9YCZNXnj%2B%2F0LfWjbg%2B3tkPyDoddBGZM2Uc6MKk7KQKh3BnWri%2BbRneC4VUN1HQWSHQT%2BR3cAEOo3i4DS3gELbOwJWDFVFirYWVLGh6WpE%2FjPR0Iw8G%2BMdPF2dmtdqfMKL35Iaz3XZuyUPD3aruSVnvd%2BStrvu6X7EIxvyvWi4to0l81RIPdIN2I%2BcQ%2FULwc%2BAzBO1Rf4ilnqVLqXerkx%2FXJjdK%2FXOO7PTVo67N6SnKPX6CFTJnlKfVsMumUixiFdkEWwsZ%2FczMHxHD9Dvn%2FQGVu5%2FhiTRaBEYI9o2utmap0Vi5rsqDbDMByv2eXHII%2FjGaINsD%2FnB6ObkRRJ2WViYfsWqfsXjxUQv3mywJX0wQqYrTBAmO0qNOI4O%2Feya1gpoFsEM8mAiChtREEJXmr4FRuosAyrHJZGnwLlpoYMQ40ewwcQgoLROkRyETJRGN1F2Yy%2BYeVvS6nxWj8HLSgMcXlzk7fREjF%2FR1uPPM3NjazGOcf029uGkeNtZnTFn8vUXnXS84h93zk%2FqBaU06B%2BxnBXy0ROmfhbJRavrKjGBGmwptQG7MaEJPFadRkeSmEjbWvo6Q%2BSz%2B1JykznYrTJuKzbyvLGQqtKtW%2BX%2B92P4aG23MlI%2Baab0dbwcXg6nz0XVODayOkN7yvSVHEURAL5xIqNpWa5UiHyj5LBAZIrUapyteKtMzxNCN82hMx9w6bjyocCnhdzVJp1BtASDoYJl1nDnoeAo2klMlKQ2E0nrwaQMZKudh9qhmPk5HE4yJ38FyRxLPQwf597O%2Fjslj63Q26hZp7Yzs%2FGQ45OMcMqzib%2FrQ650bSirPYmT4gSDG31Z2fjSOKnc89q4VWb2k7rMDgyYIi%2BRiCmw3mMNIGXVT9xrZ%2BRn6fq1dCKHMzLtWTKyfEjVRCnWg2iTkYnjwV9gOYimTBHTmvRuJveC3k3lrEljMt8jvOeeH8%2FboXdLfXsFxkRy5ExNamdgwkJqLy2t9Xdr3UaSJx6mwYZ5OEWbPkuKlhfNJ1q3ormQolUndv5fMhceTZCmYcJVbUswr00lLkzz2%2BcPl7Ts4R3esBLkiv%2FOBl6IiEa6Lr1uEh8J7mjAnSdJIUXmu7kh6wMfIGndBLN1k5gEiBn1Rq6MAcqWwW9H0MLTwrIdm0noknEfTqNm7dOo3LlBqwn8HRSPCML3luTvIB91je9o5A%2FsY%2Fh%2BCNtm0lQNk5xIpwhKfHVuzv3rJbr%2BmnOztVEm8tHmXDWLZi2Zc%2FmA%2B2bOXSMHXbrmDouFSm2R0OcKMBMlMbl%2BIFQbeoMPImcNrvU2bLnwSSiOiAejHEyIJ9KgdUJ8BmFxPOtWWBRb%2BP5631ej0mLtzNx%2BWvg0tZ8WPiysHWbcw9hHKtXS0DAeS8%2FKGxUE2cCzrTA8d9pkSv7k8zlc5uwgeTInJVaJmmXiabkIegaZU5O6lTkrTVy9CYAtxt6Hvm7c9zT%2BlVqtYMpskMxC%2F2AFUNEsoEfk9j7VpAFszwfqjQUmprNF4gstHz2qnNB%2FCWFWW0JoWkA4KX6fD%2Fso8nX2gso3CGLnHAN4%2BTvkuJlzSiqkhO7pGpVPSKEHDAG8JZ6I%2B3GdmUQ%2B0n2F9DDyc6fxkW%2Bsw1XbMd2FKO2kDjAL0LZdmowYkCkCTlcVWx4cWQ64weq%2BTcgkSoZIT%2BW60dRSA1RavIOLpSEKC3fewr0xDXm5hXvFKnaPHDIxzzzIISO3yEDngiAacYJgvzLg5uV1H3p%2FplwSaRMnZxFMqlYVXtr5a3RJssUpBuT8LbxT2flb1GOWy3HjKUz3mJXDWUj8GvN2Urs%2FrlNA%2FWk6QTZRJWdq19DxW7vACsg6Iy84%2FbScc53k8%2F%2BBnX5Q0HM492fOSxrE4lCxZpI8E7D22fR0aix0AT7%2F7PH97rs6x6%2Fy02rrqaNd8c54mJ1Yzz44dbLH9cfc4WE9sp7xuIy4lswfy%2FOQ%2BbpqZK3oio6%2BpaG3mIc9gE5IjpWnpj1PZ4dWIibC1M3NbVPXdV1bHt3IIxpBsv7T51EqR4zyWZtKc0q4ILq4ZR2c1%2BPtsRMA6odmEuF7RMaW%2BjYhYi%2FC1J9NWlTnNUtNnq%2FoTqdZXU3Y%2BqdtGvu5Y7KDGpWNhANMp4eFA0xnsxyKnSnhB2qf3LACLHVDAhi0QNag1mkiZWDKc2%2B6j84gqUkCE2oh14fVb4E5HBwGME%2Bjx%2FOJeMp76qfjolm8TU%2F9e%2BnjanM9C%2B2Hzc3y%2Bv3t4uef98NyR30BuYAPCPlc3mY1FrGzT2gNom2CvLS7AjtblrIzVrU5BSRhheV0VAZrKhR6Li3sLMKKLEVvhNNlCYlAVBvLFab%2FUxidEAmKpxwuYX8PUodbxgU5oa8VFUxsolTV8KoctxubWOlZdgvynZXegWPBSvPakY2v9CmOZS0vEqcsvnyFk0ay1iJtKVMxKlWJKgFY56LvkfLuIba%2FMyCXzGsE7cMudrBP89jVptOzEr1frs9TuCzP0BnGbO8L6tap2I4vze9VuRgv1ovT24F16F5qgMXMtWzmz0SQM8EjJDMcRjkTh%2BmyzL1FPRMLN2fa62Wg2wsw2OWRUyz%2FiLBTOxN2HlrxHSTfIR8ZQVHKkyp8DHV6EXfyIcSANNIR54k3grwWBSQfjaF5sUhrAZfLhIfCgDTxiN7QxEli%2F0E0XtGEoxSAd4sCLkh8RTXrL4yxEt9P1%2BjZnhBvnP3K%2FMb1rT94fnr8%2FVjGUJvZJAUlYVzcJMpsXtwkZ7Nq%2F%2FX5wEWbdrow7rratpC2cv%2BY2x%2FvrA9vl7e3T%2B7O3X388eXbsK6Rm5sQuq7ykjtdcpo9VXbv87ymx7mN3OyQCWKQ%2BATHYtQu9eJ6OwKP4B4%2BQU7UoPUZYjmBNXJx4YwW760VgA6iO8iN4JrNiljTYru3IrR8lyPbszN851N9px2XbBSegNrxMXIHEUJ88ZVUSibW64aPRT2eFAqSe6pIZvuhyULAF21izxPwchbwyplcvXUhX%2BkSbh%2F0wjjIjqu0ngv0x0L%2B0KDKuqggXPu2QiqrBlnLMB759u4S8m5hOvu4ZYId9Cqkqbnqm%2BH8wD1Xn72Op2ruUNaxoO6Qohb5qzptwPMgrhspdbu1zn4qa1UVtP26QGd1jIXD7lMd4x6f%2Fn4ayM9zMMnBMU6T3LklMyWNPnufV7STDn%2BvZ5GY9Acdz3K2XV1EElNWpReIlLcLqPuqxeafnyinIFJtO8JVFOqm7oW17Qe8JV2Y0Udb9wGsoKQrYpVluWyspk7ccRflHA02xao4unKUe3bWhHwYndrHKrN8l%2FY41j3EysS2r8Hu8eCklxf2nq1woyoyl%2B67C3wXBazkVjm1CoQeX8ZBJ6nCVwvyR8g508ut5eWzrFqVq7P1VoO%2FgxJnUb5gF2vGVM0yGBtfswMoCUvf6CbBAimrvzUU6jmf5tigKAJxKnD%2FqHzJGwcuZ8yl0UgHxBKStyI732LHns1%2F8m5EcoCS7dL8bohDxzwtQo5BE%2BNYkU6bnj5asfvtQiZX0nIHtIT0d0Xoj%2BfRzmm%2BePbIeILT%2BFcQZl2zVf0X5pSfkkGhwGbBvvvPFSVrmIKT4Rw5uWvX3%2BLP%2F9Ez2YPx2X0SCaGq19UHE3IQV8lKsIR681cUhOSIQZqCXzhjFE97BRuULuuD7lsQSUNSLS3HsEfJEhg7g0SCeRs9QK9rjusNySyV0BNeVJ%2BPjRUJ%2BHp1QcEaDm%2Bv38Zj%2Fvb54msChVpfubKjgMOC%2Bossuh67ZPhBtBx6rhfZvKLPIeu6o8zvPi4ym9Rx46tDh1Cz2wvHAfKE9g7vYJx6Y61IRSkoUkQX3kdBwHEiLq9AyjFAkMKQFGIi3JqNBAi%2Fd%2BgAcGORdOTDCnJc56%2FJAZhP8vpRkTEImb7aRBaA0ELbIzX8SKtQI2e%2FdFApfDzOWl20ec5OW79SeNYw3FalcD7gvlUK%2FxzZoYU5b%2B2suWiLSTZhoTHDRu0diFUzSW6vf6VV5b5A82rtmLJqtjOBfNz1CQpyp7ECXdb8PNRDKFw%2BRtt6U3SlapD9rPj5NZ2l5aysdeQXJNIeGXNo3U7%2BBZDlNz5aZT5QIKGYio982xs5kIFTSkaHCSk1qXa1oAsC7fDYiF4mVTZTySFM608aMdYPQz9yIOTVLE5D3CFfW73Oep9%2FdbMG6HZXHKsJ%2BhY0v0aWW9Bb1VqLJp4vHdv2gpDjMxtZjFxPPUC6OAK%2BpIHqhMC5Xf%2FedokdZ4u1xzXRf1PjFemB5d0e0YCVV8KSggO%2B%2B6q5AeKF0LHYCQyHjYCD3I78TFYbCJgklxor9oMk4F8K9n8EUBQr6sjAYoOftdW5BmjxKQDWmtfr2JYiuptUOY4TwuPtGr%2BIBcGAFT6mDelSyLSWY2JMoAiOf3x2A2poChHYG%2B4RAvxNFjFA5KyKpK5jyUxa3QZnTEw2fde746ISDK48Nf40yVvJCN4TVZSKK6oXNW0ge0csZosM07mVPCl9ec96lcuFpYv43NOX8SVmMWH6cYxgm8%2BuCSr12%2F8B). Specify desired analysis details for your data in the respective *essential.vars.groovy* file (see below) and run the selected pipeline *marsseq.pipeline.groovy* or *smartsseq.pipeline.groovy* as described [here](https://gitlab.rlp.net/imbforge/NGSpipe2go/-/blob/master/README.md). The analysis allows further parameter fine-tuning subsequent the initial analysis e.g. for plotting and QC thresholding. Therefore, a customisable *sc.report.Rmd* file will be generated in the output reports folder after running the pipeline. Go through the steps and modify the default settings where appropriate. Subsequently, the *sc.report.Rmd* file can be converted to a final html report using the *knitr* R-package.\r\n\r\n### The pipelines includes:\r\n- FastQC, MultiQC and other tools for rawdata quality control\r\n- Adapter trimming with Cutadapt\r\n- Mapping to the genome using STAR\r\n- generation of bigWig tracks for visualisation of alignment\r\n- Quantification with featureCounts (Subread) and UMI-tools (if UMIs are used for deduplication)\r\n- Downstream analysis in R using a pre-designed markdown report file (*sc.report.Rmd*). Modify this file to fit your custom parameter and thresholds and render it to your final html report. The Rmd file uses, among others, the following tools and methods:\r\n  - QC: the [scater](http://bioconductor.org/packages/release/bioc/html/scater.html) package.\r\n  - Normalization: the [scran](http://bioconductor.org/packages/release/bioc/html/scran.html) package.\r\n  - Differential expression analysis: the [scde](http://bioconductor.org/packages/release/bioc/html/scde.html) package.\r\n  - Trajectory analysis (pseudotime): the [monocle](https://bioconductor.org/packages/release/bioc/html/monocle.html) package.\r\n\r\n### Pipeline parameter settings\r\n- essential.vars.groovy: essential parameter describing the experiment \r\n  - project folder name\r\n  - reference genome\r\n  - experiment design\r\n  - adapter sequence, etc.\r\n- additional (more specialized) parameter can be given in the var.groovy-files of the individual pipeline modules \r\n- targets.txt: comma-separated txt-file giving information about the analysed samples. The following columns are required \r\n  - sample: sample identifier. Must be a unique substring of the input sample file name (e.g. common prefixes and suffixes may be removed). These names are grebbed against the count file names to merge targets.txt to the count data.\r\n  - plate: plate ID (number) \r\n  - row: plate row (letter)\r\n  - col: late column (number)\r\n  - cells: 0c/1c/10c (control wells)\r\n  - group: default variable for cell grouping (e.g. by condition)\r\n  \r\n  for pool-based libraries like MARSseq required additionally:\r\n  - pool: the pool ID comprises all cells from 1 library pool (i.e. a set of unique cell barcodes; the cell barcodes are re-used in other pools). Must be a unique substring of the input sample file name. For pool-based design, the pool ID is grebbed against the respective count data filename instead of the sample name as stated above.\r\n  - barcode: cell barcodes used as cell identifier in the count files. After merging the count data with targets.txt, the barcodes are replaced with sample IDs given in the sample column (i.e. here, sample names need not be a substring of input sample file name).\r\n\r\n### Programs required\r\n- FastQC\r\n- STAR\r\n- Samtools\r\n- Bedtools\r\n- Subread\r\n- Picard\r\n- UCSC utilities\r\n- RSeQC\r\n- UMI-tools\r\n- R\r\n\r\n## Resources\r\n- QC: the [scater](http://bioconductor.org/packages/release/bioc/html/scater.html) package.\r\n- Normalization: the [scran](http://bioconductor.org/packages/release/bioc/html/scran.html) package.\r\n- Trajectory analysis (pseudotime): the [monocle](https://bioconductor.org/packages/release/bioc/html/monocle.html) package.\r\n- A [tutorial](https://scrnaseq-course.cog.sanger.ac.uk/website/index.html) from Hemberg lab\r\n- Luecken and Theis 2019 [Current best practices in single\u2010cell RNA\u2010seq analysis: a tutorial](https://www.embopress.org/doi/10.15252/msb.20188746)\r\n\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "61",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/61?version=1",
        "name": "scRNA-seq MARS-seq",
        "number_of_steps": 0,
        "projects": [
            "IMBforge"
        ],
        "source": "WorkflowHub",
        "tags": [
            "mars-seq",
            "bpipe",
            "groovy",
            "scrna-seq"
        ],
        "tools": [],
        "type": "Bpipe",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-10-07",
        "creators": [
            "Sergi Sayols"
        ],
        "description": "# scRNA-Seq pipelines\r\n\r\nHere we forge the tools to analyze single cell RNA-Seq experiments. The analysis workflow is based on the Bioconductor packages [*scater*](https://bioconductor.org/packages/devel/bioc/vignettes/scater/inst/doc/overview.html) and [*scran*](https://bioconductor.org/packages/devel/bioc/vignettes/scran/inst/doc/scran.html) as well as the Bioconductor workflows by Lun ATL, McCarthy DJ, & Marioni JC [*A step-by-step workflow for low-level analysis of single-cell RNA-seq data.*](http://doi.org/10.12688/f1000research.9501.1) F1000Res. 2016 Aug 31 [revised 2016 Oct 31];5:2122 and Amezquita RA, Lun ATL et al. [*Orchestrating Single-Cell Analysis with Bioconductor*](https://osca.bioconductor.org/index.html) Nat Methods. 2020 Feb;17(2):137-145.\r\n\r\n## Implemented protocols\r\n - MARS-Seq (massively parallel single-cell RNA-sequencing): The protocol is based on the publications of Jaitin DA, et al. (2014). *Massively parallel single-cell RNA-seq for marker-free decomposition of tissues into cell types.* Science (New York, N.Y.), 343(6172), 776\u2013779. https://doi.org/10.1126/science.1247651 and Keren-Shaul H., et al. (2019). *MARS-seq2.0: an experimental and analytical pipeline for indexed sorting combined with single-cell RNA sequencing.* Nature Protocols. https://doi.org/10.1038/s41596-019-0164-4. The MARS-Seq library preparation protocol is given [here](https://github.com/imbforge/NGSpipe2go/blob/master/resources/MARS-Seq_protocol_Step-by-Step_MML.pdf). The sequencing reads are demultiplexed according to the respective pool barcodes before they are used as input for the analysis pipeline.  \r\n- Smart-seq2: Libraries are generated using the [Smart-seq2 kit](http://www.nature.com/nmeth/journal/v10/n11/full/nmeth.2639.html). \r\n\r\n## Pipeline Workflow\r\nAll analysis steps are illustrated in the pipeline [flowchart](https://www.draw.io/?lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1&title=scRNA-Seq#R7R3ZcpvK8mtUlTxIxSIh6dF2oiyVODm2Uzk5LykEI4kYAWGxrXz9nZ6FdUBIQoDjm%2BTeIwYYZqZ7eu%2BegXq1fXrn697ms2sie6BI5tNAfTNQFFmdTPB%2FoGVHW6bKjDasfctkDyUNt9YfxBol1hpZJgoyD4aua4eWl200XMdBRphp033ffcw%2BtnLt7Fc9fY0KDbeGbhdbv1tmuGGtsjZPbrxH1nrDPj1TpvTGUjfu174bOex7jusgemer827YHIONbrqPqSb17UC98l03pL%2B2T1fIhmXlK0bfW5TcjYfsIyes88J76eNqcz0L7YfNzfL6%2Fe3i55%2F3Qw6AB92O2FoMFM3GHV6a1gOsrm2tHXJD%2Bx3BUC99sgzxJf61Zv8lry39fAseEumLt5LFCHd87Tfh1sa%2FZHzP1pfIvoyX9Mq1XZ88pC7IH%2FxIEPrufQwkvIiXK9cJGUbJGoxbDzbIZD2SfuKrlWXbqU4R%2BRN3yu8QGKqXhru1DHwp4d9rWw8C9juGo0T6D%2FXQcmGFhnMpnl4aKgxQD8gP0VOqiUHpHXK3KPR3%2BBF2dzbR6CtsMw1nDAsfU6g5ZW2bFFaOJZVtCbYd1nHfCV7gHww1xGjyYFz%2Fc%2Fs9%2Bhi9%2BfPf3Ye7xY%2Fg4b%2Bh%2FKLx5K0Gf8vwZO3rpoWhfQz6NIAtykzKYEtMU1PYMpYE2KLxB0%2FBlj%2Ffwzd30r%2F%2FfVzcvPW3snn3bvdhOJELwEMmprfs0vXDjbt2Hd1%2Bm7SmIAALkzzzyXU9BpZfKAx3DIR6FLq4KYUU6MkK%2F4XXRxN29SN1580T65lc7PiFg%2Bebegkuf6TvJa%2BRq%2BQ98wLYDr40ALYAaWhcWDYfThEBMwhXCvnAjXwDVexFtsdD3V%2BjsOo59iAsfSUi%2BcjGNOwhywpFSEFexfPWd6kHPNdywiDV81doSPBzPJtk8VMZZ9lU7vnJbFb1PP5BR5CgZzyVWhj75e7L%2Fa%2FN7Ofvf%2B9CZfHz%2BrusLYez54KwRaw6PyKNZ00j0km8SN3DiRIgctYDlH8YEDBc4AdkzXsq50vwcK1exsJerpTBxeVWtxwAiuUh28LcADdeptkc%2FcY%2Bflh7JJXzCTzdybfpwAp8tMr0uglDkLYvADrKYm2FmMuOfNsbORg7lIW1Xa5cjCn45%2FW7W5iasnbxxRD%2Fb2m7S%2BDe6AH0ggWfN6YKi8C4ub4I0G%2F4udX9EP8c8fsjzL7dh1167JLBWGXSiFF%2BGPqRY%2Bgh3m2FeVb0Gq%2FrIlkAKfUvhocuEEVE61YCurzg0gU2VvbSDbwxYILGwV3e6UnQbh6sYvLQCJEZEmJshfoSv50iM1j%2FRivLsYhCokivlnDndaXA%2Fezmbbu6GRBeE3jIINwGa%2BEwUdC5XTOyAQ2f%2F%2FwQxnIntHRgh7pjxpMbwrStFRY4CdPXMU9F%2FlETbgAHn5ARhbDeKRwsGUm5MncGHUgTqEBc20mrQPK4ARVILDMJhJTcEmBF0IOfWFH0sMoIk6cCI5fuZrwhtjyNhctVKSXVXkO5sGSKYMV422E6Q0HIHypZpWCa7cBdrQJgQjkgHCboiw0Zky4lff77R0bq36eaDtKKaaKnlqimTWoIslJX12xJQ%2FgWIP%2FL8heYgDGpBCtQPQ1g6T4BRbOcNaVpS9c3kT%2FEzSAaEEmBIT6heBLG8kl8J23jvSCihIrG8Dd%2BwtNNM%2B5b6Z8kRuEXjMKnjEHucOkr29FJEpeAI3GBpgi%2FcuDkmRXaZmeIUQdL8gaBTqpDqfBUDMPUU0o5IMOla%2B4KjQURKzRjPUXfejYaVAljycPF13mLxRsMSu14t5KDRYFUX1at%2FqGxOOaKWSx133DN5qcR9yu98tHvyPIB%2BzBwcf%2FS54ub2%2BEtIHL57GoO5OD5eq5rNz5Z2qn04U3ldF93Ml9MnxuELuuOTdWJtkvkdzIvsNU2PivSqfTKxry%2Fo2kZTWInnxbuNNo6HQOMEO7m5iZhfWkhk%2F%2BDn9IrA%2FMNn2zDR%2FhSJ3MEocJrbo6sOwn8EWtni5ywi0mNRqPmpqQ7IDW64QZ0XMnUQ%2F1cU4JGEUsn0hMRS9KabLWOoaSEX6aUpPQNUMksQ7cvmMc0BCXikvtPbbSCz7j4qZVNHEwr4lNKqxOPGytEt1iqgR4ffd2Lx3WaFq1kdDJ5pglUQpFOKM8PFuTxZUqWP0CBUzp11Tw3BY6D9NkocJ5Q%2FN%2Fi4VsOlfAlL9HZaPuQ7CC4p6XuYaQPh2xXXRAZ36GWsrJ4BN7wNTGnigxsy1pUx8u3bQpPHTVVSh7g5kysZl6mNK9ulM3Ycjl60P2gEaN%2FWZcnKaAFEJ0HIHkhy3cfLKLopC28IkSrZG%2BXsUH4Gr%2BYWRZmoyj9Ln0N5iq0JEuvXC8cHS4PpZZzL4dUqjnkqbyQKfyfyOuNxdlkueNYntbjjvIRQVnHc0eRc7xVuzP%2FYG8Nz%2BMsGM9geBZGmMjqy5Nbmgp4qh3xpPVEiollhIUehP9cEZJcR4D4HWHSF%2B6IDs7VU3cFA9YfybCJbxAGi%2Fnu%2FYB50nL8fBMtR3hvV%2FByxse3eHBA8Rex2xQeA88dvvHbGNHmmNvupeoUTqVUXUR3cpQ%2BS7hVrYS%2Bk0hO1rGURfgGCP1EygWgjYuUXtYEJEo73JlYh9ALQyy5ylVCTpC9dB9boCRKNSk5H0kY1yQJE6lpknBkEGTWQT3WpBxWnBbUKMSReDVfNo40DPpiwKqcg62a82ZTJGVvJUA%2FFYfU6bRRHBKKLT1yl%2B%2FBozKxRe1MbKlLo8Z9CdTO0ygljV%2F78XE8OT8%2Bys8mtaDRSO3aqNSWBFyAlqb52q%2FVxtve6xvVfHKt%2B2t1qPWHehzKhFoAljrvxb6fqrl9n%2BUr%2B59vOEHj9uHi8Z186%2F78cjn%2F8k19%2BrrZ3g97pD3%2FvXxo2gt8PJUPTWYn8aHauv3t3cVNbc3%2BBqvuxGjseRBkdE4dPsm3wBA%2FXIkfpyBzTiW%2B4QjguZxFmo6VdvHaMmk6ZZ0lfhMa3CR9TfwpOVoHvqzUQjNXVtYQAguZdy1vLdMkpDAHk6Tlhq2EOijJbiUJzsQBTskU2714WJNL%2FA%2Bv3hUw1wke5hW%2BlpNr%2FA8e98Mr18Fd6xYBLsII%2B4iCcFBMyG0ADdQZfDxrvSnggdBMr44PxoN6MtALlVi1ukyncQPNacmFWmGL1ncPkkQJ5iSUjnBFf4lCLwK2A%2FUrYINfFJgOvvnbqGI53fmeY7%2BiG0%2FDIhyJxTUm467jPzw9YCZNXnj%2B%2F0LfWjbg%2B3tkPyDoddBGZM2Uc6MKk7KQKh3BnWri%2BbRneC4VUN1HQWSHQT%2BR3cAEOo3i4DS3gELbOwJWDFVFirYWVLGh6WpE%2FjPR0Iw8G%2BMdPF2dmtdqfMKL35Iaz3XZuyUPD3aruSVnvd%2BStrvu6X7EIxvyvWi4to0l81RIPdIN2I%2BcQ%2FULwc%2BAzBO1Rf4ilnqVLqXerkx%2FXJjdK%2FXOO7PTVo67N6SnKPX6CFTJnlKfVsMumUixiFdkEWwsZ%2FczMHxHD9Dvn%2FQGVu5%2FhiTRaBEYI9o2utmap0Vi5rsqDbDMByv2eXHII%2FjGaINsD%2FnB6ObkRRJ2WViYfsWqfsXjxUQv3mywJX0wQqYrTBAmO0qNOI4O%2Feya1gpoFsEM8mAiChtREEJXmr4FRuosAyrHJZGnwLlpoYMQ40ewwcQgoLROkRyETJRGN1F2Yy%2BYeVvS6nxWj8HLSgMcXlzk7fREjF%2FR1uPPM3NjazGOcf029uGkeNtZnTFn8vUXnXS84h93zk%2FqBaU06B%2BxnBXy0ROmfhbJRavrKjGBGmwptQG7MaEJPFadRkeSmEjbWvo6Q%2BSz%2B1JykznYrTJuKzbyvLGQqtKtW%2BX%2B92P4aG23MlI%2Baab0dbwcXg6nz0XVODayOkN7yvSVHEURAL5xIqNpWa5UiHyj5LBAZIrUapyteKtMzxNCN82hMx9w6bjyocCnhdzVJp1BtASDoYJl1nDnoeAo2klMlKQ2E0nrwaQMZKudh9qhmPk5HE4yJ38FyRxLPQwf597O%2Fjslj63Q26hZp7Yzs%2FGQ45OMcMqzib%2FrQ650bSirPYmT4gSDG31Z2fjSOKnc89q4VWb2k7rMDgyYIi%2BRiCmw3mMNIGXVT9xrZ%2BRn6fq1dCKHMzLtWTKyfEjVRCnWg2iTkYnjwV9gOYimTBHTmvRuJveC3k3lrEljMt8jvOeeH8%2FboXdLfXsFxkRy5ExNamdgwkJqLy2t9Xdr3UaSJx6mwYZ5OEWbPkuKlhfNJ1q3ormQolUndv5fMhceTZCmYcJVbUswr00lLkzz2%2BcPl7Ts4R3esBLkiv%2FOBl6IiEa6Lr1uEh8J7mjAnSdJIUXmu7kh6wMfIGndBLN1k5gEiBn1Rq6MAcqWwW9H0MLTwrIdm0noknEfTqNm7dOo3LlBqwn8HRSPCML3luTvIB91je9o5A%2FsY%2Fh%2BCNtm0lQNk5xIpwhKfHVuzv3rJbr%2BmnOztVEm8tHmXDWLZi2Zc%2FmA%2B2bOXSMHXbrmDouFSm2R0OcKMBMlMbl%2BIFQbeoMPImcNrvU2bLnwSSiOiAejHEyIJ9KgdUJ8BmFxPOtWWBRb%2BP5631ej0mLtzNx%2BWvg0tZ8WPiysHWbcw9hHKtXS0DAeS8%2FKGxUE2cCzrTA8d9pkSv7k8zlc5uwgeTInJVaJmmXiabkIegaZU5O6lTkrTVy9CYAtxt6Hvm7c9zT%2BlVqtYMpskMxC%2F2AFUNEsoEfk9j7VpAFszwfqjQUmprNF4gstHz2qnNB%2FCWFWW0JoWkA4KX6fD%2Fso8nX2gso3CGLnHAN4%2BTvkuJlzSiqkhO7pGpVPSKEHDAG8JZ6I%2B3GdmUQ%2B0n2F9DDyc6fxkW%2Bsw1XbMd2FKO2kDjAL0LZdmowYkCkCTlcVWx4cWQ64weq%2BTcgkSoZIT%2BW60dRSA1RavIOLpSEKC3fewr0xDXm5hXvFKnaPHDIxzzzIISO3yEDngiAacYJgvzLg5uV1H3p%2FplwSaRMnZxFMqlYVXtr5a3RJssUpBuT8LbxT2flb1GOWy3HjKUz3mJXDWUj8GvN2Urs%2FrlNA%2FWk6QTZRJWdq19DxW7vACsg6Iy84%2FbScc53k8%2F%2BBnX5Q0HM492fOSxrE4lCxZpI8E7D22fR0aix0AT7%2F7PH97rs6x6%2Fy02rrqaNd8c54mJ1Yzz44dbLH9cfc4WE9sp7xuIy4lswfy%2FOQ%2BbpqZK3oio6%2BpaG3mIc9gE5IjpWnpj1PZ4dWIibC1M3NbVPXdV1bHt3IIxpBsv7T51EqR4zyWZtKc0q4ILq4ZR2c1%2BPtsRMA6odmEuF7RMaW%2BjYhYi%2FC1J9NWlTnNUtNnq%2FoTqdZXU3Y%2BqdtGvu5Y7KDGpWNhANMp4eFA0xnsxyKnSnhB2qf3LACLHVDAhi0QNag1mkiZWDKc2%2B6j84gqUkCE2oh14fVb4E5HBwGME%2Bjx%2FOJeMp76qfjolm8TU%2F9e%2BnjanM9C%2B2Hzc3y%2Bv3t4uef98NyR30BuYAPCPlc3mY1FrGzT2gNom2CvLS7AjtblrIzVrU5BSRhheV0VAZrKhR6Li3sLMKKLEVvhNNlCYlAVBvLFab%2FUxidEAmKpxwuYX8PUodbxgU5oa8VFUxsolTV8KoctxubWOlZdgvynZXegWPBSvPakY2v9CmOZS0vEqcsvnyFk0ay1iJtKVMxKlWJKgFY56LvkfLuIba%2FMyCXzGsE7cMudrBP89jVptOzEr1frs9TuCzP0BnGbO8L6tap2I4vze9VuRgv1ovT24F16F5qgMXMtWzmz0SQM8EjJDMcRjkTh%2BmyzL1FPRMLN2fa62Wg2wsw2OWRUyz%2FiLBTOxN2HlrxHSTfIR8ZQVHKkyp8DHV6EXfyIcSANNIR54k3grwWBSQfjaF5sUhrAZfLhIfCgDTxiN7QxEli%2F0E0XtGEoxSAd4sCLkh8RTXrL4yxEt9P1%2BjZnhBvnP3K%2FMb1rT94fnr8%2FVjGUJvZJAUlYVzcJMpsXtwkZ7Nq%2F%2FX5wEWbdrow7rratpC2cv%2BY2x%2FvrA9vl7e3T%2B7O3X388eXbsK6Rm5sQuq7ykjtdcpo9VXbv87ymx7mN3OyQCWKQ%2BATHYtQu9eJ6OwKP4B4%2BQU7UoPUZYjmBNXJx4YwW760VgA6iO8iN4JrNiljTYru3IrR8lyPbszN851N9px2XbBSegNrxMXIHEUJ88ZVUSibW64aPRT2eFAqSe6pIZvuhyULAF21izxPwchbwyplcvXUhX%2BkSbh%2F0wjjIjqu0ngv0x0L%2B0KDKuqggXPu2QiqrBlnLMB759u4S8m5hOvu4ZYId9Cqkqbnqm%2BH8wD1Xn72Op2ruUNaxoO6Qohb5qzptwPMgrhspdbu1zn4qa1UVtP26QGd1jIXD7lMd4x6f%2Fn4ayM9zMMnBMU6T3LklMyWNPnufV7STDn%2BvZ5GY9Acdz3K2XV1EElNWpReIlLcLqPuqxeafnyinIFJtO8JVFOqm7oW17Qe8JV2Y0Udb9wGsoKQrYpVluWyspk7ccRflHA02xao4unKUe3bWhHwYndrHKrN8l%2FY41j3EysS2r8Hu8eCklxf2nq1woyoyl%2B67C3wXBazkVjm1CoQeX8ZBJ6nCVwvyR8g508ut5eWzrFqVq7P1VoO%2FgxJnUb5gF2vGVM0yGBtfswMoCUvf6CbBAimrvzUU6jmf5tigKAJxKnD%2FqHzJGwcuZ8yl0UgHxBKStyI732LHns1%2F8m5EcoCS7dL8bohDxzwtQo5BE%2BNYkU6bnj5asfvtQiZX0nIHtIT0d0Xoj%2BfRzmm%2BePbIeILT%2BFcQZl2zVf0X5pSfkkGhwGbBvvvPFSVrmIKT4Rw5uWvX3%2BLP%2F9Ez2YPx2X0SCaGq19UHE3IQV8lKsIR681cUhOSIQZqCXzhjFE97BRuULuuD7lsQSUNSLS3HsEfJEhg7g0SCeRs9QK9rjusNySyV0BNeVJ%2BPjRUJ%2BHp1QcEaDm%2Bv38Zj%2Fvb54msChVpfubKjgMOC%2Bossuh67ZPhBtBx6rhfZvKLPIeu6o8zvPi4ym9Rx46tDh1Cz2wvHAfKE9g7vYJx6Y61IRSkoUkQX3kdBwHEiLq9AyjFAkMKQFGIi3JqNBAi%2Fd%2BgAcGORdOTDCnJc56%2FJAZhP8vpRkTEImb7aRBaA0ELbIzX8SKtQI2e%2FdFApfDzOWl20ec5OW79SeNYw3FalcD7gvlUK%2FxzZoYU5b%2B2suWiLSTZhoTHDRu0diFUzSW6vf6VV5b5A82rtmLJqtjOBfNz1CQpyp7ECXdb8PNRDKFw%2BRtt6U3SlapD9rPj5NZ2l5aysdeQXJNIeGXNo3U7%2BBZDlNz5aZT5QIKGYio982xs5kIFTSkaHCSk1qXa1oAsC7fDYiF4mVTZTySFM608aMdYPQz9yIOTVLE5D3CFfW73Oep9%2FdbMG6HZXHKsJ%2BhY0v0aWW9Bb1VqLJp4vHdv2gpDjMxtZjFxPPUC6OAK%2BpIHqhMC5Xf%2FedokdZ4u1xzXRf1PjFemB5d0e0YCVV8KSggO%2B%2B6q5AeKF0LHYCQyHjYCD3I78TFYbCJgklxor9oMk4F8K9n8EUBQr6sjAYoOftdW5BmjxKQDWmtfr2JYiuptUOY4TwuPtGr%2BIBcGAFT6mDelSyLSWY2JMoAiOf3x2A2poChHYG%2B4RAvxNFjFA5KyKpK5jyUxa3QZnTEw2fde746ISDK48Nf40yVvJCN4TVZSKK6oXNW0ge0csZosM07mVPCl9ec96lcuFpYv43NOX8SVmMWH6cYxgm8%2BuCSr12%2F8B). Specify desired analysis details for your data in the respective *essential.vars.groovy* file (see below) and run the selected pipeline *marsseq.pipeline.groovy* or *smartsseq.pipeline.groovy* as described [here](https://gitlab.rlp.net/imbforge/NGSpipe2go/-/blob/master/README.md). The analysis allows further parameter fine-tuning subsequent the initial analysis e.g. for plotting and QC thresholding. Therefore, a customisable *sc.report.Rmd* file will be generated in the output reports folder after running the pipeline. Go through the steps and modify the default settings where appropriate. Subsequently, the *sc.report.Rmd* file can be converted to a final html report using the *knitr* R-package.\r\n\r\n### The pipelines includes:\r\n- FastQC, MultiQC and other tools for rawdata quality control\r\n- Adapter trimming with Cutadapt\r\n- Mapping to the genome using STAR\r\n- generation of bigWig tracks for visualisation of alignment\r\n- Quantification with featureCounts (Subread) and UMI-tools (if UMIs are used for deduplication)\r\n- Downstream analysis in R using a pre-designed markdown report file (*sc.report.Rmd*). Modify this file to fit your custom parameter and thresholds and render it to your final html report. The Rmd file uses, among others, the following tools and methods:\r\n  - QC: the [scater](http://bioconductor.org/packages/release/bioc/html/scater.html) package.\r\n  - Normalization: the [scran](http://bioconductor.org/packages/release/bioc/html/scran.html) package.\r\n  - Differential expression analysis: the [scde](http://bioconductor.org/packages/release/bioc/html/scde.html) package.\r\n  - Trajectory analysis (pseudotime): the [monocle](https://bioconductor.org/packages/release/bioc/html/monocle.html) package.\r\n\r\n### Pipeline parameter settings\r\n- essential.vars.groovy: essential parameter describing the experiment \r\n  - project folder name\r\n  - reference genome\r\n  - experiment design\r\n  - adapter sequence, etc.\r\n- additional (more specialized) parameter can be given in the var.groovy-files of the individual pipeline modules \r\n- targets.txt: comma-separated txt-file giving information about the analysed samples. The following columns are required \r\n  - sample: sample identifier. Must be a unique substring of the input sample file name (e.g. common prefixes and suffixes may be removed). These names are grebbed against the count file names to merge targets.txt to the count data.\r\n  - plate: plate ID (number) \r\n  - row: plate row (letter)\r\n  - col: late column (number)\r\n  - cells: 0c/1c/10c (control wells)\r\n  - group: default variable for cell grouping (e.g. by condition)\r\n  \r\n  for pool-based libraries like MARSseq required additionally:\r\n  - pool: the pool ID comprises all cells from 1 library pool (i.e. a set of unique cell barcodes; the cell barcodes are re-used in other pools). Must be a unique substring of the input sample file name. For pool-based design, the pool ID is grebbed against the respective count data filename instead of the sample name as stated above.\r\n  - barcode: cell barcodes used as cell identifier in the count files. After merging the count data with targets.txt, the barcodes are replaced with sample IDs given in the sample column (i.e. here, sample names need not be a substring of input sample file name).\r\n\r\n### Programs required\r\n- FastQC\r\n- STAR\r\n- Samtools\r\n- Bedtools\r\n- Subread\r\n- Picard\r\n- UCSC utilities\r\n- RSeQC\r\n- UMI-tools\r\n- R\r\n\r\n## Resources\r\n- QC: the [scater](http://bioconductor.org/packages/release/bioc/html/scater.html) package.\r\n- Normalization: the [scran](http://bioconductor.org/packages/release/bioc/html/scran.html) package.\r\n- Trajectory analysis (pseudotime): the [monocle](https://bioconductor.org/packages/release/bioc/html/monocle.html) package.\r\n- A [tutorial](https://scrnaseq-course.cog.sanger.ac.uk/website/index.html) from Hemberg lab\r\n- Luecken and Theis 2019 [Current best practices in single\u2010cell RNA\u2010seq analysis: a tutorial](https://www.embopress.org/doi/10.15252/msb.20188746)\r\n\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "62",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/62?version=1",
        "name": "scRNA-seq Smart-seq 2",
        "number_of_steps": 0,
        "projects": [
            "IMBforge"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bpipe",
            "groovy",
            "scrna-seq",
            "smart-seq 2"
        ],
        "tools": [],
        "type": "Bpipe",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-12-29",
        "creators": [],
        "description": "Abstract CWL Automatically generated from the Galaxy workflow file: GTN 'Pangeo 101 for everyone - Introduction to Xarray'.\r\n\r\nIn this tutorial, we analyze particle matter < 2.5 \u03bcm/m3 data from Copernicus Atmosphere Monitoring Service to understand Xarray Galaxy Tools:\r\n- Understand how an Xarray dataset is organized;\r\n- Get metadata from Xarray dataset such as variable names, units, coordinates (latitude, longitude, level), etc;\r\n- Plot an Xarray dataset on a geographical map and learn to customize it;\r\n- Select/Subset an Xarray dataset from coordinates values such as time selection or a subset over a geographical area;\r\n- Mask an Xarray dataset with a Where statement, for instance to only see PM2.5 > 30 \u03bcm/m and highlight on a map regions with \"high\" values;\r\n- Convert an Xarray dataset to Tabular data (pandas dataframe);\r\n- Plot tabular data to visualize the forecast PM2.5 over a single point (here Naples) using a scatterplot and/or climate stripes.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "252",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/252?version=1",
        "name": "Pangeo 101 for everyone - introduction to Xarray",
        "number_of_steps": 40,
        "projects": [
            "Galaxy Climate"
        ],
        "source": "WorkflowHub",
        "tags": [
            "climate",
            "gtn",
            "copernicus",
            "pangeo"
        ],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-11-29",
        "creators": [
            "Jos\u00e9 M\u00aa Fern\u00e1ndez",
            "Asier Gonzalez-Uriarte"
        ],
        "description": "## Description\r\n\r\nThe workflow takes an input file with Cancer Driver Genes predictions (i.e. the results provided by a participant), computes a set of metrics, and compares them against the data currently stored in OpenEBench within the TCGA community. Two assessment metrics are provided for that predictions. Also, some plots (which are optional) that allow to visualize the performance of the tool are generated. The workflow consists in three standard steps, defined by OpenEBench. The tools needed to run these steps are containerised in three Docker images, whose recipes are available in the [TCGA_benchmarking_dockers](https://github.com/inab/TCGA_benchmarking_dockers ) repository and the images are stored in the [INB GitLab container registry](https://gitlab.bsc.es/inb/elixir/openebench/workflows/tcga_benchmarking_dockers/container_registry) . Separated instances are spawned from these images for each step:\r\n1. **Validation**: the input file format is checked and, if required, the content of the file is validated (e.g check whether the submitted gene IDs exist)\r\n2. **Metrics Generation**: the predictions are compared with the 'Gold Standards' provided by the community, which results in two performance metrics - precision (Positive Predictive Value) and recall(True Positive Rate).\r\n3. **Consolidation**: the benchmark itself is performed by merging the tool metrics with the rest of TCGA data. The results are provided in JSON format and SVG format (scatter plot).\r\n\r\n![OpenEBench benchmarking workflow](https://raw.githubusercontent.com/inab/TCGA_benchmarking_workflow/1.0.8/workflow_schema.jpg)\r\n\r\n## Data\r\n\r\n* [TCGA_sample_data](./TCGA_sample_data) folder contains all the reference data required by the steps. It is derived from the manuscript:\r\n[Comprehensive Characterization of Cancer Driver Genes and Mutations](https://www.cell.com/cell/fulltext/S0092-8674%2818%2930237-X?code=cell-site), Bailey et al, 2018, Cell [![doi:10.1016/j.cell.2018.02.060](https://img.shields.io/badge/doi-10.1016%2Fj.cell.2018.02.060-green.svg)](https://doi.org/10.1016/j.cell.2018.02.060) \r\n* [TCGA_sample_out](./TCGA_sample_out) folder contains an example output for a worklow run, with two cancer types / challenges selected (ACC, BRCA). Results obtained from the default execution should be similar to those ones available in this directory. Results found in [TCGA_sample_out/results](./TCGA_sample_out/results) can be visualized in the browser using [`benchmarking_workflows_results_visualizer` javascript library](https://github.com/inab/benchmarking_workflows_results_visualizer).\r\n\r\n## Requirements\r\nThis workflow depends on three tools that have to be installed before you can run it:\r\n* [Git](https://git-scm.com/downloads): Used to download the workflow from GitHub.\r\n* [Docker](https://docs.docker.com/get-docker/): The Docker Engine is used under the hood to execute the containerised steps of the benchmarking workflow.\r\n* [Nextflow](https://www.nextflow.io/): Is the technology used to write and execute the benchmarking workflow. Note that it depends on Bash (>=3.2) and Java (>=8 , <=17). We provide the script [run_local_nextflow.bash](run_local_nextflow.bash) that automates their installation for local testing.\r\n\r\nCheck that these tools are available in your environment:\r\n```\r\n# Git\r\n> which git\r\n/usr/bin/git\r\n> git --version\r\ngit version 2.26.2\r\n\r\n# Docker\r\n> which docker\r\n/usr/bin/docker\r\n> docker --version\r\nDocker version 20.10.9-ce, build 79ea9d308018\r\n\r\n# Nextflow\r\n> which nextflow\r\n/home/myuser/bin/nextflow\r\n> nextflow -version\r\n\r\n      N E X T F L O W\r\n      version 21.04.1 build 5556\r\n      created 14-05-2021 15:20 UTC (17:20 CEST)\r\n      cite doi:10.1038/nbt.3820\r\n      http://nextflow.io\r\n```\r\nIn the case of docker, apart from being installed the daemon has to be running. On Linux distributions that use `Systemd` for service management, which includes the most popular ones as of 2021 (Ubuntu, Debian, CentOs, Red Hat, OpenSuse), the `systemctl` command can be used to check its status and manage it:\r\n\r\n```\r\n# Check status of docker daemon\r\n> sudo systemctl status docker\r\n\u25cf docker.service - Docker Application Container Engine\r\n   Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled)\r\n   Active: inactive (dead)\r\n     Docs: http://docs.docker.com\r\n\r\n# Start docker daemon\r\n> sudo systemctl start docker\r\n```\r\n\r\n### Download workflow\r\nSimply clone the repository and check out the latest tag (currently `1.0.8`):\r\n\r\n```\r\n# Clone repository\r\n> git clone https://github.com/inab/TCGA_benchmarking_dockers.git\r\n\r\n# Move to new directory\r\ncd TCGA_benchmarking_workflow/\r\n\r\n# Checkout version 1.0.8\r\n> git checkout 1.0.8 -b 1.0.8\r\n```\r\n\r\n## Usage\r\nThe workflow can be run workflow in two different ways:\r\n* Standard: `nextflow run main.nf -profile docker`\r\n* Using the bash script that installs Java and Nextflow:`./run_local_nextflow.bash run main.nf -profile docker`.\r\n\r\nArguments specifications:\r\n```\r\nUsage:\r\nRun the pipeline with default parameters:\r\nnextflow run main.nf -profile docker\r\n\r\nRun with user parameters:\r\nnextflow run main.nf -profile docker --predictionsFile {driver.genes.file} --public_ref_dir {validation.reference.file} --participant_name {tool.name} --metrics_ref_dir {gold.standards.dir} --cancer_types {analyzed.cancer.types} --assess_dir {benchmark.data.dir} --results_dir {output.dir}\r\n\r\nMandatory arguments:\r\n\t--input                 List of cancer genes prediction\r\n\t--community_id          Name or OEB permanent ID for the benchmarking community\r\n\t--public_ref_dir        Directory with list of cancer genes used to validate the predictions\r\n\t--participant_id        Name of the tool used for prediction\r\n\t--goldstandard_dir      Dir that contains metrics reference datasets for all cancer types\r\n\t--challenges_ids        List of types of cancer selected by the user, separated by spaces\r\n\t--assess_dir            Dir where the data for the benchmark are stored\r\n\r\nOther options:\r\n\t--validation_result     The output directory where the results from validation step will be saved\r\n\t--augmented_assess_dir  Dir where the augmented data for the benchmark are stored\r\n\t--assessment_results    The output directory where the results from the computed metrics step will be saved\r\n\t--outdir                The output directory where the consolidation of the benchmark will be saved\r\n\t--statsdir              The output directory with nextflow statistics\r\n\t--data_model_export_dir The output dir where json file with benchmarking data model contents will be saved\r\n\t--otherdir              The output directory where custom results will be saved (no directory inside)\r\nFlags:\r\n\t--help                  Display this message\r\n```\r\n\r\nDefault input parameters and Docker images to use for each step can be specified in the [config](./nextflow.config) file.\r\n\r\n**NOTE: In order to make your workflow compatible with the [OpenEBench VRE Nextflow Executor](https://github.com/inab/vre-process_nextflow-executor), please make sure to use the same parameter names in your workflow.**\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "244",
        "keep": true,
        "latest_version": 4,
        "license": "LGPL-3.0",
        "link": "https:/workflowhub.eu/workflows/244?version=4",
        "name": "OpenEBench TCGA Cancer Driver Genes benchmarking workflow",
        "number_of_steps": 0,
        "projects": [
            "OpenEBench"
        ],
        "source": "WorkflowHub",
        "tags": [
            "benchmarking",
            "openebench",
            "tcga"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 4
    },
    {
        "create_time": "2021-11-21",
        "creators": [],
        "description": "`atavide` is a complete workflow for metagenomics data analysis, including QC/QA, optional host removal, assembly and cross-assembly, and individual read based annotations. We have also built in some advanced analytics including tools to assign annotations from reads to contigs, and to generate metagenome-assembled genomes in several different ways, giving you the power to explore your data!\r\n\r\n`atavide` is 100% snakemake and conda, so you only need to install the snakemake workflow, and then everything else will be installed with conda.\r\n\r\nSteps:\r\n1. QC/QA with [prinseq++](https://github.com/Adrian-Cantu/PRINSEQ-plus-plus)\r\n2. optional host removal using bowtie2 and samtools, [as described previously](https://edwards.flinders.edu.au/command-line-deconseq/). To enable this, you need to provide a path to the host db and a host db.\r\n\r\nMetagenome assembly\r\n1. pairwise assembly of each sample using [megahit](https://github.com/voutcn/megahit)\r\n2. extraction of all reads that do not assemble using samtools flags\r\n3. assembly of all unassembled reads using [megahit](https://github.com/voutcn/megahit)\r\n4. compilation of _all_ contigs into a single unified set using [Flye](https://github.com/fenderglass/Flye)\r\n5. comparison of reads -> contigs to generate coverage\r\n\r\nMAG creation\r\n1. [metabat](https://bitbucket.org/berkeleylab/metabat/src/master/)\r\n2. [concoct](https://github.com/BinPro/CONCOCT)\r\n3. Pairwise comparisons using [turbocor](https://github.com/dcjones/turbocor) followed by clustering\r\n\r\nRead-based annotations\r\n1. [Kraken2](https://ccb.jhu.edu/software/kraken2/)\r\n2. [singlem](https://github.com/wwood/singlem)\r\n3. [SUPER-focus](https://github.com/metageni/SUPER-FOCUS)\r\n4. [FOCUS](https://github.com/metageni/FOCUS)\r\n\r\nWant something else added to the suite? File an issue on github and we'll add it ASAP!\r\n\r\n### Installation\r\n\r\nYou will need to install\r\n1. The NCBI taxonomy database somewhere\r\n2. The superfocus databases somewhere, and set the SUPERFOCUS_DB environmental variable\r\n\r\nEverything else should install automatically.",
        "doi": "10.48546/workflowhub.workflow.241.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "241",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/241?version=1",
        "name": "atavide",
        "number_of_steps": 0,
        "projects": [
            "FAME"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-11-16",
        "creators": [
            "Jean-Marie Burel"
        ],
        "description": "# Summary\r\nThis notebook demonstrates how to retrieve metadata associated to the paper [A SARS-CoV-2 cytopathicity dataset generated by high-content screening of a large drug repurposing collection](https://doi.org/10.1038/s41597-021-00848-4) and available in IDR at [idr0094-ellinger-sarscov2](https://idr.openmicroscopy.org/search/?query=Name:idr0094).\r\nOver 300 compounds were used in this investigation. This notebook allows the user to calculate the half maximal inhibitory concentration (IC50) for each compound. IC50 is a measure of the potency of a substance in inhibiting a specific biological or biochemical function. IC50 is a quantitative measure that indicates how much of a particular inhibitory substance (e.g. drug) is needed to inhibit, in vitro, a given biological process or biological component by 50%.\r\nUser can download the IC50 for each compound used in that study\r\n\r\nThe notebook can be launched in [My Binder](https://mybinder.org/v2/gh/IDR/idr0094-ellinger-sarscov2/master?urlpath=notebooks%2Fnotebooks%2Fidr0094-ic50.ipynb%3FscreenId%3D2603).\r\n\r\nA shiny app is also available for dynamic plotting of the IC50 curve for each compound.\r\nThis R shiny app can be launched in [My Binder](https://mybinder.org/v2/gh/IDR/idr0094-ellinger-sarscov2/master?urlpath=shiny/apps/)\r\n\r\n\r\n# Inputs\r\nParameters needed to configure the workflow:\r\n\r\n**screenId**: Identifier of a screen in IDR.\r\n\r\n# Ouputs\r\nOutput file generated:\r\n\r\n**ic50.csv**: Comma separate value file containing the IC50 for each compound.\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.238.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "238",
        "keep": true,
        "latest_version": 1,
        "license": "BSD-2-Clause",
        "link": "https:/workflowhub.eu/workflows/238?version=1",
        "name": "Calculate the half maximal inhibitory concentration (IC50) for each compound used in a SARS-CoV-2 study",
        "number_of_steps": 0,
        "projects": [
            "OME"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Jupyter",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-11-12",
        "creators": [
            "Richard Lupat"
        ],
        "description": "This is a genomics pipeline to do a single germline sample variant-calling, adapted from GATK Best Practice Workflow.\r\n\r\nThis workflow is a reference pipeline for using the Janis Python framework (pipelines assistant).\r\n- Alignment: bwa-mem\r\n- Variant-Calling: GATK HaplotypeCaller\r\n- Outputs the final variants in the VCF format.\r\n\r\n**Resources**\r\n\r\nThis pipeline has been tested using the HG38 reference set, available on Google Cloud Storage through:\r\n\r\n- https://console.cloud.google.com/storage/browser/genomics-public-data/references/hg38/v0/\r\n\r\nThis pipeline expects the assembly references to be as they appear in that storage     (\".fai\", \".amb\", \".ann\", \".bwt\", \".pac\", \".sa\", \"^.dict\").\r\nThe known sites (snps_dbsnp, snps_1000gp, known_indels, mills_indels) should be gzipped and tabix indexed.\r\n\r\n\r\nInfrastructure_deployment_metadata: Spartan (Unimelb)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "236",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/236?version=1",
        "name": "Janis Germline Variant-Calling Workflow (GATK)",
        "number_of_steps": 16,
        "projects": [
            "Janis"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "Performance summary workflow (whole genome)",
            "BGZip",
            "Perform base quality score recalibration",
            "GATK4: Gather VCFs",
            "Generate genome for BedtoolsCoverage",
            "Generating genomic intervals by chromosome",
            "Parse FastQC Adaptors",
            "Align and sort reads",
            "UncompressArchive",
            "GATK4 Germline Variant Caller",
            "BCFTools: Sort",
            "Annotate Bam Stats to Germline Vcf Workflow",
            "FastQC",
            "Merge and Mark Duplicates"
        ],
        "type": "Janis",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-11-08",
        "creators": [
            "Anna Syme"
        ],
        "description": "Combined workflow for large genome assembly\r\n\r\nThe tutorial document for this workflow is here: https://doi.org/10.5281/zenodo.5655813\r\n\r\n\r\nWhat it does:  A workflow for genome assembly, containing subworkflows:\r\n* Data QC\r\n* Kmer counting\r\n* Trim and filter reads\r\n* Assembly with Flye\r\n* Assembly polishing\r\n* Assess genome quality\r\n\r\nInputs: \r\n* long reads and short reads in fastq format\r\n* reference genome for Quast\r\n\r\nOutputs: \r\n* Data information - QC, kmers\r\n* Filtered, trimmed reads\r\n* Genome assembly, assembly graph, stats\r\n* Polished assembly, stats\r\n* Quality metrics - Busco, Quast\r\n\r\nOptions\r\n* Omit some steps - e.g. Data QC and kmer counting\r\n* Replace a module with one using a different tool - e.g. change assembly tool\r\n\r\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)",
        "doi": "10.48546/workflowhub.workflow.230.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in name",
        "id": "230",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/230?version=1",
        "name": "Combined workflows for large genome assembly",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Australia",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "large-genome-assembly"
        ],
        "tools": [
            "\n 9d0e6aba18484ea8",
            "\n 1fbcdce7d5823b15",
            "\n ec0fc1dbb6a13fe5",
            "\n ae3b48869cad659b",
            "\n 2ed2445df255451a",
            "\n c0b5fbb61b12154d"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-11-10",
        "creators": [],
        "description": "",
        "doi": null,
        "edam_operation": [
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "233",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/233?version=1",
        "name": "16S_biodiversity_for_nonoverlap_paired_end",
        "number_of_steps": 25,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metadegalaxy"
        ],
        "tools": [
            "\n addValue",
            "vsearch_chimera_detection",
            "uclust2otutable",
            "phyloseq_abundance",
            "samtools_fastx",
            "vsearch_search",
            "bwa_mem",
            "vsearch_clustering",
            "phyloseq_taxonomy",
            "biom_add_metadata",
            "symmetricPlot",
            "phyloseq_net",
            "\n Cut1",
            "vsearch_dereplication",
            "fastqc",
            "phyloseq_DESeq2",
            "picard_FilterSamReads",
            "phyloseq_richness",
            "cat_multi_datasets",
            "\n cat1",
            "biom_convert",
            "trimmomatic"
        ],
        "type": "Galaxy",
        "update_time": "2024-04-17",
        "versions": 1
    },
    {
        "create_time": "2021-11-10",
        "creators": [],
        "description": "MetaDEGalaxy: Galaxy workflow for differential abundance analysis of 16s metagenomic data",
        "doi": null,
        "edam_operation": [
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "232",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/232?version=1",
        "name": "16S_biodiversity_for_overlap_paired_end",
        "number_of_steps": 27,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metadegalaxy"
        ],
        "tools": [
            "\n addValue",
            "vsearch_chimera_detection",
            "uclust2otutable",
            "phyloseq_abundance",
            "samtools_fastx",
            "vsearch_search",
            "bwa_mem",
            "vsearch_clustering",
            "phyloseq_taxonomy",
            "picard_MergeSamFiles",
            "biom_add_metadata",
            "symmetricPlot",
            "phyloseq_net",
            "\n Cut1",
            "iuc_pear",
            "vsearch_dereplication",
            "fastqc",
            "phyloseq_DESeq2",
            "picard_FilterSamReads",
            "phyloseq_richness",
            "\n cat1",
            "biom_convert",
            "trimmomatic"
        ],
        "type": "Galaxy",
        "update_time": "2024-04-17",
        "versions": 1
    },
    {
        "create_time": "2021-11-08",
        "creators": [
            "Anna Syme"
        ],
        "description": "Assess genome quality; can run alone or as part of a combined workflow for large genome assembly. \r\n\r\n* What it does: Assesses the quality of the genome assembly: generate some statistics and determine if expected genes are present; align contigs to a reference genome.\r\n* Inputs: polished assembly;  reference_genome.fasta (e.g. of a closely-related species, if available). \r\n* Outputs:  Busco table of genes found; Quast HTML report, and link to Icarus contigs browser,  showing contigs aligned to a reference genome\r\n* Tools used: Busco, Quast\r\n* Input parameters: None required\r\n\r\nWorkflow steps: \r\n\r\nPolished assembly => Busco\r\n* First: predict genes in the assembly: using Metaeuk\r\n* Second: compare the set of predicted genes to the set of expected genes in a particular lineage. Default setting for lineage: Eukaryota\r\n\r\nPolished assembly and a reference genome => Quast\r\n* Contigs/scaffolds file: polished assembly\r\n* Type of assembly: Genome\r\n* Use a reference genome: Yes\r\n* Reference genome: Arabidopsis genome\r\n* Is the genome large (> 100Mbp)? Yes. \r\n* All other settings as defaults, except second last setting: Distinguish contigs with more than 50% unaligned bases as a separate group of contigs?: change to No\r\n\r\nOptions\r\n\r\nGene prediction: \r\n* Change tool used by Busco to predict genes in the assembly: instead of Metaeuk, use Augustus. \r\n* To do this: select: Use Augustus; Use another predefined species model; then choose from the drop down list.\r\n* Select from a database of trained species models. list here:  https://github.com/Gaius-Augustus/Augustus/tree/master/config/species\r\n* Note: if using Augustus: it may fail if the input assembly is too small (e.g. a test-size data assembly). It can't do the training part properly. \r\n\r\nCompare genes found to other lineage: \r\n* Busco has databases of lineages and their expected genes. Option to change lineage. \r\n* Not all lineages are available - there is a mix of broader and narrower lineages. - list of lineages here: https://busco.ezlab.org/list_of_lineages.html. \r\n* To see the groups in taxonomic hierarchies: Eukaryotes:  https://busco.ezlab.org/frames/euka.htm\r\n* For example,  if you have a plant species from Fabales, you could set that as the lineage. \r\n* The narrower the taxonomic group, the more total genes are expected. \r\n\r\n\r\n\r\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)\r\n",
        "doi": "10.48546/workflowhub.workflow.229.1",
        "edam_operation": [],
        "edam_topic": [
            "Sequence assembly"
        ],
        "filtered_on": "metap* in description",
        "id": "229",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/229?version=1",
        "name": "Assess genome quality",
        "number_of_steps": 2,
        "projects": [
            "Galaxy Australia",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "large-genome-assembly"
        ],
        "tools": [
            "quast",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-30",
        "versions": 1
    },
    {
        "create_time": "2021-11-08",
        "creators": [
            "Anna Syme"
        ],
        "description": "Assembly polishing subworkflow: Racon polishing with long reads\r\n\r\nInputs: long reads and assembly contigs\r\n\r\nWorkflow steps:\r\n* minimap2 : long reads are mapped to assembly => overlaps.paf. \r\n* overaps, long reads, assembly => Racon => polished assembly 1\r\n* using polished assembly 1 as input; repeat minimap2 + racon => polished assembly 2\r\n* using polished assembly 2 as input, repeat minimap2 + racon => polished assembly 3\r\n* using polished assembly 3 as input, repeat minimap2 + racon => polished assembly 4\r\n* Racon long-read polished assembly => Fasta statistics\r\n* Note: The Racon tool panel can be a bit confusing and is under review for improvement. Presently it requires sequences (= long reads), overlaps (= the paf file created by minimap2), and target sequences (= the contigs to be polished) as per \"usage\" described here https://github.com/isovic/racon/blob/master/README.md\r\n* Note: Racon: the default setting for \"output unpolished target sequences?\" is No. This has been changed to Yes for all Racon steps in these polishing workflows.  This means that even if no polishes are made in some contigs, they will be part of the output fasta file. \r\n* Note: the contigs output by Racon have new tags in their headers. For more on this see https://github.com/isovic/racon/issues/85.\r\n\r\n\r\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)",
        "doi": "10.48546/workflowhub.workflow.227.1",
        "edam_operation": [
            "Sequence assembly validation"
        ],
        "edam_topic": [
            "Sequence assembly"
        ],
        "filtered_on": "metap* in description",
        "id": "227",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/227?version=1",
        "name": "Racon polish with long reads, x4",
        "number_of_steps": 8,
        "projects": [
            "Galaxy Australia",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "large-genome-assembly"
        ],
        "tools": [
            "minimap2",
            "racon"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-30",
        "versions": 1
    },
    {
        "create_time": "2021-11-08",
        "creators": [
            "Anna Syme"
        ],
        "description": "Assembly with Flye; can run alone or as part of a combined workflow for large genome assembly. \r\n\r\n* What it does: Assembles long reads with the tool Flye\r\n* Inputs: long reads (may be raw, or filtered, and/or corrected); fastq.gz format\r\n* Outputs: Flye assembly fasta; Fasta stats on assembly.fasta; Assembly graph image from Bandage; Bar chart of contig sizes; Quast reports of genome assembly\r\n* Tools used: Flye, Fasta statistics, Bandage, Bar chart, Quast\r\n* Input parameters: None required, but recommend setting assembly mode to match input sequence type\r\n\r\nWorkflow steps:\r\n* Long reads are assembled with Flye, using default tool settings. Note: the default setting for read type (\"mode\") is nanopore raw. Change this at runtime if required. \r\n* Statistics are computed from the assembly.fasta file output, using Fasta Statistics and Quast (is genome large: Yes; distinguish contigs with more that 50% unaligned bases: no)\r\n* The graphical fragment assembly file is visualized with the tool Bandage. \r\n* Assembly information sent to bar chart to visualize contig sizes\r\n\r\nOptions\r\n* See other Flye options. \r\n* Use a different assembler (in a different workflow). \r\n* Bandage image options - change size (max size is 32767), labels - add (e.g. node lengths). You can also install Bandage on your own computer and donwload the \"graphical fragment assembly\" file to view in greater detail. \r\n\r\n\r\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.225.1",
        "edam_operation": [
            "De-novo assembly"
        ],
        "edam_topic": [
            "Sequence assembly"
        ],
        "filtered_on": "metap* in description",
        "id": "225",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/225?version=1",
        "name": "Assembly with Flye",
        "number_of_steps": 5,
        "projects": [
            "Galaxy Australia",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "large-genome-assembly"
        ],
        "tools": [
            "\n barchart_gnuplot",
            "bandage_image",
            "fasta-stats",
            "flye",
            "quast"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-30",
        "versions": 1
    },
    {
        "create_time": "2021-11-08",
        "creators": [
            "Anna Syme"
        ],
        "description": "Trim and filter reads; can run alone or as part of a combined workflow for large genome assembly. \r\n\r\n* What it does: Trims and filters raw sequence reads according to specified settings. \r\n* Inputs: Long reads (format fastq); Short reads R1 and R2 (format fastq) \r\n* Outputs: Trimmed and filtered reads: fastp_filtered_long_reads.fastq.gz (But note: no trimming or filtering is on by default), fastp_filtered_R1.fastq.gz, fastp_filtered_R2.fastq.gz\r\n* Reports: fastp report on long reads, html; fastp report on short reads, html\r\n* Tools used: fastp (Note. The latest version (0.20.1) of fastp has an issue displaying plot results. Using version 0.19.5 here instead until this is rectified). \r\n* Input parameters: None required, but recommend removing the long reads from the workflow if not using any trimming/filtering settings. \r\n\r\nWorkflow steps:\r\n\r\nLong reads: fastp settings: \r\n* These settings have been changed from the defaults (so that all filtering and trimming settings are now disabled). \r\n* Adapter trimming options: Disable adapter trimming: yes\r\n* Filter options: Quality filtering options: Disable quality filtering: yes\r\n* Filter options: Length filtering options: Disable length filtering: yes\r\n* Read modification options: PolyG tail trimming: Disable\r\n* Output options: output JSON report: yes\r\n\r\nShort reads: fastp settings:\r\n* adapter trimming (default setting: adapters are auto-detected)\r\n* quality filtering (default: phred quality 15), unqualified bases limit (default = 40%), number of Ns allowed in a read (default = 5)\r\n* length filtering (default length = min 15)\r\n* polyG tail trimming (default = on for NextSeq/NovaSeq data which is auto detected)\r\n* Output options: output JSON report: yes\r\n\r\nOptions:\r\n* Change any settings in fastp for any of the input reads. \r\n* Adapter trimming: input the actual adapter sequences. (Alternative tool for long read adapter trimming: Porechop.) \r\n* Trimming n bases from ends of reads if quality less than value x  (Alternative tool for trimming long reads: NanoFilt.)\r\n* Discard post-trimmed reads if length is < x (e.g. for long reads, 1000 bp)\r\n* Example filtering/trimming that you might do on long reads: remove adapters (can also be done with Porechop), trim bases from ends of the reads with low quality (can also be done with NanoFilt), after this can keep only reads of length x (e.g. 1000 bp) \r\n\r\n\r\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)",
        "doi": "10.48546/workflowhub.workflow.224.1",
        "edam_operation": [
            "Sequencing quality control"
        ],
        "edam_topic": [
            "Sequence assembly"
        ],
        "filtered_on": "metap* in description",
        "id": "224",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/224?version=1",
        "name": "Trim and filter reads - fastp",
        "number_of_steps": 2,
        "projects": [
            "Galaxy Australia",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "large-genome-assembly"
        ],
        "tools": [
            "fastp"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-30",
        "versions": 1
    },
    {
        "create_time": "2021-11-08",
        "creators": [
            "Anna Syme"
        ],
        "description": "Kmer counting step, can run alone or as part of a combined workflow for large genome assembly. \r\n\r\n* What it does: Estimates genome size and heterozygosity based on counts of kmers\r\n* Inputs: One set of short reads: e.g. R1.fq.gz\r\n* Outputs: GenomeScope graphs\r\n* Tools used: Meryl, GenomeScope\r\n* Input parameters: None required\r\n* Workflow steps: The tool meryl counts kmers in the input reads (k=21), then converts this into a histogram. GenomeScope: runs a model on the histogram; reports estimates. k-mer size set to 21. \r\n* Options: Use a different kmer counting tool. e.g. khmer.\r\n\r\n\r\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)",
        "doi": "10.48546/workflowhub.workflow.223.1",
        "edam_operation": [
            "Sequencing quality control"
        ],
        "edam_topic": [
            "Sequence assembly"
        ],
        "filtered_on": "metap* in description",
        "id": "223",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/223?version=1",
        "name": "kmer counting - meryl",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Australia",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "large-genome-assembly"
        ],
        "tools": [
            "genomescope",
            "meryl"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-30",
        "versions": 1
    },
    {
        "create_time": "2021-11-08",
        "creators": [
            "Anna Syme"
        ],
        "description": "Data QC step, can run alone or as part of a combined workflow for large genome assembly. \r\n\r\n* What it does: Reports statistics from sequencing reads.\r\n* Inputs: long reads (fastq.gz format), short reads (R1 and R2) (fastq.gz format).\r\n* Outputs: For long reads: a nanoplot report (the HTML report summarizes all the information). For short reads: a MultiQC report.\r\n* Tools used: Nanoplot, FastQC, MultiQC.\r\n* Input parameters: None required.\r\n* Workflow steps: Long reads are analysed by Nanoplot; Short reads (R1 and R2) are analysed by FastQC; the resulting reports are processed by MultiQC.\r\n* Options: see the tool settings options at runtime and change as required. Alternative tool option: fastp\r\n\r\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)\r\n",
        "doi": "10.48546/workflowhub.workflow.222.1",
        "edam_operation": [
            "Sequencing quality control"
        ],
        "edam_topic": [
            "Sequence assembly"
        ],
        "filtered_on": "metap* in description",
        "id": "222",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/222?version=1",
        "name": "Data QC",
        "number_of_steps": 4,
        "projects": [
            "Galaxy Australia",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "large-genome-assembly"
        ],
        "tools": [
            "nanoplot",
            "fastqc",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-30",
        "versions": 1
    },
    {
        "create_time": "2021-11-08",
        "creators": [
            "Anna Syme"
        ],
        "description": "Assembly polishing subworkflow: Racon polishing with short reads\r\n\r\nInputs: short reads and assembly (usually pre-polished with other tools first, e.g. Racon + long reads; Medaka)\r\n\r\nWorkflow steps: \r\n* minimap2: short reads (R1 only) are mapped to the assembly => overlaps.paf. Minimap2 setting is for short reads.\r\n* overlaps + short reads + assembly => Racon => polished assembly 1\r\n* using polished assembly 1 as input; repeat minimap2 + racon => polished assembly 2\r\n* Racon short-read polished assembly => Fasta statistics\r\n\r\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)",
        "doi": "10.48546/workflowhub.workflow.228.1",
        "edam_operation": [
            "Sequence assembly"
        ],
        "edam_topic": [
            "Sequence assembly"
        ],
        "filtered_on": "metap* in description",
        "id": "228",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/228?version=1",
        "name": "Racon polish with Illumina reads, x2",
        "number_of_steps": 4,
        "projects": [
            "Galaxy Australia",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "large-genome-assembly"
        ],
        "tools": [
            "minimap2",
            "racon"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-30",
        "versions": 1
    },
    {
        "create_time": "2021-11-08",
        "creators": [
            "Anna Syme"
        ],
        "description": "Assembly polishing; can run alone or as part of a combined workflow for large genome assembly. \r\n\r\n* What it does: Polishes (corrects) an assembly, using long reads (with the tools Racon and Medaka) and short reads (with the tool Racon). (Note: medaka is only for nanopore reads, not PacBio reads). \r\n* Inputs:  assembly to be polished:  assembly.fasta; long reads - the same set used in the assembly (e.g. may be raw or filtered) fastq.gz format; short reads, R1 only, in fastq.gz format\r\n* Outputs: Racon+Medaka+Racon polished_assembly. fasta; Fasta statistics after each polishing tool\r\n* Tools used: Minimap2, Racon, Fasta statistics, Medaka\r\n* Input parameters:  None required, but recommended to set the Medaka model correctly (default = r941_min_high_g360). See drop down list for options. \r\n\r\nWorkflow steps:\r\n\r\n-1-  Polish with long reads: using Racon\r\n* Long reads and assembly contigs => Racon polishing (subworkflow): \r\n* minimap2 : long reads are mapped to assembly => overlaps.paf. \r\n* overaps, long reads, assembly => Racon => polished assembly 1\r\n* using polished assembly 1 as input; repeat minimap2 + racon => polished assembly 2\r\n* using polished assembly 2 as input, repeat minimap2 + racon => polished assembly 3\r\n* using polished assembly 3 as input, repeat minimap2 + racon => polished assembly 4\r\n* Racon long-read polished assembly => Fasta statistics\r\n* Note: The Racon tool panel can be a bit confusing and is under review for improvement. Presently it requires sequences (= long reads), overlaps (= the paf file created by minimap2), and target sequences (= the contigs to be polished) as per \"usage\" described here https://github.com/isovic/racon/blob/master/README.md\r\n* Note: Racon: the default setting for \"output unpolished target sequences?\" is No. This has been changed to Yes for all Racon steps in these polishing workflows.  This means that even if no polishes are made in some contigs, they will be part of the output fasta file. \r\n* Note: the contigs output by Racon have new tags in their headers. For more on this see https://github.com/isovic/racon/issues/85.\r\n\r\n-2-  Polish with long reads: using Medaka\r\n* Racon polished assembly + long reads => medaka polishing X1 => medaka polished assembly\r\n* Medaka polished assembly => Fasta statistics\r\n\r\n-3-  Polish with short reads: using Racon\r\n* Short reads and Medaka polished assembly =>Racon polish (subworkflow):\r\n* minimap2: short reads (R1 only) are mapped to the assembly => overlaps.paf. Minimap2 setting is for short reads.\r\n* overlaps + short reads + assembly => Racon => polished assembly 1\r\n* using polished assembly 1 as input; repeat minimap2 + racon => polished assembly 2\r\n* Racon short-read polished assembly => Fasta statistics\r\n\r\nOptions\r\n* Change settings for Racon long read polishing if using PacBio reads:  The default profile setting for Racon long read polishing: minimap2 read mapping is \"Oxford Nanopore read to reference mapping\", which is specified as an input parameter to the whole Assembly polishing workflow, as text: map-ont. If you are not using nanopore reads and/or need a different setting, change this input. To see the other available settings, open the minimap2 tool, find \"Select a profile of preset options\", and click on the drop down menu. For each described option, there is a short text in brackets at the end (e.g. map-pb). This is the text to enter into the assembly polishing workflow at runtime instead of the default (map-ont).\r\n* Other options: change the number of polishes (in Racon and/or Medaka). There are ways to assess how much improvement in assembly quality has occurred per polishing round (for example, the number of corrections made; the change in Busco score - see section Genome quality assessment for more on Busco).\r\n* Option: change polishing settings for any of these tools. Note: for Racon - these will have to be changed within those subworkflows first. Then, in the main workflow, update the subworkflows, and re-save. \r\n\r\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)",
        "doi": "10.48546/workflowhub.workflow.226.1",
        "edam_operation": [
            "Sequence assembly"
        ],
        "edam_topic": [
            "Sequence assembly"
        ],
        "filtered_on": "metap* in description",
        "id": "226",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/226?version=1",
        "name": "Assembly polishing",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Australia",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "large-genome-assembly"
        ],
        "tools": [
            "medaka_consensus_pipeline",
            "\n d5a2cb013d9747c0",
            "\n 01041e6e0464607c",
            "fasta-stats"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-30",
        "versions": 1
    },
    {
        "create_time": "2021-10-19",
        "creators": [
            "Elida Schneltzer"
        ],
        "description": "This notebook is about pre-processing the Auditory Brainstem Response (ABR) raw data files provided by [Ingham et. al](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000194) to create a data set for Deep Learning models.\r\n\r\nThe unprocessed ABR data files are available at [Dryad](https://datadryad.org/stash/dataset/doi:10.5061/dryad.cv803rv).\r\n\r\nSince the ABR raw data are available as zip-archives, these have to be unzipped and the extracted raw data files parsed so that the time series corresponding to the ABR audiograms can be saved in a single csv file.\r\n\r\nThe final data set contains the ABR time series, an individual mouse identifier, stimulus frequency, stimulus sound pressure level (SPL) and a manually determined hearing threshold. For each mouse there are different time series corresponding to six different sound stimuli: broadband click, 6, 12, 18, 24, and 30 kHz, each of which was measured for a range of sound pressure levels. The exact range of sound levels can vary between the different mice and stimuli. \r\n\r\nThe following is done: \r\n\r\n* The zip archives are unpacked.\r\n* The extracted ABR raw data files are parsed and collected in one csv file per archive.\r\n* The csv files are merged into a data set of time series. Each time series corresponds to an ABR audiogram measured for a mouse at a specific frequency and sound level.\r\n* The mouse phenotyping data are available in Excel format. The individual data sheets are combined into one mouse phenotyping data set, maintaining the mouse pipeline and the cohort type mapping. In addition, the hearing thresholds are added to the ABR audiogram data set.\r\n* The data sets are curated: \r\n\r\n\t* there is a single curve per mouse, stimulus frequency and sound level,\r\n\t* each sound level is included in the list of potential sound pressure levels,\r\n\t* for each mouse for which an ABR audiogram has been measured, mouse phenotyping data are also provided.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "216",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/216?version=1",
        "name": "Preparing a data set for Deep Learning from zipped ABR raw data files",
        "number_of_steps": 0,
        "projects": [
            "Applied Computational Biology at IEG/HMGU"
        ],
        "source": "WorkflowHub",
        "tags": [
            "abr",
            "dl"
        ],
        "tools": [],
        "type": "Jupyter",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-08-18",
        "creators": [],
        "description": "The Flashlite-Supernova pipeline runs Supernova to generate phased whole-genome de novo assemblies from a Chromium prepared library on [University of Queensland's HPC, Flashlite](https://rcc.uq.edu.au/flashlite). \r\n\r\nInfrastructure\\_deployment\\_metadata: FlashLite (QRISCloud)",
        "doi": "10.48546/workflowhub.workflow.151.1",
        "edam_operation": [
            "De-novo assembly",
            "Phasing"
        ],
        "edam_topic": [
            "Sequence assembly",
            "Whole genome sequencing"
        ],
        "filtered_on": "metap* in description",
        "id": "151",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/151?version=1",
        "name": "Flashlite-Supernova",
        "number_of_steps": 0,
        "projects": [
            "Australian BioCommons",
            "Sydney Informatics Hub"
        ],
        "source": "WorkflowHub",
        "tags": [
            "10x",
            "flashlite",
            "supernova",
            "tellseq"
        ],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-08-18",
        "creators": [
            "Tracy Chew",
            "Rosemarie Sadsad",
            "Georgina Samaha"
        ],
        "description": "Flashlite-Trinity contains two workflows that run Trinity on the [University of Queensland's HPC, Flashlite](https://rcc.uq.edu.au/flashlite).  Trinity performs de novo transcriptome assembly of RNA-seq data by combining three independent software modules Inchworm, Chrysalis and Butterfly to process RNA-seq reads. The algorithm can detect isoforms, handle paired-end reads, multiple insert sizes and strandedness. Users can run Flashlite-Trinity on single samples, or smaller samples requiring <500Gb of memory or staged Trinity which is recommended for global assemblies with multiple sample inputs. Both implementations make use of Singularity containers to install software. \r\n\r\nInfrastructure\\_deployment\\_metadata: FlashLite (QRISCloud)",
        "doi": "10.48546/workflowhub.workflow.149.1",
        "edam_operation": [
            "De-novo assembly",
            "Sequence assembly",
            "Transcriptome assembly"
        ],
        "edam_topic": [
            "Gene transcripts",
            "Sequence assembly",
            "Transcriptomics"
        ],
        "filtered_on": "metap* in description",
        "id": "149",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/149?version=1",
        "name": "Flashlite-Trinity",
        "number_of_steps": 0,
        "projects": [
            "Australian BioCommons",
            "Sydney Informatics Hub"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "pbs",
            "transcriptomics",
            "container",
            "de novo",
            "global assemblies",
            "illumina",
            "rna",
            "rna-seq",
            "salmon",
            "scalable",
            "singularity",
            "strandedness",
            "transcriptome",
            "trinity"
        ],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-08-18",
        "creators": [
            "Tracy Chew",
            "Rosemarie Sadsad"
        ],
        "description": "Flashlite-Juicer is a PBS implementation of [Juicer](https://github.com/aidenlab/juicer) for University of Queensland's Flashlite HPC.\r\n\r\nInfrastructure\\_deployment\\_metadata: FlashLite (QRISCloud)",
        "doi": "10.48546/workflowhub.workflow.150.1",
        "edam_operation": [],
        "edam_topic": [
            "DNA packaging"
        ],
        "filtered_on": "metap* in description",
        "id": "150",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/150?version=1",
        "name": "Flashlite-Juicer",
        "number_of_steps": 0,
        "projects": [
            "Sydney Informatics Hub"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bwa",
            "fastq",
            "flashlite",
            "hi-c",
            "juicer",
            "pbs",
            "tad",
            "map",
            "scalable",
            "topologically associating domains"
        ],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-08-17",
        "creators": [
            "Georgina Samaha",
            "Rosemarie Sadsad",
            "Tracy Chew"
        ],
        "description": "Description: Trinity @ NCI-Gadi contains a staged [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki) workflow that can be run on the National Computational Infrastructure\u2019s (NCI) Gadi supercomputer. Trinity performs de novo transcriptome assembly of RNA-seq data by combining three independent software modules Inchworm, Chrysalis and Butterfly to process RNA-seq reads. The algorithm can detect isoforms, handle paired-end reads, multiple insert sizes and strandedness. \r\n\r\nInfrastructure\\_deployment\\_metadata: Gadi (NCI)",
        "doi": "10.48546/workflowhub.workflow.145.1",
        "edam_operation": [
            "De-novo assembly",
            "Sequence assembly",
            "Transcriptome assembly"
        ],
        "edam_topic": [
            "Transcriptomics"
        ],
        "filtered_on": "metap* in description",
        "id": "145",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/145?version=1",
        "name": "Trinity @ NCI-Gadi",
        "number_of_steps": 0,
        "projects": [
            "Australian BioCommons",
            "Sydney Informatics Hub"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "gadi",
            "nci",
            "pbs",
            "rnaseq",
            "transcriptomics",
            "rna",
            "rna-seq",
            "scalable",
            "trinity"
        ],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-08-09",
        "creators": [
            "Valentine Murigneux"
        ],
        "description": "microPIPE was developed to automate high-quality complete bacterial genome assembly using Oxford Nanopore Sequencing in combination with Illumina sequencing.\r\n\r\nTo build microPIPE we evaluated the performance of several tools at each step of bacterial genome assembly, including basecalling, assembly, and polishing. Results at each step were validated using the high-quality ST131 Escherichia coli strain EC958 (GenBank: HG941718.1). After appraisal of each step, we selected the best combination of tools to achieve the most consistent and best quality bacterial genome assemblies.\r\n\r\nThe workflow below summarises the different steps of the pipeline (with each selected tool) and the approximate run time (using GPU basecalling, averaged over 12 E. coli isolates sequenced on a R9.4 MinION flow cell). Dashed boxes correspond to optional steps in the pipeline.\r\n\r\nMicropipe has been written in Nextflow and uses Singularity containers. It can use both GPU and CPU resources.\r\n\r\nFor more information please see our publication here: https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-021-07767-z\r\n\r\nInfrastructure\\_deployment\\_metadata: Zeus (Pawsey)",
        "doi": "10.48546/workflowhub.workflow.140.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in description",
        "id": "140",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/140?version=1",
        "name": "microPIPE: a pipeline for high-quality bacterial genome construction using ONT and Illumina sequencing",
        "number_of_steps": 0,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "nextflow",
            "ont",
            "bacterial-genomics",
            "workflow"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-08-17",
        "creators": [],
        "description": "# SLURM HPC Cromwell implementation of GATK4 germline variant calling pipeline\r\nSee the [GATK](https://gatk.broadinstitute.org/hc/en-us) website for more information on this toolset \r\n## Assumptions\r\n- Using hg38 human reference genome build\r\n- Running using HPC/SLURM scheduling. This repo was specifically tested on Pawsey Zeus machine, primarily running in the `/scratch` partition. \r\n- Starting from short-read Illumina paired-end fastq files as input\r\n\r\n### Dependencies\r\nThe following versions have been tested and work, but GATK and Cromwell are regularly updated and so one must consider whether they would like to use newer versions of these tools. \r\n- BWA/0.7.15\r\n- GATK v4.0.6.0\r\n- SAMtools/1.5\r\n- picard/2.9\r\n- Python/2.7\r\n- Cromwell v61\r\n\r\n## Quick start guide\r\n### Installing and preparing environment for GATK4 with Cromwell\r\n\r\n1. Clone repository\r\n```\r\ngit clone https://github.com/SarahBeecroft/slurmCromwellGATK4.git\r\ncd slurmCromwellGATK4\r\nchmod +x *.sh\r\n```\r\n\r\n2. Install [Miniconda](https://docs.conda.io/en/latest/miniconda.html) if you haven\u2019t already. This is best placed in your `/group` directory to avoid filling your small `/home` directory, or being purged is placed in the `/scratch` directory.\r\n\r\n3. Create Conda environment using the supplied conda environment file\r\n\r\n```\r\nconda env create --file gatk4_pipeline.yml\r\n```\r\n\r\n3. Download the necessary .jar files\r\n    - The Cromwell workfow orchestration engine can be downloaded from https://github.com/broadinstitute/cromwell/releases/ \r\n    - GATK can be downloaded from https://github.com/broadinstitute/gatk/releases. Unzip the file with `unzip` \r\n    - Picard can be downloaded from https://github.com/broadinstitute/picard/releases/\r\n\r\n\r\n4. If you do not have the resource bundle files already, these need to be downloaded. In future they will be cached on Pawsey systems. The bundle data should be download from the [Google Cloud bucket](https://console.cloud.google.com/storage/browser/genomics-public-data/references/hg38/v0;tab=objects?_ga=2.98248159.1769807612.1582055494-233304531.1578854612&pli=1&prefix=&forceOnObjectsSortingFiltering=false) and not from the FTP site, which is missing various files. Refer to this handy [blog post](https://davetang.org/muse/2020/02/21/using-google-cloud-sdk-to-download-gatk-resource-bundle-files/) on how to download the resource files using Google Cloud SDK. There is a Slurm script (download_bundle.slurm) that can be used to download all hg38 files from the Google Cloud bucket. The files were downloaded in /scratch/pawsey0001/sbeecroft/hg38/v0, which needs to be moved before the data becomes purged after 30 days. Note that Homo_sapiens_assembly38.dbsnp138.vcf.gz was from the FTP bundle as this file could not be downloaded using the Conda version of Google Cloud SDK.\r\n\r\nNote that the `hg38_wgs_scattered_calling_intervals.txt` will need to be to generated using the following:\r\n\r\n```\r\ncd <your_resource_dir>\r\nfind `pwd` -name \"scattered.interval_list\" -print | sort > hg38_wgs_scattered_calling_intervals.txt\r\n```\r\n\r\nThese files are required for Multisample_Fastq_to_Gvcf_GATK4.\r\n\r\n```\r\nHomo_sapiens_assembly38.dict\r\nHomo_sapiens_assembly38.fasta\r\nHomo_sapiens_assembly38.fasta.fai\r\nHomo_sapiens_assembly38.fasta.64.alt\r\nHomo_sapiens_assembly38.fasta.64.amb\r\nHomo_sapiens_assembly38.fasta.64.ann\r\nHomo_sapiens_assembly38.fasta.64.bwt\r\nHomo_sapiens_assembly38.fasta.64.pac\r\nHomo_sapiens_assembly38.fasta.64.sa\r\nHomo_sapiens_assembly38.fasta.amb\r\nHomo_sapiens_assembly38.fasta.ann\r\nHomo_sapiens_assembly38.fasta.bwt\r\nHomo_sapiens_assembly38.fasta.pac\r\nHomo_sapiens_assembly38.fasta.sa\r\nHomo_sapiens_assembly38.dbsnp138.vcf.gz (needs to be gunzipped)\r\nHomo_sapiens_assembly38.dbsnp138.vcf.idx\r\nMills_and_1000G_gold_standard.indels.hg38.vcf.gz\r\nMills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi\r\nHomo_sapiens_assembly38.dbsnp138.vcf\r\nHomo_sapiens_assembly38.dbsnp138.vcf.idx\r\nHomo_sapiens_assembly38.known_indels.vcf.gz\r\nHomo_sapiens_assembly38.known_indels.vcf.gz.tbi\r\n```\r\n\r\nThese files are required for Multisample_jointgt_GATK4.\r\n\r\n```\r\nwgs_evaluation_regions.hg38.interval_list\r\nhg38.custom_100Mb.intervals\r\nHomo_sapiens_assembly38.dbsnp138.vcf\r\nHomo_sapiens_assembly38.dbsnp138.vcf.idx\r\n1000G_phase1.snps.high_confidence.hg38.vcf.gz\r\n1000G_phase1.snps.high_confidence.hg38.vcf.gz.tbi\r\n1000G_omni2.5.hg38.vcf.gz\r\n1000G_omni2.5.hg38.vcf.gz.tbi\r\nAxiom_Exome_Plus.genotypes.all_populations.poly.hg38.vcf.gz\r\nAxiom_Exome_Plus.genotypes.all_populations.poly.hg38.vcf.gz.tbi\r\nhapmap_3.3.hg38.vcf.gz\r\nhapmap_3.3.hg38.vcf.gz.tbi\r\n```\r\n\r\n\r\n5. Set up the config files. Files that you need to edit with the correct paths to your data/jar files or other specific configurations are:\r\n    - `Multisample_Fastq_to_Gvcf_GATK4_inputs_hg38.json`\r\n    - `Multisample_jointgt_GATK4_inputs_hg38.json`\r\n        - both json files will need the correct paths to your reference file locations, and the file specifying your inputs i.e. `samples.txt` or `gvcfs.txt`\r\n    - `samples.txt`\r\n    - `gvcfs.txt`\r\n        - These are the sample input files (tab seperated)\r\n        - The format for samples.txt is sampleID, sampleID_readgroup, path_to_fastq_R1_file, path_to_fastq_R2_file,\r\n        - The format for gvcfs.txt is sample ID, gvcf, gvcf .tbi index file\r\n        - Examples are included in this repo\r\n        - NOTE: Having tabs, not spaces, is vital for parsing the file. Visual studio code tends to introduce spaces, so if you are having issues, check the file with another text editor such as sublime. \r\n    - `launch_cromwell.sh`\r\n    - `launch_jointgt.sh`\r\n        - These are the scripts which launch the pipeline. \r\n        - `launch_cromwell.sh` launches the fastq to gvcf stage\r\n        - `launch_jointgt.sh` launched the gvcf joint genotyping to cohort vcf step. This is perfomed when you have run all samples through the fastq to gvcf stage.\r\n        - Check the paths and parameters make sense for your machine\r\n    - `slurm.conf`\r\n        - the main options here relate to the job scheduler. If you are running on Zeus at Pawsey, you should not need to alter these parameters.\r\n    - `cromwell.options`\r\n        - `cromwell.options` requires editing to provide the directory where you would like the final workflow outputs to be written\r\n    - `Multisample_Fastq_to_Gvcf_GATK4.wdl`\r\n    - `ruddle_fastq_to_gvcf_single_sample_gatk4.wdl`\r\n        - The paths to your jar files will need to be updated\r\n        - The path to your conda `activate` binary will need to be updated (e.g. `/group/projectID/userID/miniconda/bin/activate`)\r\n\r\n6. Launch the job using `sbatch launch_cromwell.sh`. When that has completed successfully, you can launch the second stage of the pipeline (joint calling) with `sbatch launch_jointgt.sh`.\r\n\r\n### Overview of the steps in `Multisample_Fastq_to_Gvcf_GATK4.wdl`\r\nThis part of the pipeline takes short-read, Illumina paired-end fastq files as the input. The outputs generated are sorted, duplicate marked bam files and their indices, duplicate metric information, and a GVCF file for each sample. The GVCF files are used as input for the second part of the pipeline (joint genotyping).\r\n\r\n```\r\nFastqToUbam\r\nGetBwaVersion\r\nSamToFastqAndBwaMem\r\nMergeBamAlignment\r\nSortAndFixTags\r\nMarkDuplicates\r\nCreateSequenceGroupingTSV\r\nBaseRecalibrator\r\nGatherBqsrReports\r\nApplyBQSR\r\nGatherBamFiles\r\nHaplotypeCaller\r\nMergeGVCFs\r\n```\r\n\r\n### Overview of the steps in `Multisample_jointgt_GATK4.wdl`\r\nThis part of the pipeline takes GVCF files (one per sample), and performs joint genotyping across all of the provided samples. This means that old previously generated GVCFs can be joint-called with new GVCFs whenever you need to add new samples. The key output from this is a joint-genotyped, cohort-wide VCF file.\r\n\r\n```\r\nGetNumberOfSamples\r\nImportGVCFs\r\nGenotypeGVCFs\r\nHardFilterAndMakeSitesOnlyVcf\r\nIndelsVariantRecalibrator\r\nSNPsVariantRecalibratorCreateModel\r\nSNPsVariantRecalibrator\r\nGatherTranches\r\nApplyRecalibration\r\nGatherVcfs\r\nCollectVariantCallingMetrics\r\nGatherMetrics\r\nDynamicallyCombineIntervals\r\n```\r\n",
        "doi": null,
        "edam_operation": [
            "Sequence alignment",
            "Variant calling",
            "Variant filtering"
        ],
        "edam_topic": [
            "Exome sequencing",
            "Genetic variation",
            "Whole genome sequencing"
        ],
        "filtered_on": "binn* in description",
        "id": "144",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/144?version=1",
        "name": "GATK4 Fastq to joint-called cohort VCF with Cromwell on SLURM",
        "number_of_steps": 0,
        "projects": [
            "Australian BioCommons",
            "Pawsey Supercomputing Research Centre"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "gatk4",
            "genomics",
            "indels",
            "snps",
            "variant_calling"
        ],
        "tools": [],
        "type": "Workflow Description Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-08-17",
        "creators": [],
        "description": "# Local Cromwell implementation of GATK4 germline variant calling pipeline\r\nSee the [GATK](https://gatk.broadinstitute.org/hc/en-us) website for more information on this toolset \r\n## Assumptions\r\n- Using hg38 human reference genome build\r\n- Running 'locally' i.e. not using HPC/SLURM scheduling, or containers. This repo was specifically tested on Pawsey Nimbus 16 CPU, 64GB RAM virtual machine, primarily running in the `/data` volume storage partition. \r\n- Starting from short-read Illumina paired-end fastq files as input\r\n\r\n### Dependencies\r\nThe following versions have been tested and work, but GATK and Cromwell are regularly updated and so one must consider whether they would like to use newer versions of these tools. \r\n- BWA/0.7.15\r\n- GATK v4.0.6.0\r\n- SAMtools/1.5\r\n- picard/2.9\r\n- Python/2.7\r\n- Cromwell v61\r\n\r\n## Quick start guide\r\n### Installing and preparing environment for GATK4 with Cromwell\r\n\r\n1. Clone repository\r\n```\r\ngit clone https://github.com/SarahBeecroft/cromwellGATK4.git\r\ncd cromwellGATK4\r\nchmod 777 *.sh\r\n```\r\n\r\n2. Install [Miniconda](https://docs.conda.io/en/latest/miniconda.html) if you haven\u2019t already. Create Conda environment using the supplied conda environment file\r\n\r\n```\r\nconda env create --file gatk4_pipeline.yml\r\n```\r\n\r\n3. Download the necessary .jar files\r\n    - The Cromwell workfow orchestration engine can be downloaded from https://github.com/broadinstitute/cromwell/releases/ \r\n    - GATK can be downloaded from https://github.com/broadinstitute/gatk/releases. Unzip the file with `unzip` \r\n    - Picard can be downloaded from https://github.com/broadinstitute/picard/releases/\r\n\r\n\r\n4. Upload the resource bundle file from IRDS using rclone or filezilla and unpack it with `tar xzvf resource.tar.gz`. Note that the `hg38_wgs_scattered_calling_intervals.txt` will need to be to generated using the following:\r\n\r\n```\r\ncd <your_resource_dir>\r\nfind `pwd` -name \"scattered.interval_list\" -print | sort > hg38_wgs_scattered_calling_intervals.txt\r\n```\r\n\r\n5. Set up the config files. Files that you need to edit with the correct paths to your data/jar files or other specific configurations are:\r\n    - `Multisample_Fastq_to_Gvcf_GATK4_inputs_hg38.json`\r\n    - `Multisample_jointgt_GATK4_inputs_hg38.json`\r\n        - both json files will need the correct paths to your reference file locations, and the file specifying your inputs i.e. `samples.txt` or `gvcfs.txt`\r\n    - `samples.txt`\r\n    - `gvcfs.txt`\r\n        - These are the sample input files (tab seperated)\r\n        - The format for samples.txt is sampleID, sampleID_readgroup, path_to_fastq_R1_file, path_to_fastq_R2_file,\r\n        - The format for gvcfs.txt is sample ID, gvcf, gvcf .tbi index file\r\n        - Examples are included in this repo\r\n        - NOTE: Having tabs, not spaces, is vital for parsing the file. Visual studio code tends to introduce spaces, so if you are having issues, check the file with another text editor such as sublime. \r\n    - `launch_cromwell.sh`\r\n    - `launch_jointgt.sh`\r\n        - These are the scripts which launch the pipeline. \r\n        - `launch_cromwell.sh` launches the fastq to gvcf stage\r\n        - `launch_jointgt.sh` launched the gvcf joint genotyping to cohort vcf step. This is perfomed when you have run all samples through the fastq to gvcf stage.\r\n        - Check the paths and parameters make sense for your machine\r\n    - `local.conf`\r\n        - the main tuneable parameters here are:\r\n        \t- `concurrent-job-limit = 5` this is the max number of concurrent jobs that can be spawned by cromwell. This depends on the computational resources available to you. 5 was determined to work reasonably well on a 16 CPU, 64GB RAM Nimbus VM (Pawsey). \r\n        \t- `call-caching enabled = true`. Setting this parameter to `false` will disable call caching (i.e. being able to resume if the job fails before completion). By default, call caching is enabled. \r\n    - `cromwell.options`\r\n        - `cromwell.options` requires editing to provide the directory where you would like the final workflow outputs to be written\r\n    - `Multisample_Fastq_to_Gvcf_GATK4.wdl`\r\n    - `ruddle_fastq_to_gvcf_single_sample_gatk4.wdl`\r\n        - The paths to your jar files will need to be updated\r\n        - The path to your conda `activate` binary will need to be updated (e.g. `/data/miniconda/bin/activate`)\r\n\r\n6. Launch the job within a `screen` or `tmux` session, using `./launch_cromwell.sh`. When that has completed successfully, you can launch the second stage of the pipeline (joint calling) with `./launch_jointgt.sh`. Ensure you pipe the stdout and stderr to a log file using (for example) `./launch_cromwell.sh &> cromwell.log`\r\n\r\n### Overview of the steps in `Multisample_Fastq_to_Gvcf_GATK4.wdl`\r\nThis part of the pipeline takes short-read, Illumina paired-end fastq files as the input. The outputs generated are sorted, duplicate marked bam files and their indices, duplicate metric information, and a GVCF file for each sample. The GVCF files are used as input for the second part of the pipeline (joint genotyping).\r\n\r\n```\r\nFastqToUbam\r\nGetBwaVersion\r\nSamToFastqAndBwaMem\r\nMergeBamAlignment\r\nSortAndFixTags\r\nMarkDuplicates\r\nCreateSequenceGroupingTSV\r\nBaseRecalibrator\r\nGatherBqsrReports\r\nApplyBQSR\r\nGatherBamFiles\r\nHaplotypeCaller\r\nMergeGVCFs\r\n```\r\n\r\n### Overview of the steps in `Multisample_jointgt_GATK4.wdl`\r\nThis part of the pipeline takes GVCF files (one per sample), and performs joint genotyping across all of the provided samples. This means that old previously generated GVCFs can be joint-called with new GVCFs whenever you need to add new samples. The key output from this is a joint-genotyped, cohort-wide VCF file. This file can be used for a GEMINI database after normalisation with VT and annotation with a tool such as VEP or SNPEFF. \r\n\r\nThe file `hg38.custom_100Mb.intervals` is required for this step of the pipeline to run. This is included in the git repo for convenience, but should be moved to your resource directory with all the other resource files. \r\n\r\n```\r\nGetNumberOfSamples\r\nImportGVCFs\r\nGenotypeGVCFs\r\nHardFilterAndMakeSitesOnlyVcf\r\nIndelsVariantRecalibrator\r\nSNPsVariantRecalibratorCreateModel\r\nSNPsVariantRecalibrator\r\nGatherTranches\r\nApplyRecalibration\r\nGatherVcfs\r\nCollectVariantCallingMetrics\r\nGatherMetrics\r\nDynamicallyCombineIntervals\r\n```\r\n",
        "doi": null,
        "edam_operation": [
            "Sequence alignment",
            "Variant calling",
            "Variant filtering"
        ],
        "edam_topic": [
            "Exome sequencing",
            "Genetic variation",
            "Whole genome sequencing"
        ],
        "filtered_on": "binn* in description",
        "id": "147",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/147?version=1",
        "name": "GATK4 Fastq to joint-called cohort VCF with Cromwell on local cluster (no job scheduler)",
        "number_of_steps": 0,
        "projects": [
            "Australian BioCommons",
            "Pawsey Supercomputing Research Centre"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "gatk4",
            "genomics",
            "indels",
            "snps",
            "variant_calling",
            "workflow"
        ],
        "tools": [],
        "type": "Workflow Description Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-08-11",
        "creators": [
            "Mike Thang"
        ],
        "description": "This is a Galaxy workflow that uses to convert the16S BIOM file to table and figures. It is part of the metaDEGalaxy workflow MetaDEGalaxy: Galaxy workflow for differential abundance analysis of 16s metagenomic data. ",
        "doi": null,
        "edam_operation": [
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "142",
        "keep": true,
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/142?version=1",
        "name": "16S_biodiversity_BIOM",
        "number_of_steps": 8,
        "projects": [
            "Galaxy Australia"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metadegalaxy"
        ],
        "tools": [
            "biom_convert",
            "phyloseq_DESeq2",
            "phyloseq_abundance",
            "phyloseq_taxonomy",
            "biom_add_metadata",
            "symmetricPlot",
            "phyloseq_richness",
            "phyloseq_net"
        ],
        "type": "Galaxy",
        "update_time": "2024-04-17",
        "versions": 1
    },
    {
        "create_time": "2021-07-08",
        "creators": [
            "Cyril Noel",
            "Alexandre Cormier",
            "Patrick Durand",
            "Laura Leroi",
            "Pierre Cuzin"
        ],
        "description": "ORSON combine state-of-the-art tools for annotation processes within a Nextflow pipeline: sequence similarity search (PLAST, BLAST or Diamond), functional annotation retrieval (BeeDeeM) and functional prediction (InterProScan). When required, BUSCO completness evaluation and eggNOG Orthogroup annotation can be activated. While ORSON results can be analyzed through the command-line, it also offers the possibility to be compatible with BlastViewer or Blast2GO graphical tools.\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.136.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "136",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/136?version=1",
        "name": "ORSON: workflow for prOteome and tRanScriptome functiOnal aNnotation",
        "number_of_steps": 0,
        "projects": [
            "SeBiMER"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "genomics",
            "nextflow",
            "proteomics",
            "transcriptomics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-06-17",
        "creators": [],
        "description": "Metagenomic dataset taxonomic classification using kraken2",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "124",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/124?version=1",
        "name": "1: Plant virus detection with kraken2 (SE)",
        "number_of_steps": 3,
        "projects": [
            "Integrated and Urban Plant Pathology Laboratory"
        ],
        "source": "WorkflowHub",
        "tags": [
            "virology",
            "kraken"
        ],
        "tools": [
            "taxonomy_krona_chart",
            "kraken2",
            "Kraken2Tax"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2021-02-04",
        "creators": [],
        "description": "Metagenomic dataset taxonomic classification using kraken2",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "101",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/101?version=1",
        "name": "1: Plant virus detection with kraken2 (PE)",
        "number_of_steps": 3,
        "projects": [
            "Integrated and Urban Plant Pathology Laboratory"
        ],
        "source": "WorkflowHub",
        "tags": [
            "virology",
            "kraken"
        ],
        "tools": [
            "taxonomy_krona_chart",
            "kraken2",
            "Kraken2Tax"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2020-10-28",
        "creators": [
            "Jasper Koehorst",
            "Bart Nijsse"
        ],
        "description": "Amplicon analysis workflow using NG-Tax\r\n\r\n**Steps:**\r\n\r\n* Quality control on the reads\r\n* Execute NGTax for ASV detection and classification\r\n\r\nFor more information about NG-Tax 2.0 have a look at https://doi.org/10.3389/fgene.2019.01366",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "16S in tags",
        "id": "45",
        "keep": true,
        "latest_version": 7,
        "license": "CC0-1.0",
        "link": "https:/workflowhub.eu/workflows/45?version=7",
        "name": "NGTax",
        "number_of_steps": 5,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "16s",
            "amplicon",
            "its"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-05-21",
        "creators": [
            "Jos\u00e9 M\u00aa Fern\u00e1ndez"
        ],
        "description": "Rare disease researchers workflow is that they submit their raw data (fastq), run the mapping and variant calling RD-Connect pipeline and obtain unannotated gvcf files to further submit to the RD-Connect GPAP or analyse on their own.\r\n\r\nThis demonstrator focuses on the variant calling pipeline. The raw genomic data is processed using the RD-Connect pipeline ([Laurie et al., 2016](https://www.ncbi.nlm.nih.gov/pubmed/27604516)) running on the standards (GA4GH) compliant, interoperable container orchestration platform.\r\n\r\nThis demonstrator will be aligned with the current implementation study on [Development of Architecture for Software Containers at ELIXIR and its use by EXCELERATE use-case communities](docs/Appendix%201%20-%20Project%20Plan%202018-biocontainers%2020171117.pdf) \r\n\r\nFor this implementation, different steps are required:\r\n\r\n1. Adapt the pipeline to CWL and dockerise elements \r\n2. Align with IS efforts on software containers to package the different components (Nextflow) \r\n3. Submit trio of Illumina NA12878 Platinum Genome or Exome to the GA4GH platform cloud (by Aspera or ftp server)\r\n4. Run the RD-Connect pipeline on the container platform\r\n5. Return corresponding gvcf files\r\n6. OPTIONAL: annotate and update to RD-Connect playground instance\r\n\r\nN.B: The demonstrator might have some manual steps, which will not be in production. \r\n\r\n## RD-Connect pipeline\r\n\r\nDetailed information about the RD-Connect pipeline can be found in [Laurie et al., 2016](https://www.ncbi.nlm.nih.gov/pubmed/?term=27604516)\r\n\r\n![alt text](https://raw.githubusercontent.com/inab/Wetlab2Variations/eosc-life/docs/RD-Connect_pipeline.jpg)\r\n\r\n## The applications\r\n\r\n**1\\. Name of the application: Adaptor removal**\r\nFunction: remove sequencing adaptors   \r\nContainer (readiness status, location, version): [cutadapt (v.1.18)](https://hub.docker.com/r/cnag/cutadapt)  \r\nRequired resources in cores and RAM: current container size 169MB  \r\nInput data (amount, format, directory..): raw fastq  \r\nOutput data: paired fastq without adaptors  \r\n\r\n**2\\. Name of the application: Mapping and bam sorting**\r\nFunction: align data to reference genome  \r\nContainer : [bwa-mem (v.0.7.17)](https://hub.docker.com/r/cnag/bwa) / [Sambamba (v. 0.6.8 )](https://hub.docker.com/r/cnag/sambamba)(or samtools)  \r\nResources :current container size 111MB / 32MB  \r\nInput data: paired fastq without adaptors  \r\nOutput data: sorted bam  \r\n\r\n**3\\. Name of the application: MarkDuplicates**  \r\nFunction: Mark (and remove) duplicates  \r\nContainer: [Picard (v.2.18.25)](https://hub.docker.com/r/cnag/picard)\r\nResources: current container size 261MB  \r\nInput data:sorted bam  \r\nOutput data: Sorted bam with marked (or removed) duplicates  \r\n\r\n**4\\. Name of the application: Base quality recalibration (BQSR)**  \r\nFunction: Base quality recalibration  \r\nContainer: [GATK (v.3.6-0)](https://hub.docker.com/r/cnag/gatk)\r\nResources: current container size 270MB  \r\nInput data: Sorted bam with marked (or removed) duplicates  \r\nOutput data: Sorted bam with marked duplicates & base quality recalculated  \r\n\r\n**5\\. Name of the application: Variant calling**  \r\nFunction: variant calling  \r\nContainer: [GATK (v.3.6-0)](https://hub.docker.com/r/cnag/gatk)\r\nResources: current container size 270MB  \r\nInput data:Sorted bam with marked duplicates & base quality recalculated  \r\nOutput data: unannotated gvcf per sample  \r\n\r\n**6\\. (OPTIONAL)Name of the application: Quality of the fastq**  \r\nFunction: report on the sequencing quality  \r\nContainer: [fastqc 0.11.8](https://hub.docker.com/r/cnag/fastqc)\r\nResources: current container size 173MB  \r\nInput data: raw fastq  \r\nOutput data: QC report \r\n\r\n## Licensing\r\n\r\nGATK declares that archived packages are made available for free to academic researchers under a limited license for non-commercial use. If you need to use one of these packages for commercial use. https://software.broadinstitute.org/gatk/download/archive ",
        "doi": "10.48546/workflowhub.workflow.106.3",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "106",
        "keep": true,
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/106?version=3",
        "name": "VariantCaller_GATK3.6",
        "number_of_steps": 0,
        "projects": [
            "EOSC-Life - Demonstrator 7: Rare Diseases"
        ],
        "source": "WorkflowHub",
        "tags": [
            "nextflow",
            "variant_calling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 3
    },
    {
        "create_time": "2021-05-05",
        "creators": [
            "Laura Rodriguez-Navas",
            "Jos\u00e9 M\u00aa Fern\u00e1ndez"
        ],
        "description": "# COnSensus Interaction Network InFErence Service\r\nInference framework for reconstructing networks using a consensus approach between multiple methods and data sources.\r\n\r\n![alt text](https://raw.githubusercontent.com/PhosphorylatedRabbits/cosifer/master/docs/_static/logo.png)\r\n\r\n## Reference\r\n[Manica, Matteo, Charlotte, Bunne, Roland, Mathis, Joris, Cadow, Mehmet Eren, Ahsen, Gustavo A, Stolovitzky, and Mar\u00eda Rodr\u00edguez, Mart\u00ednez. \"COSIFER: a python package for the consensus inference of molecular interaction networks\".Bioinformatics (2020)](https://doi.org/10.1093/bioinformatics/btaa942).",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "ITS in description",
        "id": "119",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/119?version=1",
        "name": "COSIFER",
        "number_of_steps": 0,
        "projects": [
            "iPC: individualizedPaediatricCure"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cancer",
            "cosifer",
            "pediatric",
            "rna-seq"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-04-21",
        "versions": 1
    },
    {
        "create_time": "2021-04-09",
        "creators": [
            "Yvan Le Bras"
        ],
        "description": "Workflow to take DataOne data packages (raw datasets + metadata written in Ecological Metadata Standard) as input and create a DwC occurence.csv file almost ready to put in a Dawrin core Archive using eml-annotations at the attribute level",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in tags",
        "id": "117",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/117?version=1",
        "name": "Workflow constructed from history 'test dwc from PNDB Data package EML DwC annotations'",
        "number_of_steps": 21,
        "projects": [
            "PNDB"
        ],
        "source": "WorkflowHub",
        "tags": [
            "darwin core",
            "data package",
            "dataone",
            "eml",
            "ecological metadata language",
            "galaxy",
            "galaxy-e",
            "eml-annotation"
        ],
        "tools": [
            "regexColumn1",
            "filter_tabular",
            "tp_easyjoin_tool",
            "tp_grep_tool",
            "regex_replace",
            "\n Grep1",
            "bg_uniq",
            "\n Remove beginning1",
            "datamash_transpose"
        ],
        "type": "Galaxy",
        "update_time": "2023-11-09",
        "versions": 1
    },
    {
        "create_time": "2021-03-21",
        "creators": [],
        "description": "Workflow for tracking objects in Cell Profiler:\r\nhttps://training.galaxyproject.org/training-material/topics/imaging/tutorials/object-tracking-using-cell-profiler/tutorial.html",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioimaging",
            "Imaging"
        ],
        "filtered_on": "profil* in tags",
        "id": "115",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/115?version=1",
        "name": "Object tracking using CellProfiler",
        "number_of_steps": 12,
        "projects": [
            "IBISBA Workflows",
            "Euro-BioImaging"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cellprofiler",
            "galaxy",
            "image processing",
            "imaging"
        ],
        "tools": [
            "cp_color_to_gray",
            "cp_identify_primary_objects",
            "cp_common",
            "cp_measure_object_size_shape",
            "cp_tile",
            "cp_overlay_outlines",
            "cp_export_to_spreadsheet",
            "cp_measure_object_intensity",
            "cp_save_images",
            "cp_cellprofiler",
            "unzip",
            "cp_track_objects"
        ],
        "type": "Galaxy",
        "update_time": "2023-07-03",
        "versions": 1
    },
    {
        "create_time": "2020-06-08",
        "creators": [
            "Martin Beracochea"
        ],
        "description": "<p class=\"has-line-data\" data-line-start=\"0\" data-line-end=\"1\"><img src=\"https://img.shields.io/badge/CWL-1.2.0--dev2-green\" alt=\"\"> <img src=\"https://img.shields.io/badge/nextflow-20.01.0-brightgreen\" alt=\"\"> <img src=\"https://img.shields.io/badge/uses-docker-blue.svg\" alt=\"\"> <img src=\"https://img.shields.io/badge/uses-conda-yellow.svg\" alt=\"\"> <img src=\"https://api.travis-ci.org/EBI-Metagenomics/emg-viral-pipeline.svg\" alt=\"\"></p>\r\n<h1 class=\"code-line\" data-line-start=2 data-line-end=3 ><a id=\"VIRify_2\"></a>VIRify</h1>\r\n<p class=\"has-line-data\" data-line-start=\"4\" data-line-end=\"5\"><img width=\"500px\" src=\"https://raw.githubusercontent.com/EBI-Metagenomics/emg-viral-pipeline/master/nextflow/figures/sankey.png\" alt=\"Sankey plot\"></p>\r\n<p class=\"has-line-data\" data-line-start=\"6\" data-line-end=\"7\">VIRify is a recently developed pipeline for the detection, annotation, and taxonomic classification of viral contigs in metagenomic and metatranscriptomic assemblies. The pipeline is part of the repertoire of analysis services offered by <a href=\"https://www.ebi.ac.uk/metagenomics/\">MGnify</a>. VIRify\u2019s taxonomic classification relies on the detection of taxon-specific profile hidden Markov models (HMMs), built upon a set of 22,014 orthologous protein domains and referred to as ViPhOGs.</p>\r\n<p class=\"has-line-data\" data-line-start=\"8\" data-line-end=\"9\">VIRify was implemented in CWL.</p>\r\n<h2 class=\"code-line\" data-line-start=10 data-line-end=11 ><a id=\"What_do_I_need_10\"></a>What do I need?</h2>\r\n<p class=\"has-line-data\" data-line-start=\"12\" data-line-end=\"13\">The current implementation uses CWL version 1.2 dev+2. It was tested using Toil version 4.10 as the workflow engine and conda to manage the software dependencies.</p>\r\n<h3 class=\"code-line\" data-line-start=14 data-line-end=15 ><a id=\"Docker__Singularity_support_14\"></a>Docker - Singularity support</h3>\r\n<p class=\"has-line-data\" data-line-start=\"16\" data-line-end=\"17\">Soon\u2026</p>\r\n<h2 class=\"code-line\" data-line-start=18 data-line-end=19 ><a id=\"Setup_environment_18\"></a>Setup environment</h2>\r\n<pre><code class=\"has-line-data\" data-line-start=\"21\" data-line-end=\"24\" class=\"language-bash\">conda env create <span class=\"hljs-operator\">-f</span> cwl/requirements/conda_env.yml\r\nconda activate viral_pipeline\r\n</code></pre>\r\n<h2 class=\"code-line\" data-line-start=25 data-line-end=26 ><a id=\"Basic_execution_25\"></a>Basic execution</h2>\r\n<pre><code class=\"has-line-data\" data-line-start=\"28\" data-line-end=\"31\" class=\"language-bash\"><span class=\"hljs-built_in\">cd</span> cwl/\r\nvirify.sh -h\r\n</code></pre>\r\n<h1 class=\"code-line\" data-line-start=32 data-line-end=33 ><a id=\"A_note_about_metatranscriptomes_32\"></a>A note about metatranscriptomes</h1>\r\n<p class=\"has-line-data\" data-line-start=\"34\" data-line-end=\"36\">Although VIRify has been benchmarked and validated with metagenomic data in mind, it is also possible to use this tool to detect RNA viruses in metatranscriptome assemblies (e.g. SARS-CoV-2). However, some additional considerations for this purpose are outlined below:<br>\r\n<strong>1. Quality control:</strong> As for metagenomic data, a thorough quality control of the FASTQ sequence reads to remove low-quality bases, adapters and host contamination (if appropriate) is required prior to assembly. This is especially important for metatranscriptomes as small errors can further decrease the quality and contiguity of the assembly obtained. We have used <a href=\"https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/\">TrimGalore</a> for this purpose.</p>\r\n<p class=\"has-line-data\" data-line-start=\"37\" data-line-end=\"38\"><strong>2. Assembly:</strong> There are many assemblers available that are appropriate for either metagenomic or single-species transcriptomic data. However, to our knowledge, there is no assembler currently available specifically for metatranscriptomic data. From our preliminary investigations, we have found that transcriptome-specific assemblers (e.g. <a href=\"http://cab.spbu.ru/software/spades/\">rnaSPAdes</a>) generate more contiguous and complete metatranscriptome assemblies compared to metagenomic alternatives (e.g. <a href=\"https://github.com/voutcn/megahit/releases\">MEGAHIT</a> and <a href=\"http://cab.spbu.ru/software/spades/\">metaSPAdes</a>).</p>\r\n<p class=\"has-line-data\" data-line-start=\"39\" data-line-end=\"40\"><strong>3. Post-processing:</strong> Metatranscriptomes generate highly fragmented assemblies. Therefore, filtering contigs based on a set minimum length has a substantial impact in the number of contigs processed in VIRify. It has also been observed that the number of false-positive detections of <a href=\"https://github.com/jessieren/VirFinder/releases\">VirFinder</a> (one of the tools included in VIRify) is lower among larger contigs. The choice of a length threshold will depend on the complexity of the sample and the sequencing technology used, but in our experience any contigs &lt;2 kb should be analysed with caution.</p>\r\n<p class=\"has-line-data\" data-line-start=\"41\" data-line-end=\"42\"><strong>4. Classification:</strong> The classification module of VIRify depends on the presence of a minimum number and proportion of phylogenetically-informative genes within each contig in order to confidently assign a taxonomic lineage. Therefore, short contigs typically obtained from metatranscriptome assemblies remain generally unclassified. For targeted classification of RNA viruses (for instance, to search for Coronavirus-related sequences), alternative DNA- or protein-based classification methods can be used. Two of the possible options are: (i) using <a href=\"https://github.com/marbl/MashMap/releases\">MashMap</a> to screen the VIRify contigs against a database of RNA viruses (e.g. Coronaviridae) or (ii) using <a href=\"http://hmmer.org/download.html\">hmmsearch</a> to screen the proteins obtained in the VIRify contigs against marker genes of the taxon of interest.</p>\r\n<h2>Contact us</h2>\r\n<a href=\"https://www.ebi.ac.uk/support/metagenomics\">MGnify helpdesk</a>",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "27",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/27?version=1",
        "name": "VIRify",
        "number_of_steps": 0,
        "projects": [
            "MGnify",
            "HoloFood at MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-03-24",
        "versions": 1
    },
    {
        "create_time": "2020-06-08",
        "creators": [
            "Martin Beracochea"
        ],
        "description": "<p class=\"has-line-data\" data-line-start=\"0\" data-line-end=\"1\"><img src=\"https://img.shields.io/badge/CWL-1.2.0--dev2-green\" alt=\"\"> <img src=\"https://img.shields.io/badge/nextflow-20.01.0-brightgreen\" alt=\"\"> <img src=\"https://img.shields.io/badge/uses-docker-blue.svg\" alt=\"\"> <img src=\"https://img.shields.io/badge/uses-conda-yellow.svg\" alt=\"\"> <img src=\"https://api.travis-ci.org/EBI-Metagenomics/emg-viral-pipeline.svg\" alt=\"\"></p>\r\n<h1 class=\"code-line\" data-line-start=2 data-line-end=3 ><a id=\"VIRify_2\"></a>VIRify</h1>\r\n<p class=\"has-line-data\" data-line-start=\"4\" data-line-end=\"5\"><img width=\"500px\" src=\"https://raw.githubusercontent.com/EBI-Metagenomics/emg-viral-pipeline/master/nextflow/figures/sankey.png\" alt=\"Sankey plot\"></p>\r\n<p class=\"has-line-data\" data-line-start=\"6\" data-line-end=\"7\">VIRify is a recently developed pipeline for the detection, annotation, and taxonomic classification of viral contigs in metagenomic and metatranscriptomic assemblies. The pipeline is part of the repertoire of analysis services offered by <a href=\"https://www.ebi.ac.uk/metagenomics/\">MGnify</a>. VIRify\u2019s taxonomic classification relies on the detection of taxon-specific profile hidden Markov models (HMMs), built upon a set of 22,014 orthologous protein domains and referred to as ViPhOGs.</p>\r\n<p class=\"has-line-data\" data-line-start=\"8\" data-line-end=\"9\">VIRify was implemented in CWL.</p>\r\n<h2 class=\"code-line\" data-line-start=10 data-line-end=11 ><a id=\"What_do_I_need_10\"></a>What do I need?</h2>\r\n<p class=\"has-line-data\" data-line-start=\"12\" data-line-end=\"13\">The current implementation uses CWL version 1.2 dev+2. It was tested using Toil version 4.10 as the workflow engine and conda to manage the software dependencies.</p>\r\n<h3 class=\"code-line\" data-line-start=14 data-line-end=15 ><a id=\"Docker__Singularity_support_14\"></a>Docker - Singularity support</h3>\r\n<p class=\"has-line-data\" data-line-start=\"16\" data-line-end=\"17\">Soon\u2026</p>\r\n<h2 class=\"code-line\" data-line-start=18 data-line-end=19 ><a id=\"Setup_environment_18\"></a>Setup environment</h2>\r\n<pre><code class=\"has-line-data\" data-line-start=\"21\" data-line-end=\"24\" class=\"language-bash\">conda env create <span class=\"hljs-operator\">-f</span> cwl/requirements/conda_env.yml\r\nconda activate viral_pipeline\r\n</code></pre>\r\n<h2 class=\"code-line\" data-line-start=25 data-line-end=26 ><a id=\"Basic_execution_25\"></a>Basic execution</h2>\r\n<pre><code class=\"has-line-data\" data-line-start=\"28\" data-line-end=\"31\" class=\"language-bash\"><span class=\"hljs-built_in\">cd</span> cwl/\r\nvirify.sh -h\r\n</code></pre>\r\n<h1 class=\"code-line\" data-line-start=32 data-line-end=33 ><a id=\"A_note_about_metatranscriptomes_32\"></a>A note about metatranscriptomes</h1>\r\n<p class=\"has-line-data\" data-line-start=\"34\" data-line-end=\"36\">Although VIRify has been benchmarked and validated with metagenomic data in mind, it is also possible to use this tool to detect RNA viruses in metatranscriptome assemblies (e.g. SARS-CoV-2). However, some additional considerations for this purpose are outlined below:<br>\r\n<strong>1. Quality control:</strong> As for metagenomic data, a thorough quality control of the FASTQ sequence reads to remove low-quality bases, adapters and host contamination (if appropriate) is required prior to assembly. This is especially important for metatranscriptomes as small errors can further decrease the quality and contiguity of the assembly obtained. We have used <a href=\"https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/\">TrimGalore</a> for this purpose.</p>\r\n<p class=\"has-line-data\" data-line-start=\"37\" data-line-end=\"38\"><strong>2. Assembly:</strong> There are many assemblers available that are appropriate for either metagenomic or single-species transcriptomic data. However, to our knowledge, there is no assembler currently available specifically for metatranscriptomic data. From our preliminary investigations, we have found that transcriptome-specific assemblers (e.g. <a href=\"http://cab.spbu.ru/software/spades/\">rnaSPAdes</a>) generate more contiguous and complete metatranscriptome assemblies compared to metagenomic alternatives (e.g. <a href=\"https://github.com/voutcn/megahit/releases\">MEGAHIT</a> and <a href=\"http://cab.spbu.ru/software/spades/\">metaSPAdes</a>).</p>\r\n<p class=\"has-line-data\" data-line-start=\"39\" data-line-end=\"40\"><strong>3. Post-processing:</strong> Metatranscriptomes generate highly fragmented assemblies. Therefore, filtering contigs based on a set minimum length has a substantial impact in the number of contigs processed in VIRify. It has also been observed that the number of false-positive detections of <a href=\"https://github.com/jessieren/VirFinder/releases\">VirFinder</a> (one of the tools included in VIRify) is lower among larger contigs. The choice of a length threshold will depend on the complexity of the sample and the sequencing technology used, but in our experience any contigs &lt;2 kb should be analysed with caution.</p>\r\n<p class=\"has-line-data\" data-line-start=\"41\" data-line-end=\"42\"><strong>4. Classification:</strong> The classification module of VIRify depends on the presence of a minimum number and proportion of phylogenetically-informative genes within each contig in order to confidently assign a taxonomic lineage. Therefore, short contigs typically obtained from metatranscriptome assemblies remain generally unclassified. For targeted classification of RNA viruses (for instance, to search for Coronavirus-related sequences), alternative DNA- or protein-based classification methods can be used. Two of the possible options are: (i) using <a href=\"https://github.com/marbl/MashMap/releases\">MashMap</a> to screen the VIRify contigs against a database of RNA viruses (e.g. Coronaviridae) or (ii) using <a href=\"http://hmmer.org/download.html\">hmmsearch</a> to screen the proteins obtained in the VIRify contigs against marker genes of the taxon of interest.</p>\r\n<h2>Contact us</h2>\r\n<a href=\"https://www.ebi.ac.uk/support/metagenomics\">MGnify helpdesk</a>",
        "doi": "10.48546/workflowhub.workflow.26.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "26",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/26?version=1",
        "name": "VIRify",
        "number_of_steps": 17,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [
            "krona plots",
            "ViPhOG annotations",
            "Default lenght 1kb https://github.com/EBI-Metagenomics/emg-virify-scripts/issues/6",
            "hmmscan",
            "MashMap",
            "PPR-Meta",
            "Restore fasta names",
            "Blast in a database of viral sequences including metagenomes",
            "Prodigal",
            "VirFinder",
            "Filter contigs",
            "Combine",
            "ratio evalue ViPhOG",
            "VirSorter",
            "Taxonomic assign"
        ],
        "type": "Common Workflow Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-02-18",
        "creators": [
            "Jos\u00e9 M\u00aa Fern\u00e1ndez",
            "Laura Rodriguez-Navas"
        ],
        "description": "Rare disease researchers workflow is that they submit their raw data (fastq), run the mapping and variant calling RD-Connect pipeline and obtain unannotated gvcf files to further submit to the RD-Connect GPAP or analyse on their own.\r\n\r\nThis demonstrator focuses on the variant calling pipeline. The raw genomic data is processed using the RD-Connect pipeline ([Laurie et al., 2016](https://www.ncbi.nlm.nih.gov/pubmed/27604516)) running on the standards (GA4GH) compliant, interoperable container orchestration platform.\r\n\r\nThis demonstrator will be aligned with the current implementation study on [Development of Architecture for Software Containers at ELIXIR and its use by EXCELERATE use-case communities](docs/Appendix%201%20-%20Project%20Plan%202018-biocontainers%2020171117.pdf) \r\n\r\nFor this implementation, different steps are required:\r\n\r\n1. Adapt the pipeline to CWL and dockerise elements \r\n2. Align with IS efforts on software containers to package the different components (Nextflow) \r\n3. Submit trio of Illumina NA12878 Platinum Genome or Exome to the GA4GH platform cloud (by Aspera or ftp server)\r\n4. Run the RD-Connect pipeline on the container platform\r\n5. Return corresponding gvcf files\r\n6. OPTIONAL: annotate and update to RD-Connect playground instance\r\n\r\nN.B: The demonstrator might have some manual steps, which will not be in production. \r\n\r\n## RD-Connect pipeline\r\n\r\nDetailed information about the RD-Connect pipeline can be found in [Laurie et al., 2016](https://www.ncbi.nlm.nih.gov/pubmed/?term=27604516)\r\n\r\n![alt text](https://raw.githubusercontent.com/inab/Wetlab2Variations/eosc-life/docs/RD-Connect_pipeline.jpg)\r\n\r\n## The applications\r\n\r\n**1\\. Name of the application: Adaptor removal**\r\nFunction: remove sequencing adaptors   \r\nContainer (readiness status, location, version): [cutadapt (v.1.18)](https://hub.docker.com/r/cnag/cutadapt)  \r\nRequired resources in cores and RAM: current container size 169MB  \r\nInput data (amount, format, directory..): raw fastq  \r\nOutput data: paired fastq without adaptors  \r\n\r\n**2\\. Name of the application: Mapping and bam sorting**  \r\nFunction: align data to reference genome  \r\nContainer : [bwa-mem (v.0.7.17)](https://hub.docker.com/r/cnag/bwa) / [Sambamba (v. 0.6.8 )](https://hub.docker.com/r/cnag/sambamba)(or samtools)  \r\nResources :current container size 111MB / 32MB  \r\nInput data: paired fastq without adaptors  \r\nOutput data: sorted bam  \r\n\r\n**3\\. Name of the application: MarkDuplicates**  \r\nFunction: Mark (and remove) duplicates  \r\nContainer: [Picard (v.2.18.25)](https://hub.docker.com/r/cnag/picard)\r\nResources: current container size 261MB  \r\nInput data:sorted bam  \r\nOutput data: Sorted bam with marked (or removed) duplicates  \r\n\r\n**4\\. Name of the application: Base quality recalibration (BQSR)**  \r\nFunction: Base quality recalibration  \r\nContainer: [GATK (v.3.6-0)](https://hub.docker.com/r/cnag/gatk)\r\nResources: current container size 270MB  \r\nInput data: Sorted bam with marked (or removed) duplicates  \r\nOutput data: Sorted bam with marked duplicates & base quality recalculated  \r\n\r\n**5\\. Name of the application: Variant calling**  \r\nFunction: variant calling  \r\nContainer: [GATK (v.3.6-0)](https://hub.docker.com/r/cnag/gatk)\r\nResources: current container size 270MB  \r\nInput data:Sorted bam with marked duplicates & base quality recalculated  \r\nOutput data: unannotated gvcf per sample  \r\n\r\n**6\\. (OPTIONAL)Name of the application: Quality of the fastq**  \r\nFunction: report on the sequencing quality  \r\nContainer: [fastqc 0.11.8](https://hub.docker.com/r/cnag/fastqc)\r\nResources: current container size 173MB  \r\nInput data: raw fastq  \r\nOutput data: QC report \r\n\r\n## Licensing\r\n\r\nGATK declares that archived packages are made available for free to academic researchers under a limited license for non-commercial use. If you need to use one of these packages for commercial use. https://software.broadinstitute.org/gatk/download/archive ",
        "doi": "10.48546/workflowhub.workflow.107.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "107",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/107?version=1",
        "name": "VariantCaller_GATK3.6",
        "number_of_steps": 15,
        "projects": [
            "EOSC-Life - Demonstrator 7: Rare Diseases"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cwl",
            "variant_calling"
        ],
        "tools": [
            "gatk-base_recalibration_print_reads",
            "gatk-base_recalibration",
            "gatk-haplotype_caller",
            "gatk-ir",
            "gatk3-rtc",
            "picard-MD"
        ],
        "type": "Common Workflow Language",
        "update_time": "2023-04-21",
        "versions": 1
    },
    {
        "create_time": "2021-02-02",
        "creators": [],
        "description": "Galaxy version of pre-processing of reads from COVID-19 samples. \r\nQC + human read cleaning\r\nBased on https://github.com/Finn-Lab/Metagen-FastQC/blob/master/metagen-fastqc.sh",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "99",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/99?version=1",
        "name": "COVID-19: read pre-processing",
        "number_of_steps": 4,
        "projects": [
            "IBISBA Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "samtool_filter2",
            "trim_galore",
            "bwa_mem",
            "samtools_fastx"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2020-11-23",
        "creators": [
            "Bart Nijsse"
        ],
        "description": "Workflow to build different indices for different tools from a genome and transcriptome. \r\n\r\nThis workflow expects an (annotated) genome in GBOL ttl format.\r\n\r\nSteps:\r\n  - SAPP: rdf2gtf (genome fasta)\r\n  - SAPP: rdf2fasta (transcripts fasta)\r\n  - STAR index (Optional for Eukaryotic origin)\r\n  - bowtie2 index\r\n  - kallisto index\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in description",
        "id": "75",
        "keep": true,
        "latest_version": 1,
        "license": "CC0-1.0",
        "link": "https:/workflowhub.eu/workflows/75?version=1",
        "name": "Indices builder from GBOL RDF (TTL)",
        "number_of_steps": 5,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment"
        ],
        "tools": [
            "Creates kallisto index with transcripts fasta file",
            "Convert input RDF (turtle) file to GTF file",
            "Convert input RDF (turtle) file to Genome fasta file.",
            "Creates bowtie2 index with genome fasta",
            "Creates STAR index with genome fasta and GTF file"
        ],
        "type": "Common Workflow Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-11-04",
        "creators": [],
        "description": "This WF is based on the official Covid19-Galaxy assembly workflow as available from https://covid19.galaxyproject.org/genomics/2-assembly/ . It has been adapted to suit the needs of the analysis of metagenomics sequencing data. Prior to be submitted to INDSC databases, these data need to be cleaned from contaminant reads, including reads of possible human origin. \r\n\r\nThe assembly of the SARS-CoV-2 genome is performed using both the Unicycler and the SPAdes assemblers, similar to the original WV.\r\n\r\nTo facilitate the deposition of raw sequencing reads in INDSC databases, different fastq files are saved during the different steps of the WV. Which reflect different levels of stringency/filtration:\r\n\r\n(1) Initially fastq are filtered to remove human reads. \r\n(2) Subsequently, a similarity search is performed against the reference assembly of the SARS-CoV-2 genome, to retain only SARS-CoV-2 like reads. \r\n(3) Finally, SARS-CoV-2 reads are assembled, and the bowtie2 program is used to identify (and save in the corresponding fastq files) only reads that are completely identical to the final assembly of the genome.\r\n\r\nAny of the fastq files produced in (1), (2) or (3) are suitable for being submitted in  raw reads repositories. While the files filtered according to (1) are richer and contain more data, including for example genomic sequences of different microbes living in the oral cavity; files filtered according to (3) contain only the reads that are completely identical to the final assembly. This should guarantee that any re-analysis/re-assembly of these always produce consistent and identical results. File obtained at (2) include all the reads in the sequencing reaction that had some degree of similarity with the reference SARS-CoV-2 genome, these may include subgenomic RNAs, but also polymorphic regions/variants in the case of a coinfection by multiple SARS-CoV-2 strains. Consequently, reanalysis of these data is not guarateed to produce identical and consistent results, depending on the parameters used during the assembly. However, these data contain more information.\r\n\r\nPlease feel free to comment,  ask questions and/or add suggestions\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "68",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/68?version=1",
        "name": "MC_COVID19like_Assembly_Reads",
        "number_of_steps": 7,
        "projects": [
            "Italy-Covid-data-Portal"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [
            "bowtie2",
            "unicycler",
            "spades",
            "fastp"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2020-08-05",
        "creators": [
            "Milad Miladi"
        ],
        "description": "Metagenomics: taxa classification",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in name",
        "id": "53",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/53?version=1",
        "name": "ONT -- Metagenomics-Kraken2-Krona",
        "number_of_steps": 4,
        "projects": [
            "NanoGalaxy"
        ],
        "source": "WorkflowHub",
        "tags": [
            "ont"
        ],
        "tools": [
            "taxonomy_krona_chart",
            "tp_replace_in_line",
            "kraken2",
            "datamash_reverse"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2020-07-22",
        "creators": [],
        "description": "CWL workflow for NMR spectra Peak Picking\r\nThe workflow takes as input a series of 2D 1H 15N HSQC NMR spectra and uses nmrpipe tools to convert the spectra in nmrpipe format and performs an automatic peak picking.\r\nThis test uses a protein MDM2 with different ligands and peptide and generates a peak list with 1H and 15N chemical shift values for each spectrum. The difference among these peak lists can be used to characterize the ligand binding site on the protein.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "43",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/43?version=1",
        "name": "NMR pipe",
        "number_of_steps": 4,
        "projects": [
            "NMR Workflow"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-06-18",
        "creators": [],
        "description": "Alignment, assembly and annotation of RNQSEQ reads using TOPHAT  (without filtering out host reads).",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in name",
        "id": "37",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/37?version=1",
        "name": "Assembly using Tophat2 and annotation (alternate)",
        "number_of_steps": 19,
        "projects": [
            "GalaxyProject SARS-CoV-2"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "assembly",
            "galaxy",
            "rnaseq",
            "tophat2",
            "covid-19"
        ],
        "tools": [
            "fastp",
            "gffread",
            "uniprotxml_downloader",
            "glimmer_build-icm",
            "ncbi_makeblastdb",
            "glimmer_knowlegde-based",
            "fasterq_dump",
            "ncbi_blastp_wrapper",
            "transdecoder",
            "antismash",
            "tophat2",
            "cufflinks",
            "hmmer_jackhmmer",
            "multiqc",
            "cuffmerge"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2020-06-18",
        "creators": [],
        "description": "Alignment, assembly RNASEQ reads and annotation of generated transcripts.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "38",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/38?version=1",
        "name": "Unicycler assembly and annotation",
        "number_of_steps": 21,
        "projects": [
            "GalaxyProject SARS-CoV-2"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "annotation",
            "assembly",
            "rnaseq",
            "unicycler",
            "covid-19"
        ],
        "tools": [
            "fastp",
            "unicycler",
            "uniprotxml_downloader",
            "glimmer_build-icm",
            "ncbi_makeblastdb",
            "samtools_fastx",
            "glimmer_knowlegde-based",
            "fasterq_dump",
            "ncbi_blastp_wrapper",
            "antismash",
            "picard_MergeSamFiles",
            "transdecoder",
            "samtool_filter2",
            "hmmer_jackhmmer",
            "bwa_mem",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2020-06-18",
        "creators": [],
        "description": "Alignment, assembly and annotation of RNASEQ reads as well as annotation of generated transcripts.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "39",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/39?version=1",
        "name": "StringTie assembly and annotation",
        "number_of_steps": 22,
        "projects": [
            "GalaxyProject SARS-CoV-2"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "annotation",
            "assembly",
            "rnaseq",
            "stringtie",
            "covid-19"
        ],
        "tools": [
            "fastp",
            "gffread",
            "uniprotxml_downloader",
            "glimmer_build-icm",
            "stringtie",
            "ncbi_makeblastdb",
            "samtools_fastx",
            "glimmer_knowlegde-based",
            "fasterq_dump",
            "stringtie_merge",
            "ncbi_blastp_wrapper",
            "antismash",
            "transdecoder",
            "hmmer_jackhmmer",
            "samtool_filter2",
            "hisat2",
            "bwa_mem",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2020-06-18",
        "creators": [],
        "description": "Alignment, assembly and annotation of generated transcripts from RNASEQ reads.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "annot* in tags",
        "id": "40",
        "keep": true,
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/40?version=1",
        "name": "Assembly using Tophat2 and annotation",
        "number_of_steps": 24,
        "projects": [
            "GalaxyProject SARS-CoV-2"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "annotation",
            "assembly",
            "rnaseq",
            "tophat2",
            "covid-19"
        ],
        "tools": [
            "cuffquant",
            "uniprotxml_downloader",
            "ncbi_makeblastdb",
            "samtools_fastx",
            "samtool_filter2",
            "hmmer_jackhmmer",
            "bwa_mem",
            "cuffmerge",
            "fastp",
            "gffread",
            "ncbi_blastp_wrapper",
            "cuffdiff",
            "multiqc",
            "fasterq_dump",
            "transdecoder",
            "antismash",
            "glimmer_build-icm",
            "glimmer_knowlegde-based",
            "tophat2",
            "cufflinks"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2020-05-14",
        "creators": [
            "Andreas Wilm and October SESSIONS and Paola Florez DE SESSIONS and ZHU Yuan and Shuzhen SIM and CHU Wenhan Collins"
        ],
        "description": "<!DOCTYPE html><html><head><meta charset=\"utf-8\"><style></style></head><body id=\"preview\">\r\n<h1 class=\"code-line\" data-line-start=0 data-line-end=1><a id=\"nfcoreviprhttpsrawgithubusercontentcomnfcoreviprmasterdocsimagesvipr_logosvg_0\"></a><img src=\"https://raw.githubusercontent.com/nf-core/vipr/master/docs/images/vipr_logo.png\" alt=\"nf-core/vipr\" height=\"200\" width=\"500\"></h1>\r\n<p class=\"has-line-data\" data-line-start=\"2\" data-line-end=\"3\"><a href=\"https://travis-ci.org/nf-core/vipr\"><img src=\"https://travis-ci.org/nf-core/vipr.svg?branch=master\" alt=\"Build Status\"></a> <a href=\"https://www.nextflow.io/\"><img src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.31.1-brightgreen.svg\" alt=\"Nextflow\"></a> <a href=\"https://gitter.im/nf-core/Lobby\"><img src=\"https://img.shields.io/badge/gitter-%20join%20chat%20%E2%86%92-4fb99a.svg\" alt=\"Gitter\"></a></p>\r\n<p class=\"has-line-data\" data-line-start=\"4\" data-line-end=\"5\"><a href=\"http://bioconda.github.io/\"><img src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" alt=\"install with bioconda\"></a> <a href=\"https://hub.docker.com/r/nfcore/vipr/\"><img src=\"https://img.shields.io/docker/automated/nfcore/vipr.svg\" alt=\"Docker Container available\"></a> <a href=\"https://singularity-hub.org/collections/1405\"><img src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"></a></p>\r\n<p class=\"has-line-data\" data-line-start=\"16\" data-line-end=\"17\"><strong>nf-core/vipr</strong> is a bioinformatics best-practice analysis pipeline for assembly and intrahost / low-frequency variant calling for viral samples.</p>\r\n<p class=\"has-line-data\" data-line-start=\"18\" data-line-end=\"19\">The pipeline is built using <a href=\"https://www.nextflow.io\">Nextflow</a>, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.</p>\r\n<h3 class=\"code-line\" data-line-start=20 data-line-end=21><a id=\"Pipeline_Steps_20\"></a>Pipeline Steps</h3>\r\n<table class=\"table table-striped table-bordered\">\r\n<thead>\r\n<tr>\r\n<th>Step</th>\r\n<th>Main program/s</th>\r\n</tr>\r\n</thead>\r\n<tbody>\r\n<tr>\r\n<td>Trimming, combining of read-pairs per sample and QC</td>\r\n<td>Skewer, FastQC</td>\r\n</tr>\r\n<tr>\r\n<td>Decontamination</td>\r\n<td>decont</td>\r\n</tr>\r\n<tr>\r\n<td>Metagenomics classification / Sample purity</td>\r\n<td>Kraken</td>\r\n</tr>\r\n<tr>\r\n<td>Assembly to contigs</td>\r\n<td>BBtools\u2019 Tadpole</td>\r\n</tr>\r\n<tr>\r\n<td>Assembly polishing</td>\r\n<td>ViPR Tools</td>\r\n</tr>\r\n<tr>\r\n<td>Mapping to assembly</td>\r\n<td>BWA, LoFreq</td>\r\n</tr>\r\n<tr>\r\n<td>Low frequency variant calling</td>\r\n<td>LoFreq</td>\r\n</tr>\r\n<tr>\r\n<td>Coverage and variant AF plots (two processes)</td>\r\n<td>Bedtools, ViPR Tools</td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n<h3 class=\"code-line\" data-line-start=33 data-line-end=34><a id=\"Documentation_33\"></a>Documentation</h3>\r\n<p class=\"has-line-data\" data-line-start=\"35\" data-line-end=\"36\">Documentation about the pipeline can be found in the <code>docs/</code> directory:</p>\r\n<ol>\r\n<li class=\"has-line-data\" data-line-start=\"37\" data-line-end=\"38\"><a href=\"docs/installation.md\">Installation and configuration</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"38\" data-line-end=\"39\"><a href=\"docs/usage.md\">Running the pipeline</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"39\" data-line-end=\"41\"><a href=\"docs/output.md\">Output and how to interpret the results</a></li>\r\n</ol>\r\n<h3 class=\"code-line\" data-line-start=41 data-line-end=42><a id=\"Credits_41\"></a>Credits</h3>\r\n<p class=\"has-line-data\" data-line-start=\"43\" data-line-end=\"46\">This pipeline was originally developed by Andreas Wilm (<a href=\"https://github.com/andreas-wilm\">andreas-wilm</a>) at <a href=\"https://www.a-star.edu.sg/gis/\">Genome Institute of Singapore</a>.<br>\r\nIt started out as an ecosystem around LoFreq and went through a couple of iterations.<br>\r\nThe current version had three predecessors <a href=\"https://github.com/CSB5/vipr\">ViPR 1</a>, <a href=\"https://github.com/CSB5/vipr2\">ViPR 2</a> and <a href=\"https://github.com/gis-rpd/pipelines/tree/master/germs/vipr\">ViPR 3</a>.</p>\r\n<p class=\"has-line-data\" data-line-start=\"47\" data-line-end=\"48\">An incomplete list of publications using (previous versions of) ViPR:</p>\r\n<ul>\r\n<li class=\"has-line-data\" data-line-start=\"49\" data-line-end=\"50\"><a href=\"https://www.ncbi.nlm.nih.gov/pubmed/26327586\">Sessions et. al., PLoS Negl Trop Dis., 2015</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"50\" data-line-end=\"52\"><a href=\"https://www.ncbi.nlm.nih.gov/pubmed/26325059\">Sim et al., PLoS Negl Trop Dis., 2015</a></li>\r\n</ul>\r\n<p class=\"has-line-data\" data-line-start=\"52\" data-line-end=\"53\">Plenty of people provided essential feedback, including:</p>\r\n<ul>\r\n<li class=\"has-line-data\" data-line-start=\"54\" data-line-end=\"55\"><a href=\"https://www.duke-nus.edu.sg/content/sessions-october\">October SESSIONS</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"55\" data-line-end=\"56\"><a href=\"https://www.a-star.edu.sg/gis/Our-People/Platform-Leaders\">Paola Florez DE SESSIONS</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"56\" data-line-end=\"57\">ZHU Yuan</li>\r\n<li class=\"has-line-data\" data-line-start=\"57\" data-line-end=\"58\">Shuzhen SIM</li>\r\n<li class=\"has-line-data\" data-line-start=\"58\" data-line-end=\"59\">CHU Wenhan Collins</li>\r\n</ul>\r\n</body></html>",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "20",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/20?version=1",
        "name": "nf-core/vipr",
        "number_of_steps": 0,
        "projects": [
            "nf-core viralrecon"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-05-14",
        "creators": [
            "Sarai Varona and Miguel Juli\u00e1 and Sara Monzon and Alexander Peltzer and Alison Meynert and Edgar Garriga Nogales and Erik Garrison and Gisela Gabernet and Harshil Patel and Joao Curado and Jose Espinosa-Carrasco and Katrin Sameith and Marta Pozuelo and Maxime Garcia and Michael Heuer and Phil Ewels and Simon Heumos and Stephen Kelly and Thanh Le Viet and Isabel Cuesta"
        ],
        "description": "<!DOCTYPE html><html><head><meta charset=\"utf-8\"><style></style></head><body id=\"preview\">\r\n<h1 class=\"code-line\" data-line-start=0 data-line-end=1><a id=\"nfcoreviralreconhttpsrawgithubusercontentcomnfcoreviralreconmasterdocsimagesnfcoreviralrecon_logopng_0\"></a><img src=\"https://raw.githubusercontent.com/nf-core/viralrecon/master/docs/images/nf-core-viralrecon_logo.png\" alt=\"nf-core/viralrecon\"></h1>\r\n<p class=\"has-line-data\" data-line-start=\"2\" data-line-end=\"3\"><a href=\"https://github.com/nf-core/viralrecon/actions\"><img src=\"https://github.com/nf-core/viralrecon/workflows/nf-core%20CI/badge.svg\" alt=\"GitHub Actions CI Status\"></a> <a href=\"https://github.com/nf-core/viralrecon/actions\"><img src=\"https://github.com/nf-core/viralrecon/workflows/nf-core%20linting/badge.svg\" alt=\"GitHub Actions Linting Status\"></a> <a href=\"https://www.nextflow.io/\"><img src=\"https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg\" alt=\"Nextflow\"></a> <a href=\"https://bioconda.github.io/\"><img src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" alt=\"install with bioconda\"></a></p>\r\n<p class=\"has-line-data\" data-line-start=\"4\" data-line-end=\"5\"><a href=\"https://hub.docker.com/r/nfcore/viralrecon\"><img src=\"https://img.shields.io/docker/automated/nfcore/viralrecon.svg\" alt=\"Docker\"></a> <a href=\"https://doi.org/10.5281/zenodo.3872730\"><img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3872730.svg\" alt=\"DOI\"></a></p>\r\n<p class=\"has-line-data\" data-line-start=\"8\" data-line-end=\"9\"><strong>nfcore/viralrecon</strong> is a bioinformatics analysis pipeline used to perform assembly and intrahost/low-frequency variant calling for viral samples. The pipeline currently supports metagenomics and amplicon sequencing data derived from the Illumina sequencing platform.</p>\r\n<p class=\"has-line-data\" data-line-start=\"10\" data-line-end=\"11\">This pipeline is a re-implementation of the <a href=\"https://github.com/BU-ISCIII/SARS_Cov2_consensus-nf\">SARS_Cov2_consensus-nf</a> and <a href=\"https://github.com/BU-ISCIII/SARS_Cov2_assembly-nf\">SARS_Cov2_assembly-nf</a> pipelines initially developed by <a href=\"https://github.com/svarona\">Sarai Varona</a> and <a href=\"https://github.com/saramonzon\">Sara Monzon</a> from <a href=\"https://github.com/BU-ISCIII\">BU-ISCIII</a>. Porting both of these pipelines to nf-core was an international collaboration between numerous contributors and developers, led by <a href=\"https://github.com/drpatelh\">Harshil Patel</a> from the <a href=\"https://www.crick.ac.uk/research/science-technology-platforms/bioinformatics-and-biostatistics/\">The Bioinformatics &amp; Biostatistics Group</a> at <a href=\"https://www.crick.ac.uk/\">The Francis Crick Institute</a>, London. We appreciated the need to have a portable, reproducible and scalable pipeline for the analysis of COVID-19 sequencing samples and so the Avengers Assembled! Please come and join us and add yourself to the contributor list :)</p>\r\n<p class=\"has-line-data\" data-line-start=\"12\" data-line-end=\"13\">We have integrated a number of options in the pipeline to allow you to run specific aspects of the workflow if you so wish. For example, you can skip all of the assembly steps with the <code>--skip_assembly</code> parameter. See <a href=\"docs/usage.md\">usage docs</a> for all of the available options when running the pipeline.</p>\r\n<p class=\"has-line-data\" data-line-start=\"14\" data-line-end=\"15\">Please click <a href=\"https://raw.githack.com/nf-core/viralrecon/master/docs/html/multiqc_report.html\">here</a> to see an example MultiQC report generated using the parameters defined in <a href=\"https://github.com/nf-core/viralrecon/blob/master/conf/test_full.config\">this configuration file</a> to run the pipeline on <a href=\"https://zenodo.org/record/3735111\">samples</a> which were prepared from the <a href=\"https://artic.network/ncov-2019\">ncov-2019 ARTIC Network V1 amplicon set</a> and sequenced on the Illumina MiSeq platform in 301bp paired-end format.</p>\r\n<p class=\"has-line-data\" data-line-start=\"16\" data-line-end=\"17\">The pipeline is built using <a href=\"https://www.nextflow.io\">Nextflow</a>, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker containers making installation trivial and results highly reproducible. Furthermore, automated continuous integration tests to run the pipeline on a full-sized dataset are passing on AWS cloud.</p>\r\n<h2 class=\"code-line\" data-line-start=18 data-line-end=19><a id=\"Pipeline_summary_18\"></a>Pipeline summary</h2>\r\n<ol>\r\n<li class=\"has-line-data\" data-line-start=\"20\" data-line-end=\"21\">Download samples via SRA, ENA or GEO ids (<a href=\"https://ena-docs.readthedocs.io/en/latest/retrieval/file-download.html\"><code>ENA FTP</code></a>, <a href=\"https://github.com/rvalieris/parallel-fastq-dump\"><code>parallel-fastq-dump</code></a>; <em>if required</em>)</li>\r\n<li class=\"has-line-data\" data-line-start=\"21\" data-line-end=\"22\">Merge re-sequenced FastQ files (<a href=\"http://www.linfo.org/cat.html\"><code>cat</code></a>; <em>if required</em>)</li>\r\n<li class=\"has-line-data\" data-line-start=\"22\" data-line-end=\"23\">Read QC (<a href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\"><code>FastQC</code></a>)</li>\r\n<li class=\"has-line-data\" data-line-start=\"23\" data-line-end=\"24\">Adapter trimming (<a href=\"https://github.com/OpenGene/fastp\"><code>fastp</code></a>)</li>\r\n<li class=\"has-line-data\" data-line-start=\"24\" data-line-end=\"33\">Variant calling<br>\r\ni. Read alignment (<a href=\"http://bowtie-bio.sourceforge.net/bowtie2/index.shtml\"><code>Bowtie 2</code></a>)<br>\r\nii. Sort and index alignments (<a href=\"https://sourceforge.net/projects/samtools/files/samtools/\"><code>SAMtools</code></a>)<br>\r\niii. Primer sequence removal (<a href=\"https://github.com/andersen-lab/ivar\"><code>iVar</code></a>; <em>amplicon data only</em>)<br>\r\niv. Duplicate read marking (<a href=\"https://broadinstitute.github.io/picard/\"><code>picard</code></a>; <em>removal optional</em>)<br>\r\nv. Alignment-level QC (<a href=\"https://broadinstitute.github.io/picard/\"><code>picard</code></a>, <a href=\"https://sourceforge.net/projects/samtools/files/samtools/\"><code>SAMtools</code></a>)<br>\r\nvi. Choice of multiple variant calling and consensus sequence generation routes (<a href=\"https://dkoboldt.github.io/varscan/\"><code>VarScan 2</code></a>, <a href=\"https://samtools.github.io/bcftools/bcftools.html\"><code>BCFTools</code></a>, <a href=\"https://github.com/arq5x/bedtools2/\"><code>BEDTools</code></a> <em>||</em> <a href=\"https://github.com/andersen-lab/ivar\"><code>iVar variants and consensus</code></a> <em>||</em> <a href=\"https://samtools.github.io/bcftools/bcftools.html\"><code>BCFTools</code></a>, <a href=\"https://github.com/arq5x/bedtools2/\"><code>BEDTools</code></a>)<br>\r\n- Variant annotation (<a href=\"http://snpeff.sourceforge.net/SnpEff.html\"><code>SnpEff</code></a>, <a href=\"http://snpeff.sourceforge.net/SnpSift.html\"><code>SnpSift</code></a>)<br>\r\n- Consensus assessment report (<a href=\"http://quast.sourceforge.net/quast\"><code>QUAST</code></a>)</li>\r\n<li class=\"has-line-data\" data-line-start=\"33\" data-line-end=\"43\"><em>De novo</em> assembly<br>\r\ni. Primer trimming (<a href=\"https://cutadapt.readthedocs.io/en/stable/guide.html\"><code>Cutadapt</code></a>; <em>amplicon data only</em>)<br>\r\nii. Removal of host reads (<a href=\"http://ccb.jhu.edu/software/kraken2/\"><code>Kraken 2</code></a>)<br>\r\niii. Choice of multiple assembly tools (<a href=\"http://cab.spbu.ru/software/spades/\"><code>SPAdes</code></a> <em>||</em> <a href=\"http://cab.spbu.ru/software/meta-spades/\"><code>metaSPAdes</code></a> <em>||</em> <a href=\"https://github.com/rrwick/Unicycler\"><code>Unicycler</code></a> <em>||</em> <a href=\"https://github.com/GATB/minia\"><code>minia</code></a>)<br>\r\n- Blast to reference genome (<a href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE_TYPE=BlastSearch\"><code>blastn</code></a>)<br>\r\n- Contiguate assembly (<a href=\"https://www.sanger.ac.uk/science/tools/pagit\"><code>ABACAS</code></a>)<br>\r\n- Assembly report (<a href=\"https://github.com/BU-ISCIII/plasmidID\"><code>PlasmidID</code></a>)<br>\r\n- Assembly assessment report (<a href=\"http://quast.sourceforge.net/quast\"><code>QUAST</code></a>)<br>\r\n- Call variants relative to reference (<a href=\"https://github.com/lh3/minimap2\"><code>Minimap2</code></a>, <a href=\"https://github.com/ekg/seqwish\"><code>seqwish</code></a>, <a href=\"https://github.com/vgteam/vg\"><code>vg</code></a>, <a href=\"https://github.com/rrwick/Bandage\"><code>Bandage</code></a>)<br>\r\n- Variant annotation (<a href=\"http://snpeff.sourceforge.net/SnpEff.html\"><code>SnpEff</code></a>, <a href=\"http://snpeff.sourceforge.net/SnpSift.html\"><code>SnpSift</code></a>)</li>\r\n<li class=\"has-line-data\" data-line-start=\"43\" data-line-end=\"45\">Present QC and visualisation for raw read, alignment, assembly and variant calling results (<a href=\"http://multiqc.info/\"><code>MultiQC</code></a>)</li>\r\n</ol>\r\n<h2 class=\"code-line\" data-line-start=45 data-line-end=46><a id=\"Quick_Start_45\"></a>Quick Start</h2>\r\n<p class=\"has-line-data\" data-line-start=\"47\" data-line-end=\"48\">i. Install <a href=\"https://nf-co.re/usage/installation\"><code>nextflow</code></a></p>\r\n<p class=\"has-line-data\" data-line-start=\"49\" data-line-end=\"50\">ii. Install either <a href=\"https://docs.docker.com/engine/installation/\"><code>Docker</code></a> or <a href=\"https://www.sylabs.io/guides/3.0/user-guide/\"><code>Singularity</code></a> for full pipeline reproducibility (please only use <a href=\"https://conda.io/miniconda.html\"><code>Conda</code></a> as a last resort; see <a href=\"https://nf-co.re/usage/configuration#basic-configuration-profiles\">docs</a>)</p>\r\n<p class=\"has-line-data\" data-line-start=\"51\" data-line-end=\"52\">iii. Download the pipeline and test it on a minimal dataset with a single command</p>\r\n<pre><code class=\"has-line-data\" data-line-start=\"54\" data-line-end=\"56\" class=\"language-bash\">nextflow run nf-core/viralrecon -profile <span class=\"hljs-built_in\">test</span>,&lt;docker/singularity/conda/institute&gt;\r\n</code></pre>\r\n<blockquote>\r\n<p class=\"has-line-data\" data-line-start=\"57\" data-line-end=\"58\">Please check <a href=\"https://github.com/nf-core/configs#documentation\">nf-core/configs</a> to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use <code>-profile &lt;institute&gt;</code> in your command. This will enable either <code>docker</code> or <code>singularity</code> and set the appropriate execution settings for your local compute environment.</p>\r\n</blockquote>\r\n<p class=\"has-line-data\" data-line-start=\"59\" data-line-end=\"60\">iv. Start running your own analysis!</p>\r\n<pre><code class=\"has-line-data\" data-line-start=\"62\" data-line-end=\"64\" class=\"language-bash\">nextflow run nf-core/viralrecon -profile &lt;docker/singularity/conda/institute&gt; --input samplesheet.csv --genome <span class=\"hljs-string\">'NC_045512.2'</span> -profile docker\r\n</code></pre>\r\n<p class=\"has-line-data\" data-line-start=\"65\" data-line-end=\"66\">See <a href=\"docs/usage.md\">usage docs</a> for all of the available options when running the pipeline.</p>\r\n<h2 class=\"code-line\" data-line-start=67 data-line-end=68><a id=\"Documentation_67\"></a>Documentation</h2>\r\n<p class=\"has-line-data\" data-line-start=\"69\" data-line-end=\"70\">The nf-core/viralrecon pipeline comes with documentation about the pipeline, found in the <code>docs/</code> directory:</p>\r\n<ol>\r\n<li class=\"has-line-data\" data-line-start=\"71\" data-line-end=\"72\"><a href=\"https://nf-co.re/usage/installation\">Installation</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"72\" data-line-end=\"76\">Pipeline configuration\r\n<ul>\r\n<li class=\"has-line-data\" data-line-start=\"73\" data-line-end=\"74\"><a href=\"https://nf-co.re/usage/local_installation\">Local installation</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"74\" data-line-end=\"75\"><a href=\"https://nf-co.re/usage/adding_own_config\">Adding your own system config</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"75\" data-line-end=\"76\"><a href=\"docs/usage.md#reference-genomes\">Reference genomes</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"has-line-data\" data-line-start=\"76\" data-line-end=\"77\"><a href=\"docs/usage.md\">Running the pipeline</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"77\" data-line-end=\"78\"><a href=\"docs/output.md\">Output and how to interpret the results</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"78\" data-line-end=\"80\"><a href=\"https://nf-co.re/usage/troubleshooting\">Troubleshooting</a></li>\r\n</ol>\r\n<h2 class=\"code-line\" data-line-start=80 data-line-end=81><a id=\"Credits_80\"></a>Credits</h2>\r\n<p class=\"has-line-data\" data-line-start=\"82\" data-line-end=\"83\">These scripts were originally written by <a href=\"https://github.com/svarona\">Sarai Varona</a>, <a href=\"https://github.com/MiguelJulia\">Miguel Juli\u00e1</a> and <a href=\"https://github.com/saramonzon\">Sara Monzon</a> from <a href=\"https://github.com/BU-ISCIII\">BU-ISCIII</a> and co-ordinated by Isabel Cuesta for the <a href=\"https://eng.isciii.es/eng.isciii.es/Paginas/Inicio.html\">Institute of Health Carlos III</a>, Spain. Through collaboration with the nf-core community the pipeline has now been updated substantially to include additional processing steps, to standardise inputs/outputs and to improve pipeline reporting; implemented primarily by <a href=\"https://github.com/drpatelh\">Harshil Patel</a> from <a href=\"https://www.crick.ac.uk/research/science-technology-platforms/bioinformatics-and-biostatistics/\">The Bioinformatics &amp; Biostatistics Group</a> at <a href=\"https://www.crick.ac.uk/\">The Francis Crick Institute</a>, London.</p>\r\n<p class=\"has-line-data\" data-line-start=\"84\" data-line-end=\"85\">Many thanks to others who have helped out and contributed along the way too, including (but not limited to):</p>\r\n<table class=\"table table-striped table-bordered\">\r\n<thead>\r\n<tr>\r\n<th>Name</th>\r\n<th>Affiliation</th>\r\n</tr>\r\n</thead>\r\n<tbody>\r\n<tr>\r\n<td><a href=\"https://github.com/apeltzer\">Alexander Peltzer</a></td>\r\n<td><a href=\"https://www.boehringer-ingelheim.de/\">Boehringer Ingelheim, Germany</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/ameynert\">Alison Meynert</a></td>\r\n<td><a href=\"https://www.ed.ac.uk/\">University of Edinburgh, Scotland</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/edgano\">Edgar Garriga Nogales</a></td>\r\n<td><a href=\"https://www.crg.eu/\">Centre for Genomic Regulation, Spain</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/ekg\">Erik Garrison</a></td>\r\n<td><a href=\"https://www.ucsc.edu/\">UCSC, USA</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/ggabernet\">Gisela Gabernet</a></td>\r\n<td><a href=\"https://portal.qbic.uni-tuebingen.de/portal/\">QBiC, University of T\u00fcbingen, Germany</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/jcurado-flomics\">Joao Curado</a></td>\r\n<td><a href=\"https://www.flomics.com/\">Flomics Biotech, Spain</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/JoseEspinosa\">Jose Espinosa-Carrasco</a></td>\r\n<td><a href=\"https://www.crg.eu/\">Centre for Genomic Regulation, Spain</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/ktrns\">Katrin Sameith</a></td>\r\n<td><a href=\"https://genomecenter.tu-dresden.de\">DRESDEN-concept Genome Center, Germany</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/lcabus-flomics\">Lluc Cabus</a></td>\r\n<td><a href=\"https://www.flomics.com/\">Flomics Biotech, Spain</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/mpozuelo-flomics\">Marta Pozuelo</a></td>\r\n<td><a href=\"https://www.flomics.com/\">Flomics Biotech, Spain</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/MaxUlysse\">Maxime Garcia</a></td>\r\n<td><a href=\"https://www.scilifelab.se/\">SciLifeLab, Sweden</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/heuermh\">Michael Heuer</a></td>\r\n<td><a href=\"https://https://rise.cs.berkeley.edu\">UC Berkeley, USA</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/ewels\">Phil Ewels</a></td>\r\n<td><a href=\"https://www.scilifelab.se/\">SciLifeLab, Sweden</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/subwaystation\">Simon Heumos</a></td>\r\n<td><a href=\"https://portal.qbic.uni-tuebingen.de/portal/\">QBiC, University of T\u00fcbingen, Germany</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/stevekm\">Stephen Kelly</a></td>\r\n<td><a href=\"https://www.mskcc.org/\">Memorial Sloan Kettering Cancer Center, USA</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/thanhleviet\">Thanh Le Viet</a></td>\r\n<td><a href=\"https://quadram.ac.uk/\">Quadram Institute, UK</a></td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n<blockquote>\r\n<p class=\"has-line-data\" data-line-start=\"105\" data-line-end=\"106\">Listed in alphabetical order</p>\r\n</blockquote>\r\n<h2 class=\"code-line\" data-line-start=107 data-line-end=108><a id=\"Contributions_and_Support_107\"></a>Contributions and Support</h2>\r\n<p class=\"has-line-data\" data-line-start=\"109\" data-line-end=\"110\">If you would like to contribute to this pipeline, please see the <a href=\"https://github.com/nf-core/viralrecon/blob/master/.github/CONTRIBUTING.md\">contributing guidelines</a>.</p>\r\n<p class=\"has-line-data\" data-line-start=\"111\" data-line-end=\"112\">For further information or help, don\u2019t hesitate to get in touch on <a href=\"https://nfcore.slack.com/channels/viralrecon\">Slack</a> (you can join with <a href=\"https://nf-co.re/join/slack\">this invite</a>).</p>\r\n<h2 class=\"code-line\" data-line-start=113 data-line-end=114><a id=\"Citation_113\"></a>Citation</h2>\r\n<p class=\"has-line-data\" data-line-start=\"115\" data-line-end=\"116\">If you use nf-core/viralrecon for your analysis, please cite it using the following doi: <a href=\"https://doi.org/10.5281/zenodo.3872730\">10.5281/zenodo.3872730</a></p>\r\n<p class=\"has-line-data\" data-line-start=\"117\" data-line-end=\"118\">An extensive list of references for the tools used by the pipeline can be found in the <a href=\"https://github.com/nf-core/viralrecon/blob/master/CITATIONS.md\"><code>CITATIONS.md</code></a> file.</p>\r\n<p class=\"has-line-data\" data-line-start=\"119\" data-line-end=\"120\">You can cite the <code>nf-core</code> publication as follows:</p>\r\n<blockquote>\r\n<p class=\"has-line-data\" data-line-start=\"121\" data-line-end=\"122\"><strong>The nf-core framework for community-curated bioinformatics pipelines.</strong></p>\r\n<p class=\"has-line-data\" data-line-start=\"123\" data-line-end=\"124\">Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso &amp; Sven Nahnsen.</p>\r\n<p class=\"has-line-data\" data-line-start=\"125\" data-line-end=\"127\"><em>Nat Biotechnol.</em> 2020 Feb 13. doi: <a href=\"https://dx.doi.org/10.1038/s41587-020-0439-x\">10.1038/s41587-020-0439-x</a>.<br>\r\nReadCube: <a href=\"https://rdcu.be/b1GjZ\">Full Access Link</a></p>\r\n</blockquote>\r\n</body></html>",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage* in description",
        "id": "19",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/19?version=1",
        "name": "nf-core/viralrecon",
        "number_of_steps": 0,
        "projects": [
            "nf-core viralrecon"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-05-29",
        "creators": [
            "Melchior du Lac"
        ],
        "description": "The workflow runs the RetroSynthesis algorithm to generate a collection of heterologous pathways in a host organism of choice, converts them to SBML files, performs analysis on the pathways to then rank the theoretical best performing ones.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in tags",
        "id": "25",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/25?version=1",
        "name": "Pathway Ranker",
        "number_of_steps": 12,
        "projects": [
            "IBISBA Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [
            "retrosynthesis",
            "synthetic biology",
            "metabolic engineering",
            "pathway design",
            "pathway prediction"
        ],
        "tools": [
            "\n makeSource",
            "\n rpReader",
            "\n rpExtractSink",
            "\n retropath2",
            "\n retrorules",
            "\n rpGlobalScore",
            "\n rp2paths",
            "\n rpReport",
            "\n rpThermo",
            "\n rpCofactors",
            "\n rpFBA",
            "\n rpVisualiser"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-05-29",
        "creators": [
            "Melchior du Lac"
        ],
        "description": "Generate possible metabolic routes for the production of a target molecule in an organism of choice",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap* in tags",
        "id": "24",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/24?version=1",
        "name": "RetroSynthesis",
        "number_of_steps": 7,
        "projects": [
            "IBISBA Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [
            "retrosynthesis",
            "synthetic biology",
            "metabolic engineering",
            "pathway design",
            "pathway prediction"
        ],
        "tools": [
            "\n makeSource",
            "\n rpReader",
            "\n rpExtractSink",
            "\n retropath2",
            "\n retrorules",
            "\n rp2paths",
            "\n rpCofactors"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-05-29",
        "creators": [
            "Melchior du Lac"
        ],
        "description": "This workflow converts the top-ranking predicted pathways from the \"RetroSynthesis\" and \"Pathway Analysis\" workflows to plasmids intended to be expressed in the specified organism",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "plasmid* in description",
        "id": "23",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/23?version=1",
        "name": "Genetic Design",
        "number_of_steps": 7,
        "projects": [
            "IBISBA Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [
            "retrosynthesis",
            "genetic design",
            "pathway prediction"
        ],
        "tools": [
            "\n PartsGenie",
            "\n LCRGenie",
            "\n rpOptBioDes",
            "\n DNAWeaver",
            "\n extractTaxonomy",
            "\n rpSelenzyme",
            "\n rpSBMLtoSBOL"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-04-10",
        "creators": [
            "Tim Dudgeon",
            " Simon Bray",
            " Gianmauro Cuccuru",
            " Bj\u00f6rn Gr\u00fcning",
            " Rachael Skyner",
            " Jack Scantlebury",
            " Susan Leung",
            " Frank von Delft"
        ],
        "description": "This workflow is used for the virtual screening of the SARS-CoV-2 main protease (de.NBI-cloud, STFC). It includes Charge enumeration, Generation of 3D conformations, Preparation of active site for docking using rDock, Docking, Scoring and Selection of compounds available. More info can be found at https://covid19.galaxyproject.org/cheminformatics/",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in name",
        "id": "18",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/18?version=1",
        "name": "Cheminformatics - XChem combined",
        "number_of_steps": 3,
        "projects": [
            "GalaxyProject SARS-CoV-2"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-04-10",
        "creators": [
            "Tim Dudgeon",
            " Simon Bray",
            " Gianmauro Cuccuru",
            " Bj\u00f6rn Gr\u00fcning",
            " Rachael Skyner",
            " Jack Scantlebury",
            " Susan Leung",
            " Frank von Delft"
        ],
        "description": "This workflow combines SDF files from all fragments into a single dataset and filters to include only the lowest (best) scoring pose for each compound. This file of optimal poses for all ligands is used to compare to a database of Enamine and Chemspace compounds to select the best scoring 500 matches. More info can be found at https://covid19.galaxyproject.org/cheminformatics/",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "17",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/17?version=1",
        "name": "Cheminformatics - Filter results",
        "number_of_steps": 13,
        "projects": [
            "GalaxyProject SARS-CoV-2"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-04-10",
        "creators": [
            "Tim Dudgeon",
            " Simon Bray",
            " Gianmauro Cuccuru",
            " Bj\u00f6rn Gr\u00fcning",
            " Rachael Skyner",
            " Jack Scantlebury",
            " Susan Leung",
            " Frank von Delft"
        ],
        "description": "This workflow generates binding scores that correlate well with binding affinities using an additional tool TransFS, developed at Oxford University. More info can be found at https://covid19.galaxyproject.org/cheminformatics/",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "16",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/16?version=1",
        "name": "Cheminformatics - TransFS scoring",
        "number_of_steps": 3,
        "projects": [
            "GalaxyProject SARS-CoV-2"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-04-10",
        "creators": [
            "Tim Dudgeon",
            " Simon Bray",
            " Gianmauro Cuccuru",
            " Bj\u00f6rn Gr\u00fcning",
            " Rachael Skyner",
            " Jack Scantlebury",
            " Susan Leung",
            " Frank von Delft"
        ],
        "description": "This workflow generates binding scores that correlate well with binding affinities using an additional tool SuCOS Max, developed at Oxford University. More info can be found at https://covid19.galaxyproject.org/cheminformatics/",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "15",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/15?version=1",
        "name": "Cheminformatics - SuCOS scoring",
        "number_of_steps": 5,
        "projects": [
            "GalaxyProject SARS-CoV-2"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-04-10",
        "creators": [
            "Tim Dudgeon",
            " Simon Bray",
            " Gianmauro Cuccuru",
            " Bj\u00f6rn Gr\u00fcning",
            " Rachael Skyner",
            " Jack Scantlebury",
            " Susan Leung",
            " Frank von Delft"
        ],
        "description": "This workflow generates a file describing the active site of the protein for each of the fragment screening crystal structures using rDock s rbcavity. It also creates a single hybrid molecule that contains all the ligands - the \"frankenstein\" ligand. More info can be found at https://covid19.galaxyproject.org/cheminformatics/",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in description",
        "id": "13",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/13?version=1",
        "name": "Cheminformatics - Active site generation",
        "number_of_steps": 2,
        "projects": [
            "GalaxyProject SARS-CoV-2"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-04-10",
        "creators": [
            "Dannon Baker",
            " Marius van den Beek",
            " Dave Bouvier",
            " John Chilton",
            " Nate Coraor",
            " Frederik Coppens",
            " Bert Droesbeke",
            " Ignacio Eguinoa",
            " Simon Gladman",
            " Bj\u00f6rn Gr\u00fcning",
            " Delphine Larivi\u00e8re",
            " Gildas Le Corguill\u00e9",
            " Andrew Lonie",
            " Nicholas Keener",
            " Sergei Kosakovsky Pond",
            " Wolfgang Maier",
            " Anton Nekrutenko",
            " James Taylor",
            " Steven Weaver"
        ],
        "description": "This workflow employs a recombination detection algorithm (GARD) developed by Kosakovsky Pond et al. and implemented in the hyphy package. More info can be found at https://covid19.galaxyproject.org/genomics/",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn* in name",
        "id": "10",
        "keep": true,
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/10?version=1",
        "name": "Genomics - Recombination and selection analysis",
        "number_of_steps": 6,
        "projects": [
            "GalaxyProject SARS-CoV-2"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    }
]